@inproceedings{10.1145/3342827.3342831,
author = {Hidayatullah, Ahmad Fathan and Kurniawan, Wisnu and Ratnasari, Chanifah Indah},
title = {Topic Modeling on Indonesian Online Shop Chat},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342831},
doi = {10.1145/3342827.3342831},
abstract = {This paper aims to discover topics from an Indonesian online shop chat. Moreover, we employed Latent Dirichlet Allocation to find out what kind of topics that are often discussed and conversation trends between buyers and customer service. Several tasks were performed, such as, collecting data, preprocessing, phrase aggregation, topic modeling, and topic analysis. We found several attracting findings during our experiments. In preprocessing task, product name extraction from URLs assisted to discover the intended product from the customer's conversation. On the other hand, the phrase aggregation task helped us to merge various terms which have same intended meaning, so that, we could obtain better topical model result and easier to determine the topic label.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {121–126},
numpages = {6},
keywords = {Topic modeling, Topic model, Online Shop, Latent Dirichlet Allocation, Bahasa Indonesia},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3397271.3401168,
author = {Zhang, Yu and Meng, Yu and Huang, Jiaxin and Xu, Frank F. and Wang, Xuan and Han, Jiawei},
title = {Minimally Supervised Categorization of Text with Metadata},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401168},
doi = {10.1145/3397271.3401168},
abstract = {Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1)the presence of metadata : in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2)label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into the same semantic space to encode heterogeneous signals. Then, based on the same generative process, we synthesize training samples to address the bottleneck of label scarcity. We conduct a thorough evaluation on a wide range of datasets. Experimental results prove the effectiveness of MetaCat over many competitive baselines.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1231–1240},
numpages = {10},
keywords = {weak supervision, metadata, text classification},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3511808.3557410,
author = {Hosseiny Marani, Amin and Levine, Joshua and Baumer, Eric P.S.},
title = {One Rating to Rule Them All? Evidence of Multidimensionality in Human Assessment of Topic Labeling Quality},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557410},
doi = {10.1145/3511808.3557410},
abstract = {Two general approaches are common for evaluating automatically generated labels in topic modeling: direct human assessment; or performance metrics that can be calculated without, but still correlate with, human assessment. However, both approaches implicitly assume that the quality of a topic label is single-dimensional. In contrast, this paper provides evidence that human assessments about the quality of topic labels consist of multiple latent dimensions. This evidence comes from human assessments of four simple labeling techniques. For each label, study participants responded to several items asking them to assess each label according to a variety of different criteria. Exploratory factor analysis shows that these human assessments of labeling quality have a two-factor latent structure. Subsequent analysis demonstrates that this multi-item, two-factor assessment can reveal nuances that would be missed using either a single-item human assessment of perceived label quality or established performance metrics. The paper concludes by suggesting future directions for the development of human-centered approaches to evaluating NLP and ML systems more broadly.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {768–779},
numpages = {12},
keywords = {topic modeling, performance metrics, topic labeling, exploratory factor analysis, human assessment},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3474717.3484212,
author = {Wang, Dongjie and Liu, Kunpeng and Mohaisen, David and Wang, Pengyang and Lu, Chang-Tien and Fu, Yanjie},
title = {Automated Feature-Topic Pairing: Aligning Semantic and Embedding Spaces in Spatial Representation Learning},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3484212},
doi = {10.1145/3474717.3484212},
abstract = {Automated characterization of spatial data is a kind of critical geographical intelligence. As an emerging technique for characterization, Spatial Representation Learning (SRL) uses deep neural networks (DNNs) to learn non-linear embedded features of spatial data for characterization. However, SRL extracts features by internal layers of DNNs, and thus suffers from lacking semantic labels. Texts of spatial entities, on the other hand, provide semantic understanding of latent feature labels, but is insensible to deep SRL models. How can we teach a SRL model to discover appropriate topic labels in texts and pair learned features with the labels? This paper formulates a new problem: feature-topic pairing, and proposes a novel Particle Swarm Optimization (PSO) based deep learning framework. Specifically, we formulate the feature-topic pairing problem into an automated alignment task between 1) a latent embedding feature space and 2) a textual semantic topic space. We decompose the alignment of the two spaces into: 1) point-wise alignment, denoting the correlation between a topic distribution and an embedding vector; 2) pair-wise alignment, denoting the consistency between a feature-feature similarity matrix and a topic-topic similarity matrix. We design a PSO based solver to simultaneously select an optimal set of topics and learn corresponding features based on the selected topics. We develop a closed loop algorithm to iterate between 1) minimizing losses of representation reconstruction and feature-topic alignment and 2) searching the best topics. Finally, we present extensive experiments to demonstrate the enhanced performance of our method.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {450–453},
numpages = {4},
keywords = {spatial representation learning K@multiple spaces alignment},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1145/3158670,
author = {Zhao, Wayne Xin and Zhang, Wenhui and He, Yulan and Xie, Xing and Wen, Ji-Rong},
title = {Automatically Learning Topics and Difficulty Levels of Problems in Online Judge Systems},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3158670},
doi = {10.1145/3158670},
abstract = {Online Judge (OJ) systems have been widely used in many areas, including programming, mathematical problems solving, and job interviews. Unlike other online learning systems, such as Massive Open Online Course, most OJ systems are designed for self-directed learning without the intervention of teachers. Also, in most OJ systems, problems are simply listed in volumes and there is no clear organization of them by topics or difficulty levels. As such, problems in the same volume are mixed in terms of topics or difficulty levels. By analyzing large-scale users’ learning traces, we observe that there are two major learning modes (or patterns). Users either practice problems in a sequential manner from the same volume regardless of their topics or they attempt problems about the same topic, which may spread across multiple volumes. Our observation is consistent with the findings in classic educational psychology. Based on our observation, we propose a novel two-mode Markov topic model to automatically detect the topics of online problems by jointly characterizing the two learning modes. For further predicting the difficulty level of online problems, we propose a competition-based expertise model using the learned topic information. Extensive experiments on three large OJ datasets have demonstrated the effectiveness of our approach in three different tasks, including skill topic extraction, expertise competition prediction and problem recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {27},
numpages = {33},
keywords = {online judge systems, expertise learning, Topic models}
}

@inproceedings{10.1145/3388440.3412418,
author = {Li, Yue and Nair, Pratheeksha and Wen, Zhi and Chafi, Imane and Okhmatovskaia, Anya and Powell, Guido and Shen, Yannan and Buckeridge, David},
title = {Global Surveillance of COVID-19 by Mining News Media Using a Multi-Source Dynamic Embedded Topic Model},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412418},
doi = {10.1145/3388440.3412418},
abstract = {As the COVID-19 pandemic continues to unfold, understanding the global impact of non-pharmacological interventions (NPI) is important for formulating effective intervention strategies, particularly as many countries prepare for future waves. We used a machine learning approach to distill latent topics related to NPI from large-scale international news media. We hypothesize that these topics are informative about the timing and nature of implemented NPI, dependent on the source of the information (e.g., local news versus official government announcements) and the target countries. Given a set of latent topics associated with NPI (e.g., self-quarantine, social distancing, online education, etc), we assume that countries and media sources have different prior distributions over these topics, which are sampled to generate the news articles. To model the source-specific topic priors, we developed a semi-supervised, multi-source, dynamic, embedded topic model. Our model is able to simultaneously infer latent topics and learn a linear classifier to predict NPI labels using the topic mixtures as input for each news article. To learn these models, we developed an efficient end-to-end amortized variational inference algorithm. We applied our models to news data collected and labelled by the World Health Organization (WHO) and the Global Public Health Intelligence Network (GPHIN). Through comprehensive experiments, we observed superior topic quality and intervention prediction accuracy, compared to the baseline embedded topic models, which ignore information on media source and intervention labels. The inferred latent topics reveal distinct policies and media framing in different countries and media sources, and also characterize reaction to COVID-19 and NPI in a semantically meaningful manner. Our PyTorch code is available on Github (htps://github.com/li-lab-mcgill/covid19_media).},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {34},
numpages = {14},
keywords = {text mining, coronavirus, Bayesian inference, media news, Topic models},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3366424.3382678,
author = {Li, Quanzhi and Zhang, Qiong},
title = {Abstractive Event Summarization on Twitter},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382678},
doi = {10.1145/3366424.3382678},
abstract = {This paper presents a new approach for automatically summarizing a social media event. It utilizes the BERT model as the encoder and a Transformer architecture as the decoder. The framework also includes an event topic prediction component, and the predicted event topic will help the decoder focus more on the specific aspects of the topic category when generating summary. To make the summary more succinct and coherent, the most important messages from an event cluster are selected by a message selection model and encoded by the BERT model. Our preliminary experiment shows that our approach outperforms the baseline methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {22–23},
numpages = {2},
keywords = {Twitter, event summary, event topic, social media, BERT},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3077136.3084138,
author = {Chin, Jin Yao and Bhowmick, Sourav S. and Jatowt, Adam},
title = {TOTEM: Personal Tweets Summarization on Mobile Devices},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3084138},
doi = {10.1145/3077136.3084138},
abstract = {Tweets summarization aims to find a group of representative tweets for a specific topic. In recent times, there have been several research efforts toward devising a variety of techniques to summarize tweets in Twitter. However, these techniques are either not personal (i.e., consider only tweets in the timeline of a specific user) or are too expensive to be realized on a mobile device. Given that 80% of active Twitter users access the site on mobile devices, in this demonstration we present a lightweight, personalized, on-demand, topic modeling-based tweets summarization engine called TOTEM, designed for such devices. Specifically, TOTEM summarizes most recent tweets on a user's timeline and enables her to visualize and navigate representative topics and associated tweets in a user-friendly tap-and-swipe manner.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1305–1308},
numpages = {4},
keywords = {summarization, mobile device, tweets, personal, topic modeling},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1145/3512900,
author = {Engel, Kristen and Hua, Yiqing and Zeng, Taixiang and Naaman, Mor},
title = {Characterizing Reddit Participation of Users Who Engage in the QAnon Conspiracy Theories},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512900},
doi = {10.1145/3512900},
abstract = {Widespread conspiracy theories may significantly impact our society. This paper focuses on the QAnon conspiracy theory, a consequential conspiracy theory that started on and disseminated successfully through social media. Our work characterizes how Reddit users who have participated in QAnon-focused subreddits engage in activities on the platform, especially outside their own communities. Using a large-scale Reddit moderation action against QAnon-related activities in 2018 as the source, we identified 13,000 users active in the early QAnon communities. We collected the 2.1 million submissions and 10.8 million comments posted by these users across all of Reddit from October 2016 to January 2021. The majority of these users were only active after the emergence of the QAnon conspiracy theory and decreased in activity after Reddit's 2018 QAnon ban. A qualitative analysis of a sample of 915 subreddits where the "QAnon-enthusiastic" users were especially active shows that they participated in a diverse range of subreddits, often of unrelated topics to QAnon. However, most of the users' submissions were concentrated in subreddits that have sympathetic attitudes towards the conspiracy theory, characterized by discussions that were pro-Trump, or emphasized unconstricted behavior (often anti-establishment and anti-interventionist). Further study of a sample of 1,571 of these submissions indicates that most consist of links from low-quality sources, bringing potential harm to the broader Reddit community. These results point to the likelihood that the activities of early QAnon users on Reddit were dedicated and committed to the conspiracy, providing implications on both platform moderation design and future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {53},
numpages = {22},
keywords = {QAnon, Reddit ban, online communities, conspiracy theories, content moderation}
}

@inproceedings{10.1145/3184558.3186979,
author = {Schneider, Rudolf and Arnold, Sebastian and Oberhauser, Tom and Klatt, Tobias and Steffek, Thomas and L\"{o}ser, Alexander},
title = {Smart-MD: Neural Paragraph Retrieval of Medical Topics},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186979},
doi = {10.1145/3184558.3186979},
abstract = {We demonstrate Smart-MD, an information retrieval system for medical professionals. The system supports topical queries in the form [disease topic], such as ["lyme disease", treatments]. In contrast to document-oriented retrieval systems, Smart-MD retrieves relevant paragraphs and reduces the reading load of a medical doctor drastically. We recognize diseases and topical aspects with a novel paragraph retrieval method based on bidirectional LSTM neural networks. We demonstrate Smart-MD on a dataset that contains 3,469 diseases from the English language part of Wikipedia and 6,876 distinct medical aspects extracted from Wikipedia headlines.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {203–206},
numpages = {4},
keywords = {neural information classification, paragraph retrieval},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3078971.3079004,
author = {Momeni, Elaheh and Rawassizadeh, Reza and Adar, Eytan},
title = {Leveraging Semantic Facets for Adaptive Ranking of Social Comments},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079004},
doi = {10.1145/3078971.3079004},
abstract = {An essential part of the social media ecosystem is user-generated comments. However, not all comments are useful to all people as both authors of comments and readers have different intentions and perspectives. Consequently, the development of automated approaches for the ranking of comments and the optimization of viewers' interaction experiences are becoming increasingly important. This work proposes an adaptive faceted ranking framework which enriches comments along multiple semantic facets (e.g., subjectivity, informativeness, and topics), thus enabling users to explore different facets and select combinations of facets in order to extract and rank comments that match their interests. A prototype implementation of the framework has been developed which allows us to evaluate different ranking strategies of the proposed framework. We find that adaptive faceted ranking shows significant improvements over prevalent ranking methods which are utilized by many platforms such as YouTube or The Economist. We observe substantial improvements in user experience when enriching each element of a comment along multiple explicit semantic facets rather than in a single topic or subjective facets.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {356–364},
numpages = {9},
keywords = {adaptive ranking, semantic facets, social comment},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1145/3406324.3410710,
author = {Leiva, Luis A. and Hota, Asutosh and Oulasvirta, Antti},
title = {Enrico: A Dataset for Topic Modeling of Mobile UI Designs},
year = {2020},
isbn = {9781450380522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406324.3410710},
doi = {10.1145/3406324.3410710},
abstract = {Topic modeling of user interfaces (UIs), also known as layout design categorization, contributes to a better understanding of the UI functionality. Starting from Rico, a large dataset of mobile UIs, we revised a random sample of 10k UIs and concluded to Enrico (shorthand of Enhanced Rico), a human-supervised high-quality dataset comprising 1460 UIs and 20 design topics. As a validation example, we train a deep learning model for three different UI representations (screenshots, wireframes, and embeddings). The screenshot representation provides the highest discriminative power (95% AUC) and a competitive accuracy of 75% (a random classifier achieves 5% accuracy in the same task). We discuss several applications that can be developed with this new public resource, including e.g. semantic UI captioning and tagging, explainable UI designs, smart tutorials, and improved design search capabilities.},
booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {9},
numpages = {4},
keywords = {User interface design, Layout classification, Machine learning, Neural networks},
location = {Oldenburg, Germany},
series = {MobileHCI '20}
}

@inproceedings{10.1145/3485447.3512266,
author = {Solovev, Kirill and Pr\"{o}llochs, Nicolas},
title = {Moral Emotions Shape the Virality of COVID-19 Misinformation on Social Media},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512266},
doi = {10.1145/3485447.3512266},
abstract = {While false rumors pose a threat to the successful overcoming of the COVID-19 pandemic, an understanding of how rumors diffuse in online social networks is – even for non-crisis situations – still in its infancy. Here we analyze a large sample consisting of COVID-19 rumor cascades from Twitter that have been fact-checked by third-party organizations. The data comprises N = 10,610 rumor cascades that have been retweeted more than 24 million times. We investigate whether COVID-19 misinformation spreads more viral than the truth and whether the differences in the diffusion of true vs. false rumors can be explained by the moral emotions they carry. We observe that, on average, COVID-19 misinformation is more likely to go viral than truthful information. However, the veracity effect is moderated by moral emotions: false rumors are more viral than the truth if the source tweets embed a high number of other-condemning emotion words, whereas a higher number of self-conscious emotion words is linked to a less viral spread. The effects are pronounced both for health misinformation and false political rumors. These findings offer insights into how true vs. false rumors spread and highlight the importance of considering emotions from the moral emotion families in social media content.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3706–3717},
numpages = {12},
keywords = {COVID-19, virality, explanatory modeling, Social media, computational social science, misinformation, moral emotions},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3336191.3371863,
author = {Wei, Jiaqi and Han, Shuo and Zou, Lei},
title = {VISION-KG: Topic-Centric Visualization System for Summarizing Knowledge Graph},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371863},
doi = {10.1145/3336191.3371863},
abstract = {Large scale knowledge graph (KG) has attracted wide attentions in both academia and industry recently. However, due to the complexity of SPARQL syntax and massive volume of real KG, it remains difficult for ordinary users to access KG. In this demo, we present VISION-KG, a topic-centric visualization system to help users navigate KG easily via entity summarization and entity clustering. Given a query entity v0, VISION-KG summarizes the induced subgraph of v0's neighbor nodes via our proposed facts ranking method that measures importance, relatedness and diversity. Moreover, to achieve conciseness, we split the summarized graph into several topic-centric summarized subgraph according to semantic and structural similarities among entities. We will demonstrate how VISION-KG provides a user-friendly visualization interface for navigating KG.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {857–860},
numpages = {4},
keywords = {entity summarization, entity clustering, knowledge graph visualization, topic-centric},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1145/2990507,
author = {Hoang, Tuan-Anh and Lim, Ee-Peng},
title = {Modeling Topics and Behavior of Microbloggers: An Integrated Approach},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2990507},
doi = {10.1145/2990507},
abstract = {Microblogging encompasses both user-generated content and behavior. When modeling microblogging data, one has to consider personal and background topics, as well as how these topics generate the observed content and behavior. In this article, we propose the Generalized Behavior-Topic (GBT) model for simultaneously modeling background topics and users’ topical interest in microblogging data. GBT considers multiple topical communities (or realms) with different background topical interests while learning the personal topics of each user and the user’s dependence on realms to generate both content and behavior. This differentiates GBT from other previous works that consider either one realm only or content data only. By associating user behavior with the latent background and personal topics, GBT helps to model user behavior by the two types of topics. GBT also distinguishes itself from other earlier works by modeling multiple types of behavior together. Our experiments on two Twitter datasets show that GBT can effectively mine the representative topics for each realm. We also demonstrate that GBT significantly outperforms other state-of-the-art models in modeling content topics and user profiling.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {44},
numpages = {37},
keywords = {probabilistic graphic model, Social media, behavior mining, microblogging, topic modeling, user behavior}
}

@inproceedings{10.1145/3503516.3503527,
author = {Yuan, Meng and Lin, Pauline and Zobel, Justin},
title = {Document Clustering vs Topic Models: A Case Study},
year = {2021},
isbn = {9781450395991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503516.3503527},
doi = {10.1145/3503516.3503527},
abstract = {Document collections can be characterised in a variety of ways. Two key approaches are clustering, which partitions collections into subcollections with the expectation that the contents will be thematically linked, and topic models, which describe the contents in terms of weighted lists of words that are expected to represent different themes. In this paper, we report experiments on the observed relationship between clusters and topic models in a preliminary study of a large text collection. Both produce results that appear cohesive in their own right, but surprisingly – given the very different ways in which they are formed – the descriptions of the collections that they generate are strongly similar. This unexpected mutual reinforcement creates confidence in both approaches as tools for annotating and describing the contents of document collections.},
booktitle = {Proceedings of the 25th Australasian Document Computing Symposium},
articleno = {6},
numpages = {8},
keywords = {document clustering, collection representation, topic models},
location = {Virtual Event, Australia},
series = {ADCS '21}
}

@inproceedings{10.1145/3477495.3531812,
author = {Zhang, Dake and Vakili Tahami, Amir and Abualsaud, Mustafa and Smucker, Mark D.},
title = {Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531812},
doi = {10.1145/3477495.3531812},
abstract = {When searching the web for answers to health questions, people can make incorrect decisions that have a negative effect on their lives if the search results contain misinformation. To reduce health misinformation in search results, we need to be able to detect documents with correct answers and promote them over documents containing misinformation. Determining the correct answer has been a difficult hurdle to overcome for participants in the TREC Health Misinformation Track. In the 2021 track, automatic runs were not allowed to use the known answer to a topic's health question, and as a result, the top automatic run had a compatibility-difference score of 0.043 while the top manual run, which used the known answer, had a score of 0.259. The compatibility-difference measures the ability of methods to rank correct and credible documents before incorrect and non-credible documents. By using an existing set of health questions and their known answers, we show it is possible to learn which web hosts are trustworthy, from which we can predict the correct answers to the 2021 health questions with an accuracy of 76%. Using our predicted answers, we can promote documents that we predict contain this answer and achieve a compatibility-difference score of 0.129, which is a three-fold increase in performance over the best previous automatic method.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2099–2104},
numpages = {6},
keywords = {web search, stance detection, health misinformation},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3316782.3316792,
author = {Montenegro, C. and Santana, R. and Lozano, J. A.},
title = {Data Generation Approaches for Topic Classification in Multilingual Spoken Dialog Systems},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3316792},
doi = {10.1145/3316782.3316792},
abstract = {The conception of spoken-dialog systems (SDS) usually faces the problem of extending or adapting the system to multiple languages. This implies the creation of modules specifically for the new languages, which is a time consuming process. In this paper, we propose two methods to reduce the time needed to extend the SDS to other languages. Our methods are particularly oriented to the topic classification and semantic tagging tasks and we evaluate their effectiveness on topic classification for three languages: English, Spanish, French.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {211–217},
numpages = {7},
keywords = {topic classification, bilingual datasets, neural networks, spoken dialog systems},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3132847.3133011,
author = {Saha, Tanay Kumar and Joty, Shafiq and Hassan, Naeemul and Hasan, Mohammad Al},
title = {Regularized and Retrofitted Models for Learning Sentence Representation with Context},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133011},
doi = {10.1145/3132847.3133011},
abstract = {Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences. For solving these tasks, bag-of-word based representation has been used for a long time. In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations. However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context. In this paper, we first characterize two types of contexts depending on their scope and utility. We then propose two approaches to incorporate contextual information into content-based models. We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors. Experimental results demonstrate that our proposed models outshine existing models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {547–556},
numpages = {10},
keywords = {sen2vec, classification, clustering, ranking, feature learning, distributed representation of sentences, discourse, retrofitting},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1007/s00778-018-0522-9,
author = {Zhao, Kaiqi and Cong, Gao and Chin, Jin-Yao and Wen, Rong},
title = {Exploring Market Competition over Topics in Spatio-Temporal Document Collections},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0522-9},
doi = {10.1007/s00778-018-0522-9},
abstract = {With the prominence of location-based services and social networks in recent years, huge amounts of spatio-temporal document collections (e.g., geo-tagged tweets) have been generated. These data collections often imply user's ideas on different products and thus are helpful for business owners to explore hot topics of their brands and the competition relation to other brands in different spatial regions during different periods. In this work, we aim to mine the topics and the market competition of different brands over each topic for a category of business (e.g., coffeehouses) from spatio-temporal documents within a user-specified region and time period. To support such spatio-temporal search online in an exploratory manner, we propose a novel framework equipped by (1) a generative model for mining topics and market competition, (2) an Octree-based off-line pre-training method for the model and (3) an efficient algorithm for combining pre-trained models to return the topics and market competition on each topic within a user-specified pair of region and time span. Extensive experiments show that our framework is able to improve the runtime by up to an order of magnitude compared with baselines while achieving similar model quality in terms of training log-likelihood.},
journal = {The VLDB Journal},
month = {feb},
pages = {123–145},
numpages = {23},
keywords = {Spatio-temporal data, Algorithms, Gibbs sampling, Exploratory search, Topic model}
}

@inproceedings{10.1145/3366423.3380066,
author = {Abebe, Rediet and Giorgi, Salvatore and Tedijanto, Anna and Buffone, Anneke and Schwartz, H. Andrew Andrew},
title = {Quantifying Community Characteristics of Maternal Mortality Using Social Media},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380066},
doi = {10.1145/3366423.3380066},
abstract = {While most mortality rates have decreased in the US, maternal mortality has increased and is among the highest of any OECD nation. Extensive public health research is ongoing to better understand the characteristics of communities with relatively high or low rates. In this work, we explore the role that social media language can play in providing insights into such community characteristics. Analyzing pregnancy-related tweets generated in US counties, we reveal a diverse set of latent topics including Morning Sickness, Celebrity Pregnancies, and Abortion Rights. We find that rates of mentioning these topics on Twitter predicts maternal mortality rates with higher accuracy than standard socioeconomic and risk variables such as income, race, and access to health-care, holding even after reducing the analysis to six topics chosen for their interpretability and connections to known risk factors. We then investigate psychological dimensions of community language, finding the use of less trustful, more stressed, and more negative affective language is significantly associated with higher mortality rates, while trust and negative affect also explain a significant portion of racial disparities in maternal mortality. We discuss the potential for these insights to inform actionable health interventions at the community-level.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2976–2983},
numpages = {8},
keywords = {maternal mortality, language, topic modeling, community characteristics, health disparities},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3127526.3127530,
author = {Botev, Viktor and Marinov, Kaloyan and Sch\"{a}fer, Florian},
title = {Word Importance-Based Similarity of Documents Metric (WISDM): Fast and Scalable Document Similarity Metric for Analysis of Scientific Documents},
year = {2017},
isbn = {9781450353885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127526.3127530},
doi = {10.1145/3127526.3127530},
abstract = {We present the Word importance-based similarity of documents metric (WISDM), a fast and scalable novel method for document similarity/distance computation for analysis of scientific documents. It is based on recent advancements in the area of word embeddings. WISDM combines learned word vectors together with traditional count-based models for document similarity computation, eventually achieving state-of-the-art performance and precision. The novel method first selects from two text documents those words that carry the most information and forms a word set for each document respectively. Then it relies on an existing word embeddings model to get the vector representations of the selected words. In the final step, it computes the closeness of the two sets of word vector representations, fit into a matrix, using a correlation coefficient. The presented metric was evaluated on three tasks, relevant to the analysis of scientific documents, and three data sets of open access scientific research. The results demonstrate that WISDM achieves significant performance speed-up in comparison to state-of-the-art metrics with a very marginal drop in precision.},
booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
pages = {17–23},
numpages = {7},
keywords = {document distance, document similarity, correlation coefficient, word embeddings, TF-IDF, document metric, text metric, importance-based, similarity, word2vec},
location = {Toronto, ON, Canada},
series = {WOSP 2017}
}

@inproceedings{10.1145/3543829.3544529,
author = {Meyer, Selina and Elsweiler, David and Ludwig, Bernd and Fernandez-Pichel, Marcos and Losada, David E.},
title = {Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543829.3544529},
doi = {10.1145/3543829.3544529},
abstract = {Scarcity of user data continues to be a problem in research on conversational user interfaces and often hinders or slows down technical innovation. In the past, different ways of synthetically generating data, such as data augmentation techniques have been explored. With the rise of ever improving pre-trained language models, we ask if we can go beyond such methods by simply providing appropriate prompts to these general purpose models to generate data. We explore the feasibility and cost-benefit trade-offs of using non fine-tuned synthetic data to train classification algorithms for conversational agents. We compare this synthetically generated data with real user data and evaluate the performance of classifiers trained on different combinations of synthetic and real data. We come to the conclusion that, although classifiers trained on such synthetic data perform much better than random baselines, they do not compare to the performance of classifiers trained on even very small amounts of real user data, largely because such data is lacking much of the variability found in user generated data. Nevertheless, we show that in situations where very little data and resources are available, classifiers trained on such synthetically generated data might be preferable to the collection and annotation of naturalistic data.},
booktitle = {Proceedings of the 4th Conference on Conversational User Interfaces},
articleno = {8},
numpages = {6},
keywords = {conversational ai, text generation, nlp, datasets},
location = {Glasgow, United Kingdom},
series = {CUI '22}
}

@inproceedings{10.1145/3406865.3418568,
author = {Choi, Yoonseo and Monserrat, Toni-Jan Keith and Park, Jeongeon and Shin, Hyungyu and Lee, Nyoungwoo and Kim, Juho},
title = {ProtoChat: Supporting the Conversation Design Process with Crowd Feedback},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418568},
doi = {10.1145/3406865.3418568},
abstract = {Conversation designers use iterative design to create, test, and improve conversation flows. While it is possible to iterate conversation design with existing chatbot prototyping tools, challenges remain such as recruiting participants and collecting structured feedback on specific conversational components, hindering rapid iterations, and making informed design decisions. To address these limitations, we introduce ProtoChat, a crowd-powered design tool built to support the iterative process of conversation design for a chatbot. ProtoChat enables rapid testing with the crowd and guiding the crowd workers to provide granular feedback on specific points of conversation.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {19–23},
numpages = {5},
keywords = {chatbot design, crowdsourcing, iterative design, conversation design, conversational user interface},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3394171.3413582,
author = {Zhang, Huaizheng and Luo, Yong and Ai, Qiming and Wen, Yonggang and Hu, Han},
title = {Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413582},
doi = {10.1145/3394171.3413582},
abstract = {Given the massive market of advertising and the sharply increasing online multimedia content (such as videos), it is now fashionable to promote advertisements (ads) together with the multimedia content. However, manually finding relevant ads to match the provided content is labor-intensive, and hence some automatic advertising techniques are developed. Since ads are usually hard to understand only according to its visual appearance due to the contained visual metaphor, some other modalities, such as the contained texts, should be exploited for understanding. To further improve user experience, it is necessary to understand both the ads' topic and sentiment. This motivates us to develop a novel deep multimodal multitask framework that integrates multiple modalities to achieve effective topic and sentiment prediction simultaneously for ads understanding. In particular, in our framework termed Deep$M^2$Ad, we first extract multimodal information from ads and learn high-level and comparable representations. The visual metaphor of the ad is decoded in an unsupervised manner. The obtained representations are then fed into the proposed hierarchical multimodal attention modules to learn task-specific representations for final prediction. A multitask loss function is also designed to jointly train both the topic and sentiment prediction models in an end-to-end manner, where bottom-layer parameters are shared to alleviate over-fitting. We conduct extensive experiments on a large-scale advertisement dataset and achieve state-of-the-art performance for both prediction tasks. The obtained results could be utilized as a benchmark for ads understanding.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {430–438},
numpages = {9},
keywords = {online advertising, ads understanding, neural networks, multitask learning, multimodal learning},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.5555/3382225.3382358,
author = {Arag\'{o}n, Pablo and Bermejo, Yago and G\'{o}mez, Vicen\c{c} and Kaltenbrunner, Andreas},
title = {Interactive Discovery System for Direct Democracy},
year = {2018},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Decide Madrid is the civic technology of Madrid City Council which allows users to create and support online petitions. Despite the initial success, the platform is encountering problems with the growth of petition signing because petitions are far from the minimum number of supporting votes they must gather. Previous analyses have suggested that this problem is produced by the interface: a paginated list of petitions which applies a non-optimal ranking algorithm. For this reason, we present an interactive system for the discovery of topics and petitions. This approach leads us to reflect on the usefulness of data visualization techniques to address relevant societal challenges.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {601–604},
numpages = {4},
keywords = {civic technology, decide madrid, collective action, data visualization, technopolitics, e-democracy, text clustering, online petitions, platform design},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3194452.3194474,
author = {Yang, Shanliang and Sun, Qi and Zhou, Huyong and Gong, Zhengjie and Zhou, Yangzhi and Huang, Junhong},
title = {A Topic Detection Method Based on KeyGraph and Community Partition},
year = {2018},
isbn = {9781450364195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194452.3194474},
doi = {10.1145/3194452.3194474},
abstract = {More and more media stream data is created on the Internet every day. It's more difficult for persons to obtain valuable information due to information overload. Topic detection is the method that extracts valuable hot topics from media stream data. It is the tool to help to solve the problem of overload information. The topic positive accuracy of cluster method is very low. In this paper, we proposed one topic detection method based on KeyGraph to improve the positive accuracy, and took experiments compared with baseline method on corpus marked by graduate students. In the result, the positive accuracy of KeyGraph method reaches 88.48% with great improvement. The result verified the effectiveness of our proposed method.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Artificial Intelligence},
pages = {30–34},
numpages = {5},
keywords = {KeyGraph, Topic detection, TF-IDF, Louvain},
location = {Chengdu, China},
series = {ICCAI 2018}
}

@inproceedings{10.1145/3488560.3502190,
author = {Butler, Rhys and Duggirala, Vishnu Dutt and Banaei-Kashani, Farnoush},
title = {ILFQA: A Platform for Efficient and Accurate Long-Form Question Answering},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502190},
doi = {10.1145/3488560.3502190},
abstract = {We present an efficient and accurate long-form question-answering platform, dubbed iLFQA (i.e., short for intelligent Long-Form Question Answering). The purpose of iLFQA is to function as a platform which accepts unscripted questions and efficiently produces semantically meaningful, explanatory, and accurate long-form responses. iLFQA consists of a number of modules for zero-shot classification, text retrieval, and text generation to generate answers to questions based on an open-domain knowledge base. iLFQA is unique in the question answering space because it is an example of a deployable and efficient long-form question answering system. Question answering systems exist in many forms, but long-form question answering remains relatively unexplored, and to the best of our knowledge none of the existing long-form question answering systems are shown to be sufficiently efficient to be deployable. We have made the source code and implementation details of iLFQA available for the benefit of researchers and practitioners in this field. With this demonstration, we present iLFQA as an open-domain, deployable, and accurate open-source long-form question answering platform.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1565–1568},
numpages = {4},
keywords = {text generation, long-form question answering, text retrieval, natural language processing, generalized language models},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3106237.3119875,
author = {Ellmann, Mathias},
title = {On the Similarity of Software Development Documentation},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3119875},
doi = {10.1145/3106237.3119875},
abstract = {Software developers spent 20% of their time on information seeking on Stack Overflow, YouTube or an API reference documentation. Software developers can search within Stack Overflow for duplicates or similar posts. They can also take a look on software development documentations that have similar and additional information included as a Stack Overflow post or a development screencast in order to get new inspirations on how to solve their current development problem. The linkage of same and different types of software development documentation might safe time to evolve new software solutions and might increase the productivity of the developer’s work day. In this paper we will discuss our approach to get a broader understanding of different similarity types (exact, similar and maybe) within and between software documentation as well as an understanding of how different software documentations can be extended.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {1030–1033},
numpages = {4},
keywords = {Software Development Documentation, Software Analytics, Similarity Types},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3308560.3316749,
author = {Bhargava, Preeti and Spasojevic, Nemanja and Ellinger, Sarah and Rao, Adithya and Menon, Abhinand and Fuhrmann, Saul and Hu, Guoning},
title = {Learning to Map Wikidata Entities To Predefined Topics},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316749},
doi = {10.1145/3308560.3316749},
abstract = {Recently much progress has been made in entity disambiguation and linking systems (EDL). Given a piece of text, EDL links words and phrases to entities in a knowledge base, where each entity defines a specific concept. Although extracted entities are informative, they are often too specific to be used directly by many applications. These applications usually require text content to be represented with a smaller set of predefined concepts or topics, belonging to a topical taxonomy, that matches their exact needs. In this study, we aim to build a system that maps Wikidata entities to such predefined topics. We explore a wide range of methods that map entities to topics, including GloVe similarity, Wikidata predicates, Wikipedia entity definitions, and entity-topic co-occurrences. These methods often predict entity-topic mappings that are reliable, i.e., have high precision, but tend to miss most of the mappings, i.e., have low recall. Therefore, we propose an ensemble system that effectively combines individual methods and yields much better performance, comparable with human annotators.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1194–1202},
numpages = {9},
keywords = {wikipedia, entity topic assignment, knowledge base, entity topic mapping, wikidata, natural language processing},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3173574.3173597,
author = {Niu, Xi and Abbas, Fakhri and Maher, Mary Lou and Grace, Kazjon},
title = {Surprise Me If You Can: Serendipity in Health Information},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173597},
doi = {10.1145/3173574.3173597},
abstract = {Our natural tendency to be curious is increasingly important now that we are exposed to vast amounts of information. We often cope with this overload by focusing on the familiar: information that matches our expectations. In this paper we present a framework for interactive serendipitous information discovery based on a computational model of surprise. This framework delivers information that users were not actively looking for, but which will be valuable to their unexpressed needs. We hypothesize that users will be surprised when presented with information that violates the expectations predicted by our model of them. This surprise model is balanced by a value component which ensures that the information is relevant to the user. Within this framework we have implemented two surprise models, one based on association mining and the other on topic modeling approaches. We evaluate these two models with thirty users in the context of online health news recommendation. Positive user feedback was obtained for both of the computational models of surprise compared to a baseline random method. This research contributes to the understanding of serendipity and how to "engineer" serendipity that is favored by users.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {surprise, value, health news, computational models, serendipity, information retrieval systems},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3387902.3394035,
author = {Wei, Xianglin and Liu, Jianwei and Wang, Junwei and Wang, Yangang and Fan, Jianhua},
title = {Similarity-Aware Popularity-Based Caching in Wireless Edge Computing},
year = {2020},
isbn = {9781450379564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387902.3394035},
doi = {10.1145/3387902.3394035},
abstract = {Mobile edge computing (MEC) can greatly reduce the latency experienced by mobile devices and their energy consumption through bringing data processing, computing, and caching services closer to the source of data generation. However, existing edge caching mechanisms usually focus on predicting the popularity of contents or data chunks based on their request history. This will lead to a slow start problem for the newly arrived contents and fail to fulfill MEC's context-aware requirements. Moreover, the dynamic nature of contents as well as mobile devices has not been fully studied. Both of them hinder the further promotion and application of MEC caching. In this backdrop, this paper aims to tackle the caching problem in wireless edge caching scenarios, and a new dynamic caching architecture is proposed. The mobility of users and the dynamics nature of contents are considered comprehensively in our caching architecture rather than adopting a static assumption as that in many current efforts. Based on this framework, a Similarity-Aware Popularity-based Caching (SAPoC) algorithm is proposed which considers a content's freshness, short-term popularity, and the similarity between contents when making caching decisions. Extensive simulation experiments have been conducted to evaluate SAPoC's performance, and the results have shown that SAPoC outperforms several typical proposals in both cache hit ratio and energy consumption.},
booktitle = {Proceedings of the 17th ACM International Conference on Computing Frontiers},
pages = {257–260},
numpages = {4},
keywords = {cache algorithm, dynamic caching, wireless network, mobile edge computing},
location = {Catania, Sicily, Italy},
series = {CF '20}
}

@inproceedings{10.1145/3269206.3269309,
author = {Hoang, Tuan-Anh and Vo, Khoi Duy and Nejdl, Wolfgang},
title = {W2E: A Worldwide-Event Benchmark Dataset for Topic Detection and Tracking},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269309},
doi = {10.1145/3269206.3269309},
abstract = {Topic detection and tracking in document streams is a critical task in many important applications, hence has been attracting research interest in recent decades. With the large size of data streams, there have been a number of works from different approaches that propose automatic methods for the task. However, there is only a few small benchmark datasets that are publicly available for evaluating the proposed methods. The lack of large datasets with fine-grained groundtruth implicitly restrains the development of more advanced methods. In this work, we address this issue by collecting and publishing W2E - a large dataset consisting of news articles from more than 50 prominent mass media channels worldwide. The articles cover a large set of popular events within a full year. W2E is more than 15 times larger than TREC's TDT2 dataset, which is widely used in prior work. We further conduct exploratory analysis to examine the dynamics and diversity of W2E and propose potential uses of the dataset in other research.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1847–1850},
numpages = {4},
keywords = {topic detection, benchmark dataset, topic tracking},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3425192,
author = {Joshi, Karuna Pande and Saha, Srishty},
title = {A Semantically Rich Framework for Knowledge Representation of Code of Federal Regulations},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-199X},
url = {https://doi.org/10.1145/3425192},
doi = {10.1145/3425192},
abstract = {Federal government agencies and organizations doing business with them have to adhere to the Code of Federal Regulations (CFR). The CFRs are currently available as large text documents that are not machine processable and so require extensive manual effort to parse and comprehend, especially when sections cross-reference topics spread across various titles. We have developed a novel framework to automatically extract knowledge from CFRs and represent it using a semantically rich knowledge graph. The framework captures knowledge in the form of key terms, rules, topic summaries, relationships between various terms, semantically similar terminologies, deontic expressions, and cross-referenced facts and rules. We built our framework using deep learning technologies like TensorFlow for word embeddings and text summarization, Gensim for topic modeling, and Semantic Web technologies for building the knowledge graph. In this article, we describe our framework in detail and present the results of our analysis of the Title 48 CFR knowledge base that we have built using this framework. Our framework and knowledge graph can be adopted by federal agencies and businesses to automate their internal processes that reference the CFR rules and policies.},
journal = {Digit. Gov.: Res. Pract.},
month = {nov},
articleno = {21},
numpages = {17},
keywords = {Deep learning, compliance, legal text analytics, Semantic Web}
}

@inproceedings{10.1145/3410530.3414327,
author = {Adhikary, Rishiraj and Batra, Nipun},
title = {Computational Tools for Understanding Air Pollution},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414327},
doi = {10.1145/3410530.3414327},
abstract = {Ambient fine particulate (PM2.5) is the most significant risk factor for premature death, shortening life expectancy at birth by 1.5 to 1.9 years [2]. 91% of the world's population lives in areas where air pollution exceeds safety limits1. 99% of the people in countries like India, Pakistan, Nepal, and Bangladesh experience ambient exposures of PM2.5 exceeding 75 μg/m3 to 100 μg/m3 [3]. My Ph.D. thesis will be on understanding the perception of air pollution among people using social media data. I also intend to develop a wearable air pollution exposure monitor and design an air pollution visualisation tool to reduce the entry barrier for air pollution research.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {199–203},
numpages = {5},
keywords = {air pollution wearable, visualisation, air pollution perception, social media},
location = {Virtual Event, Mexico},
series = {UbiComp-ISWC '20}
}

@inproceedings{10.1145/3485447.3512163,
author = {Min, Erxue and Rong, Yu and Bian, Yatao and Xu, Tingyang and Zhao, Peilin and Huang, Junzhou and Ananiadou, Sophia},
title = {Divide-and-Conquer: Post-User Interaction Network for Fake News Detection on Social Media},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512163},
doi = {10.1145/3485447.3512163},
abstract = {Fake News detection has attracted much attention in recent years. Social context based detection methods attempt to model the spreading patterns of fake news by utilizing the collective wisdom from users on social media. This task is challenging for three reasons: (1) There are multiple types of entities and relations in social context, requiring methods to effectively model the heterogeneity. (2) The emergence of news in novel topics in social media causes distribution shifts, which can significantly degrade the performance of fake news detectors. (3) Existing fake news datasets usually lack of great scale, topic diversity and user social relations, impeding the development of this field. To solve these problems, we formulate social context based fake news detection as a heterogeneous graph classification problem, and propose a fake news detection model named Post-User Interaction Network (PSIN), which adopts a divide-and-conquer strategy to model the post-post, user-user and post-user interactions in social context effectively while maintaining their intrinsic characteristics. Moreover,we adopt an adversarial topic discriminator for topic-agnostic feature learning, in order to improve the generalizability of our method for new-emerging topics. Furthermore, we curate a new dataset for fake news detection, which contains over 27,155 news from 5 topics, 5 million posts, 2 million users and their induced social graph with 0.2 billion edges. It has been published on https://github.com/qwerfdsaplking/MC-Fake. Extensive experiments illustrate that our method outperforms SOTA baselines in both in-topic and out-of-topic settings.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1148–1158},
numpages = {11},
keywords = {Graph Neural Network, Social Media, Fake News Detection},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3110025.3110102,
author = {Effendy, Suhendry and Yap, Roland H. C.},
title = {Using Community Structure to Categorize Computer Science Conferences: Initial Results},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110102},
doi = {10.1145/3110025.3110102},
abstract = {Research in computer science (CS) is published mainly in conferences. We investigate the possibility of automatically categorizing CS conferences by using exemplars (influential conferences). We propose an automatic exemplars selection method. Our experiments show that categorizing by exemplars matches well with curated topic classification from the Chinese CCF conference list. The results also accord with manual judgement which show promise as a practical and robust method for categorizing CS conferences.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {297–300},
numpages = {4},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3357384.3357941,
author = {Wang, Xiaobao and Jin, Di and Liu, Mengquan and He, Dongxiao and Musial, Katarzyna and Dang, Jianwu},
title = {Emotional Contagion-Based Social Sentiment Mining in Social Networks by Introducing Network Communities},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357941},
doi = {10.1145/3357384.3357941},
abstract = {The rapid development of social media services has facilitated the communication of opinions through online news, blogs, microblogs, instant-messages, and so on. This article concentrates on the mining of readers' social sentiments evoked by social media materials. Existing methods are only applicable to a minority of social media like news portals with emotional voting information, while ignore the emotional contagion between writers and readers. However, incorporating such factors is challenging since the learned hidden variables would be very fuzzy (because of the short and noisy text in social networks). In this paper, we try to solve this problem by introducing a high-order network structure, i.e. communities. We first propose a new generative model called Community-Enhanced Social Sentiment Mining (CESSM), which 1) considers the emotional contagion between writers and readers to capture precise social sentiment, and 2) incorporates network communities to capture coherent topics. We then derive an inference algorithm based on Gibbs sampling. Empirical results show that, CESSM achieves significantly superior performance against the state-of-the-art techniques for text sentiment classification and interestingness in social sentiment mining.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1763–1772},
numpages = {10},
keywords = {community, emotional contagion, social network, social sentiment},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1109/TASLP.2021.3126937,
author = {Supraja, S. and Khong, Andy W. H. and Tatinati, S.},
title = {Regularized Phrase-Based Topic Model for Automatic Question Classification With Domain-Agnostic Class Labels},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3126937},
doi = {10.1109/TASLP.2021.3126937},
abstract = {Classification of questions according to domain-agnostic class labels relies on a suitable feature extraction process. We propose the use of phrases that is more effective than using words to represent questions. The proposed phrase-based topic modeling technique employs asymmetric priors that are scaled with a new C-value for nested regular expressions. In addition, to suppress high-frequency words in phrases, we deploy term weightages computed using the modified distinguishing feature selector. The proposed approach also incorporates a new topic regularization mechanism to facilitate efficient mapping of questions to class labels. We validate the performance of our proposed model via four datasets across different domain-agnostic class labels comprising question types, reasoning capabilities, and cognitive complexities. Results obtained highlight that the proposed technique outperforms existing methods in terms of macro-average F1 score.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {3604–3616},
numpages = {13}
}

@inproceedings{10.1145/3436286.3436291,
author = {Simou, Chen},
title = {Hot Topics of Big Data Research in China},
year = {2020},
isbn = {9781450376457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436286.3436291},
doi = {10.1145/3436286.3436291},
abstract = {This paper aims at exploring the hot topics of big data research in China in the past three years. We choose Big Data, one influential and representative Chinese academic journal as our data source, and download the bibliographic data of 195 articles published in the past three years on this journal from CNKI. In order to gain the general overview of Chinese big data research status quo, we count the high frequency words and distribution of publish years. From the high frequency words list, we can tell that the times of mentions of "data" is far more than that of other frequency words, which verifies the fact that data is the most important concept in big data research. In order to extract the hot topics of big data research, we adopt LDA model to generate topics of the 195 research articles. With several experiments, we set the topic number as seven for the best outcome quality. With expert suggestion, we label the seven topics as "Big data helps government decisions", "Internet and management"," Artificial intelligence helps public security"," Big data promotes information", "Big data governance", "Artificial intelligence and 5G"," Big data prediction" based on the high frequency words in each topic. Most of the topic labels are linked to 5G, artificial intelligence and other modern techniques, which implies that the big data plays a necessary role in empowering the future techniques. We hope this paper could contribute to further big data research in China.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data and Artificial Intelligence},
pages = {20–25},
numpages = {6},
keywords = {Big Data, Hot Topics, Lda Model},
location = {Johannesburg, South Africa},
series = {ISBDAI '20}
}

@inproceedings{10.1145/3442442.3451361,
author = {Hoppe, Fabian and Dess\`{\i}, Danilo and Sack, Harald},
title = {Deep Learning Meets Knowledge Graphs for Scholarly Data Classification},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451361},
doi = {10.1145/3442442.3451361},
abstract = {The amount of scientific literature continuously grows, which poses an increasing challenge for researchers to manage, find and explore research results. Therefore, the classification of scientific work is widely applied to enable the retrieval, support the search of suitable reviewers during the reviewing process, and in general to organize the existing literature according to a given schema. The automation of this classification process not only simplifies the submission process for authors, but also ensures the coherent assignment of classes. However, especially fine-grained classes and new research fields do not provide sufficient training data to automatize the process. Additionally, given the large number of not mutual exclusive classes, it is often difficult and computationally expensive to train models able to deal with multi-class multi-label settings. To overcome these issues, this work presents a preliminary Deep Learning framework as a solution for multi-label text classification for scholarly papers about Computer Science. The proposed model addresses the issue of insufficient data by utilizing the semantics of classes, which is explicitly provided by latent representations of class labels. This study uses Knowledge Graphs as a source of these required external class definitions by identifying corresponding entities in DBpedia to improve the overall classification.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {417–421},
numpages = {5},
keywords = {Multi-Label Classification, Knowledge Graphs, Scholarly Data, Deep Learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3478905.3478922,
author = {Fu, Zhu and Ding, Weike and Guan, Peng and Ding, Xuhui},
title = {Topic Mining of Modern Poetry For Digital Humanities: Case Study of Gu Cheng Poetry},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478922},
doi = {10.1145/3478905.3478922},
abstract = {Using topic extraction and visualization method to explore the themes of Gu Cheng's complete poems, which provides a reference for the quantitative interpretation and analysis of modern poetry works. According to Gu's self-staging, we divide Gu's complete poems, and use Latent Dirichlet Allocation (LDA) model and word cloud visualization technology to conduct staged topic extraction and visual analysis of Gu's poems. Finally, we divide Gu's poems creation into four stages of nature stage, culture stage, counter-culture stage, and non-self stage. On the staged topic extraction experiment of Gu's poems, we obtain four stages of topic extraction results and word frequency cloud maps, and in-depth analyze the theme changes and reasons of Gu's poems at different stages. Results show that the used topic extraction and visualization method could objectively and comprehensively reveal the content and change process of Gu's poems.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {78–83},
numpages = {6},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.5555/3511065.3511092,
author = {Weiss, Michael},
title = {Patterns for Managing Remote Software Projects},
year = {2020},
isbn = {9781941652169},
publisher = {The Hillside Group},
address = {USA},
abstract = {The trend towards working remotely has been accelerated significantly by the COVID-19 pandemic, in particular in software development. In this paper, we describe patterns for managing remote software projects. These patterns were mined from the literature by conducting a literature review and synthesizing the findings using a research synthesis framework developed in design science (van Burg &amp; Romme, 2004) in combination with the holistic pattern mining approach by Iba &amp; Isaku (2012), but using topic modeling instead of visual clustering to identify themes. The current paper describes two of the patterns: Information Flow and Shared Mental Model. The target audience for the patterns are managers of software-intensive organizations transitioning to a remote work mode, as well as students and researchers.},
booktitle = {Proceedings of the 27th Conference on Pattern Languages of Programs},
articleno = {20},
numpages = {8},
keywords = {topic modeling, patterns, remote software projects, pattern mining},
location = {Virtual Event},
series = {PLoP '20}
}

@inproceedings{10.1145/3477495.3531817,
author = {Xia, Jinxiong and Liu, Cao and Chen, Jiansong and Li, Yuchen and Yang, Fan and Cai, Xunliang and Wan, Guanglu and Wang, Houfeng},
title = {Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531817},
doi = {10.1145/3477495.3531817},
abstract = {Dialogue topic segmentation is a challenging task in which dialogues are split into segments with pre-defined topics. Existing works on topic segmentation adopt a two-stage paradigm, including text segmentation and segment labeling. However, such methods tend to focus on the local context in segmentation, and the inter-segment dependency is not well captured. Besides, the ambiguity and labeling noise in dialogue segment bounds bring further challenges to existing models. In this work, we propose the Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to address the above issues. Specifically, we propose the parallel extraction network to perform segment extractions, optimizing the bipartite matching cost of segments to capture inter-segment dependency. Furthermore, we propose neighbor smoothing to handle the segment-bound noise and ambiguity. Experiments on a dialogue-based and a document-based topic segmentation dataset show that PEN-NS outperforms state-the-of-art models significantly.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2126–2131},
numpages = {6},
keywords = {parallel extraction, neighbor smoothing., dialogue topic segmentation, data noise, boundary ambiguity},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1109/ASONAM49781.2020.9381379,
author = {Duong, Viet and Luo, Jiebo and Pham, Phu and Yang, Tongyu and Wang, Yu},
title = {The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic},
year = {2020},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381379},
doi = {10.1109/ASONAM49781.2020.9381379},
abstract = {In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people's opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community's demographics and responses to prevalent social issues during a major crisis.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {126–130},
numpages = {5},
keywords = {college students, sentiment analysis, classification, Twitter, COVID-19},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3027385.3027438,
author = {Slater, Stefan and Baker, Ryan and Almeda, Ma. Victoria and Bowers, Alex and Heffernan, Neil},
title = {Using Correlational Topic Modeling for Automated Topic Identification in Intelligent Tutoring Systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027438},
doi = {10.1145/3027385.3027438},
abstract = {Student knowledge modeling is an important part of modern personalized learning systems, but typically relies upon valid models of the structure of the content and skill in a domain. These models are often developed through expert tagging of skills to items. However, content creators in crowdsourced personalized learning systems often lack the time (and sometimes the domain knowledge) to tag skills themselves. Fully automated approaches that rely on the covariance of correctness on items can lead to effective skill-item mappings, but the resultant mappings are often difficult to interpret. In this paper we propose an alternate approach to automatically labeling skills in a crowdsourced personalized learning system using correlated topic modeling, a natural language processing approach, to analyze the linguistic content of mathematics problems. We find a range of potentially meaningful and useful topics within the context of the ASSISTments system for mathematics problem-solving.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {393–397},
numpages = {5},
keywords = {natural language processing, topic modeling, correlational topic modeling, mathematics education, intelligent tutoring systems},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3175684.3175699,
author = {Comito, Carmela and Pizzuti, Clara and Procopio, Nicola},
title = {How People Talk about Health? Detecting Health Topics from Twitter Streams},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175699},
doi = {10.1145/3175684.3175699},
abstract = {The paper proposes an online clustering algorithm for detecting health-related topics. The method extracts from the tweets relevant terms and incrementally groups them by taking into account both term occurrences and tweet age. A detailed experimentation on the tweets posted by users in US shows that the method is capable to group tweets addressing common health issues into the pertinent topic, outperforming traditional topic model approaches, like Doc-p and LDA.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {85–90},
numpages = {6},
keywords = {Topic Detection, Twitter, e-Health},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@inproceedings{10.1145/3121257.3121260,
author = {Ellmann, Mathias and Oeser, Alexander and Fucci, Davide and Maalej, Walid},
title = {Find, Understand, and Extend Development Screencasts on YouTube},
year = {2017},
isbn = {9781450351577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121257.3121260},
doi = {10.1145/3121257.3121260},
abstract = {A software development screencast is a video that captures the screen of a developer working on a particular task and explaining implementation details. Due to the increased popularity of development screencasts e.g., on YouTube, we study how and to what extent they can be used as additional source of knowledge to answer developers’ questions, for example about the use of a specific API. We first study the difference between development screencasts and other types of screencasts using video frame analysis. When comparing frames with the Cosine algorithm, developers can expect ten development screencasts in the top 20 out of 100 different YouTube videos. We then extracted popular development topics. These were: database operations, system set-up, plug-in development, game development, and testing. We also identified six recurring tasks performed in development screencasts, such as object usage and UI operations. Finally, we conducted a similarity analysis of the screencast transcripts and the Javadoc of the corresponding screencasts.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics},
pages = {1–7},
numpages = {7},
keywords = {Similarity Analytics, API documentation, Development Screencasts},
location = {Paderborn, Germany},
series = {SWAN 2017}
}

@inproceedings{10.1145/3443467.3443751,
author = {Gongda, Qiu and Guixin, Zhang and Hui, Shi and Liqiong, Deng},
title = {A Comprehensive Sentiment Analysis Method Based on Sentiment Multi-Label and Probabilistic Hesitant Fuzzy Decision-Making},
year = {2020},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443751},
doi = {10.1145/3443467.3443751},
abstract = {Aiming at the difficulty of analysis on social text with complex sentiment and lacking of the comprehensive sentiment analysis based on historical social text set, a sentiment portrait analysis method based on multi-sentiment analysis and probabilistic hesitant fuzzy decision-making was proposed. Firstly, the BERT model was appropriately adapted to the hesitant fuzzy problem. Topic class and sentiment label of user text are acquired by topic classification and text sentiment multi-label model based on BERT model, Meanwhile the hesitant and fuzzy sentiment of different topics text are preserved by multi-level and multi-label sentiment. Secondly, the probabilistic hesitation fuzzy set is constructed based on the hesitation fuzzy and probability of text, in which the probabilistic hesitation fuzzy elements of each attribute are integrated by PHFWA operator. Finally, the modified sentiment entropy was proposed based on the statistical distribution of sentiment and information entropy, combined with its score function, deviation degree function and modified sentiment entropy, three types of sentiment distribution are defined and judged. In the experiment, four public Weibo accounts were selected and their status data were collected for analysis, which verified the effectiveness of the method in this paper.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {186–194},
numpages = {9},
keywords = {Probability hesitation fuzzy, Topic multi-classification, Modified Sentiment Entropy, Sentiment Multi-label, Complex sentiment text},
location = {Xiamen, China},
series = {EITCE 2020}
}

@inproceedings{10.1145/3338533.3366583,
author = {Chen, Yiyan and Tao, Li and Wang, Xueting and Yamasaki, Toshihiko},
title = {Weakly Supervised Video Summarization by Hierarchical Reinforcement Learning},
year = {2019},
isbn = {9781450368414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338533.3366583},
doi = {10.1145/3338533.3366583},
abstract = {Conventional video summarization approaches based on reinforcement learning have the problem that the reward can only be received after the whole summary is generated. Such kind of reward is sparse and it makes reinforcement learning hard to converge. Another problem is that labelling each shot is tedious and costly, which usually prohibits the construction of large-scale datasets. To solve these problems, we propose a weakly supervised hierarchical reinforcement learning framework, which decomposes the whole task into several subtasks to enhance the summarization quality. This framework consists of a manager network and a worker network. For each subtask, the manager is trained to set a subgoal only by a task-level binary label, which requires much fewer labels than conventional approaches. With the guide of the subgoal, the worker predicts the importance scores for video shots in the subtask by policy gradient according to both global reward and innovative defined sub-rewards to overcome the sparse problem. Experiments on two benchmark datasets show that our proposal has achieved the best performance, even better than supervised approaches.},
booktitle = {Proceedings of the ACM Multimedia Asia},
articleno = {3},
numpages = {6},
keywords = {sub-reward, hierarchical reinforcement learning, video summarization},
location = {Beijing, China},
series = {MMAsia '19}
}

@inproceedings{10.1145/3041021.3051159,
author = {Chen, Hongxu and Yin, Hongzhi and Li, Xue and Wang, Meng and Chen, Weitong and Chen, Tong},
title = {People Opinion Topic Model: Opinion Based User Clustering in Social Networks},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3051159},
doi = {10.1145/3041021.3051159},
abstract = {Mining various hot discussed topics and corresponding opinions from different groups of people in social media (e.g., Twitter) is very useful. For example, a decision maker in a company wants to know how different groups of people (customers, staff, competitors, etc.) think about their services, facilities, and things happened around. In this paper, we are focusing on the problem of finding opinion variations based on different groups of people and introducing the concept of opinion based community detection. Further, we also introduce a generative graphic model, namely People Opinion Topic (POT) model, which detects social communities, associated hot discussed topics, and perform sentiment analysis simultaneously by modelling user's social connections, common interests, and opinions in a unified way. This paper is the first attempt to study community and opinion mining together. Compared with traditional social communities detection, the detected communities by POT model are more interpretable and meaningful. In addition, we further analyse how diverse opinions distributed and propagated among various social communities. Experiments on real twitter dataset indicate our model is effective.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1353–1359},
numpages = {7},
keywords = {opinion, social network, topic model, community detection},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1145/3469886,
author = {Desolda, Giuseppe and Ferro, Lauren S. and Marrella, Andrea and Catarci, Tiziana and Costabile, Maria Francesca},
title = {Human Factors in Phishing Attacks: A Systematic Literature Review},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469886},
doi = {10.1145/3469886},
abstract = {Phishing is the fraudulent attempt to obtain sensitive information by disguising oneself as a trustworthy entity in digital communication. It is a type of cyber attack often successful because users are not aware of their vulnerabilities or are unable to understand the risks. This article presents a systematic literature review conducted to draw a “big picture” of the most important research works performed on human factors and phishing. The analysis of the retrieved publications, framed along the research questions addressed in the systematic literature review, helps in understanding how human factors should be considered to defend against phishing attacks. Future research directions are also highlighted.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {173},
numpages = {35},
keywords = {human factors, cybersecurity, Phishing}
}

@inproceedings{10.1145/3487553.3524721,
author = {Ommi, Yassaman and Yousefabadi, Matin and Faez, Faezeh and Sabour, Amirmojtaba and Soleymani Baghshah, Mahdieh and Rabiee, Hamid R.},
title = {CCGG: A Deep Autoregressive Model for Class-Conditional Graph Generation},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524721},
doi = {10.1145/3487553.3524721},
abstract = {Graph data structures are fundamental for studying connected entities. With an increase in the number of applications where data is represented as graphs, the problem of graph generation has recently become a hot topic. However, despite its significance, conditional graph generation that creates graphs with desired features is relatively less explored in previous studies. This paper addresses the problem of class-conditional graph generation that uses class labels as generation constraints by introducing the Class Conditioned Graph Generator (CCGG). We built CCGG by injecting the class information as an additional input into a graph generator model and including a classification loss in its total loss along with a gradient passing trick. Our experiments show that CCGG outperforms existing conditional graph generation methods on various datasets. It also manages to maintain the quality of the generated graphs in terms of distribution-based evaluation metrics.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1092–1098},
numpages = {7},
keywords = {Conditional Generative Models, Generative Models, Graph Generation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1109/TASLP.2017.2779862,
author = {Chien, Jen-Tzung},
title = {Bayesian Nonparametric Learning for Hierarchical and Sparse Topics},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2779862},
doi = {10.1109/TASLP.2017.2779862},
abstract = {This paper presents the Bayesian nonparametric BNP learning for hierarchical and sparse topics from natural language. Traditionally, the Indian buffet process provides the BNP prior on a binary matrix for an infinite latent feature model consisting of a flat layer of topics. The nested model paves an avenue to construct a tree model instead of a flat-layer model. This paper presents the nested Indian buffet process nIBP to achieve the sparsity and flexibility in topic model where the model complexity and topic hierarchy are learned from the groups of words. The mixed membership modeling is conducted by representing a document using the tree nodes or dishes that a document or a customer chooses according to the nIBP scenario. A tree stick-breaking process is implemented to select topic weights from a subtree for flexible topic modeling. Such an nIBP relaxes the constraint of adopting a single tree path in the nested Chinese restaurant process nCRP and, therefore, improves the variety of topic representation for heterogeneous documents. A Gibbs sampling procedure is developed to infer the nIBP topic model. Compared to the nested hierarchical Dirichlet process nhDP, the compactness of the estimated topics in a tree using nIBP is improved. Experimental results show that the proposed nIBP reduces the error rate of nCRP and nhDP by 18% and 8% on Reuters task for document classification, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {422–435},
numpages = {14}
}

@inproceedings{10.1145/3350546.3352506,
author = {Fromm, Michael and Faerman, Evgeniy and Seidl, Thomas},
title = {TACAM: Topic And Context Aware Argument Mining},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352506},
doi = {10.1145/3350546.3352506},
abstract = {In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {99–106},
numpages = {8},
keywords = {argument search, transfer learning, natural language processing, argument mining},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1145/3351108.3351133,
author = {Mazarura, Jocelyn and de Waal, Alta and de Villiers, Pieter},
title = {Semantic Representations for Under-Resourced Languages},
year = {2019},
isbn = {9781450372657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351108.3351133},
doi = {10.1145/3351108.3351133},
abstract = {Distributional semantics studies methods for learning semantic representation of natural text. The semantic similarity between words and documents can be derived from this presentation which leads to other practical NLP applications such as collaborative filtering, aspect-based sentiment analysis, intent classification for chatbots and machine translation. Under-resourced language data is small in size. Small data implies not only small corpora, but also short documents within the corpus. In this paper we investigate the performance of word embedding techniques on two under-resourced languages. We investigate two topic models, LDA and DMM as well as a word embedding word2vec. We find DMM to perform better than LDA as a topic model embedding. DMM and word2vec perform similar in a semantic evaluation task of aligned corpora.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019},
articleno = {24},
numpages = {10},
keywords = {topic models, under-resourced langauges, distributional semantics},
location = {Skukuza, South Africa},
series = {SAICSIT '19}
}

@inproceedings{10.1145/3323771.3323795,
author = {Intisar, Chowdhury Md and Watanobe, Yutaka and Poudel, Manoj and Bhalla, Subhash},
title = {Classification of Programming Problems Based on Topic Modeling},
year = {2019},
isbn = {9781450366397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323771.3323795},
doi = {10.1145/3323771.3323795},
abstract = {Programming skill is one of the most important and demanding skill in the current generation. In order to enable learners and programmers to practice programming and gain problem-solving skills, many Online Judge (OJ) systems exist. Most of these OJ systems have to be operated solely by students and learners. These students and novice programmers sometimes compete against each other or solve the programming problems by themselves in offline mode. But, most OJ systems have their problems arranged simply into volumes and various contests events. This arrangement system does not have any clear indication of the difficulties and categories of problems. Thus, in this paper, we have studied reliable techniques on the extraction of keywords and features which can categorize these OJ system's programming problems into their respective types and skills. We have leveraged two popular topic modeling algorithms, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) to extract relevant features. Afterward, six classifiers were trained on these topic modeling features and Naive TF-IDF features. From our studies, we discovered that topic modeling features were relatively smaller in dimensionality, yet matched the performance when trained on high dimensional naive TF-IDF features. Our main goal was to understand the precise trade-off between accuracy and dimensionality of the textual data of programming problem statements. This experiment has enabled us to obtain important tags, hint, and classification of Online Judge programming problems.},
booktitle = {Proceedings of the 2019 7th International Conference on Information and Education Technology},
pages = {275–283},
numpages = {9},
keywords = {Online Judge Systems, feature extraction, text classification, novice programmer, topic modeling},
location = {Aizu-Wakamatsu, Japan},
series = {ICIET 2019}
}

@inproceedings{10.1145/3374587.3374590,
author = {Wang, Jiangyao and Xu, Wenhua and Yan, Wenhao and Li, Caixia},
title = {Text Similarity Calculation Method Based on Hybrid Model of LDA and TF-IDF},
year = {2019},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374590},
doi = {10.1145/3374587.3374590},
abstract = {The traditional TF-IDF-based text similarity calculation model uses statistical methods to map text to the keyword vector space and convert the similarity of text into the distance between text vectors. Such methods have problems such as high computational dimensions, sparse data, and inability to take advantage of the semantic information contained in the text itself, so the results obtained are not as similar as the physical text. The text similarity model based on the topic model changes the traditional spatial similarity of keyword vectors, and can fully utilize the semantic information contained in the text itself. But this approach ignores the effect of words on text semantic representations with different weights. In the process of converting text into topic feature space, valuable information is lost. In view of the above problems, this paper proposes a text similarity hybrid model (L-THM) integrating LDA and TF-IDF for calculating text similarity. The model uses the semantic information contained in the text itself and the keyword information reflecting the text to comprehensively analyses and calculates the similarity between the texts. The experimental results show that the hybrid model can better represent the text information than the single model, and obtain a good F value in the cluster, which effectively improves the text similarity calculation effect.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {1–8},
numpages = {8},
keywords = {hybrid model, TF-IDF, topic model, LDA, text similarity},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/3297001.3297026,
author = {GM, Sushravya and Sengupta, Shubhashis},
title = {Unsupervised Multi-Task Learning Dialogue Management},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297026},
doi = {10.1145/3297001.3297026},
abstract = {Traditional dialog management systems used in task / goal-oriented scenario require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog management systems, in which all components are trained from the dialogs transcripts, overcome this limitation. This paper proposes a novel unsupervised dialogue-manager for goal-oriented dialogue applications. Set in the context of Corporate Information Organization (CIO) domain, our tasks require assessing user-initiated dialogues to understand the nature of the tickets, issue DB and API calls, and use the output of such calls to assist users. We show that an unsupervised dialog state-tracking and management system based on jointly trained LSTM-RNNs for utterance understanding and response generation can learn to perform non-trivial dialog management.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {196–202},
numpages = {7},
keywords = {State tracking and Dialogue management, Multi-task learning, topic modeling, Unsupervised learning},
location = {Kolkata, India},
series = {CoDS-COMAD '19}
}

@article{10.1145/3432924,
author = {Choi, Yoonseo and Monserrat, Toni-Jan Keith Palma and Park, Jeongeon and Shin, Hyungyu and Lee, Nyoungwoo and Kim, Juho},
title = {ProtoChat: Supporting the Conversation Design Process with Crowd Feedback},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432924},
doi = {10.1145/3432924},
abstract = {Similar to a design process for designing graphical user interfaces, conversation designers often apply an iterative design process by defining a conversation flow, testing with users, reviewing user data, and improving the design. While it is possible to iterate on conversation design with existing chatbot prototyping tools, there still remain challenges in recruiting participants on-demand and collecting structured feedback on specific conversational components. These limitations hinder designers from running rapid iterations and making informed design decisions. We posit that involving a crowd in the conversation design process can address these challenges, and introduce ProtoChat, a crowd-powered chatbot design tool built to support the iterative process of conversation design. ProtoChat makes it easy to recruit crowd workers to test the current conversation within the design tool. ProtoChat's crowd-testing tool allows crowd workers to provide concrete and practical feedback and suggest improvements on specific parts of the conversation. With the data collected from crowd-testing, ProtoChat provides multiple types of visualizations to help designers analyze and revise their design. Through a three-day study with eight designers, we found that ProtoChat enabled an iterative design process for designing a chatbot. Designers improved their design by not only modifying the conversation design itself, but also adjusting the persona and getting UI design implications beyond the conversation design itself. The crowd responses were helpful for designers to explore user needs, contexts, and diverse response formats. With ProtoChat, designers can successfully collect concrete evidence from the crowd and make decisions to iteratively improve their conversation design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {225},
numpages = {27},
keywords = {conversation design, conversational user interface, crowd testing, chatbot design, design process, crowdsourcing, crowd feedback, design iteration}
}

@article{10.1145/3373464.3373474,
author = {Burkhardt, Sophie and Kramer, Stefan},
title = {A Survey of Multi-Label Topic Models},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3373464.3373474},
doi = {10.1145/3373464.3373474},
abstract = {Every day, an enormous amount of text data is produced. Sources of text data include news, social media, emails, text messages, medical reports, scientific publications and fiction. To keep track of this data, there are categories, key words, tags or labels that are assigned to each text. Automatically predicting such labels is the task of multi-label text classification. Often however, we are interested in more than just the pure classification: rather, we would like to understand which parts of a text belong to the label, which words are important for the label or which labels occur together. Because of this, topic models may be used for multi-label classification as an interpretable model that is flexible and easily extensible. This survey demonstrates the manifold possibilities and flexibility of the topic model framework for the complex setting of multi-label text classification by categorizing different variants of models.},
journal = {SIGKDD Explor. Newsl.},
month = {nov},
pages = {61–79},
numpages = {19}
}

@inproceedings{10.1145/3341105.3374041,
author = {Calisir, Emre and Brambilla, Marco},
title = {Wide-Spectrum Characterization of Long-Running Political Phenomena on Social Media: The Brexit Case},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374041},
doi = {10.1145/3341105.3374041},
abstract = {In this study, we propose a wide-spectrum analysis of long-running political events on social media, with reference to an interesting real-world international case: the so-called Brexit, the process through which the United Kingdom activated the option of leaving the European Union. In this study, we model the users participating in 33 months of Twitter debate, covering their behaviour and demographics. By using publicly shared tweets, we developed a stance classification model to evaluate the change of stance over time. We also extracted the key topics of the long-running debate, studying which political side have discussed them most and what is the general sentiment on each. We also revealed the participation of bot accounts, and we found that the higher the bot score, the more likely the account is in a pro-Leave position. We conclude our study with a temporal and comparative analysis of politicians' social media accounts.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1869–1876},
numpages = {8},
keywords = {brexit referendum, political stance classification, topic discovery, automated political accounts},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3531146.3533107,
author = {Laufer, Benjamin and Jain, Sameer and Cooper, A. Feder and Kleinberg, Jon and Heidari, Hoda},
title = {Four Years of FAccT: A Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533107},
doi = {10.1145/3531146.3533107},
abstract = {Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {401–426},
numpages = {26},
keywords = {impact, community perspectives, reflexivity, topics, values, mixed methods, FAccT},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3490354.3494388,
author = {Pagliaro, Cynthia and Mehta, Dhagash and Shiao, Han-Tai and Wang, Shaofei and Xiong, Luwei},
title = {Investor Behavior Modeling by Analyzing Financial Advisor Notes: A Machine Learning Perspective},
year = {2021},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494388},
doi = {10.1145/3490354.3494388},
abstract = {Modeling investor behavior is crucial to identifying behavioral coaching opportunities for financial advisors. With the help of natural language processing (NLP) we analyze an unstructured (textual) dataset of financial advisors' summary notes, taken after every investor conversation, to gain first ever insights into advisor-investor interactions. These insights are used to predict investor needs during adverse market conditions; thus allowing advisors to coach investors and help avoid inappropriate financial decision-making. First, we perform topic modeling to gain insight into the emerging topics and trends. Based on this insight, we construct a supervised classification model to predict the probability that an advised investor will move assets to cash during volatile market periods. To the best of our knowledge, ours is the first work on exploring the advisor-investor relationship using unstructured data. This work may have far-reaching implications for both traditional and emerging financial advisory service models like robo-advising.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {23},
numpages = {8},
keywords = {natural language processing, machine learning, investor's modeling, financial advice},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3493700.3493727,
author = {Dixit, Vini},
title = {GCFI++: Embedding and Frequent Itemset Based Incremental Hierarchical Clustering with Labels and Outliers},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493727},
doi = {10.1145/3493700.3493727},
abstract = {Customer support is an essential part of any company to ensure customer satisfaction and business growth. The responsibility and scale get even higher for enterprise companies like Freshworks that build customer support platforms for its enterprise customers from multiple and previously unseen domains. A ticket may carry issues never seen before, therefore static, and predefined problem labels will not always work. These predefined problem labels are often created manually by admins. For a large customer account, roughly 10-35% of the problem labels could get discovered by this manual process and only 5-9% of the available open tickets could get linked to a problem label. So, there is a need for a system for a new incoming ticket. The system should either automatically detect an existing problem topic with its possible suggestions or systematically generate new problem topic(s) for its assignment. Our system is based on an incremental approach to mine ticket data streams and assigns them to hierarchical soft clusters with auto-generated problem/issue topics. Our system achieves an improvement in ticket assignments up to 97%. To compare with recent clustering advancements, we’ve also evaluated our approach on a public dataset [9] and achieved comparable results. Source code and results of the evaluation are available in detail at: https://github.com/vinidixit/hierarchical-labelled-clustering-evaluation.},
booktitle = {5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {135–143},
numpages = {9},
keywords = {Frequent Itemset Embeddings, Incremental Systems, Outlier Detection, Topic Detection, Customer Support Systems, Microtext, Hierarchical Labeling, Hierarchical Clustering},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.1145/3269206.3271671,
author = {Li, Ximing and Li, Changchun and Chi, Jinjin and Ouyang, Jihong and Li, Chenliang},
title = {Dataless Text Classification: A Topic Modeling Approach with Document Manifold},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271671},
doi = {10.1145/3269206.3271671},
abstract = {Recently, dataless text classification has attracted increasing attention. It trains a classifier using seed words of categories, rather than labeled documents that are expensive to obtain. However, a small set of seed words may provide very limited and noisy supervision information, because many documents contain no seed words or only irrelevant seed words. In this paper, we address these issues using document manifold, assuming that neighboring documents tend to be assigned to a same category label. Following this idea, we propose a novel Laplacian seed word topic model (LapSWTM). In LapSWTM, we model each document as a mixture of hidden category topics, each of which corresponds to a distinctive category. Also, we assume that neighboring documents tend to have similar category topic distributions. This is achieved by incorporating a manifold regularizer into the log-likelihood function of the model, and then maximizing this regularized objective. Experimental results show that our LapSWTM significantly outperforms the existing dataless text classification algorithms and is even competitive with supervised algorithms to some extent. More importantly, it performs extremely well when the seed words are scarce.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {973–982},
numpages = {10},
keywords = {dataless text classification, topic modeling, seed word, document manifold},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3340531.3412878,
author = {Hofst\"{a}tter, Sebastian and Zlabinger, Markus and Sertkan, Mete and Schr\"{o}der, Michael and Hanbury, Allan},
title = {Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412878},
doi = {10.1145/3340531.3412878},
abstract = {There are many existing retrieval and question answering datasets. However, most of them either focus on ranked list evaluation or single-candidate question answering. This divide makes it challenging to properly evaluate approaches concerned with ranking documents and providing snippets or answers for a given query. In this work, we present FiRA: a novel dataset of Fine-Grained Relevance Annotations. We extend the ranked retrieval annotations of the Deep Learning track of TREC 2019 with passage and word level graded relevance annotations for all relevant documents. We use our newly created data to study the distribution of relevance in long documents, as well as the attention of annotators to specific positions of the text. As an example, we evaluate the recently introduced TKL document ranking model. We find that although TKL exhibits state-of-the-art retrieval results for long documents, it misses many relevant passages.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3031–3038},
numpages = {8},
keywords = {relevance distribution, position bias, word-level relevance, fine-grained annotations},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3387940.3392207,
author = {Peirs, Tom and Westra, Freark and Molenaar, Sabine and Jansen, Slinger},
title = {The Influence of Technical Variety in Software Ecosystems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392207},
doi = {10.1145/3387940.3392207},
abstract = {There is a lack of empirical evidence on software ecosystem health metrics, and a need for operationalizable metrics that describe software ecosystem characteristics. This study unveils a new approach for measuring technical variety concisely. Studies show that a high variety opens up new opportunities and thus, better niche creation, and ultimately, improves software ecosystem health. Four different ecosystems are evaluated, and compared. Variety is measured in relation to robustness, and productivity metrics of the ecosystem to uncover the influence of technical variety on software ecosystems. Technical variety indicates a positive correlation with robustness, however acceptance of this statement is not confirmed with certainty due to a weak relation. Furthermore, significant relations indicate differences between ecosystem types.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {725–732},
numpages = {8},
keywords = {Software Ecosystems, Niche Creation, Technical Variety, Software Ecosystem Health},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3290688.3290735,
author = {Habibabadi, Sedigheh Khademi and Haghighi, Pari Delir},
title = {Topic Modelling for Identification of Vaccine Reactions in Twitter},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290735},
doi = {10.1145/3290688.3290735},
abstract = {Background: Detection of vaccine safety signals depends on various established reporting systems, where there is inevitably a lag between an adverse reaction to a vaccine and the reporting of it, and subsequent processing of reports. Therefore, it is desirable to try and detect safety signals earlier, ideally close to real-time. Extensive use of social media has provided a platform for sharing and seeking health-related information, and the immediacy of social media conversations mean that they are an ideal candidate for early detection of vaccine safety signals. The objective of this study is to evaluate topic models for identifying user posts on Twitter that most likely contain vaccine safety signals. This is an initial step in the overall research to determine if reliable vaccine safety signals can be detected in social media streams. The techniques used were focused on identifying the model design and number of topics that best revealed documents that contained vaccine safety signals, to assist with dimension reduction and subsequent labelling of the text data. The study compared Gensim LDA, MALLET, and jLDADMM DMM models to determine the most effective model for detecting vaccine safety signals, assisted by an evaluation process that used an adjusted F-Scoring technique over a labelled subset of the documents.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {31},
numpages = {10},
keywords = {Social media, Topic modelling, Twitter, Vaccine safety surveillance},
location = {Sydney, NSW, Australia},
series = {ACSW 2019}
}

@inproceedings{10.1145/3127526.3127529,
author = {Accuosto, Pablo and Ronzano, Francesco and Ferr\'{e}s, Daniel and Saggion, Horacio},
title = {Multi-Level Mining and Visualization of Scientific Text Collections: Exploring a Bi-Lingual Scientific Repository},
year = {2017},
isbn = {9781450353885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127526.3127529},
doi = {10.1145/3127526.3127529},
abstract = {We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).},
booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
pages = {9–16},
numpages = {8},
keywords = {Language Resources, Scientific Text Mining, Big Scientific Data, Information Extraction, PDF Conversion, Data Visualization},
location = {Toronto, ON, Canada},
series = {WOSP 2017}
}

@inproceedings{10.1145/3383455.3422537,
author = {Chen, Jiahao and Veloso, Manuela},
title = {Paying down Metadata Debt: Learning the Representation of Concepts Using Topic Models},
year = {2020},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422537},
doi = {10.1145/3383455.3422537},
abstract = {We introduce a data management problem called metadata debt, to identify the mapping between data concepts and their logical representations. We describe how this mapping can be learned using semisupervised topic models based on low-rank matrix factorizations that account for missing and noisy labels, coupled with sparsity penalties to improve localization and interpretability. We introduce a gauge transformation approach that allows us to construct explicit associations between topics and concept labels, and thus assign meaning to topics. We also show how to use this topic model for semisupervised learning tasks like extrapolating from known labels, evaluating possible errors in existing labels, and predicting missing features. We show results from this topic model in predicting subject tags on over 25,000 datasets from Kaggle.com, demonstrating the ability to learn semantically meaningful features.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {43},
numpages = {8},
keywords = {controlled vocabulary, topic model, semisupervised learning},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3123266.3123420,
author = {Chen, Shizhe and Chen, Jia and Jin, Qin and Hauptmann, Alexander},
title = {Video Captioning with Guidance of Multimodal Latent Topics},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123420},
doi = {10.1145/3123266.3123420},
abstract = {The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&amp;M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. The topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model. M&amp;M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves better generalization ability.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {multi-task, video captioning, latent topics, multimodal},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3511095.3531269,
author = {Huang, Xiaolei and Wormley, Alexandra and Cohen, Adam},
title = {Learning to Adapt Domain Shifts of Moral Values via Instance Weighting},
year = {2022},
isbn = {9781450392334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511095.3531269},
doi = {10.1145/3511095.3531269},
abstract = {Classifying moral values in user-generated text from social media is critical in understanding community cultures and interpreting user behaviors of social movements. Moral values and language usage can change across the social movements; however, text classifiers are usually trained in source domains of existing social movements and tested in target domains of new social issues without considering the variations. In this study, we examine domain shifts of moral values and language usage, quantify the effects of domain shifts on the morality classification task, and propose a neural adaptation framework via instance weighting to improve cross-domain classification tasks. The quantification analysis suggests a strong correlation between morality shifts, language usage, and classification performance. We evaluate the neural adaptation framework on a public Twitter data across 7 social movements and gain classification improvements up to 12.1%. Finally, we release a new data of the COVID-19 vaccine labeled with moral values and evaluate our approach on the new target domain. For the case study of the COVID-19 vaccine, our adaptation framework achieves up to 5.26% improvements over neural baselines. This is the first study to quantify impacts of moral shifts, propose adaptive framework to model the shifts, and conduct a case study to model COVID-19 vaccine-related behaviors from moral values.},
booktitle = {Proceedings of the 33rd ACM Conference on Hypertext and Social Media},
pages = {121–131},
numpages = {11},
keywords = {domain variation, morality, moral values, classification, instant weighting, adaptation},
location = {Barcelona, Spain},
series = {HT '22}
}

@article{10.1145/3511097,
author = {Nasim, Zarmeen and Haider, Sajjad},
title = {Automatic Labeling of Clusters for a Low-Resource Urdu Language},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511097},
doi = {10.1145/3511097},
abstract = {Document clustering techniques often produce clusters that require human intervention to interpret the meaning of such clusters. Automatic cluster labeling refers to the process of assigning a meaningful phrase to a cluster as a label. This article proposes an unsupervised method for cluster labeling that is based on noun phrase chunking. The proposed method is compared with four other statistical-based methods, including Z-Order, M-Order, T-Order, and YAKE. In addition to the statistical measures based labeling schemes, the approach is also compared with two graph-based techniques: TextRank and PositionRank. The experiments were performed on the low-resource Urdu language corpus of News Headlines. The proposed approach's effectiveness was evaluated using cosine similarity, the Jaccard index, and feedback received from human evaluators. The results show that the proposed method outperforms other methods. It was found that the labels produced were more relevant and semantically rich in contrast to other approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {93},
numpages = {22},
keywords = {low-resource language, urdu language processing, Cluster labeling}
}

@inproceedings{10.1145/3405962.3405968,
author = {Steuber, Florian and Schoenfeld, Mirco and Rodosek, Gabi Dreo},
title = {Topic Modeling of Short Texts Using Anchor Words},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405968},
doi = {10.1145/3405962.3405968},
abstract = {We present Archetypal LDA or short A-LDA, a topic model tailored to short texts containing "semantic anchors" which convey a certain meaning or implicitly build on discussions beyond their mere presence. A-LDA is an extension to Latent Dirichlet Allocation in that we guide the process of topic inference by these semantic anchors as seed words to the LDA. We identify these seed words unsupervised from the documents and evaluate their co-occurrences using archetypal analysis, a geometric approximation problem that aims for finding k points that best approximate the data set's convex hull. These so called archetypes are considered as latent topics and used to guide the LDA. We demonstrate the effectiveness of our approach using Twitter, where semantic anchor words are the hashtags assigned to tweets by users. In direct comparison to LDA, A-LDA achieves 10-13% better results. We find that representing topics in terms of hashtags corresponding to calculated archetypes alone already results in interpretable topics and the model's performance peaks for seed confidence values ranging from 0.7 to 0.9.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {210–219},
numpages = {10},
keywords = {text mining, archetypal analysis, data mining, topic modeling, short text},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3340531.3411932,
author = {Lee, Dongho and Oh, Byungkook and Seo, Seungmin and Lee, Kyong-Ho},
title = {News Recommendation with Topic-Enriched Knowledge Graphs},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411932},
doi = {10.1145/3340531.3411932},
abstract = {News recommendation systems? purpose is to tackle the immense amount of news and offer personalized recommendations to users. A major issue in news recommendation is to capture the precise news representations for the efficacy of recommended items. Commonly, news contents are filled with well-known entities of different types. However, existing recommendation systems overlook exploiting external knowledge about entities and topical relatedness among the news. To cope with the above problem, in this paper, we propose Topic-Enriched Knowledge Graph Recommendation System(TEKGR). Three encoders in TEKGR handle news titles in two perspectives to obtain news representation embedding: (1) to extract meaning of news words without considering latent knowledge features in the news and (2) to extract semantic knowledge of news through topic information and contextual information from a knowledge graph. After obtaining news representation vectors, an attention network compares clicked news to the candidate news in order to get the user's final embedding. Our TEKGR model is superior to existing news recommendation methods by manipulating topical relations among entities and contextual features of entities. Experimental results on two public datasets show that our approach outperforms state-of-the-art deep recommendation approaches.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {695–704},
numpages = {10},
keywords = {knowledge graphs, neural networks, news recommendation, recommendation system},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3209281.3209378,
author = {Deng, Qing and Cai, Guoray and Zhang, Hui and Liu, Yi and Huang, Lida and Sun, Feng},
title = {Enhancing Situation Awareness of Public Safety Events by Visualizing Topic Evolution Using Social Media},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209378},
doi = {10.1145/3209281.3209378},
abstract = {Social media contributes to enhancing transparency and openness for the purpose of innovating public services and policy-making. In disaster management, social media data can be mined to discover public perceptions and concerns on large disaster events. However, converting large data streams into useful information remains a challenge due to the unstructured nature of textual data. This study proposes an interactive topic modeling method to analyze microblog data for understanding the dynamics of public expressions immediately after a major explosion event. First, we extract topics from microblog message data. In order to test the influence of the number of topics, the topics are detected at multiple levels of granularity by varying the number of topics. Second, these topics are used to detect topical compositions of contents at different time slices and assess the topic evolution over time. The topic evolution patterns are visualized by the streamgraph method to discover informative topics to help to take further actions. Third, since the first-level topics are not informative, we conduct a second-level topic (subtopic) analysis to detect key decision elements by choosing "investigation" from the first-level topics, a hot focus in any man-made disaster. The results improve our understanding of the topic composition evolution around major man-made disasters and have implications on officials deciding what and when to release formal investigation information to the public.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {7},
numpages = {10},
keywords = {data analysis, topic evolution, massive man-made disasters, social media, situation awareness},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3097286.3097288,
author = {Addawood, Aseel and Schneider, Jodi and Bashir, Masooda},
title = {Stance Classification of Twitter Debates: The Encryption Debate as A Use Case},
year = {2017},
isbn = {9781450348478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097286.3097288},
doi = {10.1145/3097286.3097288},
abstract = {Social media have enabled a revolution in user-generated content. They allow users to connect, build community, produce and share content, and publish opinions. To better understand online users' attitudes and opinions, we use stance classification. Stance classification is a relatively new and challenging approach to deepen opinion mining by classifying a user's stance in a debate. Our stance classification use case is tweets that were related to the spring 2016 debate over the FBI's request that Apple decrypt a user's iPhone. In this "encryption debate," public opinion was polarized between advocates for individual privacy and advocates for national security. We propose a machine learning approach to classify stance in the debate, and a topic classification that uses lexical, syntactic, Twitter-specific, and argumentative features as a predictor for classifications. Models trained on these feature sets showed significant increases in accuracy relative to the unigram baseline.},
booktitle = {Proceedings of the 8th International Conference on Social Media &amp; Society},
articleno = {2},
numpages = {10},
keywords = {Argumentative Features, Supervised Machine Learning, Natural Language Processing, Stance Classification},
location = {Toronto, ON, Canada},
series = {#SMSociety17}
}

@inproceedings{10.1145/3209542.3209564,
author = {Khazaei, Taraneh and Xiao, Lu and Mercer, Robert E. and Khan, Atif},
title = {Understanding Privacy Dichotomy in Twitter},
year = {2018},
isbn = {9781450354271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209542.3209564},
doi = {10.1145/3209542.3209564},
abstract = {Balancing personalization and privacy is one of the challenges marketers commonly face. The privacy dilemmas associated with personalized services are particularly concerning in the context of social networking websites, wherein the privacy dichotomy problem is widely observed. To prevent potential privacy violations, businesses need to employ multiple safeguards beyond the current privacy settings of users. As a possible solution, companies can utilize user social footprints to detect user privacy preferences. To take a step towards this goal, we first ran a series of experiments to examine if the privacy preference attribute is homophilous in social media. As a result, we found a set of clues that users' privacy preferences are similar to the privacy behaviour of their social contacts, signaling that privacy homophily exists in social networks. We further studied users located in different neighbourhoods with varying degrees of privacy and found a set of characteristics that are specific to public users located in private neighbourhoods. These identified features can be used in a predictive model to identify public user accounts that are intended to be private, supporting companies to make an informed decision whether or not to exploit one's publicly available data for personalization purposes.},
booktitle = {Proceedings of the 29th on Hypertext and Social Media},
pages = {156–164},
numpages = {9},
keywords = {social network analysis, preference detection, social privacy},
location = {Baltimore, MD, USA},
series = {HT '18}
}

@inproceedings{10.1145/3227609.3227659,
author = {Loukachevitch, Natalia and Ivanov, Kirill and Dobrov, Boris},
title = {Thesaurus-Based Topic Models and Their Evaluation},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227659},
doi = {10.1145/3227609.3227659},
abstract = {In this paper we study thesaurus-based topic models and evaluate them from the point of view of topic coherence. Thesaurus-based topic model enhances scores of related terms found in the same text, which means that the model encourages these terms to be in the same topics. We evaluate various variants of such models. At the first step, we carry out manual evaluation of the obtained topics. At the second step, we study the possibility to use the collected manual data for evaluating new variants of thesaurus-based models, propose a method and select the best of its parameters in cross-validation. At the third step, we apply the created evaluation method to estimate the influence of word frequencies on adding thesaurus relations during generating topic models.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {11},
numpages = {9},
keywords = {thesaurus, topic models, content-based analysis},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@inproceedings{10.1145/3097983.3098122,
author = {Wang, Hao and Fu, Yanmei and Wang, Qinyong and Yin, Hongzhi and Du, Changying and Xiong, Hui},
title = {A Location-Sentiment-Aware Recommender System for Both Home-Town and Out-of-Town Users},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098122},
doi = {10.1145/3097983.3098122},
abstract = {Spatial item recommendation has become an important means to help people discover interesting locations, especially when people pay a visit to unfamiliar regions. Some current researches are focusing on modelling individual and collective geographical preferences for spatial item recommendation based on users' check-in records, but they fail to explore the phenomenon of user interest drift across geographical regions, i.e., users would show different interests when they travel to different regions. Besides, they ignore the influence of public comments for subsequent users' check-in behaviors. Specifically, it is intuitive that users would refuse to check in to a spatial item whose historical reviews seem negative overall, even though it might fit their interests. Therefore, it is necessary to recommend the right item to the right user at the right location. In this paper, we propose a latent probabilistic generative model called LSARS to mimic the decision-making process of users' check-in activities both in home-town and out-of-town scenarios by adapting to user interest drift and crowd sentiments, which can learn location-aware and sentiment-aware individual interests from the contents of spatial items and user reviews. Due to the sparsity of user activities in out-of-town regions, LSARS is further designed to incorporate the public preferences learned from local users' check-in behaviors. Finally, we deploy LSARS into two practical application scenes: spatial item recommendation and target user discovery. Extensive experiments on two large-scale location-based social networks (LBSNs) datasets show that LSARS achieves better performance than existing state-of-the-art methods.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1143},
numpages = {9},
keywords = {check-in behavior, recommendation, crowd sentiment, user interest drift},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3287560.3287583,
author = {Bountouridis, Dimitrios and Harambam, Jaron and Makhortykh, Mykola and Marrero, M\'{o}nica and Tintarev, Nava and Hauff, Claudia},
title = {SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287583},
doi = {10.1145/3287560.3287583},
abstract = {The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as "Matthew effects", "filter bubbles", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {150–159},
numpages = {10},
keywords = {diversity, recommender systems, simulation, news media},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{10.1145/3511095.3531289,
author = {Martins, Emanuelle Azevedo and Salles, Isadora and Benevenuto, Fabricio and Goussevskaia, Olga},
title = {Characterizing Sponsored Content in Facebook and Instagram},
year = {2022},
isbn = {9781450392334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511095.3531289},
doi = {10.1145/3511095.3531289},
abstract = {In this work we present a comparative analysis of influencer marketing evolution on Facebook and Instagram, spanning the pre and post Covid-19 pandemic onset periods. We collected and characterized a large-scale cross-platform dataset, comprised of 9.5 million sponsored posts. We analyzed the relative growth rates of the number of ads and of user engagement within different topics of interest, such as sports, retail, travel, and politics. We discuss which topics have been most impacted by the onset of the pandemic, both in terms of sponsored content supply and demand. With this work we hope to expand the understanding of influence dynamics on social networks and provide support for the development of more contextualized and effective branding strategies.},
booktitle = {Proceedings of the 33rd ACM Conference on Hypertext and Social Media},
pages = {52–63},
numpages = {12},
keywords = {influencer marketing, Social network analysis, Instagram, Facebook., sponsored content},
location = {Barcelona, Spain},
series = {HT '22}
}

@inproceedings{10.1145/3462757.3466085,
author = {Aumiller, Dennis and Almasian, Satya and Lackner, Sebastian and Gertz, Michael},
title = {Structural Text Segmentation of Legal Documents},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466085},
doi = {10.1145/3462757.3466085},
abstract = {The growing complexity of legal cases has lead to an increasing interest in legal information retrieval systems that can effectively satisfy user-specific information needs. However, such downstream systems typically require documents to be properly formatted and segmented, which is often done with relatively simple pre-processing steps, disregarding topical coherence of segments. Systems generally rely on representations of individual sentences or paragraphs, which may lack crucial context, or document-level representations, which are too long for meaningful search results. To address this issue, we propose a segmentation system that can predict topical coherence of sequential text segments spanning several paragraphs, effectively segmenting a document and providing a more balanced representation for downstream applications. We build our model on top of popular transformer networks and formulate structural text segmentation as topical change detection, by performing a series of independent classifications that allow for efficient fine-tuning on task-specific data. We crawl a novel dataset consisting of roughly 74,000 online Terms-of-Service documents, including hierarchical topic annotations, which we use for training. Results show that our proposed system significantly outperforms baselines, and adapts well to structural peculiarities of legal documents. We release both data and trained models to the research community for future work.1},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {2–11},
numpages = {10},
keywords = {text segmentation, document understanding, outline generation},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.1145/3383583.3398521,
author = {Zhang, Jinsong and Guo, Chun and Liu, Xiaozhong},
title = {Characterize and Evaluate Scientific Domain and Domain Context Knowledge Map},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398521},
doi = {10.1145/3383583.3398521},
abstract = {Domain knowledge map, a.k.a., scholarly network, construction as an important method can describe the significant characters of a selected domain. In this research, we will address three fundamental problems for scholarly network generation. Firstly, two different methods will be investigated to associate keywords on the graph: Co-occur Domain Distance and Citation Probability Distribution Distance. Secondly, this paper will construct domain (core journals and conference proceedings) knowledge and domain referral (domain citation) scholarly networks, and propose a novel method to integrate those graphs by optimizing the nodes and their linkage. Finally, the paper will propose an innovative method to evaluate the accuracy and coverage of scholarly networks based on training keyword oriented Labeled-LDA model and validate different domain or domain referral graphs.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {187–196},
numpages = {10},
keywords = {domain characterization, scientific domain, knowledge map, domain context},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing Feature Model by Identifying Variability-Aware Modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {feature modules, configuration, variability-aware modularity, feature model recovery, product line},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3485447.3512034,
author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
title = {Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512034},
doi = {10.1145/3485447.3512034},
abstract = {Topic models have been the prominent tools for automatic topic discovery from text corpora. Despite their effectiveness, topic models suffer from several limitations including the inability of modeling word ordering information in documents, the difficulty of incorporating external linguistic knowledge, and the lack of both accurate and efficient inference methods for approximating the intractable posterior. Recently, pretrained language models (PLMs) have brought astonishing performance improvements to a wide variety of tasks due to their superior representations of text. Interestingly, there have not been standard approaches to deploy PLMs for topic discovery as better alternatives to topic models. In this paper, we begin by analyzing the challenges of using PLM representations for topic discovery, and then propose a joint latent space learning and clustering framework built upon PLM embeddings. In the latent space, topic-word and document-topic distributions are jointly modeled so that the discovered topics can be interpreted by coherent and distinctive terms and meanwhile serve as meaningful summaries of the documents. Our model effectively leverages the strong representation power and superb linguistic features brought by PLMs for topic discovery, and is conceptually simpler than topic models. On two benchmark datasets in different domains, our model generates significantly more coherent and diverse topics than strong topic models, and offers better topic-wise document representations, based on both automatic and human evaluations.1},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3143–3152},
numpages = {10},
keywords = {Clustering, Topic Discovery, Pretrained Language Models},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3503161.3547898,
author = {Lyu, Hanjia and Luo, Jiebo},
title = {Understanding Political Polarization via Jointly Modeling Users, Connections and Multimodal Contents on Heterogeneous Graphs},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547898},
doi = {10.1145/3503161.3547898},
abstract = {Understanding political polarization on social platforms is important as public opinions may become increasingly extreme when they are circulated in homogeneous communities, thus potentially causing damage in the real world. Automatically detecting the political ideology of social media users can help better understand political polarization. However, it is challenging due to the scarcity of ideology labels, complexity of multimodal contents, and cost of time-consuming data collection process. Most previous frameworks either focus on unimodal content or do not scale up well. In this study, we adopt a heterogeneous graph neural network to jointly model user characteristics, multimodal post contents as well as user-item relations in a bipartite graph to learn a comprehensive and effective user embedding without requiring ideology labels. We apply our framework to online discussions about economy and public health topics. The learned embeddings are then used to detect political ideology and understand political polarization. Our framework outperforms the unimodal, early/late fusion baselines, and homogeneous GNN frameworks by a margin of at least 9% absolute gain in the area under the receiver operating characteristic on two social media datasets. More importantly, our work does not require a time-consuming data collection process, which allows faster detection and in turn allows the policy makers to conduct analysis and design policies in time to respond to crises. We also show that our framework learns meaningful user embeddings and can help better understand political polarization. Notable differences in user descriptions, topics, images, and levels of retweet/quote activities are observed. Our framework for decoding user-content interaction shows wide applicability in understanding political polarization. Furthermore, it can be extended to user-item bipartite information networks for other applications such as content and product recommendation.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4072–4082},
numpages = {11},
keywords = {multimedia, political polarization, user-content interaction, heterogeneous graph},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3488560.3498496,
author = {Arora, Akhil and Gerlach, Martin and Piccardi, Tiziano and Garc\'{\i}a-Dur\'{a}n, Alberto and West, Robert},
title = {Wikipedia Reader Navigation: When Synthetic Data Is Enough},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498496},
doi = {10.1145/3488560.3498496},
abstract = {Every day millions of people read Wikipedia. When navigating the vast space of available topics using hyperlinks, readers describe trajectories on the article network. Understanding these navigation patterns is crucial to better serve readers' needs and address structural biases and knowledge gaps. However, systematic studies of navigation on Wikipedia are hindered by a lack of publicly available data due to the commitment to protect readers' privacy by not storing or sharing potentially sensitive data. In this paper, we ask: How well can Wikipedia readers' navigation be approximated by using publicly available resources, most notably the Wikipedia clickstream data? We systematically quantify the differences between real navigation sequences and synthetic sequences generated from the clickstream data, in 6 analyses across 8 Wikipedia language versions. Overall, we find that the differences between real and synthetic sequences are statistically significant, but with small effect sizes, often well below 10%. This constitutes quantitative evidence for the utility of the Wikipedia clickstream data as a public resource: clickstream data can closely capture reader navigation on Wikipedia and provides a sufficient approximation for most practical downstream applications relying on reader data. More broadly, this study provides an example for how clickstream-like data can generally enable research on user navigation on online platforms while protecting users' privacy.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {16–26},
numpages = {11},
keywords = {wikipedia clickstream, wikipedia server logs, user navigation},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3366423.3380278,
author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Wang, Zihan and Zhang, Chao and Zhang, Yu and Han, Jiawei},
title = {Discriminative Topic Mining via Category-Name Guided Text Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380278},
doi = {10.1145/3366423.3380278},
abstract = {Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2121–2132},
numpages = {12},
keywords = {Text Embedding, Discriminative Analysis, Topic Mining, Text Classification},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3442381.3449805,
author = {Piccardi, Tiziano and West, Robert},
title = {Crosslingual Topic Modeling with WikiPDA},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449805},
doi = {10.1145/3442381.3449805},
abstract = {We present Wikipedia-based Polyglot Dirichlet Allocation (WikiPDA), a crosslingual topic model that learns to represent Wikipedia articles written in any language as distributions over a common set of language-independent topics. It leverages the fact that Wikipedia articles link to each other and are mapped to concepts in the Wikidata knowledge base, such that, when represented as bags of links, articles are inherently language-independent. WikiPDA works in two steps, by first densifying bags of links using matrix completion and then training a standard monolingual topic model. A human evaluation shows that WikiPDA produces more coherent topics than monolingual text-based latent Dirichlet allocation (LDA), thus offering crosslinguality at no cost. We demonstrate WikiPDA’s utility in two applications: a study of topical biases in 28 Wikipedia language editions, and crosslingual supervised document classification. Finally, we highlight WikiPDA’s capacity for zero-shot language transfer, where a model is reused for new languages without any fine-tuning. Researchers can benefit from WikiPDA as a practical tool for studying Wikipedia’s content across its 299 language editions in interpretable ways, via an easy-to-use library publicly available at https://github.com/epfl-dlab/WikiPDA.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3032–3041},
numpages = {10},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3419249.3420106,
author = {van Berkel, Niels and Papachristos, Eleftherios and Giachanou, Anastasia and Hosio, Simo and Skov, Mikael B.},
title = {A Systematic Assessment of National Artificial Intelligence Policies: Perspectives from the Nordics and Beyond},
year = {2020},
isbn = {9781450375795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419249.3420106},
doi = {10.1145/3419249.3420106},
abstract = {Echoing the evolving interest and impact of artificial intelligence on society, governments are increasingly looking for ways to strategically position themselves as both innovators and regulators in this new domain. One of the most explicit and accessible ways in which governments outline these plans is through national strategy and policy documents. We follow a systematic search strategy to identify national AI policy documents across twenty-five countries. Through an analysis of these documents, including topic modelling, clustering, and reverse topic-search, we provide an overview of the topics discussed in national AI policies and contrast the differences between countries. Furthermore, we analyse the frequency of eleven ethical principles across our corpus. Our paper outlines implications of the differences between geographical and cultural clusters in relation to the future development of artificial intelligence applications.},
booktitle = {Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society},
articleno = {10},
numpages = {12},
keywords = {strategy, national guidelines, Artificial Intelligence, topic modelling, policy, AI, ethics},
location = {Tallinn, Estonia},
series = {NordiCHI '20}
}

@inproceedings{10.1145/3340531.3412050,
author = {Aloteibi, Saad and Clark, Stephen},
title = {Learning to Personalize for Web Search Sessions},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412050},
doi = {10.1145/3340531.3412050},
abstract = {The task of session search focuses on using interaction data to improve relevance for the user's next query at the session level. In this paper, we formulate session search as a personalization task under the framework of learning to rank. Personalization approaches re-rank results to match a user model. Such user models are usually accumulated over time based on the user's browsing behaviour. We use a pre-computed and transparent set of user models based on concepts from the social science literature. Interaction data are used to map each session to these user models. Novel features are then estimated based on such models as well as sessions' interaction data. Extensive experiments on test collections from the TREC session track show statistically significant improvements over current session search algorithms.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {15–24},
numpages = {10},
keywords = {session search, retrieval model, personalization, user models},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3357384.3358048,
author = {Ahmadvand, Ali and Sahijwani, Harshita and Choi, Jason Ingyu and Agichtein, Eugene},
title = {ConCET: Entity-Aware Topic Classification for Open-Domain Conversational Agents},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358048},
doi = {10.1145/3357384.3358048},
abstract = {Identifying the topic (domain) of each user's utterance in open-domain conversational systems is a crucial step for all subsequent language understanding and response tasks. In particular, for complex domains, an utterance is often routed to a single component responsible for that domain. Thus, correctly mapping a user utterance to the right domain is critical. This is a challenging task: users could mention entities like actors, singers or locations to implicitly indicate the domain, which requires extensive domain knowledge to interpret. To address this problem, we introduce ConCET: a Concurrent Entity-aware conversational Topic classifier, which incorporates entity type information together with the utterance content features. Specifically, ConCET utilizes entity information to enrich the utterance representation, combining character, word, and entity type embeddings into a single representation. However, for rich domains with millions of available entities, unrealistic amounts of labeled training data would be required. To complement our model, we propose a simple and effective method for generating synthetic training data, to augment the typically limited amounts of labeled training data, using commonly available knowledge bases as to generate additional labeled utterances. We extensively evaluate ConCET and our proposed training method first on an openly available human-human conversational dataset called Self-Dialogue, to calibrate our approach against previous state-of-the-art methods; second, we evaluate ConCET on a large dataset of human-machine conversations with real users, collected as part of the Amazon Alexa Prize. Our results show that ConCET significantly improves topic classification performance on both datasets, reaching 8-10% improvements compared to state-of-the-art deep learning methods. We complement our quantitative results with detailed analysis of system performance, which could be used for further improvements of conversational agents.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1371–1380},
numpages = {10},
keywords = {entity-aware conversation domain classification, open-domain conversational agents, conversational topic classification},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3501712.3529735,
author = {McDermott, Tiarnach and Robson, James and Winters, Niall and Malmberg, Lars-Erik},
title = {Mapping the Changing Landscape of Child-Computer Interaction Research Through Correlated Topic Modelling},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529735},
doi = {10.1145/3501712.3529735},
abstract = {As the field of child-computer interaction (CCI) develops and forms an increasingly distinct identity, there is a need for reflection upon the state of the field, and its development thus far. This paper provides an overview of the thematic structure of the CCI field in order to support such reflection, expanding upon previous reviews through implementation of a correlated topic model, an automated, inductive content analysis method, in analysing 4,771 CCI research papers published between 2003 and 2021. Prominence of research topics, and their evolution, are explored. Results portray CCI as a vibrant and varied research landscape which has evolved dynamically over time, exhibiting increasing specialisation and emergence of distinct subfields, and progressing from a technology- to needs-driven agenda. This analysis contributes an extensive empirical mapping of the CCI research landscape, facilitating reflection upon the field and its development, and revealing gaps in extant literature and opportunities for future research.},
booktitle = {Interaction Design and Children},
pages = {82–97},
numpages = {16},
keywords = {Correlated Topic Model, Automated Text Analysis, Child-Computer Interaction, CCI Research, Literature Review},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1145/3091478.3091502,
author = {Hong, Lingzi and Fu, Cheng and Torrens, Paul and Frias-Martinez, Vanessa},
title = {Understanding Citizens' and Local Governments' Digital Communications During Natural Disasters: The Case of Snowstorms},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091502},
doi = {10.1145/3091478.3091502},
abstract = {A growing number of citizens and local governments have embraced the use of Twitter to communicate during natural disasters. Studies have shown that online communications during disasters can be explained using crisis communication taxonomies. However, such taxonomies are broad and general, and offer little insight into the detailed content of the communications. In this paper, we propose a semi-automatic framework to extract and compare, in retrospect, the digital communication footprints of citizens and governments during disasters. These footprints, which characterize the topics discussed during a disaster at different spatio-temporal scales, are computed in an unsupervised manner using topic models, and manually labelled to identify specific issues affecting the population. The end objective is to offer detailed information about issues affecting citizens during natural disasters and to compare these against local governments' communications. We evaluate the framework using Twitter communications from 18 snowstorms (including two blizzards) on the US east coast.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {141–150},
numpages = {10},
keywords = {crisis communication, spatio-temporal modeling, disaster analytics, topic models},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@article{10.5555/3122009.3242019,
author = {George, Clint P. and Doss, Hani},
title = {Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {5937–5974},
numpages = {38},
keywords = {latent dirichlet allocation, empirical bayes inference, model selection, Markov chain monte carlo, topic modelling}
}

@inproceedings{10.1145/3377325.3377491,
author = {Smith-Renner, Alison and Kumar, Varun and Boyd-Graber, Jordan and Seppi, Kevin and Findlater, Leah},
title = {Digging into User Control: Perceptions of Adherence and Instability in Transparent Models},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377491},
doi = {10.1145/3377325.3377491},
abstract = {We explore predictability and control in interactive systems where controls are easy to validate. Human-in-the-loop techniques allow users to guide unsupervised algorithms by exposing and supporting interaction with underlying model representations, increasing transparency and promising fine-grained control. However, these models must balance user input and the underlying data, meaning they sometimes update slowly, poorly, or unpredictably---either by not incorporating user input as expected (adherence) or by making other unexpected changes (instability). While prior work exposes model internals and supports user feedback, less attention has been paid to users' reactions when transparent models limit control. Focusing on interactive topic models, we explore user perceptions of control using a study where 100 participants organize documents with one of three distinct topic modeling approaches. These approaches incorporate input differently, resulting in varied adherence, stability, update speeds, and model quality. Participants disliked slow updates most, followed by lack of adherence. Instability was polarizing: some participants liked it when it surfaced interesting information, while others did not. Across modeling approaches, participants differed only in whether they noticed adherence.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {519–530},
numpages = {12},
keywords = {interactive machine learning, intelligent user interface evaluation, control, transparency, topic modeling},
location = {Cagliari, Italy},
series = {IUI '20}
}

@inproceedings{10.1145/3197026.3197052,
author = {Salatino, Angelo A. and Osborne, Francesco and Motta, Enrico},
title = {AUGUR: Forecasting the Emergence of New Research Topics},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197052},
doi = {10.1145/3197026.3197052},
abstract = {Being able to rapidly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies. The literature presents several approaches to identifying the emergence of new research topics, which rely on the assumption that the topic is already exhibiting a certain degree of popularity and consistently referred to by a community of researchers. However, detecting the emergence of a new research area at an embryonic stage, i.e., before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge. We address this issue by introducing Augur, a novel approach to the early detection of research topics. Augur analyses the diachronic relationships between research areas and is able to detect clusters of topics that exhibit dynamics correlated with the emergence of new research topics. Here we also present the Advanced Clique Percolation Method (ACPM), a new community detection algorithm developed specifically for supporting this task. Augur was evaluated on a gold standard of 1,408 debutant topics in the 2000-2011 interval and outperformed four alternative approaches in terms of both precision and recall.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {303–312},
numpages = {10},
keywords = {ontologies, topic trends, scholarly data, semantic technologies, embryonic topic, clustering algorithms, topic detection},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@article{10.1109/TASLP.2020.3012062,
author = {Kesiraju, Santosh and Plchot, Old\v{r}ich and Burget, Luk\'{a}\v{s} and Gangashetty, Suryakanth V.},
title = {Learning Document Embeddings Along With Their Uncertainties},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3012062},
doi = {10.1109/TASLP.2020.3012062},
abstract = {Majority of the text modeling techniques yield only point-estimates of document embeddings and lack in capturing the uncertainty of the estimates. These uncertainties give a notion of how well the embeddings represent a document. We present Bayesian subspace multinomial model (Bayesian SMM), a generative log-linear model that learns to represent documents in the form of Gaussian distributions, thereby encoding the uncertainty in its covariance. Additionally, in the proposed Bayesian SMM, we address a commonly encountered problem of intractability that appears during variational inference in mixed-logit models. We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings. Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the unseen test data better as compared to the state-of-the-art neural variational document model on (Fisher) speech and (20Newsgroups) text corpora. Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data. The topic ID results show that the proposed model outperforms state-of-the-art unsupervised topic models and achieve comparable results to the state-of-the-art fully supervised discriminative models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {2319–2332},
numpages = {14}
}

@inproceedings{10.1145/3313831.3376380,
author = {Yen, Yu-Chun Grace and Kim, Joy O. and Bailey, Brian P.},
title = {Decipher: An Interactive Visualization Tool for Interpreting Unstructured Design Feedback from Multiple Providers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376380},
doi = {10.1145/3313831.3376380},
abstract = {Feedback from diverse audiences can vary in focus, differ in structure, and contradict each other, making it hard to interpret and act on. While prior work has explored generating quality feedback, our work helps a designer interpret that feedback. Through a formative study with professional designers (N=10), we discovered that the interpretation process includes categorizing feedback, identifying valuable feedback, and prioritizing which feedback to incorporate in a revision. We also found that designers leverage feedback topic and sentiment, and the status of the provider to aid interpretation. Based on the findings, we created a new tool (Decipher) that enables designers to visualize and navigate a collection of feedback using its topic and sentiment structure. In a preliminary evaluation (N=20), we found that Decipher helped users feel less overwhelmed during feedback interpretation tasks and better attend to critical issues and conflicting opinions compared to using a typical document-editing tool.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {sense-making, creativity, creativity support tools, feedback},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3308558.3313617,
author = {Le, Tuan and Akoglu, Leman},
title = {ContraVis: Contrastive and Visual Topic Modeling for Comparing Document Collections},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313617},
doi = {10.1145/3308558.3313617},
abstract = {Given posts on 'abortion' and posts on 'religion' from a political forum, how can we find topics that are discriminative and those in common? In general, (1) how can we compare and contrast two or more different ('labeled') document collections? Moreover, (2) how can we visualize the data (in 2-d or 3-d) to best reflect the similarities and differences between the collections? We introduce (to the best of our knowledge) the first contrastive and visual topic model, called ContraVis, that jointly addresses both problems: (1) contrastive topic modeling, and (2) contrastive visualization. That is, ContraVis learns not only latent topics but also embeddings for the documents, topics and labels for visualization. ContraVis exhibits three key properties by design. It is (i) Contrastive: It enables comparative analysis of different document corpora by extracting latent discriminative and common topics across labeled documents; (ii) Visually-expressive: Different from numerous existing models, it also produces a visualization for all of the documents, labels, and the extracted topics, where proximity in the coordinate space is reflective of proximity in semantic space; (iii) Unified: It extracts topics and visual coordinates simultaneously under a joint model. Through extensive experiments on real-world datasets, we show ContraVis 's potential for providing visual contrastive analysis of multiple document collections. We show both qualitatively and quantitatively that ContraVis significantly outperforms both unsupervised and supervised state-of-the-art topic models in contrastive power, semantic coherence and visual effectiveness.},
booktitle = {The World Wide Web Conference},
pages = {928–938},
numpages = {11},
keywords = {visualization, comparative text mining, contrastive topic models},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.1145/3449209,
author = {Li, Xuyang and Bahursettiwar, Antara and Kogan, Marina},
title = {Hello? Is There Anybody in There? Analysis of Factors Promoting Response From Authoritative Sources in Crisis},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449209},
doi = {10.1145/3449209},
abstract = {As social media has become more present in people's day-to-day lives, many turn to these platforms in natural disasters to keep abreast of the ever-evolving crisis situation. Facing the increasing amount of crisis-related information available on the social media platforms, users tend to focus on and reach out to authoritative sources---individuals or organizations that provide authoritative and credible crisis-related information due to their official status or position. As they provide high-quality information, response from the authoritative sources can be especially valuable to social media users directly affected in natural disasters. In this study, we aim to extend the reach of credible information during crisis, and direct the attention of authoritative users to the affected users who need their help. Specifically, we investigate what factors differentiate the tweets by regular users that receive responses from authoritative accounts from those that do not. We find that regular users' popularity and official accounts' level of busyness do not seem to affect the likelihood of tweets receiving a response. We thus explore the linguistic features of the tweets' content. Topic modeling and sentiment analysis results suggest that these linguistic aspects of the tweets may affect the response rate from authoritative sources. Our findings suggest crisis-related policy implications, as well as design implications for social media platforms where such exchanges take place, which can potentially increase the reach of credible information in a crisis and help those affected obtain the safety-critical information they need.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {135},
numpages = {21},
keywords = {authoritative sources, social media data, crisis informatics, information seeking}
}

@article{10.1145/3415215,
author = {Mendu, Sanjana and Baglione, Anna and Baee, Sonia and Wu, Congyu and Ng, Brandon and Shaked, Adi and Clore, Gerald and Boukhechba, Mehdi and Barnes, Laura},
title = {A Framework for Understanding the Relationship between Social Media Discourse and Mental Health},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415215},
doi = {10.1145/3415215},
abstract = {Over 35% of the world's population uses social media. Platforms like Facebook, Twitter, and Instagram have radically influenced the way individuals interact and communicate. These platforms facilitate both public and private communication with strangers and friends alike, providing rich insight into an individual's personality, health, and wellbeing. To date, many researchers have employed a variety of methods for extracting mental health-centric features from digital text communication (DTC) data, including natural language processing, social network analysis, and extraction of temporal discourse patterns. However, none have explored a hierarchical framework for extracting features from private messages with the goal of unifying approaches across methodological domains. Furthermore, while analyses of large, public corpora abound in existing literature, limited work has been done to explore the relationship between of private textual communications, personality traits, and symptoms of mental illness. We present a framework for constructing rich feature spaces from digital text communications. We then demonstrate the efficacy of our framework by applying it to a dataset of private Facebook messages in a college student population (N=103). Our results reveal key individual differences in temporal and relational behaviors, as well as language usage in relation to validated measures of trait-level anxiety, loneliness, and personality. This work represents a critical step forward in linking features of private social media messages to validated measures of mental health, wellbeing, and personality.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {144},
numpages = {23},
keywords = {social media, language, machine learning, text mining, mental health}
}

@inproceedings{10.1145/3379597.3387472,
author = {Abdellatif, Ahmad and Costa, Diego and Badran, Khaled and Abdalkareem, Rabe and Shihab, Emad},
title = {Challenges in Chatbot Development: A Study of Stack Overflow Posts},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387472},
doi = {10.1145/3379597.3387472},
abstract = {Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development.Therefore, in this paper, we examine the Q&amp;A website, Stack Overflow, to provide insights on the topics that chatbot developers are interested and the challenges they face. In particular, we leverage topic modeling to understand the topics that are being discussed by chatbot developers on Stack Overflow. Then, we examine the popularity and difficulty of those topics. Our results show that most of the chatbot developers are using Stack Overflow to ask about implementation guidelines. We determine 12 topics that developers discuss (e.g., Model Training) that fall into five main categories. Most of the posts belong to chatbot development, integration, and the natural language understanding (NLU) model categories. On the other hand, we find that developers consider the posts of building and integrating chatbots topics more helpful compared to other topics. Specifically, developers face challenges in the training of the chatbot's model. We believe that our study guides future research to propose techniques and tools to help the community at its early stages to overcome the most popular and difficult topics that practitioners face when developing chatbots.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {174–185},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1145/3374214,
author = {Pu, Calton and Suprem, Abhijit and Lima, Rodrigo Alves and Musaev, Aibek and Wang, De and Irani, Danesh and Webb, Steve and Ferreira, Joao Eduardo},
title = {Beyond Artificial Reality: Finding and Monitoring Live Events from Social Sensors},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3374214},
doi = {10.1145/3374214},
abstract = {With billions of active social media accounts and millions of live video cameras, live new big data offer many opportunities for smart applications. However, the main consumers of the new big data have been humans. We envision the research on live knowledge, to automatically acquire real-time, validated, and actionable information. Live knowledge presents two significant and diverging technical challenges: big noise and concept drift. We describe the EBKA (evidence-based knowledge acquisition) approach, illustrated by the LITMUS landslide information system. LITMUS achieves both high accuracy and wide coverage, demonstrating the feasibility and promise of EBKA approach to achieve live knowledge.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {2},
numpages = {21},
keywords = {Artificial reality, real-time event detection, evidence-based knowledge acquisition, concept drift, true novelty, live knowledge}
}

@inproceedings{10.1145/3180155.3180218,
author = {Gao, Cuiyun and Zeng, Jichuan and Lyu, Michael R. and King, Irwin},
title = {Online App Review Analysis for Identifying Emerging Issues},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180218},
doi = {10.1145/3180155.3180218},
abstract = {Detecting emerging issues (e.g., new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (e.g., misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the identified issues can facilitate app development in practice. Moreover, we have successfully applied IDEA to several products of Tencent, which serve hundreds of millions of users.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {48–58},
numpages = {11},
keywords = {online analysis, app reviews, emerging issues},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3493612.3520470,
author = {Silva, Jorge Sassaki Resende and Freire, Andr\'{e} Pimenta and Cardoso, Paula Christina Figueira},
title = {When Headers Are Not There: Design and User Evaluation of an Automatic Topicalisation and Labelling Tool to Aid the Exploration of Web Documents by Blind Users},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520470},
doi = {10.1145/3493612.3520470},
abstract = {Using headers to grasp a document's structure has been one of the main strategies employed by blind users on web documents when using screen readers. However, when headers are not available or not appropriately marked up, they can cause serious difficulties. This paper presents the design and evaluation of a tool for automatically generating headers for screen readers with topicalisation and labelling algorithms. The proposed tool uses Natural Language Processing techniques to divide a web document into topic segments and label each segment based on its content. We conducted an initial user study with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. Results provided preliminary indicators of improvement in performance and reduction of cognitive load. The findings contribute to knowledge to improve tools to aid in text exploration. It also provides initial empirical evidence to be further explored to analyze the impact of automatically-generated headings in improving performance and reducing cognitive load for blind users.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {18},
numpages = {11},
keywords = {automatic topicalisation and labelling, natural language processing, screen readers, accessibility},
location = {Lyon, France},
series = {W4A '22}
}

@article{10.1145/2932192,
author = {Li, Yang and Jiang, Jing and Liu, Ting and Qiu, Minghui and Sun, Xiaofei},
title = {Personalized Microtopic Recommendation on Microblogs},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/2932192},
doi = {10.1145/2932192},
abstract = {Microblogging services such as Sina Weibo and Twitter allow users to create tags explicitly indicated by the # symbol. In Sina Weibo, these tags are called microtopics, and in Twitter, they are called hashtags. In Sina Weibo, each microtopic has a designate page and can be directly visited or commented on. Recommending these microtopics to users based on their interests can help users efficiently acquire information. However, it is non-trivial to recommend microtopics to users to satisfy their information needs. In this article, we investigate the task of personalized microtopic recommendation, which exhibits two challenges. First, users usually do not give explicit ratings to microtopics. Second, there exists rich information about users and microtopics, for example, users' published content and biographical information, but it is not clear how to best utilize such information. To address the above two challenges, we propose a joint probabilistic latent factor model to integrate rich information into a matrix factorization-based solution to microtopic recommendation. Our model builds on top of collaborative filtering, content analysis, and feature regression. Using two real-world datasets, we evaluate our model with different kinds of content and contextual information. Experimental results show that our model significantly outperforms a few competitive baseline methods, especially in the circumstance where users have few adoption behaviors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {77},
numpages = {21},
keywords = {Microblogs, collaborative filtering, microtopic recommendation, topic model}
}

@article{10.1145/3446209,
author = {Xu, Qianli and Molino, Ana Garcia Del and Lin, Jie and Fang, Fen and Subbaraju, Vigneshwaran and Li, Liyuan and Lim, Joo-Hwee},
title = {Lifelog Image Retrieval Based on Semantic Relevance Mapping},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3446209},
doi = {10.1145/3446209},
abstract = {Lifelog analytics is an emerging research area with technologies embracing the latest advances in machine learning, wearable computing, and data analytics. However, state-of-the-art technologies are still inadequate to distill voluminous multimodal lifelog data into high quality insights. In this article, we propose a novel semantic relevance mapping (SRM) method to tackle the problem of lifelog information access. We formulate lifelog image retrieval as a series of mapping processes where a semantic gap exists for relating basic semantic attributes with high-level query topics. The SRM serves both as a formalism to construct a trainable model to bridge the semantic gap and an algorithm to implement the training process on real-world lifelog data. Based on the SRM, we propose a computational framework of lifelog analytics to support various applications of lifelog information access, such as image retrieval, summarization, and insight visualization. Systematic evaluations are performed on three challenging benchmarking tasks to show the effectiveness of our method.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jul},
articleno = {92},
numpages = {18},
keywords = {summarization, Lifelog, image retrieval, semantic mapping}
}

@article{10.1145/3481045,
author = {Baron, Jason R. and Sayed, Mahmoud F. and Oard, Douglas W.},
title = {Providing More Efficient Access to Government Records: A Use Case Involving Application of Machine Learning to Improve FOIA Review for the Deliberative Process Privilege},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3481045},
doi = {10.1145/3481045},
abstract = {At present, the review process for material that is exempt from disclosure under the Freedom of Information Act (FOIA) in the United States of America, and under many similar government transparency regimes worldwide, is entirely manual. Public access to the records of their government is thus inhibited by the long backlogs of material awaiting such reviews. This article studies one aspect of that problem by first creating a new public test collection with annotations for one class of exempt material subject to the deliberative process privilege, and then by using that test collection to study the ability of current text classification techniques to identify those materials that are exempt from release under that privilege. Results show that when the system is trained and evaluated using annotations from the same reviewer, even difficult cases can often be reliably detected. However, results also show that differences in reviewer interpretations, differences in record custodians, and differences in topics of the records used for training and testing can pose challenges.},
journal = {J. Comput. Cult. Herit.},
month = {jan},
articleno = {5},
numpages = {19},
keywords = {freedom of information act, evaluation, deliberative process privilege, Sensitivity review}
}

@inproceedings{10.1145/3514094.3534145,
author = {Barnett, Julia and Diakopoulos, Nicholas},
title = {Crowdsourcing Impacts: Exploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic Decision Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534145},
doi = {10.1145/3514094.3534145},
abstract = {With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {56–67},
numpages = {12},
keywords = {thematic analysis, ai ethics, broader impacts, anticipatory governance},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3475716.3475786,
author = {Gonzalez, Danielle and Perez, Paola Peralta and Mirakhorli, Mehdi},
title = {Barriers to Shift-Left Security: The Unique Pain Points of Writing Automated Tests Involving Security Controls},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475786},
doi = {10.1145/3475716.3475786},
abstract = {Background: Automated unit and integration tests allow software development teams to continuously evaluate their application's behavior and ensure requirements are satisfied. Interest in explicitly testing security at the unit and integration levels has risen as more teams begin to shift security left in their workflows, but there is little insight into any potential pain points developers may experience as they learn to adapt their existing skills to write these tests. Aims: Identify security unit and integration testing pain points that could negatively impact efforts to shift security (testing) left to this level. Method: An mixed-method empirical study was conducted on 525 Stack Overflow and Security Stack Exchange posts related to security unit and integration testing. Latent Dirichlet Allocation (LDA) was applied to identify commonly discussed topics, pain points were learned through qualitative analysis, and links were analyzed to study commonly-shared resources. Results: Nine topics representing security controls, components, and scenarios were identified; Authentication was the most commonly tested control. Developers experienced seven pain points unique to security unit and integration testing, which were all influenced by the complexity of security control designs and implementations. Most linked resources were other Q&amp;A posts, but repositories and documentation for security tools and libraries were also common. Conclusions: Developers may experience several unique pain points when writing tests at this level involving security controls. Additional resources are needed to guide developers through these challenges, which should also influence the creation of strategies and tools to help shift security testing to this level. To accelerate this, actionable recommendations for practitioners and future research directions based on these findings are highlighted.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {11},
numpages = {12},
keywords = {Integration Testing, Pain Points, Latent Dirichlet Allocation, Shift-Left Security, Security Testing, Stack Overflow, Unit Testing},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/2998181.2998269,
author = {Ma, Xiao and Hancock, Jeffrey T. and Lim Mingjie, Kenneth and Naaman, Mor},
title = {Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998269},
doi = {10.1145/2998181.2998269},
abstract = {Online peer-to-peer platforms like Airbnb allow hosts to list a property (e.g. a house, or a room) for short-term rentals. In this work, we examine how hosts describe themselves on their Airbnb profile pages. We use a mixed-methods study to develop a categorization of the topics that hosts self-disclose in their profile descriptions, and show that these topics differ depending on the type of guest engagement expected. We also examine the perceived trustworthiness of profiles using topic-coded profiles from 1,200 hosts, showing that longer self-descriptions are perceived to be more trustworthy. Further, we show that there are common strategies (a mix of topics) hosts use in self-disclosure, and that these strategies cause differences in perceived trustworthiness scores. Finally, we show that the perceived trustworthiness score is a significant predictor of host choice--especially for shorter profiles that show more variation. The results are consistent with uncertainty reduction theory, reflect on the assertions of signaling theory, and have important design implications for sharing economy platforms, especially those facilitating online-to-offline social exchange.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {2397–2409},
numpages = {13},
keywords = {airbnb, sharing economy, trustworthiness, self-disclosure, social exchange},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3368089.3409760,
author = {Lou, Yiling and Chen, Zhenpeng and Cao, Yanbin and Hao, Dan and Zhang, Lu},
title = {Understanding Build Issue Resolution in Practice: Symptoms and Fix Patterns},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409760},
doi = {10.1145/3368089.3409760},
abstract = {Build systems are essential for modern software maintenance and development, while build failures occur frequently across software systems, inducing non-negligible costs in development activities. Build failure resolution is a challenging problem and multiple studies have demonstrated that developers spend non-trivial time in resolving encountered build failures; to relieve manual efforts, automated resolution techniques are emerging recently, which are promising but still limitedly effective. Understanding how build failures are resolved in practice can provide guidelines for both developers and researchers on build issue resolution. Therefore, this work presents a comprehensive study of fix patterns in practical build failures. Specifically, we study 1,080 build issues of three popular build systems Maven, Ant, and Gradle from Stack Overflow, construct a fine-granularity taxonomy of 50 categories regarding to the failure symptoms, and summarize the fix patterns for different failure types. Our key findings reveal that build issues stretch over a wide spectrum of symptoms; 67.96% of the build issues are fixed by modifying the build script code related to plugins and dependencies; and there are 20 symptom categories, more than half of whose build issues can be fixed by specific patterns. Furthermore, we also address the challenges in applying non-intuitive or simplistic fix patterns for developers.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {617–628},
numpages = {12},
keywords = {Empirical study, Build systems, Build failure resolution},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3394831,
author = {Al-Ramahi, Mohammad and Noteboom, Cherie},
title = {Mining User-Generated Content of Mobile Patient Portal: Dimensions of User Experience},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2469-7818},
url = {https://doi.org/10.1145/3394831},
doi = {10.1145/3394831},
abstract = {Patient portals are positioned as a central component of patient engagement through the potential to change the physician-patient relationship and enable chronic disease self-management. The incorporation of patient portals provides the promise to deliver excellent quality, at optimized costs, while improving the health of the population. This study extends the existing literature by extracting dimensions related to the Mobile Patient Portal Use. We use a topic modeling approach to systematically analyze users’ feedback from the actual use of a common mobile patient portal, Epic's MyChart. Comparing results of Latent Dirichlet Allocation analysis with those of human analysis validated the extracted topics. Practically, the results provide insights into adopting mobile patient portals, revealing opportunities for improvement and to enhance the design of current basic portals. Theoretically, the findings inform the social-technical systems and Task-Technology Fit theories in the healthcare field and emphasize important healthcare structural and social aspects. Further, findings inform the humanization of healthcare framework, support the results of existing studies, and introduce new important design dimensions (i.e., aspects) that influence patient satisfaction and adherence to patient portal.},
journal = {Trans. Soc. Comput.},
month = {jun},
articleno = {15},
numpages = {24},
keywords = {latent dirichlet allocation (LDA), predictive analytics, Patient portal, explanatory analysis, user-generated contents, sentiment analysis}
}

@inproceedings{10.1145/3313831.3376768,
author = {Tahaei, Mohammad and Vaniea, Kami and Saphra, Naomi},
title = {Understanding Privacy-Related Questions on Stack Overflow},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376768},
doi = {10.1145/3313831.3376768},
abstract = {We analyse Stack Overflow (SO) to understand challenges and confusions developers face while dealing with privacy-related topics. We apply topic modelling techniques to 1,733 privacy-related questions to identify topics and then qualitatively analyse a random sample of 315 privacy-related questions. Identified topics include privacy policies, privacy concerns, access control, and version changes. Results show that developers do ask SO for support on privacy-related issues. We also find that platforms such as Apple and Google are defining privacy requirements for developers by specifying what "sensitive" information is and what types of information developers need to communicate to users (e.g. privacy policies). We also examine the accepted answers in our sample and find that 28% of them link to official documentation and more than half are answered by SO users without references to any external resources.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {usable privacy, stack overflow, software developers},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.14778/3067421.3067430,
author = {Cai, Hongyun and Zheng, Vincent W. and Zhu, Fanwei and Chang, Kevin Chen-Chuan and Huang, Zi},
title = {From Community Detection to Community Profiling},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3067421.3067430},
doi = {10.14778/3067421.3067430},
abstract = {Most existing community-related studies focus on detection, which aim to find the community membership for each user from user friendship links. However, membership alone, without a complete profile of what a community is and how it interacts with other communities, has limited applications. This motivates us to consider systematically profiling the communities and thereby developing useful community-level applications. In this paper, we for the first time formalize the concept of community profiling. With rich user information on the network, such as user published content and user diffusion links, we characterize a community in terms of both its internal content profile and external diffusion profile. The difficulty of community profiling is often underestimated. We novelly identify three unique challenges and propose a joint Community Profiling and Detection (CPD) model to address them accordingly. We also contribute a scalable inference algorithm, which scales linearly with the data size and it is easily parallelizable. We evaluate CPD on large-scale real-world data sets, and show that it is significantly better than the state-of-the-art baselines in various tasks.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {817–828},
numpages = {12}
}

@article{10.1145/3449170,
author = {Adhikary, Rishiraj and Patel, Zeel B. and Srivastava, Tanmay and Batra, Nipun and Singh, Mayank and Bhatia, Udit and Guttikunda, Sarath},
title = {Vartalaap: What Drives #AirQuality Discussions: Politics, Pollution or Pseudo-Science?},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449170},
doi = {10.1145/3449170},
abstract = {Air pollution is a global challenge for cities across the globe. Understanding the public perception of air pollution can help policymakers engage better with the public and appropriately introduce policies. Accurate public perception can also help people to identify the health risks of air pollution and act accordingly. Unfortunately, current techniques for determining perception are not scalable: it involves surveying few hundred people with questionnaire-based surveys. Using the advances in natural language processing (NLP), we propose a more scalable solution called Vartalaap to gauge public perception of air pollution via the microblogging social network Twitter. We curated a dataset of more than 1.2M tweets discussing Delhi-specific air pollution. We find that (unfortunately) the public is supportive of unproven mitigation strategies to reduce pollution, thus risking their health due to a false sense of security. We also find that air quality is a year-long problem, but the discussions are not proportional to the level of pollution and spike up when pollution is more visible. The information required by Vartalaap is publicly available and, as such, it can be immediately applied to study different societal issues across the world.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {96},
numpages = {29},
keywords = {social media, air pollution, perception}
}

@article{10.1145/3466640,
author = {Li, Siqing and Li, Yaliang and Zhao, Wayne Xin and Ding, Bolin and Wen, Ji-Rong},
title = {Interpretable Aspect-Aware Capsule Network for Peer Review Based Citation Count Prediction},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3466640},
doi = {10.1145/3466640},
abstract = {Citation count prediction is an important task for estimating the future impact of research papers. Most of the existing works utilize the information extracted from the paper itself. In this article, we focus on how to utilize another kind of useful data signal (i.e., peer review text) to improve both the performance and interpretability of the prediction models.Specially, we propose a novel aspect-aware capsule network for citation count prediction based on review text. It contains two major capsule layers, namely the feature capsule layer and the aspect capsule layer, with two different routing approaches, respectively. Feature capsules encode the local semantics from review sentences as the input of aspect capsule layer, whereas aspect capsules aim to capture high-level semantic features that will be served as final representations for prediction. Besides the predictive capacity, we also enhance the model interpretability with two strategies. First, we use the topic distribution of the review text to guide the learning of aspect capsules so that each aspect capsule can represent a specific aspect in the review. Then, we use the learned aspect capsules to generate readable text for explaining the predicted citation count. Extensive experiments on two real-world datasets have demonstrated the effectiveness of the proposed model in both performance and interpretability.},
journal = {ACM Trans. Inf. Syst.},
month = {nov},
articleno = {11},
numpages = {29},
keywords = {peer review, Citation count prediction, capsule network}
}

@article{10.1145/3230706,
author = {Wang, Weiqing and Yin, Hongzhi and Du, Xingzhong and Nguyen, Quoc Viet Hung and Zhou, Xiaofang},
title = {TPM: A Temporal Personalized Model for Spatial Item Recommendation},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230706},
doi = {10.1145/3230706},
abstract = {With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important way of helping users discover interesting locations to increase their engagement with location-based services. The availability of spatial, temporal, and social information in LBSNs offers an unprecedented opportunity to enhance the spatial item recommendation. Many previous works studied spatial and social influences on spatial item recommendation in LBSNs. Due to the strong correlations between a user’s check-in time and the corresponding check-in location, which include the sequential influence and temporal cyclic effect, it is essential for spatial item recommender system to exploit the temporal effect to improve the recommendation accuracy. Leveraging temporal information in spatial item recommendation is, however, very challenging, considering (1) when integrating sequential influences, users’ check-in data in LBSNs has a low sampling rate in both space and time, which renders existing location prediction techniques on GPS trajectories ineffective, and the prediction space is extremely large, with millions of distinct locations as the next prediction target, which impedes the application of classical Markov chain models; (2) there are various temporal cyclic patterns (i.e., daily, weekly, and monthly) in LBSNs, but existing work is limited to one specific pattern; and (3) there is no existing framework that unifies users’ personal interests, temporal cyclic patterns, and the sequential influence of recently visited locations in a principled manner.In light of the above challenges, we propose a Temporal Personalized Model (TPM), which introduces a novel latent variable topic-region to model and fuse sequential influence, cyclic patterns with personal interests in the latent and exponential space. The advantages of modeling the temporal effect at the topic-region level include a significantly reduced prediction space, an effective alleviation of data sparsity, and a direct expression of the semantic meaning of users’ spatial activities. Moreover, we introduce two methods to model the effect of various cyclic patterns. The first method is a time indexing scheme that encodes the effect of various cyclic patterns into a binary code. However, the indexing scheme faces the data sparsity problem in each time slice. To deal with this data sparsity problem, the second method slices the time according to each cyclic pattern separately and explores these patterns in a joint additive model.Furthermore, we design an asymmetric Locality Sensitive Hashing (ALSH) technique to speed up the online top-k recommendation process by extending the traditional LSH. We evaluate the performance of TPM on two real datasets and one large-scale synthetic dataset. The performance of TPM in recommending cold-start items is also evaluated. The results demonstrate a significant improvement in TPM’s ability to recommend spatial items, in terms of both effectiveness and efficiency, compared with the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {nov},
articleno = {61},
numpages = {25},
keywords = {spatial-temporal modeling, online learning, location-based service, POI}
}

@article{10.1145/3458770,
author = {Biester, Laura and Matton, Katie and Rajendran, Janarthanan and Provost, Emily Mower and Mihalcea, Rada},
title = {Understanding the Impact of COVID-19 on Online Mental Health Forums},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3458770},
doi = {10.1145/3458770},
abstract = {Like many of the disasters that have preceded it, the COVID-19 pandemic is likely to have a profound impact on people’s mental health. Understanding its impact can inform strategies for mitigating negative consequences. This work seeks to better understand the impacts of COVID-19 on mental health by examining how discussions on mental health subreddits have changed in the three months following the WHO’s declaration of a global pandemic. First, the rate at which the pandemic is discussed in each community is quantified. Then, volume of activity is measured to determine whether the number of people with mental health concerns has risen, and user interactions are analyzed to determine how they have changed during the pandemic. Finally, the content of the discussions is analyzed. Each of these metrics is considered with respect to a set of control subreddits to better understand if the changes present are specific to mental health subreddits or are representative of Reddit as a whole. There are numerous changes in the three mental health subreddits that we consider, r/Anxiety, r/depression, r/SuicideWatch; there is reduced posting activity in most cases, and there are significant changes in discussion of some topics such as work and anxiety. The results suggest that there is not an overwhelming increase in online mental health support-seeking on Reddit during the pandemic, but that discussion content related to mental health has changed.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
articleno = {31},
numpages = {28},
keywords = {user interaction, COVID-19, Mental health, topic modeling, time series}
}

@article{10.1613/jair.1.13550,
author = {Javed, Rana Tallal and Nasir, Osama and Borit, Melania and Vanh\'{e}e, Lo\"{\i}s and Zea, Elias and Gupta, Shivam and Vinuesa, Ricardo and Qadir, Junaid},
title = {Get out of the BAG! Silos in AI Ethics Education: Unsupervised Topic Modeling Analysis of Global AI Curricula},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13550},
doi = {10.1613/jair.1.13550},
abstract = {The domain of Artificial Intelligence (AI) ethics is not new, with discussions going back at least 40 years. Teaching the principles and requirements of ethical AI to students is considered an essential part of this domain, with an increasing number of technical AI courses taught at several higher-education institutions around the globe including content related to ethics. By using Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, this study uncovers topics in teaching ethics in AI courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to Bloom’s taxonomy. In this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from North American universities, 11 from Asia, 36 from Europe, and 10 from other regions. Based on this analysis, we were able to synthesize a model of teaching approaches, which we call BAG (Build, Assess, and Govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. We critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. We challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how. This article appears in the AI &amp; Society track.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {33},
keywords = {philosophical foundations, discourse modelling, data mining, scientific discovery}
}

@inproceedings{10.1145/3411764.3445383,
author = {Offenwanger, Anna and Milligan, Alan John and Chang, Minsuk and Bullard, Julia and Yoon, Dongwook},
title = {Diagnosing Bias in the Gender Representation of HCI Research Participants: How It Happens and Where We Are},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445383},
doi = {10.1145/3411764.3445383},
abstract = {In human-computer interaction (HCI) studies, bias in the gender representation of participants can jeopardize the generalizability of findings, perpetuate bias in data driven practices, and make new technologies dangerous for underrepresented groups. Key to progress towards inclusive and equitable gender practices is diagnosing the current status of bias and identifying where it comes from. In this mixed-methods study, we interviewed 13 HCI researchers to identify the potential bias factors, defined a systematic data collection procedure for meta-analysis of participant gender data, and created a participant gender dataset from 1,147 CHI papers. Our analysis provided empirical evidence for the underrepresentation of women, the invisibility of non-binary participants, deteriorating representation of women in MTurk studies, and characteristics of research topics prone to bias. Based on these findings, we make concrete suggestions for promoting inclusive community culture and equitable research practices in HCI.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {399},
numpages = {18},
keywords = {dataset, participants, data schema, user studies, gender bias, meta-analysis, HCI, gender, research, human-computer interaction, human subjects, CHI},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3072591,
author = {Hou, Lei and Li, Juanzi and Li, Xiao-Li and Tang, Jie and Guo, Xiaofei},
title = {Learning to Align Comments to News Topics},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072591},
doi = {10.1145/3072591},
abstract = {With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers’ news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
articleno = {9},
numpages = {31},
keywords = {dependent topic model, cluster chain, alignment, User-generated content, density, pu learning}
}

@inproceedings{10.1145/3035918.3035937,
author = {Gao, Zekai J. and Luo, Shangyu and Perez, Luis L. and Jermaine, Chris},
title = {The BUDS Language for Distributed Bayesian Machine Learning},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3035937},
doi = {10.1145/3035918.3035937},
abstract = {We describe BUDS, a declarative language for succinctly and simply specifying the implementation of large-scale machine learning algorithms on a distributed computing platform. The types supported in BUDS--vectors, arrays, etc.--are simply logical abstractions useful for programming, and do not correspond to the actual implementation. In fact, BUDS automatically chooses the physical realization of these abstractions in a distributed system, by taking into account the characteristics of the data. Likewise, there are many available implementations of the abstract operations offered by BUDS (matrix multiplies, transposes, Hadamard products, etc.). These are tightly coupled with the physical representation. In BUDS, these implementations are co-optimized along with the representation. All of this allows for the BUDS compiler to automatically perform deep optimizations of the user's program, and automatically generate efficient implementations.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {961–976},
numpages = {16},
keywords = {distributed system, machine learning, declarative language},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@article{10.1145/3487066,
author = {Zorrilla, Asier L\'{o}pez and Torres, M. In\'{e}s},
title = {A Multilingual Neural Coaching Model with Enhanced Long-Term Dialogue Structure},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3487066},
doi = {10.1145/3487066},
abstract = {In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jul},
articleno = {16},
numpages = {47},
keywords = {coaching, Dialogue system, explainable artificial intelligence, transfer learning, multilingual}
}

@article{10.5555/3122009.3153018,
author = {Papanikolaou, Yannis and Foulds, James R. and Rubin, Timothy N. and Tsoumakas, Grigorios},
title = {Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multilabel classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2058–2115},
numpages = {58},
keywords = {multi-label classification, latent dirichlet allocation, collapsed Gibbs sampling, CVB0, Bayesian inference, unsupervised learning, topic models, text mining}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to <i>The Web Conference 2019</i>. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

