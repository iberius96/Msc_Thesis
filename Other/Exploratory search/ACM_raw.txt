@inproceedings{10.1145/3041021.3051159,
author = {Chen, Hongxu and Yin, Hongzhi and Li, Xue and Wang, Meng and Chen, Weitong and Chen, Tong},
title = {People Opinion Topic Model: Opinion Based User Clustering in Social Networks},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3051159},
doi = {10.1145/3041021.3051159},
abstract = {Mining various hot discussed topics and corresponding opinions from different groups of people in social media (e.g., Twitter) is very useful. For example, a decision maker in a company wants to know how different groups of people (customers, staff, competitors, etc.) think about their services, facilities, and things happened around. In this paper, we are focusing on the problem of finding opinion variations based on different groups of people and introducing the concept of opinion based community detection. Further, we also introduce a generative graphic model, namely People Opinion Topic (POT) model, which detects social communities, associated hot discussed topics, and perform sentiment analysis simultaneously by modelling user's social connections, common interests, and opinions in a unified way. This paper is the first attempt to study community and opinion mining together. Compared with traditional social communities detection, the detected communities by POT model are more interpretable and meaningful. In addition, we further analyse how diverse opinions distributed and propagated among various social communities. Experiments on real twitter dataset indicate our model is effective.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1353–1359},
numpages = {7},
keywords = {social network, community detection, opinion, topic model},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3488560.3498518,
author = {Harandizadeh, Bahareh and Priniski, J. Hunter and Morstatter, Fred},
title = {Keyword Assisted Embedded Topic Model},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498518},
doi = {10.1145/3488560.3498518},
abstract = {By illuminating latent structures in a corpus of text, topic models are an essential tool for categorizing, summarizing, and exploring large collections of documents. Probabilistic topic models, such as latent Dirichlet allocation (LDA), describe how words in documents are generated via a set of latent distributions called topics. Recently, the Embedded Topic Model (ETM) has extended LDA to utilize the semantic information in word embeddings to derive semantically richer topics. As LDA and its extensions are unsupervised models, they aren't defined to make efficient use of a user's prior knowledge of the domain. To this end, we propose the Keyword Assisted Embedded Topic Model (KeyETM), which equips ETM with the ability to incorporate user knowledge in the form of informative topic-level priors over the vocabulary. Using both quantitative metrics and human responses on a topic intrusion task, we demonstrate that KeyETM produces better topics than other guided, generative models in the literaturefootnoteCode for this work can be found at urlhttps://github.com/bahareharandizade/KeyETM .},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {372–380},
numpages = {9},
keywords = {clustering, embedded topic modeling, topic models, guided topic modeling, prior knowledge},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3378065.3378076,
author = {Liu, Lin and Tang, Lin},
title = {A Label Distribution Topic Model for Multi-Label Classification},
year = {2019},
isbn = {9781450361910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378065.3378076},
doi = {10.1145/3378065.3378076},
abstract = {At present, multi-label supervised topic model is a kind of effective multi-label classification model applied to various domain. However, due to the limitation of traditional label-topic correspondence in existing multi-label supervised topic model, there are still some aspects that need to be improved. This paper proposed a label distributed LDA model(LD-LDA) for providing more complete label description, which overcomes the disadvantage that labels can only be associated with a fixed hidden topic set or a set of non-overlapping hidden topics, so as to describe labels by the form of probability distribution of all hidden topics. The experimental results show that LD-LDA model has better prediction effect than comparative models in protein function prediction. Although the description of observable variables, parameters and hidden variables in LD-LDA model is based on the problem of protein function prediction, LD-LDA model is essentially a multi-label topic model, which is also applicable to various multi-label application scenarios.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Processing},
pages = {52–57},
numpages = {6},
keywords = {probabilistic graphic model, Multi-label classification, Topic model},
location = {China, China},
series = {ICIIP 2019}
}

@inproceedings{10.1145/3132847.3132942,
author = {Jiang, Haixin and Zhou, Rui and Zhang, Limeng and Wang, Hua and Zhang, Yanchun},
title = {A Topic Model Based on Poisson Decomposition},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132942},
doi = {10.1145/3132847.3132942},
abstract = {Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1489–1498},
numpages = {10},
keywords = {statistical testing, topic coherence, text classification, poisson decomposition, topic model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3412371,
author = {Li, Shuangyin and Zhang, Yu and Pan, Rong},
title = {Bi-Directional Recurrent Attentional Topic Model},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3412371},
doi = {10.1145/3412371},
abstract = {In a document, the topic distribution of a sentence depends on both the topics of its neighbored sentences and its own content, and it is usually affected by the topics of the neighbored sentences with different weights. The neighbored sentences of a sentence include the preceding sentences and the subsequent sentences. Meanwhile, it is natural that a document can be treated as a sequence of sentences. Most existing works for Bayesian document modeling do not take these points into consideration. To fill this gap, we propose a bi-Directional Recurrent Attentional Topic Model (bi-RATM) for document embedding. The bi-RATM not only takes advantage of the sequential orders among sentences but also uses the attention mechanism to model the relations among successive sentences. To support to the bi-RATM, we propose a bi-Directional Recurrent Attentional Bayesian Process (bi-RABP) to handle the sequences. Based on the bi-RABP, bi-RATM fully utilizes the bi-directional sequential information of the sentences in a document. Online bi-RATM is proposed to handle large-scale corpus. Experiments on two corpora show that the proposed model outperforms state-of-the-art methods on document modeling and classification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {sep},
articleno = {74},
numpages = {30},
keywords = {recurrent attentional Bayesian process, topic modeling, Bi-directional recurrent attentions}
}

@inproceedings{10.1145/3219819.3219995,
author = {Acharya, Ayan and Ghosh, Joydeep and Zhou, Mingyuan},
title = {A Dual Markov Chain Topic Model for Dynamic Environments},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219995},
doi = {10.1145/3219819.3219995},
abstract = {The abundance of digital text has led to extensive research on topic models that reason about documents using latent representations. Since for many online or streaming textual sources such as news outlets, the number, and nature of topics change over time, there have been several efforts that attempt to address such situations using dynamic versions of topic models. Unfortunately, existing approaches encounter more complex inferencing when their model parameters are varied over time, resulting in high computation complexity and performance degradation. This paper introduces the DM-DTM, a dual Markov chain dynamic topic model, for characterizing a corpus that evolves over time. This model uses a gamma Markov chain and a Dirichlet Markov chain to allow the topic popularities and word-topic assignments, respectively, to vary smoothly over time. Novel applications of the Negative-Binomial augmentation trick result in simple, efficient, closed-form updates of all the required conditional posteriors, resulting in far lower computational requirements as well as less sensitivity to initial conditions, as compared to existing approaches. Moreover, via a gamma process prior, the number of desired topics is inferred directly from the data rather than being pre-specified and can vary as the data changes. Empirical comparisons using multiple real-world corpora demonstrate a clear superiority of DM-DTM over strong baselines for both static and dynamic topic models.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1099–1108},
numpages = {10},
keywords = {dynamic topic model, CRT augmentation, gibbs sampling},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3238250,
author = {Li, Chenliang and Chen, Shiqian and Xing, Jian and Sun, Aixin and Ma, Zongyang},
title = {Seed-Guided Topic Model for Document Filtering and Classification},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3238250},
doi = {10.1145/3238250},
abstract = {One important necessity is to filter out the irrelevant information and organize the relevant information into meaningful categories. However, developing text classifiers often requires a large number of labeled documents as training examples. Manually labeling documents is costly and time-consuming. More importantly, it becomes unrealistic to know all the categories covered by the documents beforehand. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this article, we propose a seed-guided topic model for the dataless text filtering and classification (named DFC). Given a collection of unlabeled documents, and for each specified category a small set of seed words that are relevant to the semantic meaning of the category, DFC filters out the irrelevant documents and classifies the relevant documents into the corresponding categories through topic influence. DFC models two kinds of topics: category-topics and general-topics. Also, there are two kinds of category-topics: relevant-topics and irrelevant-topics. Each relevant-topic is associated with one specific category, representing its semantic meaning. The irrelevant-topics represent the semantics of the unknown categories covered by the document collection. And the general-topics capture the global semantic information. DFC assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that DFC learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then filtered, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that DFC consistently outperforms the state-of-the-art dataless text classifiers for both classification with filtering and classification without filtering. In many tasks, DFC can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that DFC is insensitive to the tuning parameters. Moreover, we conduct a thorough study about the impact of seed words for existing dataless text classification techniques. The results reveal that it is not using more seed words but the document coverage of the seed words for the corresponding category that affects the dataless classification performance.},
journal = {ACM Trans. Inf. Syst.},
month = {dec},
articleno = {9},
numpages = {37},
keywords = {document filtering, dataless classification, Topic model}
}

@inproceedings{10.1145/3342827.3342835,
author = {Mu, Pengyu and He, Jingsha and Zhu, Nafei},
title = {Text Classification of Network Pyramid Scheme Based on Topic Model},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342835},
doi = {10.1145/3342827.3342835},
abstract = {At present, the network pyramid scheme has become a major tumor that hinders social development. In order to curb the propagation of the network pyramid scheme and effectively identify the pyramid scheme text in the network, this study proposes a joint topic model, Paragraph Vector Latent Dirichlet Allocation (PV_LDA), based on the characteristics of high-yield, high rebate, hierarchical salary and text topic diversity described in the text. The model uses the paragraph as the minimum processing unit to generate the topic distribution matrix of "high-interest rate" and "hierarchical salary" from the network pyramid scheme text. The Gibbs sampling is used to derive the "pyramid scheme" topic distribution matrix represented by the two features, which is used for classification processing by the classifier. the classification accuracy rate for the network pyramid scheme text can reach 86.25%. The conclusions show that the topic model proposed in this paper can capture the characteristics of the pyramid scheme more reasonably.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {15–19},
numpages = {5},
keywords = {text classification, topic mining, topic model, network pyramid scheme},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3447548.3467410,
author = {Yang, Yazheng and Pan, Boyuan and Cai, Deng and Sun, Huan},
title = {TopNet: Learning from Neural Topic Model to Generate Long Stories},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467410},
doi = {10.1145/3447548.3467410},
abstract = {Long story generation (LSG) is one of the coveted goals in natural language processing. Different from most text generation tasks, LSG requires to output a long story of rich content based on a much shorter text input, and often suffers from information sparsity. In this paper, we propose TopNet to alleviate this problem, by leveraging the recent advances in neural topic modeling to obtain high-quality skeleton words to complement the short input. In particular, instead of directly generating a story, we first learn to map the short text input to a low-dimensional topic distribution (which is pre-assigned by a topic model). Based on this latent topic distribution, we can use the reconstruction decoder of the topic model to sample a sequence of inter-related words as a skeleton for the story. Experiments on two benchmark datasets show that our proposed framework is highly effective in skeleton word selection and significantly outperforms the state-of-the-art models in both automatic evaluation and human evaluation.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {1997–2005},
numpages = {9},
keywords = {natural language processing, story telling, topic model, deep learning, long story generation},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3421515.3421521,
author = {Sun, Linjia},
title = {Automatic Language Identification Using Suprasegmental Feature and Supervised Topic Model},
year = {2020},
isbn = {9781450388627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421515.3421521},
doi = {10.1145/3421515.3421521},
abstract = {Language identification is quite challenging when it comes to discriminating between closely related dialects of the same language. The fundamental issue is to explore the discriminative cue and effective representation. In this paper, the multi-dimensional language cues are used to distinguish languages, which includes the phonotactic and prosodic information and can be found in the unsupervised setting. Moreover, a novel supervised topic model is proposed to represent and learn the difference of languages. We built the system of language identification and reported the test results on the NIST LRE07 datasets and the Chinese dialect spoken corpus. Compared with other state-of-the-art methods, the experiment results show that the proposed method provides competitive performance and helps to capture robust discriminative information for short duration language identification.},
booktitle = {2020 2nd Symposium on Signal Processing Systems},
pages = {69–73},
numpages = {5},
keywords = {Topic model, Language identification, Suprasegmental feature, Dialect identification, I-vector},
location = {Guangdong, China},
series = {SSPS 2020}
}

@article{10.1109/TASLP.2019.2892232,
author = {Deng, Dong and Jing, Liping and Yu, Jian and Sun, Shaolong and Ng, Michael K.},
title = {Sentiment Lexicon Construction With Hierarchical Supervision Topic Model},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892232},
doi = {10.1109/TASLP.2019.2892232},
abstract = {In this paper, we propose a novel hierarchical supervision topic model to construct a topic-adaptive sentiment lexicon TaSL for higher-level classification tasks. It is widely recognized that sentiment lexicon as a useful prior knowledge is crucial in sentiment analysis or opinion mining. However, many existing sentiment lexicons are constructed ignoring the variability of the sentiment polarities of words in different topics or domains. For example, the word “amazing” can refer to causing great surprise or wonder but can also refer to very impressive and excellent. In TaSL, we solve this issue by jointly considering the topics and sentiments of words. Documents are represented by multiple pairs of topics and sentiments, where each pair is characterized by a multinomial distribution over words. Meanwhile, this generating process is supervised under hierarchical supervision information of documents and words. The main advantage of TaSL is that the sentiment polarity of each word in different topics can be sufficiently captured. This model is beneficial to construct a domain-specific sentiment lexicon and then effectively improve the performance of sentiment classification. Extensive experimental results on four publicly available datasets, MR, OMD, semEval13A, and semEval16B were presented to demonstrate the usefulness of the proposed approach. The results have shown that TaSL performs better than the existing manual sentiment lexicon MPQA, the topic model based domain-specific lexicon ssLDA, the expanded lexiconsWeka-ED, Weka-STS, NRC, Liu's, and deep neural network based lexicons nnLexicon, HIT, HSSWE.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {704–718},
numpages = {15}
}

@inproceedings{10.1145/3167132.3167133,
author = {Madhavan, Manu and G, Gopakumar},
title = {A Tf-Idf Based Topic Model for Identifying LncRNAs from Genomic Background},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167133},
doi = {10.1145/3167132.3167133},
abstract = {The developments in high throughput technologies identified a large number of long non-coding RNAs (lncRNAs) whose functional characterization remains an open problem. The available research confirmed that lncRNA plays a major role in genetic and epigenetic regulation, and its expression level has a significant association with some complex diseases like cancers. The identification of lncRNA and their functional characterization is an important task in RNA Bioinformatics. In spite of their abundance in the cell, lncRNAs are less conserved at their sequence level which makes the analysis challenging. Many machine learning based models are developed in the literature for the identification and analysis of lncRNAs. This paper proposes a topic model based method for the identification of lncRNAs. To investigate the applicability of topic model in lncRNA analysis, this work develops an LDA based topic model to group lncRNAs from a collection of transcriptome sequences. The features derived from transformed k-mer patterns and secondary structure of lncRNA sequences are used for the topic model. The results are promising compared to the classic algorithms and prove that the topic models are reasonable for lncRNA analysis.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {40–46},
numpages = {7},
keywords = {topic model, latent dirichlet allocation, long non-coding RNA, mRNA, clustering},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3534678.3542675,
author = {Song, Ziyang and Hu, Yuanyi and Verma, Aman and Buckeridge, David L. and Li, Yue},
title = {Automatic Phenotyping by a Seed-Guided Topic Model},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542675},
doi = {10.1145/3534678.3542675},
abstract = {Electronic health records (EHRs) provide rich clinical information and the opportunities to extract epidemiological patterns to understand and predict patient disease risks with suitable machine learning methods such as topic models. However, existing topic models do not generate identifiable topics each predicting a unique phenotype. One promising direction is to use known phenotype concepts to guide topic inference. We present a seed-guided Bayesian topic model called MixEHR-Seed with 3 contributions: (1) for each phenotype, we infer a dual-form of topic distribution: a seed-topic distribution over a small set of key EHR codes and a regular topic distribution over the entire EHR vocabulary; (2) we model age-dependent disease progression as Markovian dynamic topic priors; (3) we infer seed-guided multi-modal topics over distinct EHR data types. For inference, we developed a variational inference algorithm. Using MixEHR-Seed, we inferred 1569 PheCode-guided phenotype topics from an EHR database in Quebec, Canada covering 1.3 million patients for up to 20-year follow-up with 122 million records for 8539 and 1126 unique diagnostic and drug codes, respectively. We observed (1) accurate phenotype prediction by the guided topics, (2) clinically relevant PheCode-guided disease topics, (3) meaningful age-dependent disease prevalence. Source code is available at GitHub: https://github.com/li-lab-mcgill/MixEHR-Seed.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4713–4723},
numpages = {11},
keywords = {electronic health records, predictive healthcare, variational autoencoder, topic modeling},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3483845.3483881,
author = {Shi, Lei and Xie, Suzhen and Tao, Yongcai and Wei, Lin and Gao, Yufei},
title = {Fake Review Identification Method Based on Topic Model and Att-BiLSTM},
year = {2021},
isbn = {9781450390453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483845.3483881},
doi = {10.1145/3483845.3483881},
abstract = {The review rating system provides valuable information to potential users, but it also encourages the creation of profit-driven fake reviews. Fake reviews and comments not only drive consumers to buy low-quality products or services, but also erode consumers' long-term confidence in review rating platforms. At present, two main reasons for the low detection accuracy of fake comments in recent studies are: (1) lack of feature learning of emotional intensity of text; (2) the inaccuracy of the identification of topic words in comments. To solve the above problems, we propose a novel identification method based on topic model and Att-BiLSTM mechanism. The proposed method calculates text affective and subjective values using TextBlob, incorporating the topic feature to train the classifier for fake review recognition. Comparative experiments show that the model effect is better than other models.},
booktitle = {Proceedings of the 2021 2nd International Conference on Control, Robotics and Intelligent System},
pages = {204–208},
numpages = {5},
keywords = {Emotional Characteristics, Attention Mechanism, Topic Model, Bidirectional Long and Short-term Memory Network, Fake Review Identification},
location = {Qingdao, China},
series = {CCRIS '21}
}

@inproceedings{10.1145/3512676.3512714,
author = {Zhao, Xiaogang and Dong, Siwei and Dang, Yiwei and Shen, Hai and Zhang, Hao and Li, Ge},
title = {A Method for Ranking Products Based on LDA Topic Model and Stochastic Dominance},
year = {2022},
isbn = {9781450387422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512676.3512714},
doi = {10.1145/3512676.3512714},
abstract = {It is difficult for consumers to make purchase decisions based on massive amount of online reviews. Therefore, a product selection method based on LDA topic model and stochastic dominance rules is proposed. The method first uses the LDA topic model to extract product attributes; secondly, sentiment analysis is applied to calculate the probability distribution and expectation matrix of different sentiment orientation; further, stochastic dominance rules and PROMETHEE-Ⅱ are used to calculate the ranking value of each product with different product attributes; finally, the best product is selected through the overall ranking value calculated by the entropy method. The feasibility and practicability of the method are illustrated by an example.},
booktitle = {2022 5th International Conference on Computers in Management and Business (ICCMB)},
pages = {167–172},
numpages = {6},
keywords = {product selection, Key words: Online reviews, stochastic dominance rules, LDA topic model},
location = {Singapore, Singapore},
series = {ICCMB 2022}
}

@inproceedings{10.1145/3503491.3503498,
author = {Yang-cai, Xiao and Rui, Wang},
title = {A Study of MOOC Course Review Topics Mining Based on LDA Topic Model},
year = {2021},
isbn = {9781450390415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503491.3503498},
doi = {10.1145/3503491.3503498},
abstract = {In order to dig deeper into the implied thematic information about online course review data on MOOC learning platforms and obtain the topic concerns of course learners in the process of participating in online courses, as a demand guide to improve the quality level of online classes. This study analyzes the course review data in the form of word cloud map for word frequency, and at the same time, uses LDA topic model for semantic analysis of online course review data to extract learners' topic concerns. The results show that learners focus on course content, lecture style, course discussion, learning resources, architecture, teacher quality, exercise explanation and sound effect in the learning process of MOOC online education platform . By mining online course review data for underlying themes, it is possible to understand learners' demand tendencies, which is meaningful and valuable for improving teaching quality.},
booktitle = {3rd Africa-Asia Dialogue Network (AADN) International Conference on Advances in Business Management and Electronic Commerce Research},
pages = {44–48},
numpages = {5},
keywords = {MOOC online courses, online comments, LDA topic model},
location = {Ganzhou, China},
series = {AADNIC-ABMECR 2021}
}

@article{10.1145/3431728,
author = {Seifollahi, Sattar and Piccardi, Massimo and Jolfaei, Alireza},
title = {An Embedding-Based Topic Model for Document Classification},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3431728},
doi = {10.1145/3431728},
abstract = {Topic modeling is an unsupervised learning task that discovers the hidden topics in a collection of documents. In turn, the discovered topics can be used for summarizing, organizing, and understanding the documents in the collection. Most of the existing techniques for topic modeling are derivatives of the Latent Dirichlet Allocation which uses a bag-of-word assumption for the documents. However, bag-of-words models completely dismiss the relationships between the words. For this reason, this article presents a two-stage algorithm for topic modelling that leverages word embeddings and word co-occurrence. In the first stage, we determine the topic-word distributions by soft-clustering a random set of embedded n-grams from the documents. In the second stage, we determine the document-topic distributions by sampling the topics of each document from the topic-word distributions. This approach leverages the distributional properties of word embeddings instead of using the bag-of-words assumption. Experimental results on various data sets from an Australian compensation organization show the remarkable comparative effectiveness of the proposed algorithm in a task of document classification.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {52},
numpages = {13},
keywords = {document classification, word embedding, clustering, Topic modelling}
}

@inproceedings{10.1145/3335484.3335536,
author = {Sun, Ping and Li, JinShan and Li, Guohui},
title = {Research on Collaborative Filtering Recommendation Algorithm Based on Sentiment Analysis and Topic Model},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335536},
doi = {10.1145/3335484.3335536},
abstract = {At present, the research about the personalized recommendation algorithm is very popular, there are two problems in this kind of algorithm. First, in the pre-filling stage, user preferences cannot be expressed only by scores, because only using the score-filling matrix has a certain deviation. The second is that in the forecasting stage, the prediction method only relies on the locally relevant scores, which cannot grasp the overall user score features. In order to resolve these two problems to improve the efficiency and accuracy of recommendation algorithm, this paper proposes a collaborative filtering recommendation algorithm based on sentiment analysis and topic model. Experiments show that the proposed algorithm outperforms User-based CF, LDA-CF and BiasSVD algorithms in prediction accuracy.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {169–178},
numpages = {10},
keywords = {Pre-filling algorithm, sentiment analysis, Personalized Recommendation, Collaborative Filtering, Topic Model},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/3372454.3372465,
author = {Fujino, Iwao and Claramunt, Christophe and Boudraa, Abdel-Ouahab},
title = {Extracting 4-Attributes Vessel Courses from AIS Data with PQK-Means and Topic Model},
year = {2019},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372465},
doi = {10.1145/3372454.3372465},
abstract = {AIS (Automatic Identification System) data received from moving vessels over an area of interest can be of very much interest for deriving maritime trajectory patterns. In this paper, a novel approach to extract course patterns from AIS data of vessels is presented. From machine learning and natural language processing principles, a topic model might be used for extracting implicit patterns underlying massive and unstructured collection of incoming data. To apply topic model to AIS data, PQk-means vector quantization to convert AIS data record to code documents is introduced. Then, a topic model is applied to extract course patterns from AIS data. In fact, courses, not only encompasses trajectory locations, but also headings and speeds, are recognized by the proposed algorithm. The performance of PQk-means is evaluated using the relative root mean square error and elapsed time. The potential of the approach is illustrated by a series of experimental results derived from practical AIS data set in a region of North West France.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {129–135},
numpages = {7},
keywords = {Maritime Big Data, Topic Model, PQk-means, Vector Quantization, Automatic Identification System (AIS), Course Pattern Extraction},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/3132847.3132946,
author = {Halder, Kishaloy and Kan, Min-Yen and Sugiyama, Kazunari},
title = {Health Forum Thread Recommendation Using an Interest Aware Topic Model},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132946},
doi = {10.1145/3132847.3132946},
abstract = {We introduce a general, interest-aware topic model (IATM), in which known higher-level interests on topics expressed by each user can be modeled. We then specialize the IATM for use in consumer health forum thread recommendation by equating each user's self-reported medical conditions as interests and topics as symptoms of treatments for recommendation. The IATM additionally models the implicit interests embodied by users' textual descriptions in their profiles. To further enhance the personalized nature of the recommendations, we introduce jointly normalized collaborative topic regression (JNCTR) which captures how users interact with the various symptoms belonging to the same clinical condition. In our experiments on two real-world consumer health forums, our proposed model significantly outperforms competitive state-of-the-art baselines by over 10% in recall. Importantly, we show that our IATM+JNCTR pipeline also imbues the recommendation process with added transparency, allowing a recommendation system to justify its recommendation with respect to each user's interest in certain health conditions.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1589–1598},
numpages = {10},
keywords = {collaborative filtering, topic models, recommender systems, graphical model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1109/TASLP.2018.2879855,
author = {Imoto, Keisuke and Ono, Nobutaka},
title = {Acoustic Topic Model for Scene Analysis With Intermittently Missing Observations},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2879855},
doi = {10.1109/TASLP.2018.2879855},
abstract = {We propose a sophisticated method of acoustic scene analysis with intermittently missing observations, which analyzes acoustic scenes and restores missing observations simultaneously on the basis of the temporal correlation between acoustic words. One effective strategy for analyzing acoustic scenes is to characterize them as a combination of acoustic words. An acoustic topic model ATM is one of the techniques, which models the process generating multiple acoustic words. Here, an acoustic word corresponds to a sound category, while it has a homogenous time duration and is defined time frame by time frame. In the ATM, it is assumed that all acoustic words are observed, and therefore, it cannot be applied if any acoustic observations are missing. However, acoustic observations may sometimes be missing because of poor recording conditions, transmission loss, or privacy reasons. In the proposed method, focusing on the fact that acoustic words are temporally correlated, we consider the transition of acoustic words in two ways: First, by modeling the temporal transition of acoustic words directly using a Markov process and finally, by modeling the temporal transition of hidden states that generate acoustic words using a hidden Markov model. We then incorporate each transition model in a process generating acoustic words based on the ATM. The proposed method allows us to analyze acoustic scenes from acoustic words by restoring missing acoustic words. In our experiments, the proposed method exhibited a classification accuracy of acoustic scenes close to that for the case of no missing observations even when 50% of the observations were missing. Moreover, the model considering the hidden-state transition can classify acoustic scenes more accurately than the model considering the acoustic word transition directly.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {367–382},
numpages = {16}
}

@inproceedings{10.1145/3459930.3469543,
author = {Song, Ziyang and Toral, Xavier Sumba and Xu, Yixin and Liu, Aihua and Guo, Liming and Powell, Guido and Verma, Aman and Buckeridge, David and Marelli, Ariane and Li, Yue},
title = {Supervised Multi-Specialist Topic Model with Applications on Large-Scale Electronic Health Record Data},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469543},
doi = {10.1145/3459930.3469543},
abstract = {Motivation: Electronic health record (EHR) data provides a new venue to elucidate disease comorbidities and latent phenotypes for precision medicine. To fully exploit its potential, a realistic data generative process of the EHR data needs to be modelled.Materials and Methods: We present MixEHR-S to jointly infer specialist-disease topics from the EHR data. As the key contribution, we model the specialist assignments and ICD-coded diagnoses as the latent topics based on patient's underlying disease topic mixture in a novel unified supervised hierarchical Bayesian topic model. For efficient inference, we developed a closed-form collapsed variational inference algorithm to learn the model distributions of MixEHR-S.Results: We applied MixEHR-S to two independent large-scale EHR databases in Quebec with three targeted applications: (1) Congenital Heart Disease (CHD) diagnostic prediction among 154,775 patients; (2) Chronic obstructive pulmonary disease (COPD) diagnostic prediction among 73,791 patients; (3) future insulin treatment prediction among 78,712 patients diagnosed with diabetes as a mean to assess the disease exacerbation. In all three applications, MixEHR-S conferred clinically meaningful latent topics among the most predictive latent topics and achieved superior target prediction accuracy compared to the existing methods, providing opportunities for prioritizing high-risk patients for healthcare services.Availability and implementation: MixEHR-S source code and scripts of the experiments are freely available at https://github.com/li-lab-mcgill/mixehrS},
booktitle = {Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {6},
numpages = {26},
keywords = {variational bayesian, drug recommendation, disease prediction, topic model, text mining},
location = {Gainesville, Florida},
series = {BCB '21}
}

@article{10.1145/3502727,
author = {Ihou, Koffi Eddy and Amayri, Manar and Bouguila, Nizar},
title = {Stochastic Variational Optimization of a Hierarchical Dirichlet Process Latent Beta-Liouville Topic Model},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3502727},
doi = {10.1145/3502727},
abstract = {In topic models, collections are organized as documents where they arise as mixtures over latent clusters called topics. A topic is a distribution over the vocabulary. In large-scale applications, parametric or finite topic mixture models such as LDA (latent Dirichlet allocation) and its variants are very restrictive in performance due to their reduced hypothesis space. In this article, we address the problem related to model selection and sharing ability of topics across multiple documents in standard parametric topic models. We propose as an alternative a BNP (Bayesian nonparametric) topic model where the HDP (hierarchical Dirichlet process) prior models documents topic mixtures through their multinomials on infinite simplex. We, therefore, propose asymmetric BL (Beta-Liouville) as a diffuse base measure at the corpus level DP (Dirichlet process) over a measurable space. This step illustrates the highly heterogeneous structure in the set of all topics that describes the corpus probability measure. For consistency in posterior inference and predictive distributions, we efficiently characterize random probability measures whose limits are the global and local DPs to approximate the HDP from the stick-breaking formulation with the GEM (Griffiths-Engen-McCloskey) random variables. Due to the diffuse measure with the BL prior as conjugate to the count data distribution, we obtain an improved version of the standard HDP that is usually based on symmetric Dirichlet (Dir). In addition, to improve coordinate ascent framework while taking advantage of its deterministic nature, our model implements an online optimization method based on stochastic, at document level, variational inference to accommodate fast topic learning when processing large collections of text documents with natural gradient. The high value in the predictive likelihood per document obtained when compared to the performance of its competitors is also consistent with the robustness of our fully asymmetric BL-based HDP. While insuring the predictive accuracy of the model using the probability of the held-out documents, we also added a combination of metrics such as the topic coherence and topic diversity to improve the quality and interpretability of the topics discovered. We also compared the performance of our model using these metrics against the standard symmetric LDA. We show that online HDP-LBLA (Latent BL Allocation)’s performance is the asymptote for parametric topic models. The accuracy in the results (improved predictive distributions of the held out) is a product of the model’s ability to efficiently characterize dependency between documents (topic correlation) as now they can easily share topics, resulting in a much robust and realistic compression algorithm for information modeling.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {84},
numpages = {48},
keywords = {predictive distributions, Beta-Liouville distribution, Hierarchical dirichlet process, Bayesian nonparametric topic model, stochastic and variational optimizations}
}

@article{10.1145/3480246,
author = {Jiang, Yuanchun and Liang, Ruicheng and Zhang, Ji and Sun, Jianshan and Liu, Yezheng and Qian, Yang},
title = {Network Public Opinion Detection During the Coronavirus Pandemic: A Short-Text Relational Topic Model},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3480246},
doi = {10.1145/3480246},
abstract = {Online social media provides rich and varied information reflecting the significant concerns of the public during the coronavirus pandemic. Analyzing what the public is concerned with from social media information can support policy-makers to maintain the stability of the social economy and life of the society. In this article, we focus on the detection of the network public opinions during the coronavirus pandemic. We propose a novel Relational Topic Model for Short texts (RTMS) to draw opinion topics from social media data. RTMS exploits the feature of texts in online social media and the opinion propagation patterns among individuals. Moreover, a dynamic version of RTMS (DRTMS) is proposed to capture the evolution of public opinions. Our experiment is conducted on a real-world dataset which includes 67,592 comments from 14,992 users. The results demonstrate that, compared with the benchmark methods, the proposed RTMS and DRTMS models can detect meaningful public opinions by leveraging the feature of social media data. It can also effectively capture the evolution of public concerns during different phases of the coronavirus pandemic.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {oct},
articleno = {52},
numpages = {27},
keywords = {coronavirus pandemic, relational topic modeling for short text, Public Opinion detection, dynamic topic model}
}

@article{10.5555/3546258.3546347,
author = {Wang, Feifei and Zhang, Junni L. and Li, Yichao and Deng, Ke and Liu, Jun S.},
title = {Bayesian Text Classification and Summarization via a Class-Specified Topic Model},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We propose the class-specified topic model (CSTM) to deal with the tasks of text classification and class-specific text summarization. The model assumes that in addition to a set of latent topics that are shared across classes, there is a set of class-specific latent topics for each class. Each document is a probabilistic mixture of the class-specific topics associated with its class and the shared topics. Each class-specific or shared topic has its own probability distribution over a given dictionary. We develop a Bayesian inference of CSTM in the semisupervised scenario, with the supervised scenario as a special case. We analyze in detail the 20 Newsgroups dataset, a benchmark dataset for text classification, and demonstrate that CSTM has better performance than a two-stage approach based on latent Dirichlet allocation (LDA), several existing supervised extensions of LDA, and an L1 penalized logistic regression. The favorable performance of CSTM is also demonstrated through Monte Carlo simulations and an analysis of the Reuters dataset.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {89},
numpages = {48},
keywords = {semisupervised classification, latent topic, L1 penalization, text mining}
}

@inproceedings{10.1145/3486001.3486002,
author = {Kumar, Sanuj and Le, Tuan},
title = {A Word Embedding Topic Model for Robust Inference of Topics and Visualization},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486002},
doi = {10.1145/3486001.3486002},
abstract = {Probabilistic topic models for semantic visualization are useful for discovering and visualizing latent topics in document collections. In these models, the inference of topics and visualization is largely based on word co-occurrences within documents. Therefore, when documents in a corpus are short in length, these models may not achieve good results due to the sparsity of word co-occurrences. In this paper, we propose a word embedding topic model (WTM) that is robust to data sparsity when detecting topics and generating visualization of short texts. Extensive experiments conducted on four real-world datasets show that WTM is more effective in dealing with short texts than state-of-the-art models.},
booktitle = {The First International Conference on AI-ML-Systems},
articleno = {1},
numpages = {7},
keywords = {short texts, topic modeling, visualization},
location = {Bangalore, India},
series = {AIMLSystems 2021}
}

@inproceedings{10.5555/3398761.3399070,
author = {Xu, Yuyu and Jeong, David and Sequeira, Pedro and Gratch, Jonathan and Aslam, Javed and Marsella, Stacy},
title = {A Supervised Topic Model Approach to Learning Effective Styles within Human-Agent Negotiation},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present a method that analyzes a person's negotiation behavior to automatically detect co-occurrence of tactics and combination of tactics (i.e., negotiation styles). We first identify action features consistent with use of the common negotiation tactics based on prior research in negotiation. Next, we apply regularized linear regression over a negotiation dataset to assess how effective particular tactics are in predicting the negotiation outcome. Finally, we use a supervised variant of a topic model to derive effective negotiation styles. Results from the clusters produced by the topic models provide insights regarding the effectiveness of negotiation styles that people utilize.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2047–2049},
numpages = {3},
keywords = {explainability in human-agent systems, socially interactive agents, agents competing and collaborating with humans},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3178876.3186069,
author = {Rakesh, Vineeth and Ding, Weicong and Ahuja, Aman and Rao, Nikhil and Sun, Yifan and Reddy, Chandan K.},
title = {A Sparse Topic Model for Extracting Aspect-Specific Summaries from Online Reviews},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186069},
doi = {10.1145/3178876.3186069},
abstract = {Online reviews have become an inevitable part of a consumer's decision making process, where the likelihood of purchase not only depends on the product's overall rating, but also on the description of its aspects. Therefore, e-commerce websites such as Amazon and Walmart constantly encourage users to write good quality re- views and categorically summarize different facets of the products. However, despite such attempts, it takes a significant effort to skim through thousands of reviews and look for answers that address the query of consumers. For example, a gamer might be interested in buying a monitor with fast refresh rates and support for Gsync and Freesync technologies, while a photographer might be interested in aspects such as color depth and accuracy. To address these chal- lenges, in this paper, we propose a generative aspect summarization model called APSUM that is capable of providing fine-grained sum- maries of online reviews. To overcome the inherent problem of aspect sparsity, we impose dual constraints: (a) a spike-and-slab prior over the document-topic distribution and (b) a linguistic su- pervision over the word-topic distribution. Using a rigorous set of experiments, we show that the proposed model is capable of out- performing the state-of-the-art aspect summarization model over a variety of datasets and deliver intuitive fine-grained summaries that could simplify the purchase decisions of consumers.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1573–1582},
numpages = {10},
keywords = {aspect summarization, information re- trieval, probabilistic generative models, topic models},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3494885.3494892,
author = {Geng, Haiyan and Dong, Baoliang},
title = {The Topic Model of Voice Interactive Dialogue Based on the Localized Command and Control System},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494892},
doi = {10.1145/3494885.3494892},
abstract = {The core problem of the human-machine dialogue system in the localized command and control system is to understand the information of the dialogue, and then give a reasonable answer that is satisfactory to the command system commander or assist the commander in completing a certain task. Facing huge interactive dialogue and other short text dialogue data, it is necessary to understand it efficiently and accurately. Daily interactive dialogues are carried out following certain themes, and mining the hidden subject information is a good way to grasp the inherent characteristics of the dialogue. The length of the dialogue text in the daily dialogue of the commander is small, and it has a high degree of randomness. The oral language is serious. Its themes are intertwined and the organizational structure is more chaotic than the news and other types of texts, which makes it difficult for the traditional topic model to capture the hidden words. The co-occurrence law of the contained words and documents cannot be directly applied to this type of text. Therefore, this article constructs a "pseudo-long document" method for short text problems. The topic information in a group of dialogues is relatively similar, and the word co-occurrence in the dialogue is more representative. Using this method to construct the training corpus of the LDA model can improve the topic model's capture of the document-word co-occurrence law to a certain extent. At the same time, this paper proposes an index based on the combination of perplexity and topic similarity to determine the optimal number of topics. Based on these two aspects, an interactive dialogue topic model is constructed.},
booktitle = {2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)},
pages = {38–42},
numpages = {5},
location = {Singapore, Singapore},
series = {CSSE 2021}
}

@article{10.1145/3404995,
author = {Wang, Wei and Gong, Zhiguo and Ren, Jing and Xia, Feng and Lv, Zhihan and Wei, Wei},
title = {Venue Topic Model–Enhanced Joint Graph Modelling for Citation Recommendation in Scholarly Big Data},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3404995},
doi = {10.1145/3404995},
abstract = {Natural language processing technologies, such as topic models, have been proven to be effective for scholarly recommendation tasks with the ability to deal with content information. Recently, venue recommendation is becoming an increasingly important research task due to the unprecedented number of publication venues. However, traditional methods focus on either the author’s local network or author-venue similarity, where the multiple relationships between scholars and venues are overlooked, especially the venue–venue interaction. To solve this problem, we propose an author topic model–enhanced joint graph modeling approach that consists of venue topic modeling, venue-specific topic influence modeling, and scholar preference modeling. We first model the venue topic with Latent Dirichlet Allocation. Then, we model the venue-specific topic influence in an asymmetric and low-dimensional way by considering the topic similarity between venues, the top-influence of venues, and the top-susceptibility of venues. The top-influence characterizes venues’ capacity of exerting topic influence on other venues. The top-susceptibility captures venues’ propensity of being topically influenced by other venues. Extensive experiments on two real-world datasets show that our proposed joint graph modeling approach outperforms the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {4},
numpages = {15},
keywords = {academic information retrieval, natural language processing, Network embedding, scientific collaboration}
}

@article{10.1109/TASLP.2021.3126937,
author = {Supraja, S. and Khong, Andy W. H. and Tatinati, S.},
title = {Regularized Phrase-Based Topic Model for Automatic Question Classification With Domain-Agnostic Class Labels},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3126937},
doi = {10.1109/TASLP.2021.3126937},
abstract = {Classification of questions according to domain-agnostic class labels relies on a suitable feature extraction process. We propose the use of phrases that is more effective than using words to represent questions. The proposed phrase-based topic modeling technique employs asymmetric priors that are scaled with a new C-value for nested regular expressions. In addition, to suppress high-frequency words in phrases, we deploy term weightages computed using the modified distinguishing feature selector. The proposed approach also incorporates a new topic regularization mechanism to facilitate efficient mapping of questions to class labels. We validate the performance of our proposed model via four datasets across different domain-agnostic class labels comprising question types, reasoning capabilities, and cognitive complexities. Results obtained highlight that the proposed technique outperforms existing methods in terms of macro-average F1 score.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {3604–3616},
numpages = {13}
}

@inproceedings{10.1145/3388440.3412418,
author = {Li, Yue and Nair, Pratheeksha and Wen, Zhi and Chafi, Imane and Okhmatovskaia, Anya and Powell, Guido and Shen, Yannan and Buckeridge, David},
title = {Global Surveillance of COVID-19 by Mining News Media Using a Multi-Source Dynamic Embedded Topic Model},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412418},
doi = {10.1145/3388440.3412418},
abstract = {As the COVID-19 pandemic continues to unfold, understanding the global impact of non-pharmacological interventions (NPI) is important for formulating effective intervention strategies, particularly as many countries prepare for future waves. We used a machine learning approach to distill latent topics related to NPI from large-scale international news media. We hypothesize that these topics are informative about the timing and nature of implemented NPI, dependent on the source of the information (e.g., local news versus official government announcements) and the target countries. Given a set of latent topics associated with NPI (e.g., self-quarantine, social distancing, online education, etc), we assume that countries and media sources have different prior distributions over these topics, which are sampled to generate the news articles. To model the source-specific topic priors, we developed a semi-supervised, multi-source, dynamic, embedded topic model. Our model is able to simultaneously infer latent topics and learn a linear classifier to predict NPI labels using the topic mixtures as input for each news article. To learn these models, we developed an efficient end-to-end amortized variational inference algorithm. We applied our models to news data collected and labelled by the World Health Organization (WHO) and the Global Public Health Intelligence Network (GPHIN). Through comprehensive experiments, we observed superior topic quality and intervention prediction accuracy, compared to the baseline embedded topic models, which ignore information on media source and intervention labels. The inferred latent topics reveal distinct policies and media framing in different countries and media sources, and also characterize reaction to COVID-19 and NPI in a semantically meaningful manner. Our PyTorch code is available on Github (htps://github.com/li-lab-mcgill/covid19_media).},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {34},
numpages = {14},
keywords = {Topic models, text mining, media news, Bayesian inference, coronavirus},
location = {Virtual Event, USA},
series = {BCB '20}
}

@article{10.1145/3447760,
author = {Chauhan, Uttam and Shah, Apurva},
title = {Improving Semantic Coherence of Gujarati Text Topic Model Using Inflectional Forms Reduction and Single-Letter Words Removal},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3447760},
doi = {10.1145/3447760},
abstract = {A topic model is one of the best stochastic models for summarizing an extensive collection of text. It has accomplished an inordinate achievement in text analysis as well as text summarization. It can be employed to the set of documents that are represented as a bag-of-words, without considering grammar and order of the words. We modeled the topics for Gujarati news articles corpus. As the Gujarati language has a diverse morphological structure and inflectionally rich, Gujarati text processing finds more complexity. The size of the vocabulary plays an important role in the inference process and quality of topics. As the vocabulary size increases, the inference process becomes slower and topic semantic coherence decreases. If the vocabulary size is diminished, then the topic inference process can be accelerated. It may also improve the quality of topics. In this work, the list of suffixes has been prepared that encounters too frequently with words in Gujarati text. The inflectional forms have been reduced to the root words concerning the suffixes in the list. Moreover, Gujarati single-letter words have been eliminated for faster inference and better quality of topics. Experimentally, it has been proved that if inflectional forms are reduced to their root words, then vocabulary length is shrunk to a significant extent. It also caused the topic formation process quicker. Moreover, the inflectional forms reduction and single-letter word removal enhanced the interpretability of topics. The interpretability of topics has been assessed on semantic coherence, word length, and topic size. The experimental results showed improvements in the topical semantic coherence score. Also, the topic size grew notably as the number of tokens assigned to the topics increased.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {18},
numpages = {18},
keywords = {text summarization, morphological analysis, inflectional forms reduction, Latent Dirichlet allocation}
}

@inproceedings{10.1145/3386415.3386969,
author = {Han, Yue and Han, Weihong and Li, Shudong},
title = {Review: Topic Model Application for Social Network Public Opinion Analysis},
year = {2019},
isbn = {9781450372930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386415.3386969},
doi = {10.1145/3386415.3386969},
abstract = {In response to the explosive growth of the Internet's information volume, how to discover and track changes in social media has gradually become a key issue in the field of data mining. Especially, how to find the popular and hot topics has become an important issue in social network analysis. In this paper, we review the topic models applied into the social network analysis. At this stage, this kind of problem is mainly solved by the method based on evolutionary clustering. The method can be subdivided into two methods: the topic model and the matrix/tensor decomposition model. This article focuses on the analysis of a class of the most important Latent Dirichlet Allocation (LDA) model from the topic model, as well as a variety of topic models derived from the LDA model, and a brief discussion of future research directions.},
booktitle = {Proceedings of the 2nd International Conference on Information Technologies and Electrical Engineering},
articleno = {22},
numpages = {9},
keywords = {Social Network Analysis, LDA Model, Topic Model},
location = {Zhuzhou, Hunan, China},
series = {ICITEE-2019}
}

@inproceedings{10.1145/3342827.3342831,
author = {Hidayatullah, Ahmad Fathan and Kurniawan, Wisnu and Ratnasari, Chanifah Indah},
title = {Topic Modeling on Indonesian Online Shop Chat},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342831},
doi = {10.1145/3342827.3342831},
abstract = {This paper aims to discover topics from an Indonesian online shop chat. Moreover, we employed Latent Dirichlet Allocation to find out what kind of topics that are often discussed and conversation trends between buyers and customer service. Several tasks were performed, such as, collecting data, preprocessing, phrase aggregation, topic modeling, and topic analysis. We found several attracting findings during our experiments. In preprocessing task, product name extraction from URLs assisted to discover the intended product from the customer's conversation. On the other hand, the phrase aggregation task helped us to merge various terms which have same intended meaning, so that, we could obtain better topical model result and easier to determine the topic label.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {121–126},
numpages = {6},
keywords = {Topic model, Topic modeling, Bahasa Indonesia, Online Shop, Latent Dirichlet Allocation},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3383455.3422537,
author = {Chen, Jiahao and Veloso, Manuela},
title = {Paying down Metadata Debt: Learning the Representation of Concepts Using Topic Models},
year = {2020},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422537},
doi = {10.1145/3383455.3422537},
abstract = {We introduce a data management problem called metadata debt, to identify the mapping between data concepts and their logical representations. We describe how this mapping can be learned using semisupervised topic models based on low-rank matrix factorizations that account for missing and noisy labels, coupled with sparsity penalties to improve localization and interpretability. We introduce a gauge transformation approach that allows us to construct explicit associations between topics and concept labels, and thus assign meaning to topics. We also show how to use this topic model for semisupervised learning tasks like extrapolating from known labels, evaluating possible errors in existing labels, and predicting missing features. We show results from this topic model in predicting subject tags on over 25,000 datasets from Kaggle.com, demonstrating the ability to learn semantically meaningful features.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {43},
numpages = {8},
keywords = {semisupervised learning, topic model, controlled vocabulary},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3501247.3539507,
author = {Lykousas, Nikolaos and Patsakis, Constantinos},
title = {Topic Modeling Approaches to Counter Online Grooming},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539507},
doi = {10.1145/3501247.3539507},
abstract = {This paper describes a topic model-based approach for analyzing a large-scale dataset of chat messages exchanged between the users of LiveMe, a major social live streaming platform, in the context of broadcasts involving sexually explicit content. The analysis reveals the characteristics of predatory behavior targeting minors and its criminal dimension in an accessible and explainable manner.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {471–475},
numpages = {5},
keywords = {chats, social live streaming, topic model, grooming},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@inproceedings{10.1145/3018661.3022750,
author = {Xue, Zijun},
title = {Scalable Text Analysis},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3022750},
doi = {10.1145/3018661.3022750},
abstract = {Latent Dirichlet Allocation (LDA) is an extremely popular probabilistic topic model used for a diverse class of appications. While highly effective, one important limitation of LDA is the high memory footprint of its inferencing algorithm, making it difficult to scale to a large dataset. In my thesis, I propose sdLDA, a highly-scalable disk-based LDA that (1) leverages the plentiful space available in disk to reduce its main-memory footprint and (2) preserves the sparsity of the extracted topics during sampling to improve both memory efficiency and sampling complexity of the inferencing algorithm. Our extensive experiments show that sdLDA scales to datasets that are two orders of magnitude larger than what existing main-memory-based algorithms can handle. Furthermore, sdLDA exhibits similar (or even better) performance compared to main-memory-based LDAs in terms of running time.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {835},
numpages = {1},
keywords = {topic model, text analysis, scalability},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3430895.3460170,
author = {Liu, Zhi and Mu, Rui and Liu, Shiqi and Peng, Xian and Liu, Sannyuya},
title = {Modeling Temporal Association of Cognition-Topic in MOOC Discussion to Track Learners' Cognitive Engagement Dynamics},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460170},
doi = {10.1145/3430895.3460170},
abstract = {In the discussion forums of massive open online courses (MOOCs), cognitive processing (e.g., insight, certain) is considered an essential factor that can affect learners' learning outcomes, but the relationship between them has not been thoroughly investigated. Especially the dynamic nature of cognitive processing is still a significant research gap. In this study, we proposed an unsupervised topic model named Temporal Cognitive Topic Model (TCTM) to automatically classify cognitive processes and obtain the conditional probability with topics over time. The results indicated that completers had more active and timely cognitive engagement as time went on and tended to use certain cognitive words to discuss the topics related to the examination and certificates, which showed that they had explicit learning goals and plans. Non-completers often used exclusive cognitive words to discuss some off-task content that pointed out a distractive learning process. Using the model, teachers can capture learners' dynamic cognitive states and associated topics to improve teaching methods and increase course completion rates.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {319–322},
numpages = {4},
keywords = {cognitive processing, discussion forums, temporal cognitive topic model},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{10.1145/3132847.3133181,
author = {Le, Tuan M. V. and Lauw, Hady W.},
title = {SemVis: Semantic Visualization for Interactive Topical Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133181},
doi = {10.1145/3132847.3133181},
abstract = {Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially. We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2487–2490},
numpages = {4},
keywords = {semantic visualization, topic model, interactive topical analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3357384.3357909,
author = {Jiang, Di and Song, Yuanfeng and Tong, Yongxin and Wu, Xueyang and Zhao, Weiwei and Xu, Qian and Yang, Qiang},
title = {Federated Topic Modeling},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357909},
doi = {10.1145/3357384.3357909},
abstract = {Topic modeling has been widely applied in a variety of industrial applications. Training a high-quality model usually requires massive amount of in-domain data, in order to provide comprehensive co-occurrence information for the model to learn. However, industrial data such as medical or financial records are often proprietary or sensitive, which precludes uploading to data centers. Hence training topic models in industrial scenarios using conventional approaches faces a dilemma: a party (i.e., a company or institute) has to either tolerate data scarcity or sacrifice data privacy. In this paper, we propose a novel framework named Federated Topic Modeling (FTM), in which multiple parties collaboratively train a high-quality topic model by simultaneously alleviating data scarcity and maintaining immune to privacy adversaries. FTM is inspired by federated learning and consists of novel techniques such as private Metropolis Hastings, topic-wise normalization and heterogeneous model integration. We conduct a series of quantitative evaluations to verify the effectiveness of FTM and deploy FTM in an Automatic Speech Recognition (ASR) system to demonstrate its utility in real-life applications. Experimental results verify FTM's superiority over conventional topic modeling.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1071–1080},
numpages = {10},
keywords = {bayesian networks, topic model, text semantics},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3374587.3374590,
author = {Wang, Jiangyao and Xu, Wenhua and Yan, Wenhao and Li, Caixia},
title = {Text Similarity Calculation Method Based on Hybrid Model of LDA and TF-IDF},
year = {2019},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374590},
doi = {10.1145/3374587.3374590},
abstract = {The traditional TF-IDF-based text similarity calculation model uses statistical methods to map text to the keyword vector space and convert the similarity of text into the distance between text vectors. Such methods have problems such as high computational dimensions, sparse data, and inability to take advantage of the semantic information contained in the text itself, so the results obtained are not as similar as the physical text. The text similarity model based on the topic model changes the traditional spatial similarity of keyword vectors, and can fully utilize the semantic information contained in the text itself. But this approach ignores the effect of words on text semantic representations with different weights. In the process of converting text into topic feature space, valuable information is lost. In view of the above problems, this paper proposes a text similarity hybrid model (L-THM) integrating LDA and TF-IDF for calculating text similarity. The model uses the semantic information contained in the text itself and the keyword information reflecting the text to comprehensively analyses and calculates the similarity between the texts. The experimental results show that the hybrid model can better represent the text information than the single model, and obtain a good F value in the cluster, which effectively improves the text similarity calculation effect.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {1–8},
numpages = {8},
keywords = {TF-IDF, hybrid model, LDA, text similarity, topic model},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/3485447.3512007,
author = {Churchill, Robert and Singh, Lisa and Ryan, Rebecca and Davis-Kean, Pamela},
title = {A Guided Topic-Noise Model for Short Texts},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512007},
doi = {10.1145/3485447.3512007},
abstract = {Researchers using social media data want to understand the discussions occurring in and about their respective fields. These domain experts often turn to topic models to help them see the entire landscape of the conversation, but unsupervised topic models often produce topic sets that miss topics experts expect or want to see. To solve this problem, we propose Guided Topic-Noise Model (GTM), a semi-supervised topic model designed with large domain-specific social media data sets in mind. The input to GTM is a set of topics that are of interest to the user and a small number of words or phrases that belong to those topics. These seed topics are used to guide the topic generation process, and can be augmented interactively, expanding the seed word list as the model provides new relevant words for different topics. GTM uses a novel initialization and a new sampling algorithm called Generalized Polya Urn (GPU) seed word sampling to produce a topic set that includes expanded seed topics, as well as new unsupervised topics. We demonstrate the robustness of GTM on open-ended responses from a public opinion survey and four domain-specific Twitter data sets.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2870–2878},
numpages = {9},
keywords = {guided topic model, seed topics, social media, topic modeling, semi-supervised topic model, topic-noise model},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3335484.3335515,
author = {Xu, Aiting and Wang, Fangyan and Ying, Pingting},
title = {Xiaomi Brand Appraisal Research Based on Zhihu by Text Mining Technology},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335515},
doi = {10.1145/3335484.3335515},
abstract = {As the largest knowledge social platform on the Chinese Internet, Zhihu has gradually become an important resource for merchants to improve publicity and optimize products, and the public to understand the brand image. The topic of "Xiaomi Technology" remains hot on Zhihu. In this context, this paper takes the essences of the "Xiaomi Technology" topic on Zhihu as the research object. First we carry on the data collection and preprocessing. Then by extracting feature based on word segmentation results, we build a corpus and construct an LDA topic model for text mining. Besides, by calculating and comparing the perplexity index, we select 20 as the number of topics. According to the results, the relationship between document-topic and topic-term is analyzed to form a topic description of the text, which shows that Xiaomi products have received great attention from consumers and are often used for comparison with other brands in the same industry; Xiaomi product launches have received much attention and had a direct impact on product sales; Xiaomi is widely recognized as one of the representatives of China's future technology.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {221–225},
numpages = {5},
keywords = {LDA topic model, text mining, Gibbs sampling, Xiaomi},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/3289600.3291022,
author = {Lin, Lu and Gong, Lin and Wang, Hongning},
title = {Learning Personalized Topical Compositions with Item Response Theory},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291022},
doi = {10.1145/3289600.3291022},
abstract = {A user-generated review document is a product between the item's intrinsic properties and the user's perceived composition of those properties. Without properly modeling and decoupling these two factors, one can hardly obtain any accurate user understanding nor item profiling from such user-generated data. In this paper, we study a new text mining problem that aims at differentiating a user's subjective composition of topical content in his/her review document from the entity's intrinsic properties. Motivated by the Item Response Theory (IRT), we model each review document as a user's detailed response to an item, and assume the response is jointly determined by the individuality of the user and the property of the item. We model the text-based response with a generative topic model, in which we characterize the items' properties and users' manifestations of them in a low-dimensional topic space. Via posterior inference, we separate and study these two components over a collection of review documents. Extensive experiments on two large collections of Amazon and Yelp review data verified the effectiveness of the proposed solution: it outperforms the state-of-art topic models with better predictive power in unseen documents, which is directly translated into improved performance in item recommendation and item summarization tasks.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {609–617},
numpages = {9},
keywords = {aspect modeling, review mining, correlated topic model},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3421558.3421577,
author = {SUN, LINJIA},
title = {Using Prosodic and Acoustic Features for Chinese Dialects Identification},
year = {2020},
isbn = {9781450388412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421558.3421577},
doi = {10.1145/3421558.3421577},
abstract = {The task of spoken language identification is quite challenging when it comes to discriminating between closely related dialects of the same language. In this paper, a novel approach is proposed to learn and identify the Chinese dialects. We distinguish the difference between Chinese dialects by using the distribution of acoustic and prosodic features, and design to organize these discriminative features into a new topic model. An iterative process is further proposed to implement the model learning. Compared with other state-of-the-art methods, the experimental results show that the proposed model provides competitive performance in the task of Chinese dialects identification.},
booktitle = {2020 2nd International Conference on Image Processing and Machine Vision},
pages = {118–123},
numpages = {6},
keywords = {Topic model, Acoustic feature, Chinese dialects, Dialects identification, Prosodic feature},
location = {Bangkok, Thailand},
series = {IPMV 2020}
}

@inproceedings{10.1145/3205977.3205991,
author = {Zhang, Wenxi and Li, Hao and Zhang, Min and Lv, Zhiquan},
title = {Privacy-Aware Risk-Adaptive Access Control in Health Information Systems Using Topic Models},
year = {2018},
isbn = {9781450356664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205977.3205991},
doi = {10.1145/3205977.3205991},
abstract = {Traditional role-based access control fails to meet the privacy requirements for patient data in medical systems, as it is infeasible for policy makers to foresee what information doctors may need for diagnosis and treatment in various situations. The universal practice in hospitals is to grant doctors unlimited access, which in turn increases the risk of breaching patient privacy. In this paper, we propose a dynamic risk-adaptive access control model for health IT systems by taking into consideration the relationships between data and access behaviors. By training topic models to portray individual and group-level access behaviors, we quantify the risk for each user over a certain period of time. Malicious users are supposed to get higher risk scores than honest users due to improper requests. Thus their further access would be denied under our access control scheme. The topic model and risk scores are periodically updated to advance the self-adaptability of the system. Experimental results have shown that our solution could effectively distinguish malicious doctors even if they deliberately conceal the misconducts.},
booktitle = {Proceedings of the 23nd ACM on Symposium on Access Control Models and Technologies},
pages = {61–67},
numpages = {7},
keywords = {healthcare, privacy, topic model, risk quantification, access control},
location = {Indianapolis, Indiana, USA},
series = {SACMAT '18}
}

@inproceedings{10.1145/3366423.3380286,
author = {Wang, Chang and Wang, Bang},
title = {An End-to-End Topic-Enhanced Self-Attention Network for Social Emotion Classification},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380286},
doi = {10.1145/3366423.3380286},
abstract = {Social emotion classification is to predict the distribution of different emotions evoked by an article among its readers. Prior studies have shown that document semantic and topical features can help improve classification performance. However, how to effectively extract and jointly exploit such features have not been well researched. In this paper, we propose an end-to-end topic-enhanced self-attention network (TESAN) that jointly encodes document semantics and extracts document topics. In particular, TESAN first constructs a neural topic model to learn topical information and generates a topic embedding for a document. We then propose a topic-enhanced self-attention mechanism to encode semantic and topical information into a document vector. Finally, a fusion gate is used to compose the document representation for emotion classification by integrating the document vector and the topic embedding. The entire TESAN is trained in an end-to-end manner. Experimental results on three public datasets reveal that TESAN outperforms the state-of-the-art schemes in terms of higher classification accuracy and higher average Pearson correlation coefficient. Furthermore, the TESAN is computation efficient and can generate more coherent topics.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2210–2219},
numpages = {10},
keywords = {social emotion classification, self-attention, neural topic model},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3538969.3538993,
author = {Umezawa, Katsuyuki and Koyanagi, Hiroki and Wohlgemuth, Sven and Mishina, Yusuke and Takaragi, Kazuo},
title = {Safety and Security Analysis Using LDA Based on Case Reports: Case Study and Trust Evaluation Method},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3538993},
doi = {10.1145/3538969.3538993},
abstract = {There are many cases where the safety and security of systems are threatened by accidental or intentional human error. This study focuses on the fact that there is information available about human error in design and operation documents and case reports, and they are in natural language. Therefore, we propose a method to analyze the impact of human error on safety and security using Latent Dirichlet Allocation (LDA), which is one of the topic model methods. First, we matched the given information to create a list of similarities (co-occurrence list) between documents. Based on this co-occurrence list, a fault and attack tree was constructed. While manually considering them, the critical points were identified through sensitivity analysis. We show the effectiveness of this proposed method through two characteristic case studies of cyber-based connected car design deficiencies and physical-based manufacturing inspection fraud. Both analyzes add a way to leverage big data interoperability in manufacturing processes using the IoT.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {154},
numpages = {7},
keywords = {vulnerability information, topic model, security, natural language processing},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3424311.3424314,
author = {Jing, Chen and Liuqian, Duan and Hong, Zhang},
title = {Research on Recommendation Strategies Integrating Emotional Tendency and User Influences},
year = {2020},
isbn = {9781450377348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424311.3424314},
doi = {10.1145/3424311.3424314},
abstract = {At present, the topic model based on LDA as an information recommendation method has the defect of neglecting the emotional information in social platform. Therefore, a microblog user recommendation strategy I-TES (influence topic emotionality similarity model) is proposed which combines emotional orientation with user influence model in this paper. Firstly, the user's theme distribution and emotional tendency is obtained based on JST model, the JS distance is used to calculate the similarity of user's theme probability distribution, and the two is combined to get the user's interest similarity score. Secondly, the user's influence measurement indicators are summarized into microblog influence, user's activity and fan's influence, and the weights of the three indicators are calculated according to AHP(analytic hierarchy process). Finally, the combination of the two is used to obtain the final score as the recommendation result. The strategy is tested on Sina Weibo user data set, and the experiment shows that this strategy has higher recommendation accuracy than the traditional recommendation method.},
booktitle = {Proceedings of the 2020 International Conference on Internet Computing for Science and Engineering},
pages = {44–51},
numpages = {8},
keywords = {Recommendation strategy, Topic model, User influence, User similarity, JST},
location = {Male, Maldives},
series = {ICICSE '20}
}

@inproceedings{10.1145/3459955.3460600,
author = {Chiang, Tzu-Hang and Lin, Yung-Yu and Nagai, Yukari and Chiang, Hua-Ko},
title = {A Study on Email Topic Identification Using Latent Dirichlet Allocation Integrated with Visual Attention},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460600},
doi = {10.1145/3459955.3460600},
abstract = {Email is the most standard documentation tool for communication. Although existing studies use the topic model to support users for classifying emails, they disregard that human is not like a machine can focus on all the words in an email to determine the distribution of email topics. The Latent Dirichlet Allocation (LDA) model forms a basis for inferring topics; our work aims to discover how each word's visual attention influences the topic inference and estimates attention to a word according to its location features. By reviewing the visual-spatial research and the state-of-the-art visual attention models, we select the Bayesian Models to estimate attention and proposing a novel model-Attention orientation Latent Dirichlet Allocation model (AttLDA). We proposed the AttLDA to effectively extract the email topics to improve email message management performance, and it can be considered a feature for settling further tasks.},
booktitle = {2021 The 4th International Conference on Information Science and Systems},
pages = {54–59},
numpages = {6},
keywords = {Topic Inference, Latent Dirichlet Allocation, Visual Attention, Topic model},
location = {Edinburgh, United Kingdom},
series = {ICISS 2021}
}

@inproceedings{10.1145/3490354.3494368,
author = {Li, Yinfei and Shou, Jiafeng and Treleaven, Philip and Wang, Jun},
title = {Graph Neural Network for Merger and Acquisition Prediction},
year = {2021},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494368},
doi = {10.1145/3490354.3494368},
abstract = {This paper investigates the application of graph neural networks (GNN) in Mergers and Acquisitions (M&amp;A) prediction, which aims to quantify the relationship between companies, their founders, and investors. M&amp;A is a critical management strategy to decide if the company is to grow or downsize, and M&amp;A prediction has been a challenging research topic in the past few decades. However, the traditional methods of predicting M&amp;A probability are only based on the company's fundamentals, such as revenue, profit, or news. Instead, GNN takes full advantage of those relationship data to expand feature dimension and improve the prediction result. Our M&amp;A prediction solution integrates with the topic model for text analysis, advanced feature engineering, and several tricks to boost GNN. The approach achieves a high Area-Under-Curve score (AUC) 0.952, which is better than the previous record 0.888. The true positive rate is 83% with a low false positive rate 7.8%, which performance is better than the previous benchmark record 70.9%/10.6%.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {3},
numpages = {8},
keywords = {merger and acquisition, topic model, graph neural network, graph theory},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3331453.3362040,
author = {Han, Hongqi and Zhai, Xiaorui and Han, Jingpeng and Ran, Yaxin},
title = {Discovering Research Teams from Scientific Papers and Patents},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3362040},
doi = {10.1145/3331453.3362040},
abstract = {Most existing team discovery methods are based on collaboration networks using papers or patents data. They usually have low efficiency because they have to create the whole network containing all researchers. In addition, these methods can't immediately output research topics for each discovered team. A novel team discovery method is presented to solve these problems. The method extracts institutional names from papers and patents to build the institution base, and extracts authors and inventors to build the researcher base after name disambiguation. Then, the method exploits Author Topic model to mine distributions of topics and researchers in papers and patents and builds research topic base. The component analysis technique is used to discover teams under each research topic by analyzing its collaboration network. Experiments show the proposed method can identify teams without establishing a whole network by integrating papers and patent data. Meanwhile, the method can provide research topics for found teams.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {73},
numpages = {5},
keywords = {Team discovery, Scientific collaboration network, Topic model, Cross organization team},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3442381.3449943,
author = {Li, Xiangsheng and Mao, Jiaxin and Ma, Weizhi and Liu, Yiqun and Zhang, Min and Ma, Shaoping and Wang, Zhaowei and He, Xiuqiang},
title = {Topic-Enhanced Knowledge-Aware Retrieval Model for Diverse Relevance Estimation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449943},
doi = {10.1145/3442381.3449943},
abstract = {Relevance measures the relation between query and document which contains several different dimensions, e.g., semantic similarity, topical relatedness, cognitive relevance (the relations in the aspect of knowledge), usefulness, timeliness, utility and so on. However, existing retrieval models mainly focus on semantic similarity and cognitive relevance while ignore other possible dimensions to model relevance. Topical relatedness, as an important dimension to measure relevance, is not well studied in existing neural information retrieval. In this paper, we propose a Topic Enhanced Knowledge-aware retrieval Model (TEKM) that jointly learns semantic similarity, knowledge relevance and topical relatedness to estimate relevance between query and document. We first construct a neural topic model to learn topical information and generate topic embeddings of a query. Then we combine the topic embeddings with a knowledge-aware retrieval model to estimate different dimensions of relevance. Specifically, we exploit kernel pooling to soft match topic embeddings with word and entity in a unified embedding space to generate fine-grained topical relatedness. The whole model is trained in an end-to-end manner. Experiments on a large-scale publicly available benchmark dataset show that TEKM outperforms existing retrieval models. Further analysis also shows how topic relatedness is modeled to improve traditional retrieval model with semantic similarity and knowledge relevance.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {756–767},
numpages = {12},
keywords = {Knowledge graph, Neural IR, Neural topic model, Kernel pooling},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3501712.3529735,
author = {McDermott, Tiarnach and Robson, James and Winters, Niall and Malmberg, Lars-Erik},
title = {Mapping the Changing Landscape of Child-Computer Interaction Research Through Correlated Topic Modelling},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529735},
doi = {10.1145/3501712.3529735},
abstract = {As the field of child-computer interaction (CCI) develops and forms an increasingly distinct identity, there is a need for reflection upon the state of the field, and its development thus far. This paper provides an overview of the thematic structure of the CCI field in order to support such reflection, expanding upon previous reviews through implementation of a correlated topic model, an automated, inductive content analysis method, in analysing 4,771 CCI research papers published between 2003 and 2021. Prominence of research topics, and their evolution, are explored. Results portray CCI as a vibrant and varied research landscape which has evolved dynamically over time, exhibiting increasing specialisation and emergence of distinct subfields, and progressing from a technology- to needs-driven agenda. This analysis contributes an extensive empirical mapping of the CCI research landscape, facilitating reflection upon the field and its development, and revealing gaps in extant literature and opportunities for future research.},
booktitle = {Interaction Design and Children},
pages = {82–97},
numpages = {16},
keywords = {CCI Research, Literature Review, Correlated Topic Model, Automated Text Analysis, Child-Computer Interaction},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1109/ICSE.2017.12,
author = {Jiang, He and Zhang, Jingxuan and Ren, Zhilei and Zhang, Tao},
title = {An Unsupervised Approach for Discovering Relevant Tutorial Fragments for APIs},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.12},
doi = {10.1109/ICSE.2017.12},
abstract = {Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely <u>F</u>ragment <u>R</u>ecommender for <u>A</u>PIs with <u>P</u>ageRank and <u>T</u>opic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {38–48},
numpages = {11},
keywords = {application programming interface, unsupervised approaches, topic model, pagerank algorithm},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/3344158,
author = {Jiang, He and Zhang, Jingxuan and Li, Xiaochen and Ren, Zhilei and Lo, David and Wu, Xindong and Luo, Zhongxuan},
title = {Recommending New Features from Mobile App Descriptions},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3344158},
doi = {10.1145/3344158},
abstract = {The rapidly evolving mobile applications (apps) have brought great demand for developers to identify new features by inspecting the descriptions of similar apps and acquire missing features for their apps. Unfortunately, due to the huge number of apps, this manual process is time-consuming and unscalable. To help developers identify new features, we propose a new approach named SAFER. In this study, we first develop a tool to automatically extract features from app descriptions. Then, given an app, we leverage the topic model to identify its similar apps based on the extracted features and API names of apps. Finally, we design a feature recommendation algorithm to aggregate and recommend the features of identified similar apps to the specified app. Evaluated over a collection of 533 annotated features from 100 apps, SAFER achieves a Hit@15 score of up to 78.68% and outperforms the baseline approach KNN+ by 17.23% on average. In addition, we also compare SAFER against a typical technique of recommending features from user reviews, i.e., CLAP. Experimental results reveal that SAFER is superior to CLAP by 23.54% in terms of Hit@15.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
articleno = {22},
numpages = {29},
keywords = {feature recommender system, domain analysis, Mobile applications, topic model}
}

@inproceedings{10.1145/3132847.3133128,
author = {Nguyen, Trong T. and Lauw, Hady W.},
title = {Collaborative Topic Regression with Denoising AutoEncoder for Content and Community Co-Representation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133128},
doi = {10.1145/3132847.3133128},
abstract = {Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2231–2234},
numpages = {4},
keywords = {social collaborative filtering, collaborative deep learning, cold-start recommendation, autoencoder, topic model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3477495.3531990,
author = {Zhang, Yuxiang and Jiang, Tao and Yang, Tianyu and Li, Xiaoli and Wang, Suge},
title = {HTKG: Deep Keyphrase Generation with Neural Hierarchical Topic Guidance},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531990},
doi = {10.1145/3477495.3531990},
abstract = {Keyphrases can concisely describe the high-level topics discussed in a document that usually possesses hierarchical topic structures. Thus, it is crucial to understand the hierarchical topic structures and employ it to guide the keyphrase identification. However, integrating the hierarchical topic information into a deep keyphrase generation model is unexplored. In this paper, we focus on how to effectively exploit the hierarchical topic to improve the keyphrase generation performance (HTKG). Specifically, we propose a novel hierarchical topic-guided variational neural sequence generation method for keyphrase generation, which consists of two major modules: a neural hierarchical topic model that learns the latent topic tree across the whole corpus of documents, and a variational neural keyphrase generation model to generate keyphrases under hierarchical topic guidance. Finally, these two modules are jointly trained to help them learn complementary information from each other. To the best of our knowledge, this is the first attempt to leverage the neural hierarchical topic to guide keyphrase generation. The experimental results demonstrate that our method significantly outperforms the existing state-of-the-art methods across five benchmark datasets.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1044–1054},
numpages = {11},
keywords = {variational neural generation model, deep keyphrase generation, neural hierarchical topic model},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3291801.3291823,
author = {Fujino, Iwao and Claramunt, Christophe and Boudraa, Abdel-Ouahab},
title = {Extracting Courses of Vessels from AIS Data and Real-Time Warning Against Off-Course},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291823},
doi = {10.1145/3291801.3291823},
abstract = {Automatic Identification System (AIS) broadcasts real-time information from moving vessels at sea. AIS data collected from moving vessels over an area of interest can rapidly generates large datasets, while such data can be also of very much interest for deriving maritime trajectory patterns or anomaly events. Besides a number of route discovery and anomaly detection techniques currently derived from AIS data, it appears, that from machine learning and natural language processing principles, a topic model might provide a novel approach for extracting implicit patterns underlying massive and unstructured collection of incoming data considered as documents. Inspired by this idea, we first applied a topic model to extract course patterns from AIS data, and then developed an algorithm-based approach for real-time warning against off course. In fact, a course not only encompasses trajectory locations, but also headings. The potential of the approach is illustrated by a series of experimental results derived from practical AIS data set in a region of North West France.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {62–69},
numpages = {8},
keywords = {Off-course Warning, Vector Quantization, Automatic Identification System (AIS), Maritime Big Data, Topic Model, Course Pattern Extraction},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3178876.3186145,
author = {Cheng, Zhiyong and Ding, Ying and Zhu, Lei and Kankanhalli, Mohan},
title = {Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186145},
doi = {10.1145/3178876.3186145},
abstract = {Although latent factor models (e.g., matrix factorization) achieve good accuracy in rating prediction, they suffer from several problems including cold-start, non-transparency, and suboptimal recommendation for local users or items. In this paper, we employ textual review information with ratings to tackle these limitations. Firstly, we apply a proposed aspect-aware topic model (ATM) on the review text to model user preferences and item features from different aspects, and estimate the aspect importance of a user towards an item. The aspect importance is then integrated into a novel aspect-aware latent factor model (ALFM), which learns user's and item's latent factors based on ratings. In particular, ALFM introduces a weighted matrix to associate those latent factors with the same set of aspects discovered by ATM, such that the latent factors could be used to estimate aspect ratings. Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance. To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation. Besides, an aspect rating is weighted by an aspect importance, which is dependent on the targeted user's preferences and targeted item's features. Therefore, it is expected that the proposed method can model a user's preferences on an item more accurately for each user-item pair locally. Comprehensive experimental studies have been conducted on 19 datasets from Amazon and Yelp 2017 Challenge dataset. Results show that our method achieves significant improvement compared with strong baseline methods, especially for users with only few ratings. Moreover, our model could interpret the recommendation results in depth.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {639–648},
numpages = {10},
keywords = {matrix factorization, recommendation, aspect-aware, review-aware, topic model},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3022227.3022251,
author = {Zhao, Chen and Li, Jiaqi and Nie, Tian and Ding, Yi and Xu, Linghan and Utsuro, Takehito and Kawada, Yasuhide and Kando, Noriko},
title = {Identifying Major Contents among Web Pages with Search Engine Suggests by Modeling Topics},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022251},
doi = {10.1145/3022227.3022251},
abstract = {This paper addresses the problem of identifying irrelevant items from a small set of similar documents using Web search engine suggests. Specifically, we collected volumes of Web pages through Web search engines and inspected the page contents using topic models. Among each cluster of pages sharing the same topic indicated by the topic model, our technique discovers potential content organization in the current page cluster and identifies pages that are out of focus from that topic. The metrics in our approach mainly consist of search engine suggest frequency and inter-document similarity measures. The intuition is that Web pages collected via the same search queries are more likely to share similar contents. We verify this intuition by implementing a subtopic based document selection framework and making quantitative evaluation against human made labeled data sets. Our evaluation result reveals that suggest frequency analysis along with inter-document similarity measure is effective at filtering off-topic documents in small data sets with satisfactory performance.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {25},
numpages = {8},
keywords = {retrieval, text understanding, similarity, overview, topic model, document filtering, search engine suggest},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@article{10.1007/s00779-018-1183-9,
author = {Luo, Li-Xia},
title = {Network Text Sentiment Analysis Method Combining LDA Text Representation and GRU-CNN},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-018-1183-9},
doi = {10.1007/s00779-018-1183-9},
abstract = {In order to improve the performance of internet public sentiment analysis, a text sentiment analysis method combining Latent Dirichlet Allocation (LDA) text representation and convolutional neural network (CNN) is proposed. First, the review texts are collected from the network for preprocessing. Then, using the LDA topic model to train the latent semantic space representation (topic distribution) of the short text, and the short text feature vector representation based on the topic distribution is constructed. Finally, the CNN with gated recurrent unit (GRU) is used as a classifier. According to the input feature matrix, the GRU-CNN strengthens the relationship between words and words, text and text, so as to achieve high accurate text classification. The simulation results show that this method can effectively improve the accuracy of text sentiment classification.},
journal = {Personal Ubiquitous Comput.},
month = {jul},
pages = {405–412},
numpages = {8},
keywords = {Gated recurrent unit, Convolutional neural network, Topic model text representation, Text sentiment analysis}
}

@inproceedings{10.1145/3340531.3412024,
author = {Liu, Hongyu and He, Ruifang and Wang, Haocheng and Wang, Bo},
title = {Fusing Parallel Social Contexts within Flexible-Order Proximity for Microblog Topic Detection},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412024},
doi = {10.1145/3340531.3412024},
abstract = {Topic detection in social media is a challenging task due to large-scale short, noisy and informal nature of messages. Most existing methods only consider textual content or simultaneously model the posts and the first-order structural characteristics of social networks. They ignore the impact of larger neighborhoods in microblog conversations on topics. Moreover, the simple combination of separated content and structure representations fails to capture their nonlinear correlation and different importance in topic inference. To this end, we propose a novel random walk based Parallel Social Contexts Fusion Topic Model (PCFTM) for weibo conversations. Firstly, a user-level conversation network with content information is built by the reposting and commenting relationships among users. Through random walks of different lengths on network, we obtain the user sequences containing the parallel content and structure contexts, which are used to acquire the flexible-order proximity of users. Then we propose a self-fusion network embedding to capture the nonlinear correlation between parallel social contexts. It is achieved by taking the content embedding sequence processed by CNN as the initial value of structure embedding sequence fed to Bi-LSTM. Meanwhile, a user-level self-attention is further used to mine the different importance of users to topics. Lastly, the user sequence embedding is incorporated into neural variational inference for detecting topics, which adaptively balances the intrinsic complementarity between content and structure, and fully uses both local and global social contexts in topic inference. Extensive experiments on three real-world weibo datasets demonstrate the effectiveness of our proposed model.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {875–884},
numpages = {10},
keywords = {microblog conversations, flexible-order proximity, parallel social contexts fusion topic model},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1145/3078845,
author = {Liu, Jie and Liu, Bin and Liu, Yanchi and Chen, Huipeng and Feng, Lina and Xiong, Hui and Huang, Yalou},
title = {Personalized Air Travel Prediction: A Multi-Factor Perspective},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078845},
doi = {10.1145/3078845},
abstract = {Human mobility analysis is one of the most important research problems in the field of urban computing. Existing research mainly focuses on the intra-city ground travel behavior modeling, while the inter-city air travel behavior modeling has been largely ignored. Actually, the inter-city travel analysis can be of equivalent importance and complementary to the intra-city travel analysis. Understanding massive passenger-air-travel behavior delivers intelligence for airlines’ precision marketing and related socioeconomic activities, such as airport planning, emergency management, local transportation planning, and tourism-related businesses. Moreover, it provides opportunities to study the characteristics of cities and the mutual relationships between them. However, modeling and predicting air traveler behavior is challenging due to the complex factors of the market situation and individual characteristics of customers (e.g., airlines’ market share, customer membership, and travelers’ intrinsic interests on destinations). To this end, in this article, we present a systematic study on the personalized air travel prediction problem, namely where a customer will fly to and which airline carrier to fly with, by leveraging real-world anonymized Passenger Name Record (PNR) data. Specifically, we first propose a relational travel topic model, which combines the merits of latent factor model with a neighborhood-based method, to uncover the personal travel preferences of aviation customers and the latent travel topics of air routes and airline carriers simultaneously. Then we present a multi-factor travel prediction framework, which fuses complex factors of the market situation and individual characteristics of customers, to predict airline customers’ personalized travel demands. Experimental results on two real-world PNR datasets demonstrate the effectiveness of our approach on both travel topic discovery and customer travel prediction.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {30},
numpages = {26},
keywords = {urban computing, travel topic model, Air travel demand, latent dirichlet allocation}
}

@article{10.5555/3122009.3208008,
author = {Perrone, Valerio and Jenkins, Paul A. and Span\`{o}, Dario and Teh, Yee Whye},
title = {Poisson Random Fields for Dynamic Feature Models},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {4626–4670},
numpages = {45},
keywords = {indian buffet process, topic model, Markov chain Monte Carlo, Poisson random field, Bayesian nonparametrics}
}

@inproceedings{10.1145/3331453.3361319,
author = {Deng, Chunhui and Deng, Huifang and Liu, Yuxin},
title = {Online Hot Topic Discovery and Hotness Evaluation},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3361319},
doi = {10.1145/3331453.3361319},
abstract = {In this paper, by analyzing the inadequacies of traditional TF-IDF(Term Frequency-Inverse Document Frequency) method and taking into account the factors of the location information, named entity and feature term burstiness, we put forward an improved weight calculation formula i.e., a new TF-IDF to update the feature term weight in real time. In this way, the accuracy of news representation model can be improved to some extent. Incremental k-means clustering based on time window and multi-center topic model is proposed to tackle topic center drift problem, reduce the error caused by inadequate topic model, and therefore, improve the clustering accuracy. At last, we defined an improved energy accumulation formula. And based on media attention, topic competition, topic burstiness magnitude and topic cohesiveness, we constructed a topic hotness evaluation model to quantify the topic hotness and therefore to better distinguish the hot topics from the cold topics. The experimental results demonstrated the effectiveness of our approaches and models.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {43},
numpages = {8},
keywords = {Single pass, Vector space model, Multi-center topic model, Improved TF-IDF, Incremental k-means clustering, Online hot topic discovery, Hotness evaluation},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3022227.3022269,
author = {Li, Jiaqi and Zhao, Chen and Lin, Youchao and Baba, Mizuho and Nie, Tian and Utsuro, Takehito and Kawada, Yasuhide and Kando, Noriko},
title = {A Method of Collecting Know-How Knowledge Based on Question-Answer Examples and Search Engine Suggests},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022269},
doi = {10.1145/3022227.3022269},
abstract = {This paper presents techniques of retrieving useful information from a mixture of Web pages collected from either question-answer sites (Q&amp;A sites) or Web search engines. The proposed techniques are designed to discover the maximum possible amount of know-how knowledge from such collections of Web pages, where know-how knowledge is defined as text contents qualified as information source regarding specific domain of questions. The major intent is to build a framework that selects helpful information to provide answers to various problems of interest, such as useful tips to a question. Techniques in this paper primarily attempt to complement knowledge available on Q&amp;A sites with pages collected from search engines via topic models. In order to argue that pages collected from search engine are truly supplements to know- how knowledge on Q&amp;A sites we verify how much extra useful information the Web search engine is able to provide by manually inspecting Web pages aggregated by the topic model.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {43},
numpages = {8},
keywords = {web search engine, topic model, know-how knowledge, question-answer site, web page collection/aggregation},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@article{10.1145/3003728,
author = {Feng, Shanshan and Cao, Jian and Wang, Jie and Qian, Shiyou},
title = {Recommendations Based on Comprehensively Exploiting the Latent Factors Hidden in Items’ Ratings and Content},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3003728},
doi = {10.1145/3003728},
abstract = {To improve the performance of recommender systems in a practical manner, several hybrid approaches have been developed by considering item ratings and content information simultaneously. However, most of these hybrid approaches make recommendations based on aggregating different recommendation techniques using various strategies, rather than considering joint modeling of the item’s ratings and content, and thus fail to detect many latent factors that could potentially improve the performance of the recommender systems. For this reason, these approaches continue to suffer from data sparsity and do not work well for recommending items to individual users. A few studies try to describe a user’s preference by detecting items’ latent features from content-description texts as compensation for the sparse ratings. Unfortunately, most of these methods are still generally unable to accomplish recommendation tasks well for two reasons: (1) they learn latent factors from text descriptions or user--item ratings independently, rather than combining them together; and (2) influences of latent factors hidden in texts and ratings are not fully explored. In this study, we propose a probabilistic approach that we denote as latent random walk (LRW) based on the combination of an integrated latent topic model and random walk (RW) with the restart method, which can be used to rank items according to expected user preferences by detecting both their explicit and implicit correlative information, in order to recommend top-ranked items to potentially interested users. As presented in this article, the goal of this work is to comprehensively discover latent factors hidden in items’ ratings and content in order to alleviate the data sparsity problem and to improve the performance of recommender systems. The proposed topic model provides a generative probabilistic framework that discovers users’ implicit preferences and items’ latent features simultaneously by exploiting both ratings and item content information. On the basis of this probabilistic framework, RW can predict a user’s preference for unrated items by discovering global latent relations. In order to show the efficiency of the proposed approach, we test LRW and other state-of-the-art methods on three real-world datasets, namely, CAMRa2011, Yahoo!, and APP. The experiments indicate that our approach outperforms all comparative methods and, in addition, that it is less sensitive to the data sparsity problem, thus demonstrating the robustness of LRW for recommendation tasks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {35},
numpages = {27},
keywords = {latent factors detection, probabilistic topic model, correlative information discovering, Recommender system, random walk with restart}
}

@inproceedings{10.1145/3155133.3155154,
author = {Nguyen, Thuc and Do, Phuc},
title = {Managing and Visualizing Citation Network Using Graph Database and LDA Model},
year = {2017},
isbn = {9781450353281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3155133.3155154},
doi = {10.1145/3155133.3155154},
abstract = {In this paper, a solution of storing and inferring on citation network using graph database and topic model is proposed. Citation network is a very large directed graph containing nodes and edges. Each node is a paper and each directed edge is a link between paper and its citing papers. In citation network analysis, each node usually contains basic properties of paper such as paper ID, publication year, paper title, authors. In this research, we propose one more property called "topic vector". This property contains topic distribution of a specific paper which is gained by LDA algorithm. After that, we propose a new approach to store and manage the citation network using graph database. Finally, we use the graph query language to develop some functions of citation network analysis and visualize topic propagation through the network. We also compare our approach with the traditional method in which the relational database is used to store and manage citation network. Experimental results show that the performance of our approach is higher than the traditional one and a different view of citation network analysis is also discussed.},
booktitle = {Proceedings of the Eighth International Symposium on Information and Communication Technology},
pages = {100–105},
numpages = {6},
keywords = {graph visualization, graph database, citation network analysis, topic propagation, topic model, heterogeneous bibliographic network},
location = {Nha Trang City, Viet Nam},
series = {SoICT 2017}
}

@article{10.1145/3459089,
author = {Pal, Ratnabali and Sekh, Arif Ahmed and Dogra, Debi Prosad and Kar, Samarjit and Roy, Partha Pratim and Prasad, Dilip K.},
title = {Topic-Based Video Analysis: A Survey},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459089},
doi = {10.1145/3459089},
abstract = {Manual processing of a large volume of video data captured through closed-circuit television is challenging due to various reasons. First, manual analysis is highly time-consuming. Moreover, as surveillance videos are recorded in dynamic conditions such as in the presence of camera motion, varying illumination, or occlusion, conventional supervised learning may not work always. Thus, computer vision-based automatic surveillance scene analysis is carried out in unsupervised ways. Topic modelling is one of the emerging fields used in unsupervised information processing. Topic modelling is used in text analysis, computer vision applications, and other areas involving spatio-temporal data. In this article, we discuss the scope, variations, and applications of topic modelling, particularly focusing on surveillance video analysis. We have provided a methodological survey on existing topic models, their features, underlying representations, characterization, and applications in visual surveillance’s perspective. Important research papers related to topic modelling in visual surveillance have been summarized and critically analyzed in this article.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {118},
numpages = {34},
keywords = {topic model, unsupervised learning, Video analysis}
}

@article{10.1145/2932192,
author = {Li, Yang and Jiang, Jing and Liu, Ting and Qiu, Minghui and Sun, Xiaofei},
title = {Personalized Microtopic Recommendation on Microblogs},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/2932192},
doi = {10.1145/2932192},
abstract = {Microblogging services such as Sina Weibo and Twitter allow users to create tags explicitly indicated by the # symbol. In Sina Weibo, these tags are called microtopics, and in Twitter, they are called hashtags. In Sina Weibo, each microtopic has a designate page and can be directly visited or commented on. Recommending these microtopics to users based on their interests can help users efficiently acquire information. However, it is non-trivial to recommend microtopics to users to satisfy their information needs. In this article, we investigate the task of personalized microtopic recommendation, which exhibits two challenges. First, users usually do not give explicit ratings to microtopics. Second, there exists rich information about users and microtopics, for example, users' published content and biographical information, but it is not clear how to best utilize such information. To address the above two challenges, we propose a joint probabilistic latent factor model to integrate rich information into a matrix factorization-based solution to microtopic recommendation. Our model builds on top of collaborative filtering, content analysis, and feature regression. Using two real-world datasets, we evaluate our model with different kinds of content and contextual information. Experimental results show that our model significantly outperforms a few competitive baseline methods, especially in the circumstance where users have few adoption behaviors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {77},
numpages = {21},
keywords = {Microblogs, microtopic recommendation, topic model, collaborative filtering}
}

@article{10.1007/s00778-018-0522-9,
author = {Zhao, Kaiqi and Cong, Gao and Chin, Jin-Yao and Wen, Rong},
title = {Exploring Market Competition over Topics in Spatio-Temporal Document Collections},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0522-9},
doi = {10.1007/s00778-018-0522-9},
abstract = {With the prominence of location-based services and social networks in recent years, huge amounts of spatio-temporal document collections (e.g., geo-tagged tweets) have been generated. These data collections often imply user's ideas on different products and thus are helpful for business owners to explore hot topics of their brands and the competition relation to other brands in different spatial regions during different periods. In this work, we aim to mine the topics and the market competition of different brands over each topic for a category of business (e.g., coffeehouses) from spatio-temporal documents within a user-specified region and time period. To support such spatio-temporal search online in an exploratory manner, we propose a novel framework equipped by (1) a generative model for mining topics and market competition, (2) an Octree-based off-line pre-training method for the model and (3) an efficient algorithm for combining pre-trained models to return the topics and market competition on each topic within a user-specified pair of region and time span. Extensive experiments show that our framework is able to improve the runtime by up to an order of magnitude compared with baselines while achieving similar model quality in terms of training log-likelihood.},
journal = {The VLDB Journal},
month = {feb},
pages = {123–145},
numpages = {23},
keywords = {Exploratory search, Spatio-temporal data, Algorithms, Gibbs sampling, Topic model}
}

@article{10.1145/3091108,
author = {Li, Chenliang and Duan, Yu and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
title = {Enhancing Topic Modeling for Short Texts with Auxiliary Word Embeddings},
year = {2017},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3091108},
doi = {10.1145/3091108},
abstract = {Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of “shortness” is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts. When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized P\'{o}lya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPU-PDMM outperforms GPU-DMM at the price of higher computational costs.},
journal = {ACM Trans. Inf. Syst.},
month = {aug},
articleno = {11},
numpages = {30},
keywords = {word embeddings, short texts, Topic model}
}

@inproceedings{10.1145/3282373.3282409,
author = {Fukuyama, Satoshi and Wakabayashi, Kei},
title = {Extracting Time Series Variation of Topic Popularity in Microblogs},
year = {2018},
isbn = {9781450364799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282373.3282409},
doi = {10.1145/3282373.3282409},
abstract = {Extracting topics and their popularities in microblogs is a promising approach to discover popular topics in the world. To challenge this task, some methods that estimate popularity of topics based on Latent Dirichlet Allocation (LDA) has been proposed. However, LDA fails to extract favorable topics on a collection of short text documents such as microblogs because the word co-occurrence information in an individual document is sparse. Therefore, in order to extract topics from microblogs, we should use a model specialized for short text documents. In this paper, we propose a topic popularity estimation method using Biterm TopicModel (BTM), which can alleviate the problem caused by document level word co-occurrence sparsity. We extract topics from the microblog documents with BTM for each time period and estimate the frequency of each topic occurrence. The proposed method can analyze the popularity of topics in a real time because we apply anefficient inference algorithm for BTMonsmall batches of tweets. Experiments on tweets collection show that some of the topics extracted by the proposed method correspond to the real world events and a topic burstiness gets higher when the event occurs.},
booktitle = {Proceedings of the 20th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {365–369},
numpages = {5},
keywords = {Biterm Topic Model, microblogs, topic popularity},
location = {Yogyakarta, Indonesia},
series = {iiWAS2018}
}

@inproceedings{10.1145/3077136.3080806,
author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
title = {Jointly Learning Word Embeddings and Latent Topics},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080806},
doi = {10.1145/3077136.3080806},
abstract = {Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step'' methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {375–384},
numpages = {10},
keywords = {word embedding, document modeling, topic model},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3360774.3360814,
author = {Lu, Yuhuan and He, Zhaocheng and Luo, Liangkui},
title = {Learning Trajectories as Words: A Probabilistic Generative Model for Destination Prediction},
year = {2019},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360774.3360814},
doi = {10.1145/3360774.3360814},
abstract = {Destination prediction is crucial for many location based services such as sightseeing places recommendation and targeted advertisements push. Most existing techniques utilize the historical trajectories to predict destinations, but they fail to well describe the spatio-temporal characteristics of trajectories and suffer the trajectory sparsity problem, i.e., the available historical trajectories are hard to cover all probable trajectories. The temporal sensitivity of historical trajectories highlights the sparsity problem even more. In this paper, we address this problem by building a probabilistic generative model to capture the spatio-temporal features of trajectories. We develop an extended Latent Dirichlet Allocation (LDA) model to characterize the generative mechanism of track points in each trajectory. In this model, trajectory, track point of trajectory and destination are regarded as document, word and response respectively. To address the trajectory sparsity problem, each trajectory is expressed by the distribution of trajectory patterns which are the topics discovered from historical trajectories. Then, the most likely destination is predicted through the trajectory patterns. The experiments performed on a real-world taxi trajectory dataset from Guangzhou confirm the advantage of the probabilistic generative model in destination prediction, achieving remarkable accuracy and strong interpretability.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {464–472},
numpages = {9},
keywords = {trajectory, topic model, destination prediction},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@inproceedings{10.1145/3383583.3398572,
author = {Zhenni, Ni and Yuxing, Qian},
title = {The Status, Hot Topics in the Field of Electronic Health Records: A Literature Review Based on Lda2vec},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398572},
doi = {10.1145/3383583.3398572},
abstract = {Electronic health records(EHRs) has become very popular in last few years. To describe the current landscape of EHR-related research,based on lda2vec and co-occurrence analysis, this study focuses on its research areas and topic distribution. It is found that studies about population health and risk prediction have grown rapidly, and much attention has been paid to the application of AI in medicine. Moreover, the standards for knowledge representation and information sharing have been basically improved.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {479–480},
numpages = {2},
keywords = {bibliometric, topic model, EHRs, machine learning},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3240508.3240596,
author = {Zhou, Zhengzhong and Di, Xiu and Zhou, Wei and Zhang, Liqing},
title = {Fashion Sensitive Clothing Recommendation Using Hierarchical Collocation Model},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240596},
doi = {10.1145/3240508.3240596},
abstract = {Automatic clothing recommendation grows dramatically due to the booming of apparel e-commerce. In this paper, we propose a novel clothing recommendation approach which is sensitive to the fashion trend. The proposed approach incorporates the expert knowledge into multiple dimensional information including purchase behaviors, image contents and product descriptions so as to provide recommendation of clothing in line with the forefront of fashion. Meanwhile, to meet with human visual aesthetics and user's collocation experience, we propose the integration of the convolutional neural network and the hierarchical collocation model (HCM) into our framework. The former is to extract effective visual features and attribute descriptors from the clothing items, while the latter embeds them into the concept of style topics which interpret the collocation pattern from a higher level of semantic knowledge. Such a data driven recommendation approach is able to learn clothing collocation metric from multi-dimensional clothing information. Experimental results show that our HCM method achieves better performance than other state-of-the-art baselines. Besides, it also ensures the fashion sensitivity of the recommended outfits.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1119–1127},
numpages = {9},
keywords = {hierarchical collocation model, clothing recommendation, topic model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3409256.3409839,
author = {Sarwar, Sheikh Muhammad and Addanki, Raghavendra and Montazeralghaem, Ali and Pal, Soumyabrata and Allan, James},
title = {Search Result Diversification with Guarantee of Topic Proportionality},
year = {2020},
isbn = {9781450380676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409256.3409839},
doi = {10.1145/3409256.3409839},
abstract = {Search result diversification based on topic proportionality considers a document as a bag of weighted topics and aims to reorder or down-sample a ranked list in a way that maintains topic proportionality. The goal is to show the topic distribution from an ambiguous query at all points in the revised list, hoping to satisfy all users in expectation. One effective approach, PM-2, greedily selects the best topic that maintains proportionality at each ranking position and then selects the document that best represents that topic. From a theoretical perspective, this approach does not provide any guarantee that topic proportionality holds in the small ranked list. Moreover, this approach does not take query-document relevance into account. We propose a Linear Programming (LP) formulation, LP-QL, that maintains topic proportionality and simultaneously maximizes relevance. We show that this approach satisfies topic proportionality constraints in expectation. Empirically, it achieves a 5.5% performance gain (significant) in terms of alpha-NDCG compared to PM-2 when we use LDA as the topic modelling approach. Furthermore, we propose LP-PM-2 that integrates the solution of LP-QL with PM-2. LP-PM-2 achieves 3.2% performance gain (significant) over PM-2 in terms of alpha-NDCG with term based topic modeling approach. All of our experiments are based on a popular web document collection, ClueWeb09 Category B, and the queries are taken from TREC Web Track's diversity task.},
booktitle = {Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval},
pages = {53–60},
numpages = {8},
keywords = {topic model, search result diversification, linear program},
location = {Virtual Event, Norway},
series = {ICTIR '20}
}

@inproceedings{10.1145/3041021.3053051,
author = {Sarkar, Santonu and Lakdawala, Rumana and Datta, Subhajit},
title = {Predicting the Impact of Software Engineering Topics: An Empirical Study},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053051},
doi = {10.1145/3041021.3053051},
abstract = {Predicting the future is hard, more so in active research areas. In this paper, we customize an established model for citation prediction of research papers and apply it on research topics. We argue that research topics, rather than individual publications, have wider relevance in the research ecosystem, for individuals as well as organizations. In this study, topics are extracted from a corpus of software engineering publications covering 55,000+ papers written by more than 70,000 authors across 56 publication venues, over a span of 38 years, using natural language processing techniques. We demonstrate how critical aspects of the original paper-based prediction model are valid for a topic-based approach. Our results indicate the customized model is able to predict citations for many of the topics considered in our study with reasonably high accuracy. Insights from these results indicate the promise of citation of prediction of research topics, and its utility for individual researchers, as well as research groups.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1251–1257},
numpages = {7},
keywords = {software engineering publication, topic model, citation prediction},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3308558.3313732,
author = {Zhang, Xuan and Qiao, Zhilei and Ahuja, Aman and Fan, Weiguo and Fox, Edward A. and Reddy, Chandan K.},
title = {Discovering Product Defects and Solutions from Online User Generated Contents},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313732},
doi = {10.1145/3308558.3313732},
abstract = {The recent increase in online user generated content (UGC) has led to the availability of a large number of posts about products and services. Often, these posts contain complaints that the consumers purchasing the products and services have. However, discovering and summarizing product defects and the related knowledge from large quantities of user posts is a difficult task. Traditional aspect opinion mining models, that aim to discover the product aspects and their corresponding opinions, are not sufficient to discover the product defect information from the user posts. In this paper, we propose the Product Defect Latent Dirichlet Allocation model (PDLDA), a probabilistic model that identifies domain-specific knowledge about product issues using interdependent three-dimensional topics: Component, Symptom, and Resolution. A Gibbs sampling based inference method for PDLDA is also introduced. To evaluate our model, we introduce three novel product review datasets. Both qualitative and quantitative evaluations show that the proposed model results in apparent improvement in the quality of discovered product defect information. Our model has the potential to benefit customers, manufacturers, and policy makers, by automatically discovering product defects from online data.},
booktitle = {The World Wide Web Conference},
pages = {3441–3447},
numpages = {7},
keywords = {Topic Model, Opinion Mining, Product Defect Discovery},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3206185.3206203,
author = {Khanthaapha, Passakorn and Pipanmaekaporn, Luepol and Kamonsantiroj, Suwatchai},
title = {Topic-Based User Profile Model for POI Recommendations},
year = {2018},
isbn = {9781450364126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206185.3206203},
doi = {10.1145/3206185.3206203},
abstract = {Location-based services (LBSs) provide users with points of interest (POIs) based on a limited distance from the user current location. Recently, the increased number of POIs in LBSs often results in overwhelming place suggestions generated to users more than before. To facilitate a user's exploration, POI recommendation is demanded. Unlike traditional recommendations, POI recommendation has no explicit user rating and location awareness. We propose in this paper a novel method to discover user profiles for recommendation of POI in LBS. We first use topic modeling to learn user's interest topics from content information associated with POIs that he/she has visited. We then infer topic distribution of POI based on the learned word-topic distribution. Because the user's interest topics learnt from texts may be broadly and imprecisely, we propose a method to re-weight the user interest topics for personalized place recommendation based on extended relevance feedback. The experiments shown that the proposed recommendation algorithm achieved better performance compared to state-of-the-art algorithms on real-world LBS dataset.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {143–147},
numpages = {5},
keywords = {topic model, POI recommendation, Location-based service},
location = {Phuket, Thailand},
series = {ISMSI '18}
}

@inproceedings{10.1145/3139923.3139936,
author = {Park, Mookyu and Seo, Junwoo and Kim, Kyoungmin and Park, Moosung and Lee, Kyungho},
title = {Criminal Minds: Reasoning Prime Suspect in Russian Hacking Scandal by Perspective of Insiders},
year = {2017},
isbn = {9781450351775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139923.3139936},
doi = {10.1145/3139923.3139936},
abstract = {Recent cyber attacks are affecting the cyberspace as well as the real world, causing social confusion. Russia Scandal in the US presidential election in 2016 is a representative example. This case is being discussed up to the impeachment of the president. Cyber influence attacks are attacks that affect cyberspace to real space. An effectiveness of such attacks boosts when insiders execute. This study analyzed the keywords related to cyber attacks of Russia during the US presidential election in 2016. Based on this, the paper used sentiment analysis for analyzing the psychology of the current Russian Scandal suspect.},
booktitle = {Proceedings of the 2017 International Workshop on Managing Insider Security Threats},
pages = {97–100},
numpages = {4},
keywords = {russian scandal, sentiment analysis, insider's profiling, topic model},
location = {Dallas, Texas, USA},
series = {MIST '17}
}

@inproceedings{10.1145/3477495.3536329,
author = {Vianna, Daniela and Silva de Moura, Edleno},
title = {Organizing Portuguese Legal Documents through Topic Discovery},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3536329},
doi = {10.1145/3477495.3536329},
abstract = {A significant challenge in the legal domain is to organize and summarize a constantly growing collection of legal documents, uncovering hidden topics, or themes, that later can support tasks such as legal case retrieval and legal judgment prediction. This massive amount of digital legal documents, combined with the inherent complexity of judiciary systems worldwide, presents a promising scenario for Machine Learning solutions, mainly those taking advantage of all the advancements in the area of Natural Language Processing (NLP). It is in this scenario that Jusbrasil, the largest legal tech company in Brazil, is situated. Using a dataset partially curated by the Jusbrasil legal team, we explore topic modeling solutions using state of the art language models, trained with legal Portuguese documents, to automatically organize and summarize this complex collection of documents. Instead of using an entire legal case, which usually is composed of many pages, we show that it is possible to efficiently organize the collection using the syllabus (in Portuguese, ementa jurisprudencial) from each court decision as they concisely summarize the main points presented by the entire decision.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3388–3392},
numpages = {5},
keywords = {language models, legal cases, law tech, topic model},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3308558.3313561,
author = {Liu, Luyang and Huang, Heyan and Gao, Yang and Zhang, Yongfeng and Wei, Xiaochi},
title = {Neural Variational Correlated Topic Modeling},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313561},
doi = {10.1145/3308558.3313561},
abstract = {With the rapid development of the Internet, millions of documents, such as news and web pages, are generated everyday. Mining the topics and knowledge on them has attracted a lot of interest on both academic and industrial areas. As one of the prevalent unsupervised data mining tools, topic models are usually explored as probabilistic generative models for large collections of texts. Traditional probabilistic topic models tend to find a closed form solution of model parameters and approach the intractable posteriors via approximation methods, which usually lead to the inaccurate inference of parameters and low efficiency when it comes to a quite large volume of data. Recently, an emerging trend of neural variational inference can overcome the above issues, which offers a scalable and powerful deep generative framework for modeling latent topics via neural networks. Interestingly, a common assumption for the most neural variational topic models is that topics are independent and irrelevant to each other. However, this assumption is unreasonable in many practical scenarios. In this paper, we propose a novel Centralized Transformation Flow to capture the correlations among topics by reshaping topic distributions. Furthermore, we present the Transformation Flow Lower Bound to improve the performance of the proposed model. Extensive experiments on two standard benchmark datasets have well-validated the effectiveness of the proposed approach.},
booktitle = {The World Wide Web Conference},
pages = {1142–1152},
numpages = {11},
keywords = {Natural language processing;topic model;neural variational inference},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3461778.3462010,
author = {Chapko, Dorota and Andr\'{e}s P\'{e}rez Rothstein, Pedro Andr\'{e}s and Emeh, Lizzie and Frumiento, Pino and Kennedy, Donald and McNicholas, David and Orjiekwe, Ifeoma and Overton, Michaela and Snead, Mark and Steward, Robyn and Sutton, Jenny and Bradshaw, Melissa and Jeffreys, Evie and Gallia, Will and Ewans, Sarah and Williams, Mark and Grierson, Mick},
title = {Supporting Remote Survey Data Analysis by Co-Researchers with Learning Disabilities through Inclusive and Creative Practices and Data Science Approaches.},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462010},
doi = {10.1145/3461778.3462010},
abstract = {Through a process of robust co-design, we created a bespoke accessible survey platform to explore the role of co-researchers with learning disabilities (LDs) in research design and analysis. A team of co-researchers used this system to create an online survey to challenge public understanding of LDs [3]. Here, we describe and evaluate the process of remotely co-analyzing the survey data across 30 meetings in a research team consisting of academics and non-academics with diverse abilities amid new COVID-19 lockdown challenges. Based on survey data with &gt;1,500 responses, we first co-analyzed demographics using graphs and art &amp; design approaches. Next, co-researchers co-analyzed the output of machine learning-based structural topic modelling (STM) applied to open-ended text responses. We derived an efficient five-steps STM co-analysis process for creative, inclusive, and critical engagement of data by co-researchers. Co-researchers observed that by trying to understand and impact public opinion, their own perspectives also changed.},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1668–1681},
numpages = {14},
keywords = {co-design, topic model, survey, Learning disability},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3366424.3382088,
author = {Yang, Hui-Kuo},
title = {Learning Topic Map from Large Scale Social Media Data},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382088},
doi = {10.1145/3366424.3382088},
abstract = {Geo-tagged social media data are hugely generated every day. Those content provide rich sources to explore keywords and topics in any region of the world thanks to the widely popularized mobile internet services. The association between geographic regions and their keywords/topics in social media has raised a lot of research attentions. Such association provides important information to event prediction, source detection, and news propagation in various applications. For instance, the disaster control and management, and evaluation of sales marketing campaign are just two of the many examples. The association between regions and keywords/topics are analogous to that between geographic coordinates and spatial features, such as streets intersections, building compounds and so on in a conventional street map, and we propose a new model, called “topic map” to mimic such an analogy and to encode and represent text features with geographic regions that reside in Geo-tagged social media data. Applications based on topic map will be explored during my research, and extensions to temporal data will be investigated further.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {279–283},
numpages = {5},
keywords = {Spatial Model, Topic Model, Geo-tagging, Social Media Data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3178876.3186168,
author = {Luiz, Washington and Viegas, Felipe and Alencar, Rafael and Mour\~{a}o, Fernando and Salles, Thiago and Carvalho, D\'{a}rlinton and Gon\c{c}alves, Marcos Andre and Rocha, Leonardo},
title = {A Feature-Oriented Sentiment Rating for Mobile App Reviews},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186168},
doi = {10.1145/3178876.3186168},
abstract = {In this paper, we propose a general framework that allows developers to filter, summarize and analyze user reviews written about applications on App Stores. Our framework extracts automatically relevant features from reviews of apps (e.g., information about functionalities, bugs, requirements, etc) and analyzes the sentiment associated with each of them. Our framework has three main building blocks, namely, (i) topic modeling, (ii) sentiment analysis and (iii) summarization interface. The topic modeling block aims at finding semantic topics from textual comments, extracting the target features based on the most relevant words of each discovered topic. The sentiment analysis block detects the sentiment associated with each discovered feature. The summarization interface provides to developers an intuitive visualization of the features (i.e., topics) and their associated sentiment, providing richer information than a 'star rating' strategy. Our evaluation shows that the topic modeling block is able to organize information provided by users in subcategories that facilitate the understanding of which features more positively/negatively impact the overall evaluation of the application. Regarding user satisfaction, we can observe that, in spite of the star rating being a good measure of evaluation, the Sentiment Analysis technique is more accurate in capturing the sentiment transmitted by the user by means of a comment.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1909–1918},
numpages = {10},
keywords = {topic model, sentiment analysis, analysis of online reviews},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3321454.3321473,
author = {Tran, Tu Cam Thi and Huynh, Hiep Xuan and Tran, Phuc Quang and Truong, Dinh Quoc},
title = {Text Classification Based on Keywords with Different Thresholds},
year = {2019},
isbn = {9781450366335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321454.3321473},
doi = {10.1145/3321454.3321473},
abstract = {Text classification is a supervised learning task for assigning text document to one or more predefined classes/topics. These topics are determined by a set of training documents. In order to construct a classification model, a machine learning algorithm was used. The training model is used to predict a class for new coming document. In this paper, we propose a text classification approach based on automatic keywords extraction with different thresholes. We use 3000 Vietnamese text documents, which belong to ten topics, downloaded from two electronic magazines vnexpress.net and vietnamnet.vn to create ten sets of the keywords. These keywords are used to predict the topic of new text document. The experimental results confirm the feasibility of proposed model.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Technology},
pages = {101–106},
numpages = {6},
keywords = {topic model, Text classification, automatic keywords extraction, threshols},
location = {Da, Nang, Viet Nam},
series = {ICIIT '19}
}

@inproceedings{10.1145/3014812.3014865,
author = {Albishre, Khaled and Li, Yuefeng and Xu, Yue},
title = {Effective Pseudo-Relevance for Microblog Retrieval},
year = {2017},
isbn = {9781450347686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014812.3014865},
doi = {10.1145/3014812.3014865},
abstract = {Microblog services such as Twitter have become a part of daily life for many users, with thousands of documents published each second. Microblog documents are often too short, overwhelming in their use of informal language and hard to understand due to a lack of contextual clues. Retrieving relevant documents from microblogs is somewhat challenging because of its nature and the massive scale of the data. However, microblog retrieval models suffer from a vocabulary mismatch problem that leads to insufficient performance. In this paper, we address microblog retrieval limitations by proposing a pseudo-relevance feedback model. Our model considers discriminative expansion to meet user interests. Experimental results on TREC 2011 and 2012 microblog datasets show that our model demonstrates significant improvements over the baseline models.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {51},
numpages = {6},
keywords = {topic model, query expansion, language model, microblog search},
location = {Geelong, Australia},
series = {ACSW '17}
}

@inproceedings{10.1145/3164541.3164580,
author = {Akayama, Ikuto and Hijikata, Yoshinori and Kuramochi, Toshiya and Sakata, Nobuchika},
title = {Proposal of Network Generation Model Based on Latent Preference Topic},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164580},
doi = {10.1145/3164541.3164580},
abstract = {People select whom to follow on social networking sites based on the topics that interest them. In this paper, we propose a new generation model for complex networks to mimic people's following behavior. In our proposed model, a node selects a target node to make a directed link based on the latent topic. We examine the features of the networks generated by our model through computer simulation. In the simulations, we calculate the average path length, clustering coefficient, and power exponent, which are representative evaluation indices of the network, and check whether they satisfy the properties of complex networks.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {89},
numpages = {7},
keywords = {Complex network, Topic Model, Network generation model, Simulation},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@inproceedings{10.1145/3123024.3124424,
author = {Ihianle, Isibor Kennedy and Naeem, Usman and Islam, Syed},
title = {Ontology-Driven Activity Recognition from Patterns of Object Use},
year = {2017},
isbn = {9781450351904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123024.3124424},
doi = {10.1145/3123024.3124424},
abstract = {The recognition of the activities of daily living of the elderly and the cognitively impaired has made it possible to provide assistance and support for them. We describe in this paper activity recognition from patterns of object use for activities and a combined activity ontology. Activity-object use patterns are discovered to provide the knowledge of object concepts for routine activity concepts for an enhanced ontology-driven activity recognition.},
booktitle = {Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {654–657},
numpages = {4},
keywords = {activity recognition, ontology model, latent dirichlet allocation, topic model},
location = {Maui, Hawaii},
series = {UbiComp '17}
}

@inproceedings{10.1145/3037697.3037740,
author = {Li, Kaiwei and Chen, Jianfei and Chen, Wenguang and Zhu, Jun},
title = {SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037740},
doi = {10.1145/3037697.3037740},
abstract = {Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics.In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {497–509},
numpages = {13},
keywords = {palellel computing, topic model, lda, gpu},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{10.1145/3093337.3037740,
author = {Li, Kaiwei and Chen, Jianfei and Chen, Wenguang and Zhu, Jun},
title = {SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/3093337.3037740},
doi = {10.1145/3093337.3037740},
abstract = {Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics.In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before.},
journal = {SIGARCH Comput. Archit. News},
month = {apr},
pages = {497–509},
numpages = {13},
keywords = {gpu, topic model, palellel computing, lda}
}

@article{10.1145/3093336.3037740,
author = {Li, Kaiwei and Chen, Jianfei and Chen, Wenguang and Zhu, Jun},
title = {SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093336.3037740},
doi = {10.1145/3093336.3037740},
abstract = {Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics.In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {497–509},
numpages = {13},
keywords = {topic model, palellel computing, lda, gpu}
}

@article{10.1145/3072591,
author = {Hou, Lei and Li, Juanzi and Li, Xiao-Li and Tang, Jie and Guo, Xiaofei},
title = {Learning to Align Comments to News Topics},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072591},
doi = {10.1145/3072591},
abstract = {With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers’ news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
articleno = {9},
numpages = {31},
keywords = {alignment, cluster chain, dependent topic model, density, pu learning, User-generated content}
}

@inproceedings{10.1145/3308560.3316455,
author = {Freire-Vidal, Yerka and Graells-Garrido, Eduardo},
title = {Characterization of Local Attitudes Toward Immigration Using Social Media},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316455},
doi = {10.1145/3308560.3316455},
abstract = {Migration is a worldwide phenomenon that may generate different reactions in the population. Attitudes vary from those that support multiculturalism and communion between locals and foreigners, to contempt and hatred toward immigrants. Since anti-immigration attitudes are often materialized in acts of violence and discrimination, it is important to identify factors that characterize these attitudes. However, doing so is expensive and impractical, as traditional methods require enormous efforts to collect data. In this paper, we propose to leverage Twitter to characterize local attitudes toward immigration, with a case study on Chile, where immigrant population has drastically increased in recent years. Using semi-supervised topic modeling, we situated 49K users into a spectrum ranging from in-favor to against immigration. We characterized both sides of the spectrum in two aspects: the emotions and lexical categories relevant for each attitude, and the discussion network structure. We found that the discussion is mostly driven by Haitian immigration; that there are temporal trends in tendency and polarity of discussion; and that assortative behavior on the network differs with respect to attitude. These insights may inform policy makers on how people feel with respect to migration, with potential implications on communication of policy and the design of interventions to improve inter-group relations.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {783–790},
numpages = {8},
keywords = {Semi-supervised Topic model, Twitter, Immigration, Public attitude},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3077136.3084139,
author = {Son, Jeong-Woo and Park, Wonjoo and Lee, Sang-Yun and Kim, Jinwoo and Kim, Sun-Joong},
title = {Smart Media Generation System for Broadcasting Contents},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3084139},
doi = {10.1145/3077136.3084139},
abstract = {Broadcasting contents are the most plausiable resources for services with video contents. Even though we already have huge amount of produced broadcasting contents, there rarely exists a system to analyze and generate information on broadcasting contents to support content retrieval and recommendation services. This paper proposes a new system for this purpose. In the proposed system, a broadcasting content is segmented into semantic units, scenes, based on its multiple characteristics. The proposed system analyzes scenes and generates their keywords, topics, and stories. Connections among scenes are automatically establishing based on shared keywords, similar topics, and consistency in stories. To support operators, the proposed system offers two tools: Scene Studio and SceneViz. We prepare several Open APIs in the proposed system to provide information and connections for service providers. The feasibility of the proposed system is shown with numerical evaluations on the qualities of generated information. We also introduce two video clip services implemented with our system.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1297–1300},
numpages = {4},
keywords = {topic model, broadcasting content, scene segmentation, content mining, smart media},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3110025.3120990,
author = {Lee, Won Kyung and Sohn, So Young},
title = {Weak Ties Based Recommendation for Interdisciplinary Research Collaboration},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3120990},
doi = {10.1145/3110025.3120990},
abstract = {This study investigates recommendations for interdisciplinary research collaboration based on the weak ties theory. Contents-based features are proposed to recommend interdisciplinary collaboration considering that some researchers who have shown a preference for interdisciplinary collaboration could be connected even if they have dissimilar research profiles. Therefore, we inferred the preference of interdisciplinary research collaboration for every researcher, and considered features such as highlighting dissimilar researchers depending on their preferences. The features are designed to have typical similarity measures when the researchers do not prefer interdisciplinary research collaboration. We evaluated our proposed features with the baseline features of patent application datasets and the former methods outperformed the latter methods.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1199–1200},
numpages = {2},
keywords = {topic model, weighted network, link prediction, Interdisciplinary research collaboration},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@article{10.1145/3099473,
author = {Phani, Shanta and Lahiri, Shibamouli and Biswas, Arindam},
title = {A Supervised Learning Approach for Authorship Attribution of Bengali Literary Texts},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3099473},
doi = {10.1145/3099473},
abstract = {Authorship Attribution is a long-standing problem in Natural Language Processing. Several statistical and computational methods have been used to find a solution to this problem. In this article, we have proposed methods to deal with the authorship attribution problem in Bengali. More specifically, we proposed a supervised framework consisting of lexical and shallow features and investigated the possibility of using topic-modeling-inspired features, to classify documents according to their authors. We have created a corpus from nearly all the literary works of three eminent Bengali authors, consisting of 3,000 disjoint samples. Our models showed better performance than the state-of-the-art, with more than 98% test accuracy for the shallow features and 100% test accuracy for the topic-based features. Further experiments with GloVe vectors [Pennington et al. 2014] showed comparable results, but flexible patterns based on content words and high-frequency words [Schwartz et al. 2013] failed to perform as well as expected.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {28},
numpages = {15},
keywords = {machine learning, lexical features, Authorship attribution, Naive bayes, topic model}
}

@inproceedings{10.1145/3358695.3360922,
author = {Zeyu, Lyu},
title = {Towards an Understanding of Online Extremism in Japan},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3360922},
doi = {10.1145/3358695.3360922},
abstract = {Using a large amount of social media data, this study employed a variety of computational methods to investigate online extremism in Japan. In order to explain the increase of online extremism, this study identifies extremists by estimating the ideological position of social media users based on the follower-followee relationship. Following this, this study characterizes the behavioral patterns of such individuals from two perspectives: comparison of profile information and preference in online discussions among different ideological groups.Computational methods provide many insights into the online extremism in Japan. First, this study finds that although online extremism has been frequently debated about in the recent years, it is somewhat surprising there were a relatively limited number of extremists. Moreover, this study finds that such individuals are more likely to spread information and express their views than moderate users. They particularly exhibit a significant preference to engage in discussions related to political issues or social issues. As a consequence, their behavior and views are more likely to capture a lot of attention and generate influence as a consequence. Taken together, the findings in this study suggest that online extremism in Japan is attributed to the behavioral patterns of extremists, rather than their increasing number.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {7–13},
numpages = {7},
keywords = {Ideological Position Estimation, Social Media Data, Structural Topic Model, Extremism},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/3184558.3186354,
author = {Lin, Wen Hua and Chen, Kuan-Ting and Chiang, Hung Yueh and Hsu, Winston},
title = {Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186354},
doi = {10.1145/3184558.3186354},
abstract = {Recently, deep neural network models have achieved promising results in image captioning task. Yet, "vanilla'' sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose Netizen Style Commenting (NSC), to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid "netizen'' style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {395–402},
numpages = {8},
keywords = {deep learning, image captioning, diversity, commenting, topic model, fashion},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3077136.3080772,
author = {Cheng, Zhiyong and Shen, Jialie and Nie, Liqiang and Chua, Tat-Seng and Kankanhalli, Mohan},
title = {Exploring User-Specific Information in Music Retrieval},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080772},
doi = {10.1145/3077136.3080772},
abstract = {With the advancement of mobile computing technology and cloud-based streaming music service, user-centered music retrieval has become increasingly important. User-specific information has a fundamental impact on personal music preferences and interests. However, existing research pays little attention to the modeling and integration of user-specific information in music retrieval algorithms/models to facilitate music search. In this paper, we propose a novel model, named User-Information-Aware Music Interest Topic (UIA-MIT) model. The model is able to effectively capture the influence of user-specific information on music preferences, and further associate users' music preferences and search terms under the same latent space. Based on this model, a user information aware retrieval system is developed, which can search and re-rank the results based on age- and/or gender-specific music preferences. A comprehensive experimental study demonstrates that our methods can significantly improve the search accuracy over existing text-based music retrieval methods.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {655–664},
numpages = {10},
keywords = {semantic music retrieval, topic model, re-ranking, user demographic information},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3445815.3445859,
author = {Tantisuwankul, Jirateep and Manaskasemsak, Bundit and Rungsawang, Arnon},
title = {Identifying Influencers in Thai Internet Forum Based on Topic-Oriented Gravity Model},
year = {2020},
isbn = {9781450388436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445815.3445859},
doi = {10.1145/3445815.3445859},
abstract = {The task of identifying influencers provides a lot of benefits for various practical applications such as recommendation systems, viral marketing, and information monitoring. This issue can traditionally be solved via a network structure with several proposed graph algorithms. However, most of them employ a global computation with much time-consuming; some consider only undirected and unweighted networks which may be inconsistent with the nature of data. Inspired by the law of gravity in Physics, we present the Topic-oriented Gravity Model (TopicGM) that investigates a directed and weighted network incorporating users' topical aspects. The key concept is that an individual is first represented as a textual content he created or read. Afterwards, TopicGM simply adopts a topic modeling, i.e., the Hierarchical Dirichlet Process (HDP), to classify topics over those contents. A topical network is then constructed where nodes represent individuals and an edge connects two individuals in the direction from the poster to the reader with a topical confidence weight. Finally, we apply the gravity formula to calculate influence scores and rank individuals. The experimental results, conducted on real-world data gathered from Pantip.com (famous Thai web forum), show that our approach outperforms many state-of-the-art baselines by accurately identifying influencers within the top of rankings.},
booktitle = {2020 4th International Conference on Computer Science and Artificial Intelligence},
pages = {271–277},
numpages = {7},
keywords = {influencer identification, social network, gravity model, topic model, viral marketing},
location = {Zhuhai, China},
series = {CSAI 2020}
}

@inproceedings{10.1145/3331184.3331287,
author = {Wang, Chang and Wang, Bang and Xiang, Wei and Xu, Minghua},
title = {Encoding Syntactic Dependency and Topical Information for Social Emotion Classification},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331287},
doi = {10.1145/3331184.3331287},
abstract = {Social emotion classification is to estimate the distribution of readers' emotion evoked by an article. In this paper, we design a new neural network model by encoding sentence syntactic dependency and document topical information into the document representation. We first use a dependency embedded recursive neural network to learn syntactic features for each sentence, and then use a gated recurrent unit to transform the sentences' vectors into a document vector. We also use a multi-layer perceptron to encode the topical information of a document into a topic vector. Finally, a gate layer is used to compose the document representation from the gated summation of the document vector and the topic vector. Experiment results on two public datasets indicate that our proposed model outperforms the state-of-the-art methods in terms of better average Pearson correlation coefficient and MicroF1 performance.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {881–884},
numpages = {4},
keywords = {dependency embedding, topic model, recursive neural network, social emotion classification},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1145/3386041,
author = {Shi, Min and Tang, Yufei and Zhu, Xingquan and Liu, Jianxun},
title = {Topic-Aware Web Service Representation Learning},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3386041},
doi = {10.1145/3386041},
abstract = {The advent of Service-Oriented Architecture (SOA) has brought a fundamental shift in the way in which distributed applications are implemented. An overwhelming number of Web-based services (e.g., APIs and Mashups) have leveraged this shift and furthered development. Applications designed with SOA principles are typically characterized by frequent dependencies with one another in the form of heterogeneous networks, i.e., annotation relations between tags and services, and composition relations between Mashups and APIs. Although prior work has shown the utility gained by exploring these networks, their analysis is still in its infancy. This article develops an approach to learning representations of the Web service network, which seeks to embed Web services in low-dimensional continuous vectors with preserved information of the network structure, functional tags, and service descriptions, such that services with similar functional properties and network structures are mapped together in the learned latent space. We first propose a topic generative model for constructing two topic distribution networks (Mashup-Topic and API-Topic) from the service content. Then, we present an efficient optimization process to derive low-dimensional vector representations of Web services from a tri-layer bipartite network with the Mashup-Topic and API-Topic networks on two ends and the Mashup-API composition network in the middle. Experiments on real-word datasets have verified that our approach is effective to learn robust low-rank service representations, i.e., 25% F1-measure gain over the state-of-the-art in Web service recommendation task.},
journal = {ACM Trans. Web},
month = {apr},
articleno = {9},
numpages = {23},
keywords = {probabilistic topic model, service representation, Mashups, network embedding, Web services}
}

@inproceedings{10.1145/3095140.3095152,
author = {Shirota, Yukari and Hashimoto, Takako and Chakraborty, Basabi},
title = {Visualization Challenge on Time Series Statistical Data},
year = {2017},
isbn = {9781450352284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095140.3095152},
doi = {10.1145/3095140.3095152},
abstract = {It has been very significant to visualize time series big data. In the paper we shall discuss design on time series statistical data. As an example, we present an animation of Gibbs sampling process to clarify the time changes. Gibbs sampling is widely used MCMC algorithm in the deep learning field. We consider that an additional z-axis coordinate or a time line are helpful for the visualization and the functions could be implemented automatically by some kind of chart-wizards. We shall discuss the design rules and tips on visualization of time series data.},
booktitle = {Proceedings of the Computer Graphics International Conference},
articleno = {12},
numpages = {4},
keywords = {MCMC, simple topic model, Gibbs sampler, shape analysis, visualization, bayesian inference},
location = {Yokohama, Japan},
series = {CGI '17}
}

@inproceedings{10.1145/3358528.3358592,
author = {He, Xun and Wang, Lianhai and Zhang, Weinan and Zhang, Peijun},
title = {Research on the Quality Prediction of Online Chinese Question Answering Community Answers Based on Comments},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358592},
doi = {10.1145/3358528.3358592},
abstract = {With the rapid development of online Community Question Answer (CQA), a large volume of valuable data has been accumulated in CQA sites, as well as a huge number of low-quality answers. To improve the user-friendliness of CQA sites and help users find high-quality answers quickly, in this paper, we propose a supervised learning model to evaluate the quality of answers on Chinese CQA sites. We build a quality evaluation model based on the pairwise learning-to-rank algorithm and combine a set of features to rank answers according to their quality. In the quality evaluation process, we also propose an innovative type of feature named "the sentiment polarity of comments". Experimental results on real CQA data show that our method can efficiently produce a quality-ranking list of answers. Moreover, the proposed sentiment polarity feature can improve the performance of the quality evaluation model significantly.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {114–120},
numpages = {7},
keywords = {topic model, community question answer, ranking learning, CQA, answer quality prediction},
location = {Jinan, China},
series = {ICBDT2019}
}

@inproceedings{10.1145/3287921.3287930,
author = {Nguyen, Thuc and Do, Phuc},
title = {CitationLDA++: An Extension of LDA for Discovering Topics in Document Network},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287930},
doi = {10.1145/3287921.3287930},
abstract = {Along with rapid development of electronic scientific publication repositories, automatic topics identification from papers has helped a lot for the researchers in their research. Latent Dirichlet Allocation (LDA) model is the most popular method which is used to discover hidden topics in texts basing on the co-occurrence of words in a corpus. LDA algorithm has achieved good results for large documents. However, article repositories usually only store title and abstract that are too short for LDA algorithm to work effectively. In this paper, we propose CitationLDA++ model that can improve the performance of the LDA algorithm in inferring topics of the papers basing on the title or/and abstract and citation information. The proposed model is based on the assumption that the topics of the cited papers also reflects the topics of the original paper. In this study, we divide the dataset into two sets. The first one is used to build prior knowledge source using LDA algorithm. The second is training dataset used in CitationLDA++. In the inference process with Gibbs sampling, CitationLDA++ algorithm use topics distribution of prior knowledge source and citation information to guide the process of assigning the topic to words in the text. The use of topics of cited papers helps to tackle the limit of word co-occurrence in case of linked short text. Experiments with the AMiner dataset including title or/and abstract of papers and citation information, CitationLDA++ algorithm gains better perplexity measurement than no additional knowledge. Experimental results suggest that the citation information can improve the performance of LDA algorithm to discover topics of papers in the case of full content of them are not available.},
booktitle = {Proceedings of the Ninth International Symposium on Information and Communication Technology},
pages = {31–37},
numpages = {7},
keywords = {text mining, topic model, distributed computing, document network, citation network analysis},
location = {Danang City, Viet Nam},
series = {SoICT 2018}
}

@inproceedings{10.1145/3239438.3239459,
author = {Cheng, Ching-Hsue and Hung, Wei-Lun},
title = {Tea in Benefits of Health: A Literature Analysis Using Text Mining and Latent Dirichlet Allocation},
year = {2018},
isbn = {9781450363891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239438.3239459},
doi = {10.1145/3239438.3239459},
abstract = {Tea originated in Asian, which was initially used as a medicinal herb. The variety of tea is according to different manufacturing processes and levels of oxidation. The different varieties of tea have different level of effects on health, thus this study adopted text mining technique and Latent Dirichlet Allocation (LDA) to analyze literature for tea in health effect. This study chose Web of Science as the database of literature source, and the search literature from 2007 to 2017. The total 1230 journal articles were collected in this study. The title, abstract, and keywords of the collected journal articles were used as a dataset for the experiment. Experimental results show that the VEM method is significantly lower than Gibbs sampling in perplexity. Hence, this study chooses K=150 when VEM method and Gibbs sampling reach the minimal perplexity in the same time. Many topics that related with tea and compounds of tea, however some topics had terms that related to health and disease. The top 10 topics show that tea could reduce the risk of diseases and benefit of health.},
booktitle = {Proceedings of the 2nd International Conference on Medical and Health Informatics},
pages = {148–155},
numpages = {8},
keywords = {Text mining, Topic model, Literature analysis, Tea, LDA, Latent Dirichlet Allocation, Health},
location = {Tsukuba, Japan},
series = {ICMHI '18}
}

@inproceedings{10.1145/3428029.3428565,
author = {Gr\"{o}nberg, Niku and Knutas, Antti and Hynninen, Timo and Hujala, Maija},
title = {An Online Tool for Analyzing Written Student Feedback},
year = {2020},
isbn = {9781450389211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428029.3428565},
doi = {10.1145/3428029.3428565},
abstract = {Collecting student feedback is commonplace in universities. Feedback surveys usually have both open-ended questions and Likert-type questions, but the answers to open questions tend not to be analysed further than simply reading them. This paper presents a tool for analyzing written student feedback using topic modeling and emotion analysis. We demonstrate the utility of this tool using course survey responses from a software engineering (SE) programme.},
booktitle = {Koli Calling '20: Proceedings of the 20th Koli Calling International Conference on Computing Education Research},
articleno = {40},
numpages = {2},
keywords = {text mining, emotion analysis, student evaluation of teaching, structural topic model},
location = {Koli, Finland},
series = {Koli Calling '20}
}

@article{10.1145/3469654,
author = {Shen, Dazhong and Qin, Chuan and Zhu, Hengshu and Xu, Tong and Chen, Enhong and Xiong, Hui},
title = {Joint Representation Learning with Relation-Enhanced Topic Models for Intelligent Job Interview Assessment},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3469654},
doi = {10.1145/3469654},
abstract = {The job interview is considered as one of the most essential tasks in talent recruitment, which forms a bridge between candidates and employers in fitting the right person for the right job. While substantial efforts have been made on improving the job interview process, it is inevitable to have biased or inconsistent interview assessment due to the subjective nature of the traditional interview process. To this end, in this article, we propose three novel approaches to intelligent job interview by learning the large-scale real-world interview data. Specifically, we first develop a preliminary model, named Joint Learning Model on Interview Assessment (JLMIA), to mine the relationship among job description, candidate resume, and interview assessment. Then, we further design an enhanced model, named Neural-JLMIA, to improve the representative capability by applying neural variance inference. Last, we propose to refine JLMIA with Refined-JLMIA (R-JLMIA) by modeling individual characteristics for each collection, i.e., disentangling the core competences from resume and capturing the evolution of the semantic topics over different interview rounds. As a result, our approaches can effectively learn the representative perspectives of different job interview processes from the successful job interview records in history. In addition, we exploit our approaches for two real-world applications, i.e., person-job fit and skill recommendation for interview assessment. Extensive experiments conducted on real-world data clearly validate the effectiveness of our models, which can lead to substantially less bias in job interviews and provide an interpretable understanding of job interview assessment.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {15},
numpages = {36},
keywords = {Interview assessment, representation disentanglement, neural topic model, latent variable model, sequential data}
}

@article{10.1145/3446343,
author = {Bashar, Md Abul and Nayak, Richi},
title = {Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3446343},
doi = {10.1145/3446343},
abstract = {Language model (LM) has become a common method of transfer learning in Natural Language Processing (NLP) tasks when working with small labeled datasets. An LM is pretrained using an easily available large unlabelled text corpus and is fine-tuned with the labelled data to apply to the target (i.e., downstream) task. As an LM is designed to capture the linguistic aspects of semantics, it can be biased to linguistic features. We argue that exposing an LM model during fine-tuning to instances that capture diverse semantic aspects (e.g., topical, linguistic, semantic relations) present in the dataset will improve its performance on the underlying task. We propose a Mixed Aspect Sampling (MAS) framework to sample instances that capture different semantic aspects of the dataset and use the ensemble classifier to improve the classification performance. Experimental results show that MAS performs better than random sampling as well as the state-of-the-art active learning models to abuse detection tasks where it is hard to collect the labelled data for building an accurate classifier.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {24},
numpages = {24},
keywords = {imbalanced dataset, active learning, transfer learning, Misogynistic tweet, hate speech, topic model}
}

@article{10.1145/3072588,
author = {He, Jiangning and Liu, Hongyan},
title = {Mining Exploratory Behavior to Improve Mobile App Recommendations},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072588},
doi = {10.1145/3072588},
abstract = {With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals.},
journal = {ACM Trans. Inf. Syst.},
month = {aug},
articleno = {32},
numpages = {37},
keywords = {personalized item recommendation, Mobile app recommendation, probabilistic generative model, topic model, exploratory behavior}
}

@inproceedings{10.1145/3289600.3290619,
author = {Spitz, Andreas and Almasian, Satya and Gertz, Michael},
title = {TopExNet: Entity-Centric Network Topic Exploration in News Streams},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290619},
doi = {10.1145/3289600.3290619},
abstract = {The recent introduction of entity-centric implicit network representations of unstructured text offers novel ways for exploring entity relations in document collections and streams efficiently and interactively. Here, we present TopExNet as a tool for exploring entity-centric network topics in streams of news articles. The application is available as a web service at https://topexnet.ifi.uni-heidelberg.de.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {798–801},
numpages = {4},
keywords = {entity network, network topic, topic visualization, implicit network, topic model},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3097983.3098151,
author = {Xing, Zhengming and Hillygus, Sunshine and Carin, Lawrence},
title = {Evaluating U.S. Electoral Representation with a Joint Statistical Model of Congressional Roll-Calls, Legislative Text, and Voter Registration Data},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098151},
doi = {10.1145/3097983.3098151},
abstract = {Extensive information on 3 million randomly sampled United States citizens is used to construct a statistical model of constituent preferences for each U.S. congressional district. This model is linked to the legislative voting record of the legislator from each district, yielding an integrated model for constituency data, legislative roll-call votes, and the text of the legislation. The model is used to examine the extent to which legislators' voting records are aligned with constituent preferences, and the implications of that alignment (or lack thereof) on subsequent election outcomes. The analysis is based on a Bayesian formalism, with fast inference via a stochastic variational Bayesian analysis.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1205–1214},
numpages = {10},
keywords = {matrix factorization, stochastic variational inference, topic model, ideal point, multiplicative gamma process, hierarchical dirichlet process},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3529372.3530923,
author = {Kashyapi, Sumanta and Dietz, Laura},
title = {Query-Specific Subtopic Clustering},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3530923},
doi = {10.1145/3529372.3530923},
abstract = {We propose a Query-Specific Siamese Similarity Metric (QS3M) for query-specific clustering of text documents. Our approach uses fine-tuned BERT embeddings to train a non-linear projection into a query-specific similarity space. We build on the idea of Siamese networks but include a third component, a representation of the query. QS3M is able to model the fine-grained similarity between text passages about the same broad topic and also generalizes to new unseen queries during evaluation. The empirical evaluation for clustering employs two TREC datasets and a set of academic abstracts from arXiv. When used to obtain query-relevant clusters, QS3M achieves a 12% performance improvement on the TREC datasets over a strong BERT-based reference method and many baselines such as TF-IDF and topic models. A similar improvement is observed for the arXiv dataset suggesting the general applicability of QS3M to different domains. Qualitative evaluation is carried out to gain insight into the strengths and limitations of the model.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {11},
numpages = {9},
keywords = {clustering, neural networks, query-specific clustering, topic model, siamese neural networks, similarity metric, topic detection},
location = {Cologne, Germany},
series = {JCDL '22}
}

@inproceedings{10.5555/3340730.3340748,
author = {Aljedaani, Wajdi and Nagappan, Meiyappan and Adams, Bram and Godfrey, Michael},
title = {A Comparison of Bugs across the IOS and Android Platforms of Two Open Source Cross Platform Browser Apps},
year = {2019},
publisher = {IEEE Press},
abstract = {Mobile app developers want to maximize their revenue and hence want to reach as large an audience as possible. In order to do this, they need to build apps for multiple platforms - like Google's Android and Apple's iOS, and maintain them in parallel. Past research has examined properties of the issues addressed in either Android or iOS, but not to compare the work between both. Our main motivation has been to determine if there were differences in how issues manifest themselves in iOS and Android, when we control for the projects, by considering the same apps across multiple platforms. In this paper, we compare issues across two mobile platforms --- iOS and Android --- for two open source browsers --- Mozilla Firefox and Google Chromium. We consider three dimensions of study: frequency of issue report submission, fixing time of issues, and type of issues (using topic modeling on the issue description to generate the categories). We found that there were indeed differences; in particular, we found that there were more issues in the Android version of the apps and the gap with the iOS version is increasing. We observe that in both apps the fix time and type of issues are different for each platform. We also noted certain kinds of issues that may be more prevalent for different browser/platform combinations. This can advise project leads in identifying and allocating development resources to address key problem areas. Hence, issue reports seem more dependent on the platform than on the mobile app, making development and maintenance effort hard to estimate.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {76–86},
numpages = {11},
keywords = {empirical studies, Google chromium, issue reports, Mozilla Firefox, issue repository, issue fixing, topic model},
location = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@inproceedings{10.1145/3393822.3432335,
author = {Aljedaani, Wajdi and Javed, Yasir and Alenezi, Mamdouh},
title = {LDA Categorization of Security Bug Reports in Chromium Projects},
year = {2020},
isbn = {9781450377621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393822.3432335},
doi = {10.1145/3393822.3432335},
abstract = {Security bug reports (SBR) depict potential security vulnerabilities in software systems. Bug tracking systems (BTS) usually contain huge numbers of bug reports including security-related ones. Malicious attackers could exploit these SBRs. Henceforth, it is very critical to pinpoint SBRs swiftly and correctly. In this work, we studied the security bug reports of the Chromium project. We looked into three main aspects of these bug reports, namely: frequencies of reporting them, how quickly they get fixed and is LDA effective in grouping these reports to known vulnerabilities types. We report our findings in these aspects.},
booktitle = {Proceedings of the 2020 European Symposium on Software Engineering},
pages = {154–161},
numpages = {8},
keywords = {Google Chromium, empirical studies, bug reports, topic model, security bug reports, Bug repository, bug fixing},
location = {Rome, Italy},
series = {ESSE 2020}
}

@inproceedings{10.1145/3132847.3133162,
author = {Srivastava, Avikalp and Datt, Madhav},
title = {Soft Seeded SSL Graphs for Unsupervised Semantic Similarity-Based Retrieval},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133162},
doi = {10.1145/3132847.3133162},
abstract = {Semantic similarity based retrieval is playing an increasingly important role in many IR systems such as modern web search, question-answering, similar document retrieval etc. Improvements in retrieval of semantically similar content are very significant to applications like Quora, Stack Overflow, Siri etc. We propose a novel unsupervised model for semantic similarity based content retrieval, where we construct semantic flow graphs for each query, and introduce the concept of "soft seeding" in graph based semi-supervised learning (SSL) to convert this into an unsupervised model.We demonstrate the effectiveness of our model on an equivalent question retrieval problem on the Stack Exchange QA dataset, where our unsupervised approach significantly outperforms the state-of-the-art unsupervised models, and produces comparable results to the best supervised models. Our research provides a method to tackle semantic similarity based retrieval without any training data, and allows seamless extension to different domain QA communities, as well as to other semantic equivalence tasks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2315–2318},
numpages = {4},
keywords = {soft seeded semi-supervised learning graphs, document representation, similar question retrieval, topic model application, semantic similarity},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3397271.3401168,
author = {Zhang, Yu and Meng, Yu and Huang, Jiaxin and Xu, Frank F. and Wang, Xuan and Han, Jiawei},
title = {Minimally Supervised Categorization of Text with Metadata},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401168},
doi = {10.1145/3397271.3401168},
abstract = {Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1)the presence of metadata : in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2)label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into the same semantic space to encode heterogeneous signals. Then, based on the same generative process, we synthesize training samples to address the bottleneck of label scarcity. We conduct a thorough evaluation on a wide range of datasets. Experimental results prove the effectiveness of MetaCat over many competitive baselines.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1231–1240},
numpages = {10},
keywords = {text classification, weak supervision, metadata},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3175684.3175699,
author = {Comito, Carmela and Pizzuti, Clara and Procopio, Nicola},
title = {How People Talk about Health? Detecting Health Topics from Twitter Streams},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175699},
doi = {10.1145/3175684.3175699},
abstract = {The paper proposes an online clustering algorithm for detecting health-related topics. The method extracts from the tweets relevant terms and incrementally groups them by taking into account both term occurrences and tweet age. A detailed experimentation on the tweets posted by users in US shows that the method is capable to group tweets addressing common health issues into the pertinent topic, outperforming traditional topic model approaches, like Doc-p and LDA.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {85–90},
numpages = {6},
keywords = {Twitter, e-Health, Topic Detection},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@article{10.1109/TASLP.2017.2779862,
author = {Chien, Jen-Tzung},
title = {Bayesian Nonparametric Learning for Hierarchical and Sparse Topics},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2779862},
doi = {10.1109/TASLP.2017.2779862},
abstract = {This paper presents the Bayesian nonparametric BNP learning for hierarchical and sparse topics from natural language. Traditionally, the Indian buffet process provides the BNP prior on a binary matrix for an infinite latent feature model consisting of a flat layer of topics. The nested model paves an avenue to construct a tree model instead of a flat-layer model. This paper presents the nested Indian buffet process nIBP to achieve the sparsity and flexibility in topic model where the model complexity and topic hierarchy are learned from the groups of words. The mixed membership modeling is conducted by representing a document using the tree nodes or dishes that a document or a customer chooses according to the nIBP scenario. A tree stick-breaking process is implemented to select topic weights from a subtree for flexible topic modeling. Such an nIBP relaxes the constraint of adopting a single tree path in the nested Chinese restaurant process nCRP and, therefore, improves the variety of topic representation for heterogeneous documents. A Gibbs sampling procedure is developed to infer the nIBP topic model. Compared to the nested hierarchical Dirichlet process nhDP, the compactness of the estimated topics in a tree using nIBP is improved. Experimental results show that the proposed nIBP reduces the error rate of nCRP and nhDP by 18% and 8% on Reuters task for document classification, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {422–435},
numpages = {14}
}

@inproceedings{10.1145/3442381.3449805,
author = {Piccardi, Tiziano and West, Robert},
title = {Crosslingual Topic Modeling with WikiPDA},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449805},
doi = {10.1145/3442381.3449805},
abstract = {We present Wikipedia-based Polyglot Dirichlet Allocation (WikiPDA), a crosslingual topic model that learns to represent Wikipedia articles written in any language as distributions over a common set of language-independent topics. It leverages the fact that Wikipedia articles link to each other and are mapped to concepts in the Wikidata knowledge base, such that, when represented as bags of links, articles are inherently language-independent. WikiPDA works in two steps, by first densifying bags of links using matrix completion and then training a standard monolingual topic model. A human evaluation shows that WikiPDA produces more coherent topics than monolingual text-based latent Dirichlet allocation (LDA), thus offering crosslinguality at no cost. We demonstrate WikiPDA’s utility in two applications: a study of topical biases in 28 Wikipedia language editions, and crosslingual supervised document classification. Finally, we highlight WikiPDA’s capacity for zero-shot language transfer, where a model is reused for new languages without any fine-tuning. Researchers can benefit from WikiPDA as a practical tool for studying Wikipedia’s content across its 299 language editions in interpretable ways, via an easy-to-use library publicly available at https://github.com/epfl-dlab/WikiPDA.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3032–3041},
numpages = {10},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3368958,
author = {Nosakhare, Ehimwenma and Picard, Rosalind},
title = {Toward Assessing and Recommending Combinations of Behaviors for Improving Health and Well-Being},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3368958},
doi = {10.1145/3368958},
abstract = {Multiple behaviors typically work together to influence health, making it hard to understand how one behavior might compensate for another. Rich multi-modal datasets from mobile sensors and advances in machine learning are today enabling new kinds of associations to be made between combinations of behaviors objectively assessed from daily life and self-reported levels of stress, mood, and health. In this article, we present a framework to (1) map multi-modal messy data collected in the “wild” to meaningful feature representations of health-related behaviors, (2) uncover latent patterns comprising combinations of behaviors that best predict health and well-being, and (3) use these learned patterns to make evidence-based recommendations that may improve health and well-being. We show how to use supervised latent Dirichlet allocation to model the observed behaviors, and we apply variational inference to uncover the latent patterns. Implementing and evaluating the model on 5,397 days of data from a group of 244 college students, we find that these latent patterns are indeed predictive of daily self-reported levels of stressed-calm, sad-happy, and sick-healthy states. We investigate the patterns of modifiable behaviors present on different days and uncover several ways in which they relate to stress, mood, and health. This work contributes a new method using objective data analysis to help advance understanding of how combinations of modifiable human behaviors may promote human health and well-being.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {4},
numpages = {29},
keywords = {supervised topic model, mood, health, machine learning, stress, Mental health and well-being, recommending health behaviors, latent variable modeling}
}

@inproceedings{10.1145/3424978.3425097,
author = {Lee, Ruijia and Lyu, Jiangyi},
title = {Sentiment Analysis of Product Reviews Based on JST Model},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425097},
doi = {10.1145/3424978.3425097},
abstract = {Product reviews are information that users comment after purchasing products online, and it contains user's sentiment information about the product. Considering that the e-commerce platform implements personal recommendation of products based on browsing information of product. We propose a sentiment analysis method of product reviews based on the Joint Sentiment/Topic model, which can implement the personal recommendation of products based on the sentiment orientation of product reviews. Firstly, we build a sentiment dictionary for analyzing product reviews by integrating multiple external sentiment dictionaries. Secondly, we give a method to mark the sentiment polarity of the product reviews text. It can tag the sentiment polarity of the product reviews text to generate prior knowledge for the Joint Sentiment/Topic model. Finally, we give the formula for calculating the value of sentiment orientation on product reviews based on the Joint Sentiment/Topic model. Experiments show that the proposed method can effectively obtain the sentiment orientation of product reviews, making the product recommendation of the e-commerce platform more scientific and reasonable.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {115},
numpages = {7},
keywords = {JST model, Gibbs sampling, Sentiment analysis, Product reviews, Sentiment dictionary},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/3121050.3121053,
author = {Cummins, Ronan},
title = {Improved Query-Topic Models Using Pseudo-Relevant P\'{o}lya Document Models},
year = {2017},
isbn = {9781450344906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121050.3121053},
doi = {10.1145/3121050.3121053},
abstract = {Query-expansion via pseudo-relevance feedback is a popular method of overcoming the problem of vocabulary mismatch and of increasing average retrieval effectiveness. In this paper, we develop a new method that estimates a query-topic model from a set of pseudo-relevant documents using a new language modelling framework. We assume that documents are generated via a mixture of multivariate Polya distributions, and we show that by identifying the topical terms in each document, we can appropriately select terms that are likely to belong to the query-topic model. The results of experiments on several TREC collections show that the new approach compares favourably to current state-of-the-art expansion methods.},
booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {101–108},
numpages = {8},
keywords = {relevance models, pseudo-relevance feedback, query models},
location = {Amsterdam, The Netherlands},
series = {ICTIR '17}
}

@inproceedings{10.1145/3351108.3351133,
author = {Mazarura, Jocelyn and de Waal, Alta and de Villiers, Pieter},
title = {Semantic Representations for Under-Resourced Languages},
year = {2019},
isbn = {9781450372657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351108.3351133},
doi = {10.1145/3351108.3351133},
abstract = {Distributional semantics studies methods for learning semantic representation of natural text. The semantic similarity between words and documents can be derived from this presentation which leads to other practical NLP applications such as collaborative filtering, aspect-based sentiment analysis, intent classification for chatbots and machine translation. Under-resourced language data is small in size. Small data implies not only small corpora, but also short documents within the corpus. In this paper we investigate the performance of word embedding techniques on two under-resourced languages. We investigate two topic models, LDA and DMM as well as a word embedding word2vec. We find DMM to perform better than LDA as a topic model embedding. DMM and word2vec perform similar in a semantic evaluation task of aligned corpora.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019},
articleno = {24},
numpages = {10},
keywords = {topic models, under-resourced langauges, distributional semantics},
location = {Skukuza, South Africa},
series = {SAICSIT '19}
}

@article{10.1145/3158670,
author = {Zhao, Wayne Xin and Zhang, Wenhui and He, Yulan and Xie, Xing and Wen, Ji-Rong},
title = {Automatically Learning Topics and Difficulty Levels of Problems in Online Judge Systems},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3158670},
doi = {10.1145/3158670},
abstract = {Online Judge (OJ) systems have been widely used in many areas, including programming, mathematical problems solving, and job interviews. Unlike other online learning systems, such as Massive Open Online Course, most OJ systems are designed for self-directed learning without the intervention of teachers. Also, in most OJ systems, problems are simply listed in volumes and there is no clear organization of them by topics or difficulty levels. As such, problems in the same volume are mixed in terms of topics or difficulty levels. By analyzing large-scale users’ learning traces, we observe that there are two major learning modes (or patterns). Users either practice problems in a sequential manner from the same volume regardless of their topics or they attempt problems about the same topic, which may spread across multiple volumes. Our observation is consistent with the findings in classic educational psychology. Based on our observation, we propose a novel two-mode Markov topic model to automatically detect the topics of online problems by jointly characterizing the two learning modes. For further predicting the difficulty level of online problems, we propose a competition-based expertise model using the learned topic information. Extensive experiments on three large OJ datasets have demonstrated the effectiveness of our approach in three different tasks, including skill topic extraction, expertise competition prediction and problem recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {27},
numpages = {33},
keywords = {online judge systems, expertise learning, Topic models}
}

@inproceedings{10.1145/3227609.3227659,
author = {Loukachevitch, Natalia and Ivanov, Kirill and Dobrov, Boris},
title = {Thesaurus-Based Topic Models and Their Evaluation},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227659},
doi = {10.1145/3227609.3227659},
abstract = {In this paper we study thesaurus-based topic models and evaluate them from the point of view of topic coherence. Thesaurus-based topic model enhances scores of related terms found in the same text, which means that the model encourages these terms to be in the same topics. We evaluate various variants of such models. At the first step, we carry out manual evaluation of the obtained topics. At the second step, we study the possibility to use the collected manual data for evaluating new variants of thesaurus-based models, propose a method and select the best of its parameters in cross-validation. At the third step, we apply the created evaluation method to estimate the influence of word frequencies on adding thesaurus relations during generating topic models.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {11},
numpages = {9},
keywords = {thesaurus, topic models, content-based analysis},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@article{10.1145/3373464.3373474,
author = {Burkhardt, Sophie and Kramer, Stefan},
title = {A Survey of Multi-Label Topic Models},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3373464.3373474},
doi = {10.1145/3373464.3373474},
abstract = {Every day, an enormous amount of text data is produced. Sources of text data include news, social media, emails, text messages, medical reports, scientific publications and fiction. To keep track of this data, there are categories, key words, tags or labels that are assigned to each text. Automatically predicting such labels is the task of multi-label text classification. Often however, we are interested in more than just the pure classification: rather, we would like to understand which parts of a text belong to the label, which words are important for the label or which labels occur together. Because of this, topic models may be used for multi-label classification as an interpretable model that is flexible and easily extensible. This survey demonstrates the manifold possibilities and flexibility of the topic model framework for the complex setting of multi-label text classification by categorizing different variants of models.},
journal = {SIGKDD Explor. Newsl.},
month = {nov},
pages = {61–79},
numpages = {19}
}

@inproceedings{10.1145/3197026.3197038,
author = {Risch, Julian and Krestel, Ralf},
title = {My Approach = Your Apparatus?},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197038},
doi = {10.1145/3197026.3197038},
abstract = {Comparative text mining extends from genre analysis and political bias detection to the revelation of cultural and geographic differences, through to the search for prior art across patents and scientific papers. These applications use cross-collection topic modeling for the exploration, clustering, and comparison of large sets of documents, such as digital libraries. However, topic modeling on documents from different collections is challenging because of domain-specific vocabulary. We present a cross-collection topic model combined with automatic domain term extraction and phrase segmentation. This model distinguishes collection-specific and collection-independent words based on information entropy and reveals commonalities and differences of multiple text collections. We evaluate our model on patents, scientific papers, newspaper articles, forum posts, and Wikipedia articles. In comparison to state-of-the-art cross-collection topic modeling, our model achieves up to 13% higher topic coherence, up to 4% lower perplexity, and up to 31% higher document classification accuracy. More importantly, our approach is the first topic model that ensures disjunct general and specific word distributions, resulting in clear-cut topic representations.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {283–292},
numpages = {10},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3477495.3531877,
author = {Rajanala, Sailaja and Pal, Arghya and Singh, Manish and Phan, Rapha\"{e}l C.-W. and Wong, KokSheik},
title = {DeSCoVeR: Debiased Semantic Context Prior for Venue Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531877},
doi = {10.1145/3477495.3531877},
abstract = {We present a novel semantic context prior-based venue recommendation system that uses only the title and the abstract of a paper. Based on the intuition that the text in the title and abstract have both semantic and syntactic components, we demonstrate that a joint training of a semantic feature extractor and syntactic feature extractor collaboratively leverages meaningful information that helps to provide venues for papers. The proposed methodology that we call DeSCoVeR at first elicits these semantic and syntactic features using a Neural Topic Model and text classifier respectively. The model then executes a transfer learning optimization procedure to perform a contextual transfer between the feature distributions of the Neural Topic Model and the text classifier during the training phase. DeSCoVeR also mitigates the document-level label bias using a Causal back-door path criterion and a sentence-level keyword bias removal technique. Experiments on the DBLP dataset show that DeSCoVeR outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2456–2461},
numpages = {6},
keywords = {joint learning, topic modeling, document classification, mutual transfer, causal debiasing},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3336499.3338013,
author = {Chen, Chi-Hung},
title = {Shipment Supplier Inference Using Topic Modeling},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338013},
doi = {10.1145/3336499.3338013},
abstract = {This research applies Latent Dirichlet Allocation on United States Automated Manifest System Bill of Lading data. We define a "bag of word" where each Harmonized tariff code represents a document, each shipper name be a token and count of shipments to be element of matrix. The result shows that topic model is able to classify some shippers of the same industries.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {10},
numpages = {1},
keywords = {Bill of Lading, Latent Dirichlet Allocation, Automated Manifest System},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.1145/3372923.3404790,
author = {Oghaz, Toktam A. and Mutlu, Ece \c{C}i\u{g}dem and Jasser, Jasser and Yousefi, Niloofar and Garibay, Ivan},
title = {Probabilistic Model of Narratives Over Topical Trends in Social Media: A Discrete Time Model},
year = {2020},
isbn = {9781450370981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372923.3404790},
doi = {10.1145/3372923.3404790},
abstract = {Online social media platforms are turning into the prime source of news and narratives about worldwide events. However, a systematic summarization-based narrative extraction that can facilitate communicating the main underlying events is lacking. To address this issue, we propose a novel event-based narrative summary extraction framework. Our proposed framework is designed as a probabilistic topic model, with categorical time distribution, followed by extractive text summarization. Our topic model identifies topics' recurrence over time with a varying time resolution. This framework not only captures the topic distributions from the data, but also approximates the user activity fluctuations over time. Furthermore, we define significance-dispersity trade-off (SDT) as a comparison measure to identify the topic with the highest lifetime attractiveness in a timestamped corpus. We evaluate our model on a large corpus of Twitter data, including more than one million tweets in the domain of the disinformation campaigns conducted against the White Helmets of Syria. Our results indicate that the proposed framework is effective in identifying topical trends, as well as extracting narrative summaries from text corpus with timestamped data.},
booktitle = {Proceedings of the 31st ACM Conference on Hypertext and Social Media},
pages = {281–290},
numpages = {10},
keywords = {narrative extraction, graphical models, online social media, topic modeling, topic detection and tracking, extractive text summarization},
location = {Virtual Event, USA},
series = {HT '20}
}

@inproceedings{10.1145/3498851.3499008,
author = {Gu, Zhiqiang and Zhang, Yuejin},
title = {Research on Online Medical Community Doctor Recommendation Based on Information Fusion},
year = {2021},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3499008},
doi = {10.1145/3498851.3499008},
abstract = {At present, more and more methods are used to solve the problem of doctor recommendation in online medical community. LDA topic model, vector space model, AHP method, knowledge map and other methods have shown good characteristics and recommendation effect. However, different recommendation methods often get different results, which often leads to inconsistent recommendation, so there are some deficiencies. This paper introduces information fusion technology to fuse the recommendation results obtained by different recommendation methods, obtains further optimization results, and solves the problem of inconsistent results obtained by different recommendation methods. Based on LDA model and word2vec model, this paper established a doctor recommendation model based on information fusion. In the empirical research, Haodaifu (www.haodf.com) has been selected as the research object. The empirical results show that the doctor recommendation model based on information fusion is better than the separate LDA topic model and word2vec model in accuracy and effectiveness.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {514–519},
numpages = {6},
keywords = {Word2vec, Information fusion, LDA, Doctor recommendation},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3269206.3271671,
author = {Li, Ximing and Li, Changchun and Chi, Jinjin and Ouyang, Jihong and Li, Chenliang},
title = {Dataless Text Classification: A Topic Modeling Approach with Document Manifold},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271671},
doi = {10.1145/3269206.3271671},
abstract = {Recently, dataless text classification has attracted increasing attention. It trains a classifier using seed words of categories, rather than labeled documents that are expensive to obtain. However, a small set of seed words may provide very limited and noisy supervision information, because many documents contain no seed words or only irrelevant seed words. In this paper, we address these issues using document manifold, assuming that neighboring documents tend to be assigned to a same category label. Following this idea, we propose a novel Laplacian seed word topic model (LapSWTM). In LapSWTM, we model each document as a mixture of hidden category topics, each of which corresponds to a distinctive category. Also, we assume that neighboring documents tend to have similar category topic distributions. This is achieved by incorporating a manifold regularizer into the log-likelihood function of the model, and then maximizing this regularized objective. Experimental results show that our LapSWTM significantly outperforms the existing dataless text classification algorithms and is even competitive with supervised algorithms to some extent. More importantly, it performs extremely well when the seed words are scarce.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {973–982},
numpages = {10},
keywords = {dataless text classification, topic modeling, seed word, document manifold},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3077136.3080781,
author = {Cai, Renqin and Wang, Chi and Wang, Hongning},
title = {Accounting for the Correspondence in Commented Data},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080781},
doi = {10.1145/3077136.3080781},
abstract = {One important way for people to make their voice heard is to comment on the articles they have read online, such as news reports and each other's posts. The user-generated comments together with the commented documents form a unique correspondence structure. Properly modeling the dependency in such data is thus vital for one to obtain accurate insight of people's opinions and attention.In this work, we develop a Commented Correspondence Topic Model to model correspondence in commented text data. We focus on two levels of correspondence. First, to capture topic-level correspondence, we treat the topic assignments in commented documents as the prior to their comments' topic proportions. This captures the thematic dependency between commented documents and their comments. Second, to capture word-level correspondence, we utilize the Dirichlet compound multinomial distribution to model topics. This captures the word repetition patterns within the commented data. By integrating these two aspects, our model demonstrated encouraging performance in capturing the correspondence sturcture, which provides improved results in modeling user-generated content, spam comment detection, and sentence-based comment retrieval compared with state-of-the-art topic model solutions for correspondence modeling.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {365–374},
numpages = {10},
keywords = {topic models, social media, user comments, text correspondence modeling},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3405962.3405968,
author = {Steuber, Florian and Schoenfeld, Mirco and Rodosek, Gabi Dreo},
title = {Topic Modeling of Short Texts Using Anchor Words},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405968},
doi = {10.1145/3405962.3405968},
abstract = {We present Archetypal LDA or short A-LDA, a topic model tailored to short texts containing "semantic anchors" which convey a certain meaning or implicitly build on discussions beyond their mere presence. A-LDA is an extension to Latent Dirichlet Allocation in that we guide the process of topic inference by these semantic anchors as seed words to the LDA. We identify these seed words unsupervised from the documents and evaluate their co-occurrences using archetypal analysis, a geometric approximation problem that aims for finding k points that best approximate the data set's convex hull. These so called archetypes are considered as latent topics and used to guide the LDA. We demonstrate the effectiveness of our approach using Twitter, where semantic anchor words are the hashtags assigned to tweets by users. In direct comparison to LDA, A-LDA achieves 10-13% better results. We find that representing topics in terms of hashtags corresponding to calculated archetypes alone already results in interpretable topics and the model's performance peaks for seed confidence values ranging from 0.7 to 0.9.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {210–219},
numpages = {10},
keywords = {text mining, topic modeling, data mining, short text, archetypal analysis},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3275116.3275152,
author = {Poyane, Roman},
title = {Toxic Communication during Streams on Twitch.Tv. The Case of Dota 2},
year = {2018},
isbn = {9781450365895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275116.3275152},
doi = {10.1145/3275116.3275152},
abstract = {This paper is devoted to the study of how toxic communication is structured in chats during streams on Twitch.tv and how chat size affects it. The data from Twitch chat logs was used to create a topic model of themes which are discussed by viewers during stream. The result indicate that there are statistically significant differences in the types of communication used in channels of different sizes.},
booktitle = {Proceedings of the 22nd International Academic Mindtrek Conference},
pages = {262–265},
numpages = {4},
keywords = {streaming, toxic communication, Dota 2, Twitch},
location = {Tampere, Finland},
series = {Mindtrek '18}
}

@article{10.5555/3447307.3447316,
author = {Kim, Jung Hee and Banks, Joelle and Bui, Duy and Glass, Michael},
title = {Automated Classification of Collaboration Skills in Typed-Chat Collaborative Problem-Solving},
year = {2021},
issue_date = {January 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {36},
number = {5},
issn = {1937-4771},
abstract = {This experiment trained classifiers to monitor the dialogue of students working together in a Java programming class. The classifiers recognized four activities within the problem-solving conversation: sharing ideas, negotiating, regulating, and maintaining conversation. These dialogue acts are characteristic of problem-solving conversations. This experiment trained classifiers that utilize specific words in the dialogue. It also trained classifiers that use a statistical topic model built from the dialogue transcripts. If dialogue acts can be recognized, then the counts of student interactions could be used for computer monitoring of online student collaborative group exercises.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {97–106},
numpages = {10}
}

@inproceedings{10.1145/3383455.3422557,
author = {Glasserman, Paul and Krstovski, Kriste and Laliberte, Paul and Mamaysky, Harry},
title = {Choosing News Topics to Explain Stock Market Returns},
year = {2020},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422557},
doi = {10.1145/3383455.3422557},
abstract = {We analyze methods for selecting topics in news articles to explain stock returns. We find, through empirical and theoretical results, that supervised Latent Dirichlet Allocation (sLDA) implemented through Gibbs sampling in a stochastic EM algorithm will often overfit returns to the detriment of the topic model. We obtain better out-of-sample performance through a random search of plain LDA models. A branching procedure that reinforces effective topic assignments often performs best. We test these methods on an archive of over 90,000 news articles about S&amp;P 500 firms.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {39},
numpages = {8},
keywords = {finance, text analysis, supervised topic models},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3274856.3274881,
author = {Blekanov, Ivan and Tarasov, Nikita and Maksimov, Alexey},
title = {Topic Modeling of Conflict Ad Hoc Discussions in Social Networks},
year = {2018},
isbn = {9781450365161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274856.3274881},
doi = {10.1145/3274856.3274881},
abstract = {In this paper authors carry out the research of three different topic models in the context of analyzing different large scale twitter ad hoc discussions, obtained from social network on different social and political events. An experiment is being conducted to test the effectiveness of models in analyzing three known discussions: the riots in Biryulyovo (Russia), Ferguson unrest (USA), Charlie Hebdo shooting (France). The results of the experiment show that the BTM topic model in terms of both Umass and Npmi, yielded the best results on all discussions in comparison to the baseline LDA model and another model for short unbalanced texts - WNTM.},
booktitle = {Proceedings of the 3rd International Conference on Applications in Information Technology},
pages = {122–126},
numpages = {5},
keywords = {ad hoc discussions, BTM, social networks, WNTM, LDA, Topic modeling},
location = {Aizu-Wakamatsu, Japan},
series = {ICAIT'2018}
}

@inproceedings{10.1145/3057148.3057151,
author = {Rossetti, Marco and Vargas, Sa\'{u}l and Magatti, Davide and Pettit, Benjamin and Kershaw, Daniel and Hristakeva, Maya and Jack, Kris},
title = {Effectively Identifying Users' Research Interests for Scholarly Reference Management and Discovery},
year = {2017},
isbn = {9781450352406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3057148.3057151},
doi = {10.1145/3057148.3057151},
abstract = {Discovering users' interests is essential in order to help them explore resources in large digital repositories. In particular, correctly identifying users' interests is commonly a good approach for organising information and providing personalised recommendations. We consider here the case of discovering users' research interests in Mendeley a research platform for scholarly article management and discovery. Prior work in this area has considered approaches such as matrix factorisation and text-based topic modelling for inferring topics of interest in recommendation scenarios. These approaches present several problems, such as little or no interpretability of the inferred topics and difficulty handling similarities in vocabulary in different research disciplines. We present an effective solution for extracting coherent and interpretable research topics that leverages the reference management data in Mendeley in a three-step approach: 1) a topic model based on the interactions between users and articles rather than article content, 2) keyword extraction to label the topics using article titles and author-declared keywords and 3) identifying the research interests of users based on the articles that they have added to their libraries. An evaluation comprised of a research interest prediction task and an article recommendation task shows the validity of our proposal in different research disciplines (clearly outperforming a text-based latent topic model) and provides further insights regarding the effects of number of latent topics in the model and the trade-off between recency and quantity of the users' libraries.},
booktitle = {Proceedings of the 1st Workshop on Scholarly Web Mining},
pages = {17–24},
numpages = {8},
keywords = {topic modelling, topic labelling, explanations, scholarly article recommendations, collaborative filtering},
location = {Cambridge, United Kingdom},
series = {SWM '17}
}

@inproceedings{10.1145/3378936.3378939,
author = {Montenegro, Carlos and Navarrete, Rosa},
title = {Improving Core Topics Discovery in Semantic Markup Literature: A Combined Approach},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378939},
doi = {10.1145/3378936.3378939},
abstract = {This research configures a corpus of articles related to the aspects being investigated in Semantic Markup, knowledge domain that has evolved and expanded over the last decade and conduct a manual process to identify the Topics being addressed. Then, it is used LDA, an unsupervised probabilistic topic model, and other tools, for automatically recognize the topics of interest within this corpus; this aims to interpret, validate and complement the results manually obtained. The results let us argue that using combined techniques contribute to improving the human expert analysis, and it is helpfully for the discovery of core topics in Semantic Markup Literature.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {237–243},
numpages = {7},
keywords = {Latent Dirichlet Allocation, Systematic mapping, Embedded semantic markup, Topic Models},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/3356422.3356449,
author = {Yang, Lu and Li, Jie and Lu, Wenhuan and Chen, Yi and Zhang, Kang and Li, Yan},
title = {How to Improve Semantics Understanding of Word Clouds},
year = {2019},
isbn = {9781450376266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356422.3356449},
doi = {10.1145/3356422.3356449},
abstract = {Word cloud is a text visualization technique which is widely applied in helping improve semantic understanding about target materials. One of the most important features is the font size, which represents words frequencies of a document. As the result, in this paper, we explore how to set font sizes of words, and its influence on semantic understanding through people's performance with qualitative and controlled experiments. Adopting an machine learning algorithm LDA (Latent Dirichlet Allocation) topic model, we quantify semantics of the document and judge participants' accuracy performance. The experimental results show the influence of different font size on semantic understanding performance and provide insights for ways in promoting semantic understanding of word cloud.},
booktitle = {Proceedings of the 12th International Symposium on Visual Information Communication and Interaction},
articleno = {13},
numpages = {5},
keywords = {semantic understanding, text visualization, LDA method, word cloud},
location = {Shanghai, China},
series = {VINCI'2019}
}

@inproceedings{10.1145/3132847.3133153,
author = {Tan, Jiaxing and Kotov, Alexander and Pir Mohammadiani, Rojiar and Huo, Yumei},
title = {Sentence Retrieval with Sentiment-Specific Topical Anchoring for Review Summarization},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133153},
doi = {10.1145/3132847.3133153},
abstract = {We propose Topic Anchoring-based Review Summarization (TARS), a two-step extractive summarization method, which creates review summaries from the sentences that represent the most important aspects of a review. In the first step, the proposed method utilizes Topic Aspect Sentiment Model (TASM), a novel sentiment-topic model, to identify aspects of sentiment-specific topics in a collection of reviews. The output of TASM is utilized in the second step of TARS to rank review sentences based on how representative of the most important review aspects their words are. Qualitative and quantitative evaluation of review summaries using two collections indicate the effectiveness of structuring review summaries around aspects of sentiment-specific topics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2323–2326},
numpages = {4},
keywords = {opinion mining, topic models, text summarization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3078564.3078574,
author = {Sun, Feiqiang and Wang, Peng and Liu, Xiaoxia and Zhai, Lidong},
title = {A Recommendation Algorithm of We-Media Articles},
year = {2017},
isbn = {9781450352109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078564.3078574},
doi = {10.1145/3078564.3078574},
abstract = {Since the explosive growth1 of we-medias today, personalized recommendation is playing an increasingly important role. How to help users to find their target articles in vast amounts of data, has become a very challenging job. Deep learning, on the other hand, have been shown good results in the image processing, computer vision, natural language processing and other fields. But it's a relatively blank in the application of we-media articles recommendation. Combining with the new features of we-media articles, this paper puts forward a recommendation algorithm of we-media articles based on topic model, Latent Dirichlet Allocation (LDA), and deep learning algorithm, Recurrent Neural Networks (RNNs).},
booktitle = {Proceedings of the 6th International Conference on Information Engineering},
articleno = {12},
numpages = {5},
keywords = {Wei Mi, We-media article recommendation, Recommendation algorithm, RNNs, LSTM, LDA},
location = {Dalian Liaoning, China},
series = {ICIE '17}
}

@inproceedings{10.1145/3308558.3313617,
author = {Le, Tuan and Akoglu, Leman},
title = {ContraVis: Contrastive and Visual Topic Modeling for Comparing Document Collections},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313617},
doi = {10.1145/3308558.3313617},
abstract = {Given posts on 'abortion' and posts on 'religion' from a political forum, how can we find topics that are discriminative and those in common? In general, (1) how can we compare and contrast two or more different ('labeled') document collections? Moreover, (2) how can we visualize the data (in 2-d or 3-d) to best reflect the similarities and differences between the collections? We introduce (to the best of our knowledge) the first contrastive and visual topic model, called ContraVis, that jointly addresses both problems: (1) contrastive topic modeling, and (2) contrastive visualization. That is, ContraVis learns not only latent topics but also embeddings for the documents, topics and labels for visualization. ContraVis exhibits three key properties by design. It is (i) Contrastive: It enables comparative analysis of different document corpora by extracting latent discriminative and common topics across labeled documents; (ii) Visually-expressive: Different from numerous existing models, it also produces a visualization for all of the documents, labels, and the extracted topics, where proximity in the coordinate space is reflective of proximity in semantic space; (iii) Unified: It extracts topics and visual coordinates simultaneously under a joint model. Through extensive experiments on real-world datasets, we show ContraVis 's potential for providing visual contrastive analysis of multiple document collections. We show both qualitatively and quantitatively that ContraVis significantly outperforms both unsupervised and supervised state-of-the-art topic models in contrastive power, semantic coherence and visual effectiveness.},
booktitle = {The World Wide Web Conference},
pages = {928–938},
numpages = {11},
keywords = {comparative text mining, contrastive topic models, visualization},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1109/JCDL.2019.00124,
author = {Bainbridge, David and Nichols, David M. and Hinze, Annika and Downie, J. Stephen},
title = {Using the HTRC Data Capsule Model to Promote Reuse and Evolution of Experimental Analysis of Digital Library Data: A Case Study of Topic Modeling},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00124},
doi = {10.1109/JCDL.2019.00124},
abstract = {We report on a case-study to independently reproduce the work given in a publicly available blog on how to develop a topic model sourced from a collection of texts, where both the data set and source code used are readily available. More specifically, we detail the steps necessary---and the challenges that had to be overcome---to replicate the work using the HathiTrust Research Center's virtual machine Data Capsule platform. From this we make recommendations for authors to follow, based on the lessons learned. We also show that the Data Capsule model can be put to work in a way that is of benefit to those interested in supporting computational reproducibility within their organizations.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {463–464},
numpages = {2},
keywords = {digital libraries, experimental reproducibility, virtual machine},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.5555/3382225.3382360,
author = {Kalyoncu, Feyzullah and Zeydan, Engin and Yigit, Ibrahim Onuralp and Yildirim, Ahmet},
title = {A Customer Complaint Analysis Tool for Mobile Network Operators},
year = {2018},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Mobile Network Operators (MNOs) are eager to learn more about complaint behaviour of their subscribers. In this demo, we study topic modeling approach for extracting relevant problems experienced by subscribers of MNOs in Turkey and visualize the topic distributions using LDAvis data analytics tool. For building topic models using Latent Dirchlet Allocation (LDA), we have built customer complaint text dataset of subscriber complaints for each MNOs from Turkey's largest customer complaint website. The proposed analysis tool can be used as customer complaint analysis service by MNOs in Turkey to gain more insight. We have also validated our generated topic model using another dataset obtained from Turkey's largest online community website. Our results indicate similar and dissimilar topics of complaints as well as some of the distinctive problems of MNOs in Turkey based on their subscriber's experiences and feedback.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {609–612},
numpages = {4},
keywords = {text analytics, subscribers, complaints, MNOs, topic modeling},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3297156.3297229,
author = {Min, Daozhen and Huang, Lei},
title = {Research on Recommendation Methods Based on Sentiment Analysis and BTM Topic Modeling},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297229},
doi = {10.1145/3297156.3297229},
abstract = {With the rapid development of the Internet and e-commerce, the importance of recommendation algorithms has become increasingly prominent. Data sparsity and scoring dependence are problems with most current recommended algorithms. In this paper, we propose the recommendation model of SABTMCF (sentiment analysis and BTM collaborative filtering). Based on the traditional collaborative filtering algorithm, the sentiment analysis and BTM topic model are used to mine the review data to obtain the user's real potential emotional emotions and different attributes of the product. The scoring matrix of the theme can alleviate the above two problems, and then calculate the similarity of the user's emotional preferences to construct the recommendation model. The paper uses Dangdang's comment data set as experimental data, and the results show that the SABTMCF algorithm can improve the data sparse problem to a certain extent and has better recommendation accuracy.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {425–430},
numpages = {6},
keywords = {BTM, sentiment analysis, Recommended algorithm},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1145/3383972.3383979,
author = {Ma, Changyi and Tan, Boyun and Li, Wenye},
title = {Functional Regions Discovery and Evaluation Using Spatial and Semantic Data},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3383979},
doi = {10.1145/3383972.3383979},
abstract = {Urbanization of modern cities necessitates the division of different regions for different functions in city planning. And in turn, the intelligent identification of these functional regions has great business value and attracts considerable research interest. Specifically, in this paper, our work proposes an LDA based topic model to discover regions with different functions in a city, based on the semantic and spatial information, User Check-in Records and Point-of-Interests (POIs), respectively. Our model infers the thematic meaning of each region, and further identifies the intensity to illustrate our model consider both spatial distance and user density. We evaluate our model extensively on a benchmarked large-scale dataset, and compared our results model with several baseline methods in both thematic and spatial aspects. Experimental results exhibit the advantages of our proposed method and high potentials to be applied in practical applications.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {202–207},
numpages = {6},
keywords = {User Check-in Records, POI, LDA, Functional Regions},
location = {Shenzhen, China},
series = {ICMLC 2020}
}

@inproceedings{10.1145/3197026.3197043,
author = {Bunk, Stefan and Krestel, Ralf},
title = {WELDA: Enhancing Topic Models by Incorporating Local Word Context},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197043},
doi = {10.1145/3197026.3197043},
abstract = {The distributional hypothesis states that similar words tend to have similar contexts in which they occur. Word embedding models exploit this hypothesis by learning word vectors based on the local context of words. Probabilistic topic models on the other hand utilize word co-occurrences across documents to identify topically related words. Due to their complementary nature, these models define different notions of word similarity, which, when combined, can produce better topical representations. In this paper we propose WELDA, a new type of topic model, which combines word embeddings (WE) with latent Dirichlet allocation (LDA) to improve topic quality. We achieve this by estimating topic distributions in the word embedding space and exchanging selected topic words via Gibbs sampling from this space. We present an extensive evaluation showing that WELDA cuts runtime by at least 30% while outperforming other combined approaches with respect to topic coherence and for solving word intrusion tasks.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {293–302},
numpages = {10},
keywords = {word embeddings, document representations, topic models},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3106426.3106459,
author = {Zhou, Xujuan and Tao, Xiaohui and Rahman, Md Mostafijur and Zhang, Ji},
title = {Coupling Topic Modelling in Opinion Mining for Social Media Analysis},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106459},
doi = {10.1145/3106426.3106459},
abstract = {Many of social media platforms such as Facebook and Twitter make it easy for everyone to share their thoughts on literally anything. Topic and opinion detection in social media facilitates the identification of emerging societal trends, analysis of public reactions to policies and business products. In this paper, we proposed a new method that combines the opining mining and context-based topic modelling to analyse public opinions on social media data. Context based topic modelling is used to categorise data in groups and discover hidden communities in data group. The unwanted data group discovered by the topic model then will be discarded. A lexicon based opinion mining method will be applied to the remaining data groups to spot out the public sentiment about the entities. A set of Tweets data on Australian Federal Election 2010 was used in our experiments. Our experimental results demonstrate that, with the help of topic modelling, our social media analysis model is accurate and effective.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {533–540},
numpages = {8},
keywords = {social media analysis, opinion mining, topic modelling, online social networks},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3411764.3445425,
author = {Crisan, Anamaria and Correll, Michael},
title = {User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text Analytics},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445425},
doi = {10.1145/3411764.3445425},
abstract = {Topic models are widely used analysis techniques for clustering documents and surfacing thematic elements of text corpora. These models remain challenging to optimize and often require a “human-in-the-loop” approach where domain experts use their knowledge to steer and adjust. However, the fragility, incompleteness, and opacity of these models means even minor changes could induce large and potentially undesirable changes in resulting model. In this paper we conduct a simulation-based analysis of human-centered interactions with topic models, with the objective of measuring the sensitivity of topic models to common classes of user actions. We find that user interactions have impacts that differ in magnitude but often negatively affect the quality of the resulting modelling in a way that can be difficult for the user to evaluate. We suggest the incorporation of sensitivity and “multiverse” analyses to topic model interfaces to surface and overcome these deficiencies. Code and Data Availability: https://osf.io/zgqaw},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {600},
numpages = {16},
keywords = {text analytics, topic modelling, human-in-the-loop ML, unsupervised clustering},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3057282,
author = {Liang, Shangsong and Yilmaz, Emine and Shen, Hong and Rijke, Maarten De and Croft, W. Bruce},
title = {Search Result Diversification in Short Text Streams},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057282},
doi = {10.1145/3057282},
abstract = {We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method -- second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
articleno = {8},
numpages = {35},
keywords = {Diversity, data streams, ad hoc retrieval}
}

@article{10.5555/3122009.3242019,
author = {George, Clint P. and Doss, Hani},
title = {Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {5937–5974},
numpages = {38},
keywords = {model selection, latent dirichlet allocation, Markov chain monte carlo, empirical bayes inference, topic modelling}
}

@inproceedings{10.1145/3240876.3240922,
author = {Fu, Jie and Huang, Shucheng and Zhang, Tianzhu and Xu, Changsheng},
title = {Learning Semantic Topics for Domain-Adapted Textual Knowledge Transfer},
year = {2018},
isbn = {9781450365208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240876.3240922},
doi = {10.1145/3240876.3240922},
abstract = {Traditional text classification methods make a basic assumption: the training and test data are homologous, while this naive assumption may not hold in the real world. Hence, this paper studies the problem of domain-adapted news text classification, hereby a model is trained on labeled data from one source domain and is able to be deployed on the other. To realize the cross-domain text classification, we propose a domain-adapted text classification method based on topic model LDA and TextCNN model, named TextLDACNN. Specifically, our work calculates the topic similarity between source and target domain, which is severed as an effective constraint to regularize the training process and hence improve the generalization of the source model to the target domain. Text classifier trained with unsupervised topic feature representation clearly outperforms the baseline TextCNN model. The result shows that our method achieves an approximately 4.0% improvement compared to the state-of-the-art method.},
booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
articleno = {31},
numpages = {5},
keywords = {domain adaptation, textCNN, text classification, LDA},
location = {Nanjing, China},
series = {ICIMCS '18}
}

@inproceedings{10.1145/3126686.3126757,
author = {Chen, Sih-Huei and Wu, Shao-Hui and Lee, Yuan-Shan and Lo, Rocky and Wang, Jia-Ching},
title = {Hierarchical Representation Based on Bayesian Nonparametric Tree-Structured Mixture Model for Playing Technique Classification},
year = {2017},
isbn = {9781450354165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126686.3126757},
doi = {10.1145/3126686.3126757},
abstract = {This work develops a topic model-based hierarchical representation for identifying the latent characteristics behind the frame-level musical features. Frame-level features and music clips are regarded as acoustic words and acoustic documents, respectively. A Gaussian hierarchical latent Dirichlet allocation (G-hLDA) is proposed to find the latent topics behind the acoustic document. The G-hLDA directly handles the continuous features instead of transforming them into discrete words, reducing information loss from discretization-based vector quantization. Specially, each latent topic that is identified by G-hLDA is represented as a node in the infinitely deep, infinitely branching tree. For a music clip, the number of its acoustic words at each node is computed to form the hierarchical representation. The proposed representation hierarchically captures not only the shared components but also the unique components among music clips, resulting in improved performance. The experimental results on the guitar playing technique database demonstrate that the proposed method outperforms baselines.},
booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017},
pages = {537–543},
numpages = {7},
keywords = {hierarchical representation, playing technique classification},
location = {Mountain View, California, USA},
series = {Thematic Workshops '17}
}

@inproceedings{10.1145/3213586.3225236,
author = {Hashavit, Anat and Tepper, Naama and Ronen, Inbal and Leiba, Lior and Cohen, Amir DN},
title = {Implicit User Modeling in Group Chat},
year = {2018},
isbn = {9781450357845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213586.3225236},
doi = {10.1145/3213586.3225236},
abstract = {In recent years, enterprise group chat collaboration tools such as Slack, IBM's Watson Workspace and Microsoft Teams, have presented unprecedented growth. With all the potential benefits of these tools " productivity increase and improved group communication " come significant challenges. Specifically, users find it hard to focus their attention on content that is relevant to them due to the load of conversational content. This load can be handled by personalized content presentation and summarization mitigated by user profiling. We present an unsupervised approach for implicitly modeling group chat users through a combination of a probabilistic topic model and social analysis. We evaluate our approach by testing it on a task of conversation participation prediction, serving as a proxy for anticipating user interests, and show that by utilizing our approach, a system successfully predicts users participation in conversations. We further analyze the contribution of the various user model components and show them to be significant.},
booktitle = {Adjunct Publication of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {275–280},
numpages = {6},
keywords = {summarization, group chat, unsupervised learning, multiparticipant chat},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inproceedings{10.1145/3460210.3493586,
author = {Harrando, Ismail and Troncy, Rapha\"{e}l},
title = {Discovering Interpretable Topics by Leveraging Common Sense Knowledge},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493586},
doi = {10.1145/3460210.3493586},
abstract = {Traditional topic modeling approaches generally rely on document-term co-occurrence statistics to find latent topics in a collection of documents. However, relying only on such statistics can yield incoherent or hard to interpret results for the end-users in many applications where the interest lies in interpreting the resulting topics (e.g. labeling documents, comparing corpora, guiding content exploration, etc.). In this work, we propose to leverage external common sense knowledge, i.e. information from the real world beyond word co-occurrence, to find topics that are more coherent and more easily interpretable by humans. We introduce the Common Sense Topic Model (CSTM), a novel and efficient approach that augments clustering with knowledge extracted from the ConceptNet knowledge graph. We evaluate this approach on several datasets alongside commonly used models using both automatic and human evaluation, and we show how it shows superior affinity to human judgement. The code for the experiments as well as the training data and human evaluation are available at https://github.com/D2KLab/CSTM.},
booktitle = {Proceedings of the 11th on Knowledge Capture Conference},
pages = {265–268},
numpages = {4},
keywords = {topic modeling, interpretable topics, common sense knowledge},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@inproceedings{10.1145/3132847.3133145,
author = {Lin, Junjie and Mao, Wenji and Zhang, Yuhao},
title = {An Enhanced Topic Modeling Approach to Multiple Stance Identification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133145},
doi = {10.1145/3132847.3133145},
abstract = {People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2167–2170},
numpages = {4},
keywords = {topic modeling, constrained Nonnegative Matrix Factorization, Multiple stance identification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3220162.3220187,
author = {Cai, Weihong and Chen, Yecong and Liu, Jianquan and Hu, Jiang},
title = {Analyzing Accessed Content Sequences with HDP-Based Models},
year = {2018},
isbn = {9781450364577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220162.3220187},
doi = {10.1145/3220162.3220187},
abstract = {The need for auditing network users' behaviors based on their accessed content urges for a new method for modeling and analysis. Topic model is a probabilistic generative model for data mixture of varying length that can extract features from individual instances of content. The hidden Markov model can be used for analyzing sequences of content. By introducing Hierarchical Dirichlet Processes on top of topic mixtures and HMMs, we can tackle down the challenges of unknown numbers of mixtures in both models by resorting to nonparametric approach. We employ variational inference for model calculation, and cluster the extracted user patterns in the form of HMM parameters to detect potential anomalies in access behavior. The proposed scheme is then verified by carrying out experiments on the LAN users' log data in an institute.},
booktitle = {Proceedings of the 3rd International Conference on Multimedia Systems and Signal Processing},
pages = {142–149},
numpages = {8},
keywords = {behavioral analytics, hierarchical Dirichlet Process, variational inference, anomaly detection},
location = {Shenzhen, China},
series = {ICMSSP '18}
}

@inproceedings{10.1145/3383313.3412207,
author = {Pe\~{n}a, Francisco J. and O'Reilly-Morgan, Diarmuid and Tragos, Elias Z. and Hurley, Neil and Duriakova, Erika and Smyth, Barry and Lawlor, Aonghus},
title = {Combining Rating and Review Data by Initializing Latent Factor Models with Topic Models for Top-N Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412207},
doi = {10.1145/3383313.3412207},
abstract = {Nowadays we commonly have multiple sources of data associated with items. Users may provide numerical ratings, or implicit interactions, but may also provide textual reviews. Although many algorithms have been proposed to jointly learn a model over both interactions and textual data, there is room to improve the many factorization models that are proven to work well on interactions data, but are not designed to exploit textual information. Our focus in this work is to propose a simple, yet easily applicable and effective, method to incorporate review data into such factorization models. In particular, we propose to build the user and item embeddings within the topic space of a topic model learned from the review data. This has several advantages: we observe that initializing the user and item embeddings in topic space leads to faster convergence of the factorization algorithm to a model that out-performs models initialized randomly, or with other state-of-the-art initialization strategies. Moreover, constraining user and item factors to topic space allows for the learning of an interpretable model that users can visualise.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {438–443},
numpages = {6},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3240323.3240363,
author = {Al-Ghossein, Marie and Murena, Pierre-Alexandre and Abdessalem, Talel and Barr\'{e}, Anthony and Cornu\'{e}jols, Antoine},
title = {Adaptive Collaborative Topic Modeling for Online Recommendation},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240363},
doi = {10.1145/3240323.3240363},
abstract = {Collaborative filtering (CF) mainly suffers from rating sparsity and from the cold-start problem. Auxiliary information like texts and images has been leveraged to alleviate these problems, resulting in hybrid recommender systems (RS). Due to the abundance of data continuously generated in real-world applications, it has become essential to design online RS that are able to handle user feedback and the availability of new items in real-time. These systems are also required to adapt to drifts when a change in the data distribution is detected. In this paper, we propose an adaptive collaborative topic modeling approach, CoAWILDA, as a hybrid system relying on adaptive online Latent Dirichlet Allocation (AWILDA) to model newly available items arriving as a document stream and incremental matrix factorization for CF. The topic model is maintained up-to-date in an online fashion and is retrained in batch when a drift is detected using documents automatically selected by an adaptive windowing technique. Our experiments on real-world datasets prove the effectiveness of our approach for online recommendation.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {338–346},
numpages = {9},
keywords = {topic modeling, collaborative fittering, concept drift, online recommendation},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.1145/3184558.3186926,
author = {Jiang, Zhile and Yu, Shuai and Qu, Qiang and Yang, Min and Luo, Junyu and Liu, Juncheng},
title = {Multi-Task Learning for Author Profiling with Hierarchical Features},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186926},
doi = {10.1145/3184558.3186926},
abstract = {Author profiling is an important but challenging task. In this paper, we propose a novel Multi-Task learning framework for Author Profiling (MTAP), in which a document modeling module is shared across three different author profiling tasks (i.e., age, gender and job classification tasks). To further boost author profiling, we integrate hierarchical features learned by different models. Concretely, we employ CNN, LSTM and topic model to learn the character-level, word-level and topic-level features, respectively. MTAP thus leverages the benefits of supervised deep neural neural networks as well as an unsupervised probabilistic generative model to enhance the document representation learning. Experimental results on a real-life blog dataset show that MTAP has robust superiority over competitors and sets state-of-the-art for all the three author profiling tasks},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {55–56},
numpages = {2},
keywords = {multi-task learning, author profiling, hierarchical features},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3085504.3085527,
author = {Sun, Guandong and Xiong, Yun and Zhu, Yangyong},
title = {How the Passengers Flow in Complex Metro Networks?},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085504.3085527},
doi = {10.1145/3085504.3085527},
abstract = {The understanding of passenger flow assignment in metro network is critical for public transit management. However, the route chosen by one passenger is difficult to be directly obtained according to the transaction records only including each trip's tap-in and tap-out time stamp and stations. In this paper, a two-stage framework for calculating passenger flow assignment in complex metro networks is proposed, named PaFA (Passenger Flow Assignment), by using smart card data. First, we design an acceleration search process to obtain all routes for each O-D pair and select the candidate routes under rules. Then, inspired by topic model, we realize similar latent relationships also can be found among O-D pair, candidate routes and passenger's travel time. Along this line, we obtain the distribution of passenger flow in different candidate routes. Finally, a comprehensive evaluation with real-world data is conducted. The results demonstrate the enhanced performance of the proposed method.},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {23},
numpages = {6},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@inproceedings{10.1145/3132847.3132969,
author = {Sun, Jiabao and Xu, Jiajie and Zheng, Kai and Liu, Chengfei},
title = {Interactive Spatial Keyword Querying with Semantics},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132969},
doi = {10.1145/3132847.3132969},
abstract = {Conventional spatial keyword queries confront the difficulty of returning desired objects that are synonyms but morphologically different to query keywords. To overcome this flaw, this paper investigates the interactive spatial keyword querying with semantics. It aims to enhance the conventional queries by not only making sense of the query keywords, but also refining the understanding of query semantics through interactions. On top of the probabilistic topic model, a novel interactive strategy is proposed to precisely infer the latent query semantics by learning from user feedbacks. In each interaction, the returned objects are carefully selected to ensure effective inference of user intended query semantics. Query processing is carried out on a small candidate object set at each round of interaction, and the whole querying process terminates when the latent query semantics learned from user feedback becomes explicit enough. The experimental results on real check-in dataset demonstrates that the quality of results has been significantly improved through limited number of interactions.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1727–1736},
numpages = {10},
keywords = {spatial database, interactive query, spatial keyword query},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1613/jair.1.13550,
author = {Javed, Rana Tallal and Nasir, Osama and Borit, Melania and Vanh\'{e}e, Lo\"{\i}s and Zea, Elias and Gupta, Shivam and Vinuesa, Ricardo and Qadir, Junaid},
title = {Get out of the BAG! Silos in AI Ethics Education: Unsupervised Topic Modeling Analysis of Global AI Curricula},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13550},
doi = {10.1613/jair.1.13550},
abstract = {The domain of Artificial Intelligence (AI) ethics is not new, with discussions going back at least 40 years. Teaching the principles and requirements of ethical AI to students is considered an essential part of this domain, with an increasing number of technical AI courses taught at several higher-education institutions around the globe including content related to ethics. By using Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, this study uncovers topics in teaching ethics in AI courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to Bloom’s taxonomy. In this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from North American universities, 11 from Asia, 36 from Europe, and 10 from other regions. Based on this analysis, we were able to synthesize a model of teaching approaches, which we call BAG (Build, Assess, and Govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. We critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. We challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how. This article appears in the AI &amp; Society track.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {33},
keywords = {discourse modelling, data mining, scientific discovery, philosophical foundations}
}

@inproceedings{10.1145/3416028.3416043,
author = {Sun, Xin and An, Xin and Xu, Shuo and Hao, Liyuan and Li, Jinghong},
title = {Identifying Important Citations by Incorporating Generative Model into Discriminative Classifiers},
year = {2020},
isbn = {9781450375467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416028.3416043},
doi = {10.1145/3416028.3416043},
abstract = {Since Budapest open access initiative was launched, a large number of full-text articles in the format of XML are available, which further promotes the technology management on the basis of citation context analysis, such as emerging technology forecasting, technology opportunity detection and innovation measurement. Inspired by the success of kernel functions utilized to promote the performance of SVM (Support Vector Machine) model, we explore the potential of combining generative and discriminative models for the task of citation function and importance classification. In more details, generative features are generated from a topic model, Citation Influence Model (CIM), and then fed to two state-of-the-art discriminative models, SVM and RF (Random Forest), with other 13 features derived from citation contexts directly to identify important citations from a brand new perspective. Extensive experimental results on a dataset from the Association for Computational Linguistics anthology indicate that our approach outperforms the counterparts.},
booktitle = {Proceedings of the 2020 3rd International Conference on Information Management and Management Science},
pages = {72–76},
numpages = {5},
keywords = {Citation context analysis, Discriminative model, Generative model, Important citations},
location = {London, United Kingdom},
series = {IMMS 2020}
}

@inproceedings{10.1145/3297662.3365789,
author = {Albalawi, Rania and Yeap, Tet Hin and Benyoucef, Morad},
title = {Toward A Real-Time Social Recommendation System},
year = {2019},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365789},
doi = {10.1145/3297662.3365789},
abstract = {Recent research has investigated approaches and models to produce optimal results in social recommendation systems (SRSs) particularly in text-based form. The aim is to analyze the user generated-content (UGC) to suggest appropriate recommendations to interested users. However, users are often not satisfied with the initial recommendations because some models do not elicit their preferences at the beginning of the interaction nor do they understand their actual needs. In this paper, we propose a real-time SRSs called ChatWithRec that aims to improve the accuracy of recommendations by analyzing the user's contextual conversation dynamically, detect the topic, and then match it with a suitable advertisement. We used the Latent Dirichlet Allocation topic model (LDA) to analyze the user's conversation and perceive topics. We evaluated our system by applying several metrics like coherence, and F-score to evaluate the performance of ChatWithRec recommendation system. The results are encouraging, indicating that the system is fast, satisfies users by getting exactly what they seek in their conversation flow.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {336–340},
numpages = {5},
keywords = {Topic Modeling, LDA, Social Recommendation System, NLP},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3106237.3106284,
author = {Murali, Vijayaraghavan and Chaudhuri, Swarat and Jermaine, Chris},
title = {Bayesian Specification Learning for Finding API Usage Errors},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106284},
doi = {10.1145/3106237.3106284},
abstract = {We present a Bayesian framework for learning probabilistic specifications from large, unstructured code corpora, and then using these specifications to statically detect anomalous, hence likely buggy, program behavior. Our key insight is to build a statistical model that correlates all specifications hidden inside a corpus with the syntax and observed behavior of programs that implement these specifications. During the analysis of a particular program, this model is conditioned into a posterior distribution that prioritizes specifications that are relevant to the program. The problem of finding anomalies is now framed quantitatively, as a problem of computing a distance between a "reference distribution" over program behaviors that our model expects from the program, and the distribution over behaviors that the program actually produces. We implement our ideas in a system, called Salento, for finding anomalous API usage in Android programs. Salento learns specifications using a combination of a topic model and a neural network model. Our encouraging experimental results show that the system can automatically discover subtle errors in Android applications in the wild, and has high precision and recall compared to competing probabilistic approaches.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {151–162},
numpages = {12},
keywords = {Specification Learning, Bug Finding, Anomaly Detection, APIs},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3397271.3401409,
author = {Noh, Yunseok and Shin, Yongmin and Park, Junmo and Kim, A-Yeong and Choi, Su Jeong and Song, Hyun-Je and Park, Seong-Bae and Park, Seyoung},
title = {WIRE: An Automated Report Generation System Using Topical and Temporal Summarization},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401409},
doi = {10.1145/3397271.3401409},
abstract = {The demand for a tool for summarizing emerging topics is increasing in modern life since the tool can deliver well-organized information to its users. Even though there are already a number of successful search systems, the system which automatically summarizes and organizes the content of emerging topics is still in its infancy. To fulfill such demand, we introduce an automated report generation system that generates a well-summarized human-readable report for emerging topics. In this report generation system, emerging topics are automatically discovered by a topic model and news articles are indexed by the discovered topics. Then, a topical summary and a timeline summary for each topic is generated by a topical multi-document summarizer and a timeline summarizer respectively. In order to enhance the apprehensibility of the users, the proposed report system provides two report modes. One is Today's Briefing which summarizes five discovered topics of every day, and the other is Full Report which shows a long-term view of each topic with a detailed topical summary and an important event timeline.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2169–2172},
numpages = {4},
keywords = {report generation, timeline summarization, image retrieval, text summarization, topic discovery, deep neural networks, text retrieval},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3418283,
author = {Jiang, Di and Tong, Yongxin and Song, Yuanfeng and Wu, Xueyang and Zhao, Weiwei and Peng, Jinhua and Lian, Rongzhong and Xu, Qian and Yang, Qiang},
title = {Industrial Federated Topic Modeling},
year = {2021},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3418283},
doi = {10.1145/3418283},
abstract = {Probabilistic topic modeling has been applied in a variety of industrial applications. Training a high-quality model usually requires a massive amount of data to provide comprehensive co-occurrence information for the model to learn. However, industrial data such as medical or financial records are often proprietary or sensitive, which precludes uploading to data centers. Hence, training topic models in industrial scenarios using conventional approaches faces a dilemma: A party (i.e., a company or institute) has to either tolerate data scarcity or sacrifice data privacy. In this article, we propose a framework named Industrial Federated Topic Modeling (iFTM), in which multiple parties collaboratively train a high-quality topic model by simultaneously alleviating data scarcity and maintaining immunity to privacy adversaries. iFTM is inspired by federated learning, supports two representative topic models (i.e., Latent Dirichlet Allocation and SentenceLDA) in industrial applications, and consists of novel techniques such as private Metropolis-Hastings, topic-wise normalization, and heterogeneous model integration. We conduct quantitative evaluations to verify the effectiveness of iFTM and deploy iFTM in two real-life applications to demonstrate its utility. Experimental results verify iFTM’s superiority over conventional topic modeling.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {2},
numpages = {22},
keywords = {differential privacy, Topic models, federated learning}
}

@inproceedings{10.1145/3172944.3172997,
author = {Zhang, Yanxia and Olenick, Jeffrey and Chang, Chu-Hsiang and Kozlowski, Steve W. J. and Hung, Hayley},
title = {The I in Team: Mining Personal Social Interaction Routine with Topic Models from Long-Term Team Data},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172997},
doi = {10.1145/3172944.3172997},
abstract = {Social interaction plays a key role in assessing teamwork and collaboration. It becomes particularly critical in team performance when coupled with isolated, confined, and extreme conditions such as undersea missions. This work investigates how social interactions of individual members in a small team evolve during the course of a long duration mission. We propose to use a topic model to mine individual social interaction patterns and examine how the dynamics of these patterns have an effect on self-assessment of mood and team cohesion. Specifically, we analyzed data from a 6-person crew wearing Sociometric badges over a 4-month mission. Our results show that our method can extract the latent structure of social contexts without supervision. We demonstrate how the extracted patterns based on probabilistic models can provide insights on common behaviors at various temporal resolutions and exhibit links with self-report affective states and team cohesion.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {421–426},
numpages = {6},
keywords = {wearable, team dynamics, machine learning},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3162957.3162996,
author = {Murfi, Hendri},
title = {Accuracy of Separable Nonnegative Matrix Factorization for Topic Extraction},
year = {2017},
isbn = {9781450353656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3162957.3162996},
doi = {10.1145/3162957.3162996},
abstract = {Topic extraction is an automatic method to extract topics in textual data. The popular method of topic extraction is latent Dirichlet allocation (LDA) which is a probabilistic topic model. Because of some limitations of learning the model parameters, e.g. NP-hard, several researchers continue the work to design methods with polynomial complexities. The developing alternative approach is the nonnegative matrix factorization (NMF) based method. Under a separability assumption, a direct method that runs in polynomial time is proposed. In general, this algorithm works in three steps: first, generating a word cooccurrence matrix, choosing anchor words for each topic, and then in the recovery step, it directly reconstructs the topics given the anchor words. In this paper, we examine the accuracy of the separable nonnegative matrix factorization (SNMF). Firstly the accuracy of SNMF is strongly influenced by the anchor words. In this case, the accuracy of SNMF is significantly improved when we find the anchr words in Eigenspace, instead of random space. Moreover, SNMF gives the higher accuracy than LDA, however, the lower accuracy than NMF.},
booktitle = {Proceedings of the 3rd International Conference on Communication and Information Processing},
pages = {226–230},
numpages = {5},
keywords = {topic extraction, singular value decomposition, separable nonnegative matrix factorization, eigenspace},
location = {Tokyo, Japan},
series = {ICCIP '17}
}

@inproceedings{10.1145/3357384.3358022,
author = {Chu, Zhendong and Cai, Renqin and Wang, Hongning},
title = {Accounting for Temporal Dynamics in Document Streams},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358022},
doi = {10.1145/3357384.3358022},
abstract = {Textual information, such as news articles, social media, and online forum discussions, often comes in a form of sequential text streams. Events happening in the real world trigger a set of articles talking about them or related events over a period of time. In the meanwhile, even one event is fading out, another related event could raise public attention. Hence, it is important to leverage the information about how topics influence each other over time to obtain a better understanding and modeling of document streams. In this paper, we explicitly model mutual influence among topics over time, with the purpose to better understand how events emerge, fade and inherit. We propose a temporal point process model, referred to as Correlated Temporal Topic Model (CoTT), to capture the temporal dynamics in a latent topic space. Our model allows for efficient online inference, scaling to continuous time document streams. Extensive experiments on real-world data reveal the effectiveness of our model in recovering meaningful temporal dependency structure among topics and documents.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1813–1822},
numpages = {10},
keywords = {temporal topic modeling, online clustering, hawkes process},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1145/3274290,
author = {Asthana, Sumit and Halfaker, Aaron},
title = {With Few Eyes, All Hoaxes Are Deep},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274290},
doi = {10.1145/3274290},
abstract = {Quality control is critical to open production communities like Wikipedia. Wikipedia editors enact border quality control with edits (counter-vandalism) and new article creations (new page patrolling) shortly after they are saved. In this paper, we describe a long-standing set of inefficiencies that have plagued new page patrolling by drawing a contrast to the more efficient, distributed processes for counter-vandalism. Further, to address this issue, we demonstrate an effective automated topic model based on a labeling strategy that leverages a folksonomy developed by subject specific working groups in Wikipedia (WikiProject tags) and a flexible ontology (WikiProjects Directory) to arrive at a hierarchical and uniform label set. We are able to attain very high fitness measures (macro ROC-AUC: 95.2%, macro PR-AUC: 74.5%) and real-time performance using word2vec-based features. Finally, we present a proposal for how incorporating this model into current tools will shift the dynamics of new article review positively.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {21},
numpages = {18},
keywords = {collaborative review, social recommendation, wikipedia, topic modeling}
}

@inproceedings{10.1145/3106426.3106494,
author = {Xu, Ke and Cai, Yi and Min, Huaqing and Zheng, Xushen and Xie, Haoran and Wong, Tak-Lam},
title = {UIS-LDA: A User Recommendation Based on Social Connections and Interests of Users in Uni-Directional Social Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106494},
doi = {10.1145/3106426.3106494},
abstract = {The rapid growth of population has posed a challenge to people for discovering new followees in uni-directional social networks. Intuitively, a user's adoption of others as followees may motivated by her interest as well as social connection. Therefore, it is worth-while to consider both factors at the same time for better recommendations. Previous recommender works on implicit follow or not feedbacks become unqualified, mainly because of the coarse users' preferences inferring, which cannot distinguish whether one follows the other is based on her social connection or individual interest. In this paper, we present a new user recommendation method which is capable of recommending candidate followees who have similar interest and closer social connection relevant to a target user. As its core, a novel topic model namely UIS-LDA is designed to jointly model a user's preferences with respect to the set of latent interest topics and social topics. The experiments using Twitter dataset proves that our proposed method effective in improving the Precision, Conversion Rate F1 score and NDCG.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {260–265},
numpages = {6},
keywords = {user recommendation, uni-directional social networks, topic modeling},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3220228.3220263,
author = {Hamdy, Abeer and Elsayed, Mohamed},
title = {Topic Modelling for Automatic Selection of Software Design Patterns},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220263},
doi = {10.1145/3220228.3220263},
abstract = {Design pattern is a high-quality and reusable solution to a recurring software design problem. It is considered an important concept in the software engineering field due to its ability to enhance some of the quality attributes of the software systems including maintainability and extensibility. However, novice developers need to be provided by a tool to assist them in selecting the fit design pattern to solve a design problem. The paper proposes a novel approach for the automatic selection of the fit design pattern. This approach is based on using Latent Dirichlet Allocation (LDA) topic model. The topic is a set of words that often appear together. LDA is able to relate words with similar meaning and to differentiate between uses of words with multiple meanings. In this paper LDA is used to analyze the textual descriptions of design patterns and extract the topics then discover the similarity between the target problem scenario and the collection of patterns using Improved Sqrt-Cosine similarity measure (ISCS). The proposed approach was evaluated using Gang of four design patterns. The experimental results showed that the proposed approach outperforms approach based on the traditional vector space model of Unigrams.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {41–46},
numpages = {6},
keywords = {DP recommendation, LDA and vector space model, gang of four, topic modelling, information retrieval, design pattern selection, text mining},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3164541.3164616,
author = {Minami, Daichi and Ushijima, Mio and Ushiama, Taketoshi},
title = {How Do Viewers React to Drama? Extraction of Scene Features of Dramas from Live Commentary Tweets},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164616},
doi = {10.1145/3164541.3164616},
abstract = {TV drama has various genres such as "romantic" and "crime." However, recent TV dramas cannot be categorized into a single genre, and features of a variety of genres sometimes coexist. In this study, we analyze the user's reaction according to the progress of a TV drama from live commentary tweets and clarify the features of the scene. A "development pattern" is extracted based on the characteristics of scenes, and a clustering of TV dramas is performed, which is different from clustering based on genre. The features of each scene in dramas is modeled by a topic model. The topic distribution of a scene is extracted using LDA as one document by combining the live comments posted during one scene. The "development pattern" of a drama is represented as the wave of the topic connecting the topic distribution of each scene.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {87},
numpages = {4},
keywords = {Social viewing, Scene feature, Live tweets, LDA},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@inproceedings{10.1145/3269206.3269256,
author = {Chen, Guandan and Xu, Nan and Mao, Weiji},
title = {An Encoder-Memory-Decoder Framework for Sub-Event Detection in Social Media},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269256},
doi = {10.1145/3269206.3269256},
abstract = {Sub-event detection can help faster and deeper understanding of an event by providing human-friendly clusters, and thus has become an important research topic in Web mining and knowledge management. In existing sub-event detection methods, clustering based methods are brittle for using heuristic similarity metric to judge whether documents belong to the same sub-event, while topic model based methods are limited to the bag of words assumption. To overcome these drawbacks in previous research, in this paper, we propose an encoder-memory-decoder framework for sub-event detection. Our model learns document and sub-event representations suitable for the similarity metric in a data-driven manner, and transforms sub-event detection into selecting the most proper sub-event representation that can maximize text reconstruction probability. Considering the case of over-fitting, we also apply transfer learning in our model. To the best of our knowledge, our model is the first to develop an unsupervised deep neural model for sub-event detection. We use Twitter as an examplar social media platform for our study, and experimental results show that our model outperforms baseline methods for sub-event detection.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1575–1578},
numpages = {4},
keywords = {sub-event detection, deep neural network, encoder-memory-decoder framework},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3123266.3123369,
author = {Fan, Mengdi and Wang, Wenmin and Dong, Peilei and Han, Liang and Wang, Ronggang and Li, Ge},
title = {Cross-Media Retrieval by Learning Rich Semantic Embeddings of Multimedia},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123369},
doi = {10.1145/3123266.3123369},
abstract = {Cross-media retrieval aims at seeking the semantic association between different media types. Most existing methods paid much attention on learning mapping functions or finding the optimal spaces, but neglected how people accurately cognize images and texts. This paper proposes a brain inspired cross-media retrieval framework to learn rich semantic embeddings of multimedia. Different from directly using off-the-shelf image features, we combine the visual and descriptive senses for an image from the view of human perception via a joint model, called multi-sensory fusion network (MSFN). A topic model based TextNet maps texts into the same semantic space as images according to their shared ground truth labels. Moreover, in order to overcome the limitations of insufficient data for training neural networks and less complexity in text form, we introduce a large-scale image-text dataset, called Britannica dataset. Extensive experiments show the effectiveness of our framework for different lengths of texts on three benchmark datasets as well as Britannica dataset. Most of all, we report the best known average results of Img2Text and Text2Img compared with several state-of-the-art methods.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1698–1706},
numpages = {9},
keywords = {cross-media retrieval, multi-sensory fusion, rich semantic embeddings, textnet},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3184558.3186571,
author = {Kapugama Geeganage, Dakshi Tharanga},
title = {Concept Embedded Topic Modeling Technique},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186571},
doi = {10.1145/3184558.3186571},
abstract = {Text contents are overloaded with the digitization of the data and new contents are transmitted through many sources by generating a large volume of information, which spreads all over the world through different communication media. Therefore, text data is available everywhere and reading, understanding and analysing the text data has become a main activity in daily routine. With the increment of the volume and the variety of information, organizing and searching, the required information has become vital. Topic modelling is the state of the art for information organization, understanding and extracting the content. Most of the prevailing topic models use the probabilistic approaches and consider the frequency and the co-occurrence to discover the topics from collections of documents. The proposed research aims to address the existing problems of topic modeling by introducing a concept embedded topic model which generates the most relevant and meaningful topics by understanding the content. The research includes approaches to understand the semantic elements from the content, domain identification of concepts and provide most suitable topics without getting the number of topics from the user beforehand. Capturing the semantics of document collections and generating the most related set of topics according to the actual meaning will be the significance of this research.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {831–835},
numpages = {5},
keywords = {topic modeling, semantics, concepts},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3442381.3450045,
author = {Xie, Qianqian and Huang, Jimin and Du, Pan and Peng, Min and Nie, Jian-Yun},
title = {Graph Topic Neural Network for Document Representation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450045},
doi = {10.1145/3442381.3450045},
abstract = {Graph Neural Networks (GNNs) such as GCN can effectively learn document representations via the semantic relation graph among documents and words. However, despite a few exceptions, most of the previous work in this line of research does not consider the underlying topical semantics inherited in document contents and the relation graph, making the representations less effective and hard to interpret. In a few recent studies trying to incorporate latent topics into GNNs, the topics have been learned independently from the relation graph modeling. Intuitively, topic extraction can benefit much from the information propagation of the relation graph structure - directly and indirectly connected documents and words have similar topics. In this paper, we propose a novel Graph Topic Neural Network (GTNN) model to mine latent topic semantics for interpretable document representation learning, taking into account the document-document, document-word, and word-word relationships in the graph. We also show that our model can be viewed as semi-amortized inference for relational topic model based on Poisson distribution, with high order correlations. We test our model in several settings: unsupervised, semi-supervised, and supervised representation learning, for both connected and unconnected documents. In all the cases, our model outperforms the state-of-the-art models for these tasks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3055–3065},
numpages = {11},
keywords = {topic models, document representation, graph neural networks},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3377713.3377786,
author = {Liu, Gang and Zhang, Hanmo and Liu, Wangyang and Cao, Yang and Fu, Weiping},
title = {Research on Paper Intelligent Plagiarism Detection Method Based on Idea Tendency},
year = {2019},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377786},
doi = {10.1145/3377713.3377786},
abstract = {This paper proposes a method that analyzes the tendency of the paper for the originality detection of papers. This method is aiming at the behavior of idea plagiarism. The plagiarism detection task can be divided into three steps which are retrieval of candidate source document set, selection of source document set and post processing. The texts are modeled by the topic model and then are divided into several topic blocks by combining the topic modeling with the classical text segmentation method TextTiling. Then, the topic blocks which may be the source block of plagiarism are filtered out by calculating topic similarities among these topic fragments. On this basis, topic words of the topic block to be measured are extracted and its idea tendency aiming at each topic word are analyzed by the sentiment analysis method based on the dictionary. And further the source topic block set is filtered out based on the idea tendency Finally, a method of evaluating the original degree of the text is given for the intelligent plagiarism detection by the fusion of topic similarity and the result of the analysis of idea tendencies. Through experimental verification, this method can effectively evaluate the originality of the paper and has a good accuracy.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {433–438},
numpages = {6},
keywords = {Text Segmentation, Plagiarism detection, Idea Tendency, Topic modeling},
location = {Sanya, China},
series = {ACAI 2019}
}

@inproceedings{10.1145/3178876.3186009,
author = {Shi, Tian and Kang, Kyeongpil and Choo, Jaegul and Reddy, Chandan K.},
title = {Short-Text Topic Modeling via Non-Negative Matrix Factorization Enriched with Local Word-Context Correlations},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186009},
doi = {10.1145/3178876.3186009},
abstract = {Being a prevalent form of social communications on the Internet, billions of short texts are generated everyday. Discovering knowledge from them has gained a lot of interest from both industry and academia. The short texts have a limited contextual information, and they are sparse, noisy and ambiguous, and hence, automatically learning topics from them remains an important challenge. To tackle this problem, in this paper, we propose a semantics-assisted non-negative matrix factorization (SeaNMF) model to discover topics for the short texts. It effectively incorporates the word-context semantic correlations into the model, where the semantic relationships between the words and their contexts are learned from the skip-gram view of the corpus. The SeaNMF model is solved using a block coordinate descent algorithm. We also develop a sparse variant of the SeaNMF model which can achieve a better model interpretability. Extensive quantitative evaluations on various real-world short text datasets demonstrate the superior performance of the proposed models over several other state-of-the-art methods in terms of topic coherence and classification accuracy. The qualitative semantic analysis demonstrates the interpretability of our models by discovering meaningful and consistent topics. With a simple formulation and the superior performance, SeaNMF can be an effective standard topic model for short texts.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1105–1114},
numpages = {10},
keywords = {word embedding, short texts, non-negative matrix factorization, topic modeling},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3132847.3133122,
author = {Wang, Cheng and Fang, Yujuan and Tan, Zheng and He, Yuan},
title = {Improving the Gain of Visual Perceptual Behaviour on Topic Modeling for Text Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133122},
doi = {10.1145/3132847.3133122},
abstract = {Internet information services have been greatly improved profiting from the growing performance of interest mining technology. Visual perceptual behaviours, a new hotspot of mining user's interests, have resulted in great gains in some typical Internet information services, e.g., information retrieval and recommendation. It is validated that combining the subjective visual perceptual behaviours with the objective contents can significantly improve these services' performance. However, the existing methods usually treat the contents and visual perceptual behaviours as two independent parts in the calculating process. The gain of visual perceptual behaviours has not been fully exploited. In this paper, we mainly aim at improving the gain of visual perceptual behaviour for text recommendation, by integrating the objective contents with subjective visual perceptual behaviours. We investigate the correlation between user's reading interests and records of real-time interaction on texts, and then design a real-time visual perceptual behaviour based method for text recommendation, which is able to: (1) build a joint interest model, called ViP-LDA (Visual Perceptual LDA), by integrating the user's visual perceptual behaviours into topic model; (2) make more accurate text recommendation based on ViP-LDA with feedback adjustment. Several experiments on a real data set are implemented to demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2339–2342},
numpages = {4},
keywords = {visual perceptual behaviour, interest model, vip-lda, text recommendation, eye tracking, lda},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3534678.3539310,
author = {Zhang, Delvin Ce and Lauw, Hady W.},
title = {Variational Graph Author Topic Modeling},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539310},
doi = {10.1145/3534678.3539310},
abstract = {While Variational Graph Auto-Encoder (VGAE) has presented promising ability to learn representations for documents, most existing VGAE methods do not model a latent topic structure and therefore lack semantic interpretability. Exploring hidden topics within documents and discovering key words associated with each topic allow us to develop a semantic interpretation of the corpus. Moreover, documents are usually associated with authors. For example, news reports have journalists specializing in writing certain type of events, academic papers have authors with expertise in certain research topics, etc. Modeling authorship information could benefit topic modeling, since documents by the same authors tend to reveal similar semantics. This observation also holds for documents published on the same venues. However, most topic models ignore the auxiliary authorship and publication venues. Given above two challenges, we propose a Variational Graph Author Topic Model for documents to integrate both semantic interpretability and authorship and venue modeling into a unified VGAE framework. For authorship and venue modeling, we construct a hierarchical multi-layered document graph with both intra- and cross-layer topic propagation. For semantic interpretability, three word relations (contextual, syntactic, semantic) are modeled and constitute three word sub-layers in the document graph. We further propose three alternatives for variational divergence. Experiments verify the effectiveness of our model on supervised and unsupervised tasks.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2429–2438},
numpages = {10},
keywords = {author topic modeling, variational graph auto-encoder, graph neural networks, text mining},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3269206.3271696,
author = {Bai, Haoli and Chen, Zhuangbin and Lyu, Michael R. and King, Irwin and Xu, Zenglin},
title = {Neural Relational Topic Models for Scientific Article Analysis},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271696},
doi = {10.1145/3269206.3271696},
abstract = {Topic modelling and citation recommendation of scientific articles are important yet challenging research problems in scientific article analysis. In particular, the inference on coherent topics can be easily affected by irrelevant contents in articles. Meanwhile, the extreme sparsity of citation networks brings difficulty to a valid citation recommendation. Intuitively, articles with similar topics are more likely to cite each other, and cited articles tend to share similar themes. Motivated from this intuition, we aim to boost the performance of both topic modelling and citation recommendation by effectively leverage this underlying correlation between latent topics and citation networks. To this end, we propose a novel Bayesian deep generative model termed as Neural Relational Topic Model (NRTM), which is composed with a Stacked Variational Auto-Encoder (SVAE) and a multilayer perception (MLP). Specifically, the SVAE utilizes an inference network to learn more representative topics of document contents, which can help to enrich the latent factors in collaborative filtering of citations. Furthermore, the MLP network conducts nonlinear collaborative filtering of citations, which can further benefit the inference of topics by leveraging the knowledge of citation networks. Extensive experiments on two real-world datasets demonstrate that our model can effectively take advantages of the coherence between topic learning and citation recommendation, and significantly outperform the state-of-the-art methods on both tasks.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {27–36},
numpages = {10},
keywords = {deep learning, topic modelling, citation recommendation},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3330430.3333615,
author = {Kuzi, Saar and Cope, William and Ferguson, Duncan and Geigle, Chase and Zhai, ChengXiang},
title = {Automatic Assessment of Complex Assignments Using Topic Models},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333615},
doi = {10.1145/3330430.3333615},
abstract = {Automated assessment of complex assignments is crucial for scaling up learning of complex skills such as critical thinking. To address this challenge, one previous work has applied supervised machine learning to automate the assessment by learning from examples of graded assignments by humans. However, in the previous work, only simple lexical features, such as words or n-grams, have been used. In this paper, we propose to use topics as features for this task, which are more interpretable than those simple lexical features and can also address polysemy and synonymy of lexical semantics. The topics can be learned automatically from the student assignment data by using a probabilistic topic model. We propose and study multiple approaches to construct topical features and to combine topical features with simple lexical features. We evaluate the proposed methods using clinical case assignments performed by veterinary medicine students. The experimental results show that topical features are generally very effective and can substantially improve performance when added on top of the lexical features. However, their effectiveness is highly sensitive to how the topics are constructed and a combination of topics constructed using multiple views of the text data works the best. Our results also show that combining the prediction results of using different types of topical features and of topical and lexical features is more effective than pooling all features together to form a larger feature space.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {13},
numpages = {10},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{10.1145/3025171.3025194,
author = {Watanabe, Kento and Matsubayashi, Yuichiroh and Inui, Kentaro and Nakano, Tomoyasu and Fukayama, Satoru and Goto, Masataka},
title = {LyriSys: An Interactive Support System for Writing Lyrics Based on Topic Transition},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025194},
doi = {10.1145/3025171.3025194},
abstract = {This paper presents LyriSys, a novel lyric-writing support system. Previous systems for lyric writing can fully automatically only generate a single line of lyrics that satisfies given constraints on accent and syllable patterns or an entire lyric. In contrast to such systems, LyriSys allows users to create and revise their work incrementally in a trial-and-error manner. Through fine-grained interactions with the system, the user can create the specifications of the musical structure and the story of the lyrics in terms of the verse-bridge-chorus structure, the number of lines, words and syllables, and most importantly, the transition over semantic topics such as "scene", "dark" and "sweet love". This paper provides an overview of the design of the system and its user interface and describes how the writing process is guided by a state-of-the-art probabilistic generative topic model that is trained without supervision. The system works for both Japanese and English.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {559–563},
numpages = {5},
keywords = {computational creativity, song lyrics, topic transition, linguistic creativity, semantic},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3132847.3133109,
author = {Wang, Yiren and Seyler, Dominic and Santu, Shubhra Kanti Karmaker and Zhai, ChengXiang},
title = {A Study of Feature Construction for Text-Based Forecasting of Time Series Variables},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133109},
doi = {10.1145/3132847.3133109},
abstract = {Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2347–2350},
numpages = {4},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3292500.3330781,
author = {Zhou, Xiao and Mascolo, Cecilia and Zhao, Zhongxiang},
title = {Topic-Enhanced Memory Networks for Personalised Point-of-Interest Recommendation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330781},
doi = {10.1145/3292500.3330781},
abstract = {Point-of-Interest (POI) recommender systems play a vital role in people's lives by recommending unexplored POIs to users and have drawn extensive attention from both academia and industry. Despite their value, however, they still suffer from the challenges of capturing complicated user preferences and fine-grained user-POI relationship for spatio-temporal sensitive POI recommendation. Existing recommendation algorithms, including both shallow and deep approaches, usually embed the visiting records of a user into a single latent vector to model user preferences: this has limited power of representation and interpretability. In this paper, we propose a novel topic-enhanced memory network (TEMN), a deep architecture to integrate the topic model and memory network capitalising on the strengths of both the global structure of latent patterns and local neighbourhood-based features in a nonlinear fashion. We further incorporate a geographical module to exploit user-specific spatial preference and POI-specific spatial influence to enhance recommendations. The proposed unified hybrid model is widely applicable to various POI recommendation scenarios. Extensive experiments on real-world WeChat datasets demonstrate its effectiveness (improvement ratio of 3.25% and 29.95% for context-aware and sequential recommendation, respectively). Also, qualitative analysis of the attention weights and topic modeling provides insight into the model's recommendation process and results.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3018–3028},
numpages = {11},
keywords = {topic modeling, recommender systems, neural networks},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3126686.3126774,
author = {Li, Xiaopeng and She, James},
title = {Relational Variational Autoencoder for Link Prediction with Multimedia Data},
year = {2017},
isbn = {9781450354165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126686.3126774},
doi = {10.1145/3126686.3126774},
abstract = {As a fundamental task, link prediction has pervasive applications in social networks, webpage networks, information retrieval and bioinformatics. Among link prediction methods, latent variable models, such as relational topic model and its variants, which jointly model both network structure and node attributes, have shown promising performance for predicting network structures and discovering latent representations. However, these methods are still limited in their representation learning capability from high-dimensional data or consider only text modality of the content. Thus they are very limited in current multimedia scenario. This paper proposes a Bayesian deep generative model called relational variational autoencoder (RVAE) that considers both links and content for link prediction in the multimedia scenario. The model learns deep latent representations from content data in an unsupervised manner, and also learns network structures from both content and link information. Unlike previous deep learning methods with denoising criteria, the proposed RVAE learns a latent distribution for content in latent space, instead of observation space, through an inference network, and can be easily extended to multimedia modalities other than text. Experiments show that RVAE is able to significantly outperform the state-of-the-art link prediction methods with more robust performance.},
booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017},
pages = {93–100},
numpages = {8},
keywords = {link prediction, generative models, bayesian, deep learning, autoencoder, variational inference},
location = {Mountain View, California, USA},
series = {Thematic Workshops '17}
}

@inproceedings{10.1145/3148011.3148030,
author = {Osborne, Francesco and Mannocci, Andrea and Motta, Enrico},
title = {Forecasting the Spreading of Technologies in Research Communities},
year = {2017},
isbn = {9781450355537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148011.3148030},
doi = {10.1145/3148011.3148030},
abstract = {Technologies such as algorithms, applications and formats are an important part of the knowledge produced and reused in the research process. Typically, a technology is expected to originate in the context of a research area and then spread and contribute to several other fields. For example, Semantic Web technologies have been successfully adopted by a variety of fields, e.g., Information Retrieval, Human Computer Interaction, Biology, and many others. Unfortunately, the spreading of technologies across research areas may be a slow and inefficient process, since it is easy for researchers to be unaware of potentially relevant solutions produced by other research communities. In this paper, we hypothesise that it is possible to learn typical technology propagation patterns from historical data and to exploit this knowledge i) to anticipate where a technology may be adopted next and ii) to alert relevant stakeholders about emerging and relevant technologies in other fields. To do so, we propose the Technology-Topic Framework, a novel approach which uses a semantically enhanced technology-topic model to forecast the propagation of technologies to research areas. A formal evaluation of the approach on a set of technologies in the Semantic Web and Artificial Intelligence areas has produced excellent results, confirming the validity of our solution.},
booktitle = {Proceedings of the Knowledge Capture Conference},
articleno = {1},
numpages = {8},
keywords = {Technology, Semantic Web, Scholarly Data, Ontology},
location = {Austin, TX, USA},
series = {K-CAP 2017}
}

@article{10.1145/3290047,
author = {Yuan, Bo and Gao, Xinbo and Niu, Zhenxing and Tian, Qi},
title = {Discovering Latent Topics by Gaussian Latent Dirichlet Allocation and Spectral Clustering},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3290047},
doi = {10.1145/3290047},
abstract = {Today, diversifying the retrieval results of a certain query will improve customers’ search efficiency. Showing the multiple aspects of information provides users an overview of the object, which helps them fast target their demands. To discover aspects, research focuses on generating image clusters from initially retrieved results. As an effective approach, latent Dirichlet allocation (LDA) has been proved to have good performance on discovering high-level topics. However, traditional LDA is designed to process textual words, and it needs the input as discrete data. When we apply this algorithm to process continuous visual images, a common solution is to quantize the continuous features into discrete form by a bag-of-visual-words algorithm. During this process, quantization error will lead to information that inevitably is lost. To construct a topic model with complete visual information, this work applies Gaussian latent Dirichlet allocation (GLDA) on the diversity issue of image retrieval. In this model, traditional multinomial distribution is substituted with Gaussian distribution to model continuous visual features. In addition, we propose a two-phase spectral clustering strategy, called dual spectral clustering, to generate clusters from region level to image level. The experiments on the challenging landmarks of the DIV400 database show that our proposal improves relevance and diversity by about 10% compared to traditional topic models.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {25},
numpages = {18},
keywords = {image retrieval, Latent Dirichlet allocation, Gaussian, spectral clustering, diversity}
}

@article{10.1145/3365662,
author = {Jr., Guy L. Steele and Tristan, Jean-Baptiste},
title = {Using Butterfly-Patterned Partial Sums to Draw from Discrete Distributions},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3365662},
doi = {10.1145/3365662},
abstract = {We describe a simd technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate (“butterfly-patterned”) form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search. Measurements using cuda 7.5 on an nvidia Titan Black gpu show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1,024 topics about 13% faster (when using single-precision floating-point data) or about 35% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses.},
journal = {ACM Trans. Parallel Comput.},
month = {nov},
articleno = {22},
numpages = {30},
keywords = {machine learning, multithreading, transposed memory access, random sampling, parallel computing, simd, gpu, lda, Butterfly, coalesced memory access, memory bottleneck, latent Dirichlet allocation, discrete distribution}
}

@inproceedings{10.1145/3018743.3018757,
author = {Steele, Guy L. and Tristan, Jean-Baptiste},
title = {Using Butterfly-Patterned Partial Sums to Draw from Discrete Distributions},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018757},
doi = {10.1145/3018743.3018757},
abstract = {We describe a SIMD technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate ("butterfly-patterned") form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search. Measurements using CUDA 7.5 on an NVIDIA Titan Black GPU show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1024 topics about about 13% faster (when using single-precision floating-point data) or about 35% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {341–355},
numpages = {15},
keywords = {parallel computing, latent dirichlet allocation, random sampling, discrete distribution, multithreading, butterfly, lda, transposed memory access, simd, gpu, memory bottleneck, machine learning, coalesced memory access},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018757,
author = {Steele, Guy L. and Tristan, Jean-Baptiste},
title = {Using Butterfly-Patterned Partial Sums to Draw from Discrete Distributions},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018757},
doi = {10.1145/3155284.3018757},
abstract = {We describe a SIMD technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate ("butterfly-patterned") form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search. Measurements using CUDA 7.5 on an NVIDIA Titan Black GPU show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1024 topics about about 13% faster (when using single-precision floating-point data) or about 35% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {341–355},
numpages = {15},
keywords = {lda, simd, gpu, butterfly, machine learning, transposed memory access, parallel computing, coalesced memory access, memory bottleneck, latent dirichlet allocation, random sampling, discrete distribution, multithreading}
}

@inproceedings{10.1145/3307681.3325407,
author = {Xie, Xiaolong and Liang, Yun and Li, Xiuhong and Tan, Wei},
title = {CuLDA: Solving Large-Scale LDA Problems on GPUs},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307681.3325407},
doi = {10.1145/3307681.3325407},
abstract = {Latent Dirichlet Allocation(LDA) is a popular topic model. Given the fact that the input corpus of LDA algorithms consists of millions to billions of tokens, the LDA training process is very time-consuming, which prevents the adoption of LDA in many scenarios, e.g., online service. GPUs have benefited modern machine learning algorithms and big data analysis as they can provide high memory bandwidth and tremendous computation power. Therefore, many frameworks, e.g. TensorFlow, Caffe, CNTK, support GPUs for accelerating various data-intensive machine learning algorithms. However, we observe that the performance of existing LDA solutions on GPUs is not satisfying. In this paper, we present CuLDA, a GPU-based efficient and scalable approach to accelerate large-scale LDA problems. CuLDA is designed to efficiently solve LDA problems at high throughput. To this end, we first delicately design workload partitioning and synchronization mechanism to exploit multiple GPUs. Then, we offload the LDA sampling process to each individual GPU by optimizing from the sampling algorithm, parallelization, and data compression perspectives. Experiment evaluations show that compared with the state-of-the-art LDA solutions, CuLDA outperforms them by a large margin (up to 7.3X) on a single GPU. CuLDA is able to achieve an extra 7.5X speedup on 8 GPUs for large data sets.},
booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {195–205},
numpages = {11},
keywords = {GPU, topic modeling, LDA},
location = {Phoenix, AZ, USA},
series = {HPDC '19}
}

@article{10.1109/TASLP.2016.2632521,
author = {Zhao, Rui and Mao, Kezhi and Rui Zhao and Kezhi Mao},
title = {Topic-Aware Deep Compositional Models for Sentence Classification},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2632521},
doi = {10.1109/TASLP.2016.2632521},
abstract = {In recent years, deep compositional models have emerged as a popular technique for representation learning of sentence in computational linguistic and natural language processing. These models normally train various forms of neural networks on top of pretrained word embeddings using a task-specific corpus. However, most of these works neglect the multisense nature of words in the pretrained word embeddings. In this paper we introduce topic models to enrich the word embeddings for multisenses of words. The integration of the topic model with various semantic compositional processes leads to topic-aware convolutional neural network and topic-aware long short term memory networks. Different from previous multisense word embeddings models that assign multiple independent and sense-specific embeddings to each word, our proposed models are lightweight and have flexible frameworks that regard word sense as the composition of two parts: a general sense derived from a large corpus and a topic-specific sense derived from a task-specific corpus. In addition, our proposed models focus on semantic composition instead of word understanding. With the help of topic models, we can integrate the topic-specific sense at word-level before the composition and sentence-level after the composition. Comprehensive experiments on five public sentence classification datasets are conducted and the results show that our proposed topic-aware deep compositional models produce competitive or better performance than other text representation learning methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {248–260},
numpages = {13}
}

@inproceedings{10.1145/3132847.3132889,
author = {Wang, Hongwei and Wang, Jia and Zhao, Miao and Cao, Jiannong and Guo, Minyi},
title = {Joint Topic-Semantic-Aware Social Recommendation for Online Voting},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132889},
doi = {10.1145/3132847.3132889},
abstract = {Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {347–356},
numpages = {10},
keywords = {matrix factorization, online voting, recommender systems, topic-enhanced word embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132897,
author = {Thonet, Thibaut and Cabanac, Guillaume and Boughanem, Mohand and Pinel-Sauvagnat, Karen},
title = {Users Are Known by the Company They Keep: Topic Models for Viewpoint Discovery in Social Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132897},
doi = {10.1145/3132847.3132897},
abstract = {Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions. There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized P\'{o}lya Urn sampling scheme (SNVDM-GPU) to leverage "acquaintances of acquaintances" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {87–96},
numpages = {10},
keywords = {topic modeling, viewpoint discovery, social networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3404835.3463004,
author = {Li, Ziming and Kiseleva, Julia and de Rijke, Maarten},
title = {Improving Response Quality with Backward Reasoning in Open-Domain Dialogue Systems},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463004},
doi = {10.1145/3404835.3463004},
abstract = {Being able to generate informative and coherent dialogue responses is crucial when designing human-like open-domain dialogue systems. Encoder-decoder-based dialogue models tend to produce generic and dull responses during the decoding step because the most predictable response is likely to be a non-informative response instead of the most suitable one. To alleviate this problem, we propose to train the generation model in a bidirectional manner by adding a backward reasoning step to the vanilla encoder-decoder training. The proposed backward reasoning step pushes the model to produce more informative and coherent content because the forward generation step's output is used to infer the dialogue context in the backward direction. The advantage of our method is that the forward generation and backward reasoning steps are trained simultaneously through the use of a latent variable to facilitate bidirectional optimization. Our method can improve response quality without introducing side information (e.g., a pre-trained topic model). The proposed bidirectional response generation method achieves state-of-the-art performance for response quality.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1940–1944},
numpages = {5},
keywords = {open-domain dialogue system, response generation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3219819.3220009,
author = {Sun, Ying and Zhu, Hengshu and Zhuang, Fuzhen and Gu, Jingjing and He, Qing},
title = {Exploring the Urban Region-of-Interest through the Analysis of Online Map Search Queries},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220009},
doi = {10.1145/3219819.3220009},
abstract = {Urban Region-of-Interest (ROI) refers to the integrated urban areas with specific functionalities that attract people's attentions and activities, such as the recreational business districts, transportation hubs, and city landmarks. Indeed, at the macro level, ROI is one of the representatives for agglomeration economies, and plays an important role in urban business planning. At the micro level, ROI provides a useful venue for understanding the urban lives, demands and mobilities of people. However, due to the vague and diversified nature of ROI, it still lacks of quantitative ways to investigate ROIs in a holistic manner. To this end, in this paper we propose a systematic study on ROI analysis through mining the large-scale online map query logs, which provides a new data-driven research paradigm for ROI detection and profiling. Specifically, we first divide the urban area into small region grids, and calculate their PageRank value as visiting popularity based on the transition information extracted from map queries. Then, we propose a density-based clustering method for merging neighboring region grids with high popularity into integrated ROIs. After that, to further explore the profiles of different ROIs, we develop a spatial-temporal latent factor model URPTM (Urban Roi Profiling Topic Model) to identify the latent travel patterns and Point-of-Interest (POI) demands of ROI visitors. Finally, we implement extensive experiments to empirically evaluate our approaches based on the large-scale real-world data collected from Beijing. Indeed, by visualizing the results obtained from URPTM, we can successfully obtain many meaningful travel patterns and interesting discoveries on urban lives.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2269–2278},
numpages = {10},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3487553.3524927,
author = {Dupuy, Jean and Guille, Adrien and Jacques, Julien},
title = {Anchor Prediction: A Topic Modeling Approach},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524927},
doi = {10.1145/3487553.3524927},
abstract = {Networks of documents connected by hyperlinks, such as Wikipedia, are ubiquitous. Hyperlinks are inserted by the authors to enrich the text and facilitate the navigation through the network. However, authors tend to insert only a fraction of the relevant hyperlinks, mainly because this is a time consuming task. In this paper we address an annotation, which we refer to as anchor prediction. Even though it is conceptually close to link prediction or entity linking, it is a different task that require developing a specific method to solve it. Given a source document and a target document, this task consists in automatically identifying anchors in the source document, i.e words or terms that should carry a hyperlink pointing towards the target document. We propose a contextualized relational topic model, CRTM, that models directed links between documents as a function of the local context of the anchor in the source document and the whole content of the target document. The model can be used to predict anchors in a source document, given the target document, without relying on a dictionary of previously seen mention or title, nor any external knowledge graph. Authors can benefit from CRTM, by letting it automatically suggest hyperlinks, given a new document and the set of target document to connect to. It can also benefit to readers, by dynamically inserting hyperlinks between the documents they’re reading. Experiments conducted on several Wikipedia corpora (in English, Italian and German) highlight the practical usefulness of anchor prediction and demonstrate the relevancy of our approach.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1310–1318},
numpages = {9},
keywords = {Document network, Topic modeling, Annotation, Anchor prediction},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3512576.3512638,
author = {Wang, Jing and Wang, Mo and Song, Yulun},
title = {A Study on Smart City Research Activity Using Bibliometric and Natural Language Processing Methods},
year = {2021},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512638},
doi = {10.1145/3512576.3512638},
abstract = {Smart cities have become a new urban development paradigm and draw much interest from the research community and society. Based on academic publications of smart city-related research, this study employs bibliometrics, natural language machine learning methods to analyze 10,000 papers indexed by Web of Science from 2009 to 2020. Bibliometrics results show that: (1) A total of 114 countries or regions worldwide have participated in smart city research, and China is the country with the highest amount of participation in the field of smart cities. (2) Smart city research has gone through three stages: the initial stage (2009-2012), the in-depth advancement stage (2013-2016), and the leap-up stage (2017-2020). Researchers paid more attention to urban attractiveness indicators such as sustainability in the early stage. In the later period, most of the research topics were clustered on improving the overall function of the city. Latent Dirichlet Allocation (LDA) topic model results revealed that research topics could be categorized into five aspects: policy research on the status quo of smart cities, data analysis and application, infrastructure construction, urban governance, and network security. Current research on smart city technologies mainly focuses on theoretical systems, technologies, and application fields. There is a lack of in-depth research and exploration in long-term construction and operation mechanisms. This research provides insight into the research status of smart city technologies and helps researchers decide on future study direction.},
booktitle = {2021 The 9th International Conference on Information Technology: IoT and Smart City},
pages = {346–352},
numpages = {7},
keywords = {Research hotspots, Natural language machine learning, Smart city, Text data mining},
location = {Guangzhou, China},
series = {ICIT 2021}
}

@inproceedings{10.1145/3038912.3052701,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas},
title = {Detecting Duplicate Posts in Programming QA Communities via Latent Semantics and Association Rules},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052701},
doi = {10.1145/3038912.3052701},
abstract = {Programming community-based question-answering (PCQA) websites such as Stack Overflow enable programmers to find working solutions to their questions. Despite detailed posting guidelines, duplicate questions that have been answered are frequently created. To tackle this problem, Stack Overflow provides a mechanism for reputable users to manually mark duplicate questions. This is a laborious effort, and leads to many duplicate questions remain undetected. Existing duplicate detection methodologies from traditional community based question-answering (CQA) websites are difficult to be adopted directly to PCQA, as PCQA posts often contain source code which is linguistically very different from natural languages. In this paper, we propose a methodology designed for the PCQA domain to detect duplicate questions. We model the detection as a classification problem over question pairs. To extract features for question pairs, our methodology leverages continuous word vectors from the deep learning literature, topic model features and phrases pairs that co-occur frequently in duplicate questions mined using machine translation systems. These features capture semantic similarities between questions and produce a strong performance for duplicate detection. Experiments on a range of real-world datasets demonstrate that our method works very well; in some cases over 30% improvement compared to state-of-the-art benchmarks. As a product of one of the proposed features, the association score feature, we have mined a set of associated phrases from duplicate questions on Stack Overflow and open the dataset to the public.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1221–1229},
numpages = {9},
keywords = {question quality, community-based question answering, latent semantics, classification, association rules},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/3161607,
author = {Chen, Qin and Hu, Qinmin and Huang, Jimmy Xiangji and He, Liang},
title = {Modeling Queries with Contextual Snippets for Information Retrieval},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3161607},
doi = {10.1145/3161607},
abstract = {Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of single terms, which can generate plenty of irrelevant query terms and decrease retrieval performance. To alleviate this problem, we propose an approach that adapts the PRF-based contextual snippets into a context-aware topic model to enhance query representations. Specifically, instead of selecting a series of independent terms, we make full use of the query contextual information and focus on the snippets with the length of n in the PRF documents. Furthermore, we propose a context-aware topic (CAT) model to mine the topic distributions of the query-relevant snippets, namely, fine contextual snippets. In contrast to the traditional topic models that infer the topics from the whole corpus, we establish a bridge between the snippets and the corresponding PRF documents, which can be used for modeling the topics more precisely and efficiently. Finally, the topic distributions of the fine snippets are used for context-aware and topic-sensitive query representations. To evaluate the performance of our approach, we integrate the obtained queries into a topic-based hybrid retrieval model and conduct extensive experiments on various TREC collections. The experimental results show that our query-modeling approach is more effective in boosting retrieval performance compared with the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {47},
numpages = {26},
keywords = {query representation, Contextual snippet, topic modeling}
}

@inproceedings{10.1145/3097983.3098067,
author = {Wang, Pengfei and Fu, Yanjie and Liu, Guannan and Hu, Wenqing and Aggarwal, Charu},
title = {Human Mobility Synchronization and Trip Purpose Detection with Mixture of Hawkes Processes},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098067},
doi = {10.1145/3097983.3098067},
abstract = {While exploring human mobility can benefit many applications such as smart transportation, city planning, and urban economics, there are two key questions that need to be answered: (i) What is the nature of the spatial diffusion of human mobility across regions with different urban functions? (ii) How to spot and trace the trip purposes of human mobility trajectories? To answer these questions, we study large-scale and city-wide taxi trajectories; and furtherly organize them as arrival sequences according to the chronological arrival time. We figure out an important property across different regions from the arrival sequences, namely human mobility synchronization effect, which can be exploited to explain the phenomenon that two regions have similar arrival patterns in particular time periods if they share similar urban functions. In addition, the arrival sequences are mixed by arrival events with distinct trip purposes, which can be revealed by the regional environment of both the origins and destinations. To that end, in this paper, we develop a joint model that integrates Mixture of Hawkes Process (MHP) with a hierarchical topic model to capture the arrival sequences with mixed trip purposes. Essentially, the human mobility synchronization effect is encoded as a synchronization rate in the MHP; while the regional environment is modeled by introducing latent Trip Purpose and POI Topic to generate the Point of Interests (POIs) in the regions. Moreover, we provide an effective inference algorithm for parameter learning. Finally, we conduct intensive experiments on synthetic data and real-world data, and the experimental results have demonstrated the effectiveness of the proposed model.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {495–503},
numpages = {9},
keywords = {synchronization, trip purpose, variational inference, hawkes process, human mobility},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3038912.3052614,
author = {Lee, Roy Ka-Wei and Hoang, Tuan-Anh and Lim, Ee-Peng},
title = {On Analyzing User Topic-Specific Platform Preferences Across Multiple Social Media Sites},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052614},
doi = {10.1145/3038912.3052614},
abstract = {Topic modeling has traditionally been studied for single text collections and applied to social media data represented in the form of text documents. With the emergence of many social media platforms, users find themselves using different social media for posting content and for social interaction. While many topics may be shared across social media platforms, users typically show preferences of certain social media platform(s) over others for certain topics. Such platform preferences may even be found at the individual level. To model social media topics as well as platform preferences of users, we propose a new topic model known as MultiPlatform-LDA (MultiLDA). Instead of just merging all posts from different social media platforms into a single text collection, MultiLDA keeps one text collection for each social media platform but allowing these platforms to share a common set of topics. MultiLDA further learns the user-specific platform preferences for each topic. We evaluate MultiLDA against TwitterLDA, the state-of-the-art method for social media content modeling, on two aspects: (i) the effectiveness in modeling topics across social media platforms, and (ii) the ability to predict platform choices for each post. We conduct experiments on three real-world datasets from Twitter, Instagram and Tumblr sharing a set of common users. Our experiments results show that the MultiLDA outperforms in both topic modeling and platform choice prediction tasks. We also show empirically that among the three social media platforms, "Daily matters" and "Relationship matters" are dominant topics in Twitter, "Social gathering", "Outing" and "Fashion" are dominant topics in Instagram, and "Music", "Entertainment" and "Fashion" are dominant topics in Tumblr.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1351–1359},
numpages = {9},
keywords = {user preference, multiple social networks, topic modeling},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1109/MSR.2019.00022,
author = {Treude, Christoph and Wagner, Markus},
title = {Predicting Good Configurations for GitHub and Stack Overflow Topic Models},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00022},
doi = {10.1109/MSR.2019.00022},
abstract = {Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as a text-mining tool for the discovery of hidden semantic structures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by grouping texts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflow text corpora, (ii) an a-posteriori characterisation of text corpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {84–95},
numpages = {12},
keywords = {algorithm portfolio, topic modelling, corpus features},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3126858.3126861,
author = {Rodrigues do Carmo, Rodrigo and Lacerda, An\'{\i}sio Mendes and Dalip, Daniel Hasan},
title = {A Majority Voting Approach for Sentiment Analysis in Short Texts Using Topic Models},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126861},
doi = {10.1145/3126858.3126861},
abstract = {Nowadays people can provide feedback on products and services on the web. Site owners can use this kind of information in order to understand more their public preferences. Sentiment Analysis can help in this task, providing methods to infer the polarity of the reviews. In these methods, the classifier can use hints about the polarity of the words and the subject being discussed in order to infer the polarity of the text. However, many of these texts are short and, because of that, the classifier can have difficulties to infer these hints. We here propose a new sentiment analysis method that uses topic models to infer the polarity of short texts. The intuition of this approach is that, by using topics, the classifier is able to better understand the context and improve the performance in this task. In this approach, we first use methods to infer topics such as LDA, BTM and MedLDA in order to represent the review and, then, we apply a classifier (e.g. Linear SVM, Random Forest or Logistic Regression). In this method, we combine the results of classifiers and text representations in two ways: (1) by using single topic representation and multiple classifiers; (2) and using multiple topic representations and a single classifier. We also analyzed the impact of expanding these texts since the topic model methods can have difficulties to deal with the data sparsity present in these reviews. The proposed approach could achieve gains of up to 8.5% compared to our baseline. Moreover, we were able to determine the best classifier (Random Forest) and the best topic detection method (MedLDA).},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {449–455},
numpages = {7},
keywords = {sentiment analysis, text expansion, topic models},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@article{10.1145/3072606,
author = {Liang, Shangsong and Ren, Zhaochun and Zhao, Yukun and Ma, Jun and Yilmaz, Emine and Rijke, Maarten De},
title = {Inferring Dynamic User Interests in Streams of Short Texts for User Clustering},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072606},
doi = {10.1145/3072606},
abstract = {User clustering has been studied from different angles. In order to identify shared interests, behavior-based methods consider similar browsing or search patterns of users, whereas content-based methods use information from the contents of the documents visited by the users. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of streams of short texts. User clustering in this setting is more challenging than in the case of long documents, as it is difficult to capture the users’ dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (UCT). UCT adaptively tracks changes of each user’s time-varying topic distributions based both on the short texts the user posts during a given time period and on previously estimated distributions. To infer changes, we propose a Gibbs sampling algorithm where a set of word pairs from each user is constructed for sampling. UCT can be used in two ways: (1) as a short-term dependency model that infers a user’s current topic distribution based on the user’s topic distributions during the previous time period only, and (2) as a long-term dependency model that infers a user’s current topic distributions based on the user’s topic distributions during multiple time periods in the past. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed short-term and long-term dependency user clustering models compared to state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
articleno = {10},
numpages = {37},
keywords = {ad hoc retrieval, data streams, Diversity}
}

@inproceedings{10.1145/3275219.3275222,
author = {Zheng, Zhiwen and Wang, Liang and Xu, Jingwei and Wu, Tianheng and Wu, Simeng and Tao, Xianping},
title = {Measuring and Predicting the Relevance Ratings between FLOSS Projects Using Topic Features},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275222},
doi = {10.1145/3275219.3275222},
abstract = {Understanding the relevance between the Free/Libra Open Source Software projects is important for developers to perform code and design reuse, discover and develop new features, keep their projects up-to-date, and etc. However, it is challenging to perform relevance ratings between the FLOSS projects mainly because: 1) beyond simple code similarity, there are complex aspects considered when measuring the relevance; and 2) the prohibitive large amount of FLOSS projects available. To address the problem, in this paper, we propose a method to measure and further predict the relevance ratings between FLOSS projects. Our method uses topic features extracted by the LDA topic model to describe the characteristics of a project. By using the topic features, multiple aspects of FLOSS projects such as the application domain, technology used, and programming language are extracted and further used to measure and predict their relevance ratings. Based on the topic features, our method uses matrix factorization to leverage the partially known relevance ratings between the projects to learn the mapping between different topic features to the relevance ratings. Finally, our method combines the topic modeling and matrix factorization technologies to predict the relevance ratings between software projects without human intervention, which is scalable to a large amount of projects. We evaluate the performance of the proposed method by applying our topic extraction and relevance modeling methods using 300 projects from GitHub. The result of topic extraction experiment shows that, for topic modeling, our LDA-based approach achieves the highest hit rate of 98.3% and the highest average accuracy of 29.8%. And the relevance modeling experiment shows that our relevance modeling approach achieves the minimum average predict error of 0.093, suggesting the effectiveness of applying the proposed method on real-world data sets.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {12},
numpages = {10},
keywords = {Matrix Factorization, Relevance Rating, FLOSS Projects, Topic Modeling},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3485447.3512138,
author = {Sepahpour-Fard, Melody and Quayle, Michael},
title = {How Do Mothers and Fathers Talk About Parenting to Different Audiences? Stereotypes and Audience Effects: An Analysis of r/Daddit, r/Mommit, and r/Parenting Using Topic Modelling},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512138},
doi = {10.1145/3485447.3512138},
abstract = {While major strides have been made towards gender equality in public life, serious inequality remains in the domestic sphere, especially around parenting. The present study analyses discussions about parenting on Reddit (i.e., a content aggregation website) to explore audience effects and gender stereotypes. It suggests a novel method to study topical variation in individuals’ language when interacting with different audiences. Comments posted in 2020 were collected from three parenting subreddits (i.e., topical communities), described as being for fathers (r/Daddit), mothers (r/Mommit), and all parents (r/Parenting). Users posting on r/Parenting and r/Daddit or on r/Parenting and r/Mommit were assumed to identify as fathers or mothers, respectively, allowing gender comparison. Users’ comments on r/Parenting (to a mixed-gender audience) were compared with their comments to single-gender audiences on r/Daddit or r/Mommit using Latent Dirichlet Allocation (LDA) topic modelling. Results show that the most discussed topic among parents is about education and family advice, a topic mainly discussed in the mixed-gender subreddit and more by fathers than mothers. The topic model also indicates that, when it comes to the basic needs of children (sleep, food, and medical care), mothers seem to be more concerned regardless of the audience. In contrast, topics such as birth and pregnancy announcements and physical appearance are more discussed by fathers in the father-centric subreddit. Overall, findings seem to show that mothers are generally more concerned about the practical sides of parenting while fathers’ expressed concerns are more contextual: with other fathers, there seems to be a desire to show their fatherhood and be recognized for it while they discuss education with mothers. These results demonstrate that concerns expressed by parents on Reddit are context-sensitive but also consistent with gender stereotypes, potentially reflecting a persistent gendered and unequal division of labour in parenting.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2696–2706},
numpages = {11},
keywords = {Reddit, parenting, audience, social identity performance, computational social science, social psychology, gender stereotypes, LDA topic modelling, natural language processing},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3428502.3428514,
author = {Papadopoulos, Theodoros and Charalabidis, Yannis},
title = {What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428514},
doi = {10.1145/3428502.3428514},
abstract = {The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {100–111},
numpages = {12},
keywords = {document similarity, Automated Text Analysis, topic modelling, AI strategies, machine learning, NLP},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3078971.3079007,
author = {Qian, Shengsheng and Zhang, Tianzhu and Xu, Changsheng},
title = {A Generic Framework for Social Event Analysis},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079007},
doi = {10.1145/3078971.3079007},
abstract = {Social event is something that occurs at specific place and time associated with some specific actions, and it consists of many stories over time. With the explosion of Web 2.0 platforms, a popular social event that is happening around us and around the world can spread very fast. As a result, social event analysis becomes more and more important for users to understand the whole evolutionary trend of social event over time. However, it is very challenging to do social event analysis because social event data from different social media sites have multi-modal, multi-domain, and large-scale properties. The goal of our research is to design advanced multimedia techniques to deal with the above issues and establish an effective and robust social event analysis framework for social event representation, detection, tracking and evolution analysis. (1) For social event representation, we propose a novel cross-domain collaborative learning algorithm based on non-parametric Bayesian dictionary learning model. It can make use of the shared domain priors and modality priors to collaboratively learn the data's representations by considering the domain discrepancy and the multi-modal property.(2) For social event detection, we propose a boosted multi-modal supervised Latent Dirichlet Allocation model. It can effectively exploit multi-modality information and utilize boosting weighted sampling strategy for large-scale data processing. (3) For social event tracking, we propose a novel multi-modal event topic model, which can effectively model the correlations between textual and visual modalities, and obtain their topics over time. (4) For social event evolution analysis, we propose a novel multi-modal multi-view topic-opinion mining model to conduct fined-grained topic and opinion analysis for social events from multiple social media sites collaboratively. It can discover multi-modal topics and the corresponding opinions over time to understand the evolutionary processes of social event. Extensive experimental results show that the proposed algorithms perform favorably against state-of-the-art methods for social event analysis.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {480–483},
numpages = {4},
keywords = {multi-modal, large-scale, multi-domain, social event},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1145/3429395.3429403,
author = {Hao, Pei-Yi and Ou, Jen-Bing and Zhuang, Kai-Xiang},
title = {Forecasting the Trends of Stock Price through Social Networks by Fuzzy Support Vector Machine with Possibility Measures},
year = {2020},
isbn = {9781450389457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429395.3429403},
doi = {10.1145/3429395.3429403},
abstract = {Obviously, there are many advantages for us to correctly forecast the price trends of stock. According to the hypothesis of efficiency market, the prices of stock are evaluated by all the current useful information. The mood of society plays an important role that affects the trend of current stock market price. The entire social mood corresponding to a given company is a critical factor which influences the stock price for that company. In recent years, the popularity of online social networks provides huge number of available social mood data. Hence, mining information from social networks and historical stock prices can improve the ability of predicting trend in stock market.In this study, we extracted the hidden topic model and emotional information from user's posts in social networks. Besides, we developed a fuzzy support vector machine to merge the abundant information from the on-line posts, which can be used to forecast the trend of stock prices. Fuzzy set theory is very useful for this study because the texts are fuzzy in itself (such as high/low and big/small), and there is an ambiguous boundary between rise and fall categories. For example, going up either 10% or 1% belongs to rise category, but is different in degree. In comparison with traditional support vector machine, the method proposed in this study is significantly better than the forecasting model of traditional support vector machine.},
booktitle = {Proceedings of the 7th Multidisciplinary in International Social Networks Conference and The 3rd International Conference on Economics, Management and Technology},
articleno = {8},
numpages = {7},
keywords = {fuzzy set theory, Social networks, sentiment analysis, stock price forecast, support vector machine, text mining},
location = {Kaohsiung, Taiwan},
series = {MISNC2020&amp;IEMT2020}
}

@inproceedings{10.1145/3366424.3382084,
author = {Mossie, Zewdie},
title = {Social Media Dark Side Content Detection Using Transfer Learning Emphasis on Hate and Conflict},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382084},
doi = {10.1145/3366424.3382084},
abstract = {Although online content continues to grow, the prevalence of dark side content such as hate, misinformation, disinformation, conflicting, fake, and so on continues to grow and has become a problem for online and offline society. Consequently, work into automated analytical and detection methods has gained much attention. The scarcity of the labeled dataset has, however, become one of the major challenges in both machine and deep learning to develop an effective supervised learning model. As a result, most State-of-the-Art (SOTA) approaches focus on English languages for the detection of such content. The identification task of such content has become a problem due to the diversity of languages used on social media platforms. We propose transfer learning since it needs only access to a large unlabeled text available on social media platforms. Since we use data from Amharic Language, which is in the low-resource language family for machine leaarning, transfer learning is found effective. First, we prepare a topic and word embedding models using Facebook data as a task-specific and a general corpus from different web domains respectively. Second, we combine topic embedding and word embedding and then send the features to a fully-connected Recurrent Neural Networks (RNNs). Our preliminary experimental results from the newly proposed attention-based topic model combined with word embedding outperform the baselines.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {259–263},
numpages = {5},
keywords = {Low-resource language, Word-topic embeddings, Transfer learning, Social media dark side, Pre-trained model},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3291060,
author = {Cheng, Zhiyong and Chang, Xiaojun and Zhu, Lei and Kanjirathinkal, Rose C. and Kankanhalli, Mohan},
title = {MMALFM: Explainable Recommendation by Leveraging Reviews and Images},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3291060},
doi = {10.1145/3291060},
abstract = {Personalized rating prediction is an important research problem in recommender systems. Although the latent factor model (e.g., matrix factorization) achieves good accuracy in rating prediction, it suffers from many problems including cold-start, non-transparency, and suboptimal results for individual user-item pairs. In this article, we exploit textual reviews and item images together with ratings to tackle these limitations. Specifically, we first apply a proposed multi-modal aspect-aware topic model (MATM) on text reviews and item images to model users’ preferences and items’ features from different aspects, and also estimate the aspect importance of a user toward an item. Then, the aspect importance is integrated into a novel aspect-aware latent factor model (ALFM), which learns user’s and item’s latent factors based on ratings. In particular, ALFM introduces a weight matrix to associate those latent factors with the same set of aspects in MATM, such that the latent factors could be used to estimate aspect ratings. Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance. To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation. Besides, every aspect rating is weighted by its aspect importance, which is dependent on the targeted user’s preferences and the targeted item’s features. Therefore, it is expected that the proposed method can model a user’s preferences on an item more accurately for each user-item pair. Comprehensive experimental studies have been conducted on the Yelp 2017 Challenge dataset and Amazon product datasets. Results show that (1) our method achieves significant improvement compared to strong baseline methods, especially for users with only few ratings; (2) item visual features can improve the prediction performance—the effects of item image features on improving the prediction results depend on the importance of the visual features for the items; and (3) our model can explicitly interpret the predicted results in great detail.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {16},
numpages = {28},
keywords = {Aspect, rating prediction, latent factor model, multi-modal, explainable recommendation}
}

@article{10.1145/3284971.3284976,
author = {Madhavan, Manu and Nair, Gopakumar Gopalakrishnan},
title = {An Effective Sequence Structure Representation for Long Non-Coding RNA Identification and Cancer Association Using Machine Learning Methods},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3284971.3284976},
doi = {10.1145/3284971.3284976},
abstract = {The invent of high-throughput technologies and consequent developments in Bioinformatics research unveiled many important non-coding transcript molecules such as Long non-coding RNAs (lncRNAs). The available studies confirmed that lncRNAs play important genetic and epigenetic roles in higher-order species like the human and their differential expressions leads to complex diseases like cancer. Even though there are arrays of studies and related tools for the analysis, less conserved patterns in the sequences and intractable structural properties challenge the understanding of varying functionalities of lncRNAs. For the better approximation of these characteristics, higher quality feature representation is required. This paper proposes an extended hybrid sequence-structure feature set for machine learning based lncRNA analysis. Here, the sequence features are derived from various frequencies of k-mer patterns, GC content and molecular weight. The structure representations consider the context of different secondary structure elements which include stems, interior loops, multi-loops and hairpin loops. These features are used for the classification of lncRNA/mRNA and cancerous/non-cancerous lncRNAs. The classifications use machine learning algorithms such as LDA based topic model, Random Forest, SVM and Na\"{\i}ve Bayes. The results show that the proposed feature set is effective in classifying lncRNAs and provide a direction towards the analysis of the role of secondary structure elements in cancer-related lncRNAs.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {oct},
pages = {49–58},
numpages = {10},
keywords = {na\"{\i}ve bayes, cancerous lncRNAs, RNA secondary structure, feature selection, long non coding RNAs, random forest, latent dirichlet allocation, support vector machine}
}

@inproceedings{10.1145/3366423.3380102,
author = {Yang, Liang and Wu, Fan and Gu, Junhua and Wang, Chuan and Cao, Xiaochun and Jin, Di and Guo, Yuanfang},
title = {Graph Attention Topic Modeling Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380102},
doi = {10.1145/3366423.3380102},
abstract = {Existing topic modeling approaches possess several issues, including the overfitting issue of Probablistic Latent Semantic Indexing (pLSI), the failure of capturing the rich topical correlations among topics in Latent Dirichlet Allocation (LDA), and high inference complexity. In this paper, we provide a new method to overcome the overfitting issue of pLSI by using the amortized inference with word embedding as input, instead of the Dirichlet prior in LDA. For generative topic model, the large number of free latent variables is the root of overfitting. To reduce the number of parameters, the amortized inference replaces the inference of latent variable with a function which possesses the shared (amortized) learnable parameters. The number of the shared parameters is fixed and independent of the scale of the corpus. To overcome the limited application of amortized inference to independent and identically distributed (i.i.d) data, a novel graph neural network, Graph Attention TOpic Network (GATON), is proposed to model the topic structure of non-i.i.d documents according to the following two observations. First, pLSI can be interpreted as stochastic block model (SBM) on a specific bi-partite graph. Second, graph attention network (GAT) can be explained as the semi-amortized inference of SBM, which relaxes the i.i.d data assumption of vanilla amortized inference. GATON provides a novel scheme, i.e. graph convolution operation based scheme, to integrate word similarity and word co-occurrence structure. Specifically, the bag-of-words document representation is modeled as a bi-partite graph topology. Meanwhile, word embedding, which captures the word similarity, is modeled as attribute of the word node and the term frequency vector is adopted as the attribute of the document node. Based on the weighted (attention) graph convolution operation, the word co-occurrence structure and word similarity patterns are seamlessly integrated for topic identification. Extensive experiments demonstrate that the effectiveness of GATON on topic identification not only benefits the document classification, but also significantly refines the input word embedding.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {144–154},
numpages = {11},
keywords = {Graph Attention Network, Graph Neural Network, Stochastic Block Model, Topic Modeling, Bipartite Network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3457913.3457935,
author = {Huang, Yuexin and Sun, Hailong},
title = {Best Answerers Prediction With Topic Based GAT In Q&amp;A Sites},
year = {2020},
isbn = {9781450388191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457913.3457935},
doi = {10.1145/3457913.3457935},
abstract = {Q&amp;A communities are playing an important role in online knowledge sharing, where a large number of users with various knowledge background make tremendous contributions to solving many technical problems based on crowd intelligence. However, as new questions are increasingly posted, it is a non-trivial issue to find a matching answerer for each question. As a result, many questions fail to receive satisfying answers in time. This paper addresses the problem by predicting the best answerer for the new question. Many existing efforts are devoted to predicting the best answerer mainly by calculating the textual similarity between questions and a user’s historical post documents. Some works consider other features, such as the similarity of tags between questions and users, the average quality of a user’s historical answers, and so on. But few works consider interaction within the community. In recent years, works that take account of the interaction between community items (such as GCN and GAT) have made considerable progress in graph mining tasks like item recommendation, node representation, node classification, and link prediction. This kind of graph mining method can easily leverage interactive information in the community and encode it in an easy-to-use way which is very helpful for downstream tasks such as recommendation. However, questions that need to be recommended to answerers are new coming ones and with no interaction with any other node in the community yet. How to make reasonable use of collaborative information to improve recommendation performance is a real challenge. In this paper, we use the interactive information between candidate answerers and combine text information to make our best answerer recommendation. There are two main parts, LDA(Latent Dirichlet Allocation) topic model is used to capture the text information and graph attention networks (GATs) for interaction. We evaluated our approach on a real dataset from Stack Exchange. The result shows that our approach outperforms all the baseline methods.},
booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
pages = {156–164},
numpages = {9},
keywords = {LDA, Graph Mining, Q&amp;A Community, Best Answerer Recommendation},
location = {Singapore, Singapore},
series = {Internetware '20}
}

@article{10.1145/3458537.3458542,
author = {Fang, Anjie},
title = {Analysing Political Events on Twitter: Topic Modelling and User Community Classification},
year = {2021},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3458537.3458542},
doi = {10.1145/3458537.3458542},
abstract = {Recently, political events, such as elections, have raised a lot of discussions on social media networks, in particular, Twitter. This brings new opportunities for social scientists to address social science tasks, such as understanding what communities said or identifying whether a community has an influence on another. However, identifying these communities and extracting what they said from social media data are challenging and non-trivial tasks.We aim to make progress towards understanding 'who' (i.e. communities) said 'what' (i.e. discussed topics) and 'when' (i.e. time) during political events on Twitter. While identifying the 'who' can benefit from Twitter user community classification approaches, 'what' they said and 'when' can be effectively addressed on Twitter by extracting their discussed topics using topic modelling approaches that also account for the importance of time on Twitter. To evaluate the quality of these topics, it is necessary to investigate how coherent these topics are to humans. Accordingly, we propose a series of approaches in this thesis.First, we investigate how to effectively evaluate the coherence of the topics generated using a topic modelling approach. The topic coherence metric evaluates the topical coherence by examining the semantic similarity among words in a topic. We argue that the semantic similarity of words in tweets can be effectively captured by using word embeddings trained using a Twitter background dataset. Through a user study, we demonstrate that our proposed word embedding-based topic coherence metric can assess the coherence of topics like humans [1, 2]. In addition, inspired by the precision at k metric, we propose to evaluate the coherence of a topic model (containing many topics) by averaging the top-ranked topics within the topic model [3]. Our proposed metrics can not only evaluate the coherence of topics and topic models, but also can help users to choose the most coherent topics.Second, we aim to extract topics with a high coherence from Twitter data. Such topics can be easily interpreted by humans and they can assist to examine 'what' has been discussed and 'when'. Indeed, we argue that topics can be discussed in different time periods (see [4]) and therefore can be effectively identified and distinguished by considering their time periods. Hence, we propose an effective time-sensitive topic modelling approach by integrating the time dimension of tweets (i.e. 'when') [5]. We show that the time dimension helps to generate topics with a high coherence. Hence, we argue that 'what' has been discussed and 'when' can be effectively addressed by our proposed time-sensitive topic modelling approach.Next, to identify 'who' participated in the topic discussions, we propose approaches to identify the community affiliations of Twitter users, including automatic ground-truth generation approaches and a user community classification approach. We show that the mentioned hashtags and entities in the users' tweets can indicate which community a Twitter user belongs to. Hence, we argue that they can be used to generate the ground-truth data for classifying users into communities. On the other hand, we argue that different communities favour different topic discussions and their community affiliations can be identified by leveraging the discussed topics. Accordingly, we propose a Topic-Based Naive Bayes (TBNB) classification approach to classify Twitter users based on their words and discussed topics [6]. We demonstrate that our TBNB classifier together with the ground-truth generation approaches can effectively identify the community affiliations of Twitter users.Finally, to show the generalisation of our approaches, we apply our approaches to analyse 3.6 million tweets related to US Election 2016 on Twitter [7]. We show that our TBNB approach can effectively identify the 'who', i.e. classify Twitter users into communities. To investigate 'what' these communities have discussed, we apply our time-sensitive topic modelling approach to extract coherent topics. We finally analyse the community-related topics evaluated and selected using our proposed topic coherence metrics.Overall, we contribute to provide effective approaches to assist social scientists towards analysing political events on Twitter. These approaches include topic coherence metrics, a time-sensitive topic modelling approach and approaches for classifying the community affiliations of Twitter users. Together they make progress to study and understand the connections and dynamics among communities on Twitter.Supervisors: Iadh Ounis, Craig Macdonald, Philip HabelThe thesis is available at http://theses.gla.ac.uk/41135/},
journal = {SIGIR Forum},
month = {mar},
pages = {38–39},
numpages = {2}
}

@inproceedings{10.1145/3107411.3107483,
author = {Kho, Soon Jye and Yalamanchili, Hima Bindu and Raymer, Michael L. and Sheth, Amit P.},
title = {A Novel Approach for Classifying Gene Expression Data Using Topic Modeling},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107483},
doi = {10.1145/3107411.3107483},
abstract = {Understanding the role of differential gene expression in cancer etiology and cellular process is a complex problem that continues to pose a challenge due to sheer number of genes and inter-related biological processes involved. In this paper, we employ an unsupervised topic model, Latent Dirichlet Allocation (LDA) to mitigate overfitting of high-dimensionality gene expression data and to facilitate understanding of the associated pathways. LDA has been recently applied for clustering and exploring genomic data but not for classification and prediction. Here, we proposed to use LDA in clustering as well as in classification of cancer and healthy tissues using lung cancer and breast cancer messenger RNA (mRNA) sequencing data. We describe our study in three phases: clustering, classification, and gene interpretation. First, LDA is used as a clustering algorithm to group the data in an unsupervised manner. Next we developed a novel LDA-based classification approach to classify unknown samples based on similarity of co-expression patterns. Evaluation to assess the effectiveness of this approach shows that LDA can achieve high accuracy compared to alternative approaches. Lastly, we present a functional analysis of the genes identified using a novel topic profile matrix formulation. This analysis identified several genes and pathways that could potentially be involved in differentiating tumor samples from normal. Overall, our results project LDA as a promising approach for classification of tissue types based on gene expression data in cancer studies.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {388–393},
numpages = {6},
keywords = {classification, latent dirichlet allocation, cancer, topic modeling, gene expression, clustering, machine learning},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@article{10.1145/3495530,
author = {Zhang, Peng and Liu, Baoxi and Lu, Tun and Ding, Xianghua and Gu, Hansu and Gu, Ning},
title = {Jointly Predicting Future Content in Multiple Social Media Sites Based on Multi-Task Learning},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3495530},
doi = {10.1145/3495530},
abstract = {User-generated contents (UGC) in social media are the direct expression of users’ interests, preferences, and opinions. User behavior prediction based on UGC has increasingly been investigated in recent years. Compared to learning a person’s behavioral patterns in each social media site separately, jointly predicting user behavior in multiple social media sites and complementing each other (cross-site user behavior prediction) can be more accurate. However, cross-site user behavior prediction based on UGC is a challenging task due to the difficulty of cross-site data sampling, the complexity of UGC modeling, and uncertainty of knowledge sharing among different sites. For these problems, we propose a Cross-Site Multi-Task (CSMT) learning method to jointly predict user behavior in multiple social media sites. CSMT mainly derives from the hierarchical attention network and multi-task learning. Using this method, the UGC in each social media site can obtain fine-grained representations in terms of words, topics, posts, hashtags, and time slices as well as the relevances among them, and prediction tasks in different social media sites can be jointly implemented and complement each other. By utilizing two cross-site datasets sampled from Weibo, Douban, Facebook, and Twitter, we validate our method’s superiority on several classification metrics compared with existing related methods.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {79},
numpages = {28},
keywords = {user-generated contents, Social media, hierarchical attention network, behavioral analytics, multi-task}
}

@article{10.1145/3130332.3130347,
author = {Sebastian, Yakub},
title = {Literature-Based Discovery by Learning Heterogeneous Bibliographic Information Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3130332.3130347},
doi = {10.1145/3130332.3130347},
abstract = {Literature-based discovery (LBD) research aims at finding effective computational methods for retrieving previously unknown information connections between clusters of research papers from disparate research areas. Existing methods encompass two general approaches. The first approach searches for these unknown connections by examining the textual contents of research papers. In addition to the existing textual features, the second approach incorporates structural features of scientific literatures, such as citation structures. These approaches, however, have not considered research papers' latent bibliographic metadata structures as important features that can be used for predicting previously unknown relationships between themThis thesis investigates a new graph-based LBD method that exploits the latent bibliographic metadata connections between pairs of research papers. The heterogeneous bibliographic information network is proposed as an efficient graph-based data structure for modeling the complex relationships between these metadata. In contrast to previous approaches, this method seamlessly combines textual and citation information in the form of path-based metadata features for predicting future co-citation links between research papers from disparate research fields. The results reported in this thesis provide evidence that the method is effective for reconstructing the historical literature-based discovery hypothesesThis thesis also investigates the effects of semantic modeling and topic modeling on the performance of the proposed method. For semantic modeling, a general-purpose word sense disambiguation technique is proposed to reduce the lexical ambiguity in the title and abstract of research papers. The experimental results suggest that the reduced lexical ambiguity did not necessarily lead to a better performance of the method. This thesis discusses some of the possible contributing factors to these results.Finally, topic modeling is used for learning the latent topical relations between research papers. The learned topic model is incorporated into the heterogeneous bibliographic information network graph and allows new predictive features to be learned. The results in this thesis suggest that topic modeling improves the performance of the proposed method by increasing the overall accuracy for predicting the future co-citation links between disparate research papers.},
journal = {SIGIR Forum},
month = {aug},
pages = {75–76},
numpages = {2}
}

@inproceedings{10.1145/3289600.3291036,
author = {Zaheer, Manzil and Ahmed, Amr and Wang, Yuan and Silva, Daniel and Najork, Marc and Wu, Yuchen and Sanan, Shibani and Chatterjee, Surojit},
title = {Uncovering Hidden Structure in Sequence Data via Threading Recurrent Models},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291036},
doi = {10.1145/3289600.3291036},
abstract = {Long Short-Term Memory (LSTM) is one of the most powerful sequence models for user browsing history citetan2016improved,korpusik2016recurrent or natural language text citemikolov2010recurrent.Despite the strong performance, it has not gained popularity for user-facing applications, mainly owing to a large number of parameters and lack of interpretability. Recently citetzaheer2017latent introduced latent LSTM Allocation (LLA) to address these problems by incorporating topic models with LSTM, where the topic model maps observed words in each sequence to topics that evolve using an LSTM model. In our experiments, we found the resulting model, although powerful and interpretable, to show shortcomings when applied to sequence data that exhibit multi-modes of behaviors with abrupt dynamic changes. To address this problem we introduce thLLA: a threading LLA model. thLLA has the ability to break each sequence into a set of segments and then model the dynamic in each segment using an LSTM mixture. In that way, thLLA can model abrupt changes in sequence dynamics and provides a better fit for sequence data while still being interpretable and requiring fewer parameters. In addition, thLLA uncovers hidden themes in the data via its dynamic mixture components. However, such generalization and interpretability come at a cost of complex dependence structure, for which inference would be extremely non-trivial. To remedy this, we present an efficient sampler based on particle MCMC method for inference that can draw from the joint posterior directly. Experimental results confirm the superiority of thLLA and the stability of the new inference algorithm on a variety of domains.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {186–194},
numpages = {9},
keywords = {sequence clustering, topic models, interpretable recurrent neural network},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3490486.3538354,
author = {Cai, Yang and Daskalakis, Constantinos},
title = {Recommender Systems Meet Mechanism Design},
year = {2022},
isbn = {9781450391504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490486.3538354},
doi = {10.1145/3490486.3538354},
abstract = {Machine learning has developed a variety of tools for learning and representing high-dimensional distributions with structure. Recent years have also seen big advances in designing multi-item mechanisms. Akin to overfitting, however, these mechanisms can be extremely sensitive to the Bayesian prior that they target, which becomes problematic when that prior is only approximately known. At the same time, even if access to the exact Bayesian prior is given, it is known that optimal or even approximately optimal multi-item mechanisms run into sample, computational, representation and communication intractability barriers.We consider a natural class of multi-item mechanism design problems with very large numbers of items, but where the bidders' value distributions can be well-approximated by a topic model akin to those used in recommendation systems with very large numbers of possible recommendations. We propose a mechanism design framework for this setting, building on a recent robustification framework by Brustle et al., which disentangles the statistical challenge of estimating a multi-dimensional prior from the task of designing a good mechanism for it, and robustifies the performance of the latter against the estimation error of the former. We provide an extension of this framework appropriate for our setting, which allows us to exploit the expressive power of topic models to reduce the effective dimensionality of the mechanism design problem and remove the dependence of its computational, communication and representation complexity on the number of items.},
booktitle = {Proceedings of the 23rd ACM Conference on Economics and Computation},
pages = {897–914},
numpages = {18},
keywords = {communication-efficient query protocols, indirect mechanisms, robustness, topic models, matrix factorization, multi-item auctions, revenue maximization},
location = {Boulder, CO, USA},
series = {EC '22}
}

@article{10.14778/3137628.3137649,
author = {Yut, Lele and Zhang, Ce and Shao, Yingxia and Cui, Bin},
title = {LDA*: A Robust and Large-Scale Topic Modeling System},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137649},
doi = {10.14778/3137628.3137649},
abstract = {We present LDA*, a system that has been deployed in one of the largest Internet companies to fulfil their requirements of "topic modeling as an internal service"---relying on thousands of machines, engineers in different sectors submit their data, some are as large as 1.8TB, to LDA* and get results back in hours. LDA* is motivated by the observation that none of the existing topic modeling systems is robust enough---Each of these existing systems is designed for a specific point in the tradeoff space that can be sub-optimal, sometimes by up to 10\texttimes{}, across workloads.Our first contribution is a systematic study of all recently proposed samplers: AliasLDA, F+LDA, LightLDA, and WarpLDA. We discovered a novel system tradeoff among these samplers. Each sampler has different sampling complexity and performs differently, sometimes by 5\texttimes{}, on documents with different lengths. Based on this tradeoff, we further developed a hybrid sampler that uses different samplers for different types of documents. This hybrid approach works across a wide range of workloads and outperforms the fastest sampler by up to 2x. We then focused on distributed environments in which thousands of workers, each with different performance (due to virtualization and resource sharing), coordinate to train a topic model. Our second contribution is an asymmetric parameter server architecture that pushes some computation to the parameter server side. This architecture is motivated by the skew of the word frequency distribution and a novel tradeoff we discovered between communication and computation. With this architecture, we outperform the traditional, symmetric architecture by up to 2\texttimes{}.With these two contributions, together with a carefully engineered implementation, our system is able to outperform existing systems by up to 10\texttimes{} and has already been running to provide topic modeling services for more than six months.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1406–1417},
numpages = {12}
}

@inproceedings{10.1145/3077136.3084138,
author = {Chin, Jin Yao and Bhowmick, Sourav S. and Jatowt, Adam},
title = {TOTEM: Personal Tweets Summarization on Mobile Devices},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3084138},
doi = {10.1145/3077136.3084138},
abstract = {Tweets summarization aims to find a group of representative tweets for a specific topic. In recent times, there have been several research efforts toward devising a variety of techniques to summarize tweets in Twitter. However, these techniques are either not personal (i.e., consider only tweets in the timeline of a specific user) or are too expensive to be realized on a mobile device. Given that 80% of active Twitter users access the site on mobile devices, in this demonstration we present a lightweight, personalized, on-demand, topic modeling-based tweets summarization engine called TOTEM, designed for such devices. Specifically, TOTEM summarizes most recent tweets on a user's timeline and enables her to visualize and navigate representative topics and associated tweets in a user-friendly tap-and-swipe manner.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1305–1308},
numpages = {4},
keywords = {personal, summarization, mobile device, topic modeling, tweets},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1145/2990507,
author = {Hoang, Tuan-Anh and Lim, Ee-Peng},
title = {Modeling Topics and Behavior of Microbloggers: An Integrated Approach},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2990507},
doi = {10.1145/2990507},
abstract = {Microblogging encompasses both user-generated content and behavior. When modeling microblogging data, one has to consider personal and background topics, as well as how these topics generate the observed content and behavior. In this article, we propose the Generalized Behavior-Topic (GBT) model for simultaneously modeling background topics and users’ topical interest in microblogging data. GBT considers multiple topical communities (or realms) with different background topical interests while learning the personal topics of each user and the user’s dependence on realms to generate both content and behavior. This differentiates GBT from other previous works that consider either one realm only or content data only. By associating user behavior with the latent background and personal topics, GBT helps to model user behavior by the two types of topics. GBT also distinguishes itself from other earlier works by modeling multiple types of behavior together. Our experiments on two Twitter datasets show that GBT can effectively mine the representative topics for each realm. We also demonstrate that GBT significantly outperforms other state-of-the-art models in modeling content topics and user profiling.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {44},
numpages = {37},
keywords = {user behavior, microblogging, Social media, probabilistic graphic model, behavior mining, topic modeling}
}

@inproceedings{10.1145/3503516.3503527,
author = {Yuan, Meng and Lin, Pauline and Zobel, Justin},
title = {Document Clustering vs Topic Models: A Case Study},
year = {2021},
isbn = {9781450395991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503516.3503527},
doi = {10.1145/3503516.3503527},
abstract = {Document collections can be characterised in a variety of ways. Two key approaches are clustering, which partitions collections into subcollections with the expectation that the contents will be thematically linked, and topic models, which describe the contents in terms of weighted lists of words that are expected to represent different themes. In this paper, we report experiments on the observed relationship between clusters and topic models in a preliminary study of a large text collection. Both produce results that appear cohesive in their own right, but surprisingly – given the very different ways in which they are formed – the descriptions of the collections that they generate are strongly similar. This unexpected mutual reinforcement creates confidence in both approaches as tools for annotating and describing the contents of document collections.},
booktitle = {Proceedings of the 25th Australasian Document Computing Symposium},
articleno = {6},
numpages = {8},
keywords = {document clustering, collection representation, topic models},
location = {Virtual Event, Australia},
series = {ADCS '21}
}

@inproceedings{10.5555/3511065.3511092,
author = {Weiss, Michael},
title = {Patterns for Managing Remote Software Projects},
year = {2020},
isbn = {9781941652169},
publisher = {The Hillside Group},
address = {USA},
abstract = {The trend towards working remotely has been accelerated significantly by the COVID-19 pandemic, in particular in software development. In this paper, we describe patterns for managing remote software projects. These patterns were mined from the literature by conducting a literature review and synthesizing the findings using a research synthesis framework developed in design science (van Burg &amp; Romme, 2004) in combination with the holistic pattern mining approach by Iba &amp; Isaku (2012), but using topic modeling instead of visual clustering to identify themes. The current paper describes two of the patterns: Information Flow and Shared Mental Model. The target audience for the patterns are managers of software-intensive organizations transitioning to a remote work mode, as well as students and researchers.},
booktitle = {Proceedings of the 27th Conference on Pattern Languages of Programs},
articleno = {20},
numpages = {8},
keywords = {remote software projects, pattern mining, topic modeling, patterns},
location = {Virtual Event},
series = {PLoP '20}
}

@inproceedings{10.1145/3290688.3290735,
author = {Habibabadi, Sedigheh Khademi and Haghighi, Pari Delir},
title = {Topic Modelling for Identification of Vaccine Reactions in Twitter},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290735},
doi = {10.1145/3290688.3290735},
abstract = {Background: Detection of vaccine safety signals depends on various established reporting systems, where there is inevitably a lag between an adverse reaction to a vaccine and the reporting of it, and subsequent processing of reports. Therefore, it is desirable to try and detect safety signals earlier, ideally close to real-time. Extensive use of social media has provided a platform for sharing and seeking health-related information, and the immediacy of social media conversations mean that they are an ideal candidate for early detection of vaccine safety signals. The objective of this study is to evaluate topic models for identifying user posts on Twitter that most likely contain vaccine safety signals. This is an initial step in the overall research to determine if reliable vaccine safety signals can be detected in social media streams. The techniques used were focused on identifying the model design and number of topics that best revealed documents that contained vaccine safety signals, to assist with dimension reduction and subsequent labelling of the text data. The study compared Gensim LDA, MALLET, and jLDADMM DMM models to determine the most effective model for detecting vaccine safety signals, assisted by an evaluation process that used an adjusted F-Scoring technique over a labelled subset of the documents.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {31},
numpages = {10},
keywords = {Topic modelling, Twitter, Social media, Vaccine safety surveillance},
location = {Sydney, NSW, Australia},
series = {ACSW 2019}
}

@inproceedings{10.1145/3357384.3357941,
author = {Wang, Xiaobao and Jin, Di and Liu, Mengquan and He, Dongxiao and Musial, Katarzyna and Dang, Jianwu},
title = {Emotional Contagion-Based Social Sentiment Mining in Social Networks by Introducing Network Communities},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357941},
doi = {10.1145/3357384.3357941},
abstract = {The rapid development of social media services has facilitated the communication of opinions through online news, blogs, microblogs, instant-messages, and so on. This article concentrates on the mining of readers' social sentiments evoked by social media materials. Existing methods are only applicable to a minority of social media like news portals with emotional voting information, while ignore the emotional contagion between writers and readers. However, incorporating such factors is challenging since the learned hidden variables would be very fuzzy (because of the short and noisy text in social networks). In this paper, we try to solve this problem by introducing a high-order network structure, i.e. communities. We first propose a new generative model called Community-Enhanced Social Sentiment Mining (CESSM), which 1) considers the emotional contagion between writers and readers to capture precise social sentiment, and 2) incorporates network communities to capture coherent topics. We then derive an inference algorithm based on Gibbs sampling. Empirical results show that, CESSM achieves significantly superior performance against the state-of-the-art techniques for text sentiment classification and interestingness in social sentiment mining.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1763–1772},
numpages = {10},
keywords = {emotional contagion, social network, social sentiment, community},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3323771.3323795,
author = {Intisar, Chowdhury Md and Watanobe, Yutaka and Poudel, Manoj and Bhalla, Subhash},
title = {Classification of Programming Problems Based on Topic Modeling},
year = {2019},
isbn = {9781450366397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323771.3323795},
doi = {10.1145/3323771.3323795},
abstract = {Programming skill is one of the most important and demanding skill in the current generation. In order to enable learners and programmers to practice programming and gain problem-solving skills, many Online Judge (OJ) systems exist. Most of these OJ systems have to be operated solely by students and learners. These students and novice programmers sometimes compete against each other or solve the programming problems by themselves in offline mode. But, most OJ systems have their problems arranged simply into volumes and various contests events. This arrangement system does not have any clear indication of the difficulties and categories of problems. Thus, in this paper, we have studied reliable techniques on the extraction of keywords and features which can categorize these OJ system's programming problems into their respective types and skills. We have leveraged two popular topic modeling algorithms, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) to extract relevant features. Afterward, six classifiers were trained on these topic modeling features and Naive TF-IDF features. From our studies, we discovered that topic modeling features were relatively smaller in dimensionality, yet matched the performance when trained on high dimensional naive TF-IDF features. Our main goal was to understand the precise trade-off between accuracy and dimensionality of the textual data of programming problem statements. This experiment has enabled us to obtain important tags, hint, and classification of Online Judge programming problems.},
booktitle = {Proceedings of the 2019 7th International Conference on Information and Education Technology},
pages = {275–283},
numpages = {9},
keywords = {text classification, topic modeling, feature extraction, novice programmer, Online Judge Systems},
location = {Aizu-Wakamatsu, Japan},
series = {ICIET 2019}
}

@article{10.1145/3425192,
author = {Joshi, Karuna Pande and Saha, Srishty},
title = {A Semantically Rich Framework for Knowledge Representation of Code of Federal Regulations},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-199X},
url = {https://doi.org/10.1145/3425192},
doi = {10.1145/3425192},
abstract = {Federal government agencies and organizations doing business with them have to adhere to the Code of Federal Regulations (CFR). The CFRs are currently available as large text documents that are not machine processable and so require extensive manual effort to parse and comprehend, especially when sections cross-reference topics spread across various titles. We have developed a novel framework to automatically extract knowledge from CFRs and represent it using a semantically rich knowledge graph. The framework captures knowledge in the form of key terms, rules, topic summaries, relationships between various terms, semantically similar terminologies, deontic expressions, and cross-referenced facts and rules. We built our framework using deep learning technologies like TensorFlow for word embeddings and text summarization, Gensim for topic modeling, and Semantic Web technologies for building the knowledge graph. In this article, we describe our framework in detail and present the results of our analysis of the Title 48 CFR knowledge base that we have built using this framework. Our framework and knowledge graph can be adopted by federal agencies and businesses to automate their internal processes that reference the CFR rules and policies.},
journal = {Digit. Gov.: Res. Pract.},
month = {nov},
articleno = {21},
numpages = {17},
keywords = {Semantic Web, legal text analytics, compliance, Deep learning}
}

@inproceedings{10.1145/3383583.3398521,
author = {Zhang, Jinsong and Guo, Chun and Liu, Xiaozhong},
title = {Characterize and Evaluate Scientific Domain and Domain Context Knowledge Map},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398521},
doi = {10.1145/3383583.3398521},
abstract = {Domain knowledge map, a.k.a., scholarly network, construction as an important method can describe the significant characters of a selected domain. In this research, we will address three fundamental problems for scholarly network generation. Firstly, two different methods will be investigated to associate keywords on the graph: Co-occur Domain Distance and Citation Probability Distribution Distance. Secondly, this paper will construct domain (core journals and conference proceedings) knowledge and domain referral (domain citation) scholarly networks, and propose a novel method to integrate those graphs by optimizing the nodes and their linkage. Finally, the paper will propose an innovative method to evaluate the accuracy and coverage of scholarly networks based on training keyword oriented Labeled-LDA model and validate different domain or domain referral graphs.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {187–196},
numpages = {10},
keywords = {knowledge map, domain context, domain characterization, scientific domain},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3478905.3478922,
author = {Fu, Zhu and Ding, Weike and Guan, Peng and Ding, Xuhui},
title = {Topic Mining of Modern Poetry For Digital Humanities: Case Study of Gu Cheng Poetry},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478922},
doi = {10.1145/3478905.3478922},
abstract = {Using topic extraction and visualization method to explore the themes of Gu Cheng's complete poems, which provides a reference for the quantitative interpretation and analysis of modern poetry works. According to Gu's self-staging, we divide Gu's complete poems, and use Latent Dirichlet Allocation (LDA) model and word cloud visualization technology to conduct staged topic extraction and visual analysis of Gu's poems. Finally, we divide Gu's poems creation into four stages of nature stage, culture stage, counter-culture stage, and non-self stage. On the staged topic extraction experiment of Gu's poems, we obtain four stages of topic extraction results and word frequency cloud maps, and in-depth analyze the theme changes and reasons of Gu's poems at different stages. Results show that the used topic extraction and visualization method could objectively and comprehensively reveal the content and change process of Gu's poems.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {78–83},
numpages = {6},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3474717.3484212,
author = {Wang, Dongjie and Liu, Kunpeng and Mohaisen, David and Wang, Pengyang and Lu, Chang-Tien and Fu, Yanjie},
title = {Automated Feature-Topic Pairing: Aligning Semantic and Embedding Spaces in Spatial Representation Learning},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3484212},
doi = {10.1145/3474717.3484212},
abstract = {Automated characterization of spatial data is a kind of critical geographical intelligence. As an emerging technique for characterization, Spatial Representation Learning (SRL) uses deep neural networks (DNNs) to learn non-linear embedded features of spatial data for characterization. However, SRL extracts features by internal layers of DNNs, and thus suffers from lacking semantic labels. Texts of spatial entities, on the other hand, provide semantic understanding of latent feature labels, but is insensible to deep SRL models. How can we teach a SRL model to discover appropriate topic labels in texts and pair learned features with the labels? This paper formulates a new problem: feature-topic pairing, and proposes a novel Particle Swarm Optimization (PSO) based deep learning framework. Specifically, we formulate the feature-topic pairing problem into an automated alignment task between 1) a latent embedding feature space and 2) a textual semantic topic space. We decompose the alignment of the two spaces into: 1) point-wise alignment, denoting the correlation between a topic distribution and an embedding vector; 2) pair-wise alignment, denoting the consistency between a feature-feature similarity matrix and a topic-topic similarity matrix. We design a PSO based solver to simultaneously select an optimal set of topics and learn corresponding features based on the selected topics. We develop a closed loop algorithm to iterate between 1) minimizing losses of representation reconstruction and feature-topic alignment and 2) searching the best topics. Finally, we present extensive experiments to demonstrate the enhanced performance of our method.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {450–453},
numpages = {4},
keywords = {spatial representation learning K@multiple spaces alignment},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@inproceedings{10.1145/3514094.3534145,
author = {Barnett, Julia and Diakopoulos, Nicholas},
title = {Crowdsourcing Impacts: Exploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic Decision Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534145},
doi = {10.1145/3514094.3534145},
abstract = {With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {56–67},
numpages = {12},
keywords = {anticipatory governance, ai ethics, broader impacts, thematic analysis},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3475716.3475786,
author = {Gonzalez, Danielle and Perez, Paola Peralta and Mirakhorli, Mehdi},
title = {Barriers to Shift-Left Security: The Unique Pain Points of Writing Automated Tests Involving Security Controls},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475786},
doi = {10.1145/3475716.3475786},
abstract = {Background: Automated unit and integration tests allow software development teams to continuously evaluate their application's behavior and ensure requirements are satisfied. Interest in explicitly testing security at the unit and integration levels has risen as more teams begin to shift security left in their workflows, but there is little insight into any potential pain points developers may experience as they learn to adapt their existing skills to write these tests. Aims: Identify security unit and integration testing pain points that could negatively impact efforts to shift security (testing) left to this level. Method: An mixed-method empirical study was conducted on 525 Stack Overflow and Security Stack Exchange posts related to security unit and integration testing. Latent Dirichlet Allocation (LDA) was applied to identify commonly discussed topics, pain points were learned through qualitative analysis, and links were analyzed to study commonly-shared resources. Results: Nine topics representing security controls, components, and scenarios were identified; Authentication was the most commonly tested control. Developers experienced seven pain points unique to security unit and integration testing, which were all influenced by the complexity of security control designs and implementations. Most linked resources were other Q&amp;A posts, but repositories and documentation for security tools and libraries were also common. Conclusions: Developers may experience several unique pain points when writing tests at this level involving security controls. Additional resources are needed to guide developers through these challenges, which should also influence the creation of strategies and tools to help shift security testing to this level. To accelerate this, actionable recommendations for practitioners and future research directions based on these findings are highlighted.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {11},
numpages = {12},
keywords = {Pain Points, Shift-Left Security, Latent Dirichlet Allocation, Unit Testing, Security Testing, Stack Overflow, Integration Testing},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3027385.3027438,
author = {Slater, Stefan and Baker, Ryan and Almeda, Ma. Victoria and Bowers, Alex and Heffernan, Neil},
title = {Using Correlational Topic Modeling for Automated Topic Identification in Intelligent Tutoring Systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027438},
doi = {10.1145/3027385.3027438},
abstract = {Student knowledge modeling is an important part of modern personalized learning systems, but typically relies upon valid models of the structure of the content and skill in a domain. These models are often developed through expert tagging of skills to items. However, content creators in crowdsourced personalized learning systems often lack the time (and sometimes the domain knowledge) to tag skills themselves. Fully automated approaches that rely on the covariance of correctness on items can lead to effective skill-item mappings, but the resultant mappings are often difficult to interpret. In this paper we propose an alternate approach to automatically labeling skills in a crowdsourced personalized learning system using correlated topic modeling, a natural language processing approach, to analyze the linguistic content of mathematics problems. We find a range of potentially meaningful and useful topics within the context of the ASSISTments system for mathematics problem-solving.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {393–397},
numpages = {5},
keywords = {natural language processing, topic modeling, correlational topic modeling, intelligent tutoring systems, mathematics education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3377325.3377491,
author = {Smith-Renner, Alison and Kumar, Varun and Boyd-Graber, Jordan and Seppi, Kevin and Findlater, Leah},
title = {Digging into User Control: Perceptions of Adherence and Instability in Transparent Models},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377491},
doi = {10.1145/3377325.3377491},
abstract = {We explore predictability and control in interactive systems where controls are easy to validate. Human-in-the-loop techniques allow users to guide unsupervised algorithms by exposing and supporting interaction with underlying model representations, increasing transparency and promising fine-grained control. However, these models must balance user input and the underlying data, meaning they sometimes update slowly, poorly, or unpredictably---either by not incorporating user input as expected (adherence) or by making other unexpected changes (instability). While prior work exposes model internals and supports user feedback, less attention has been paid to users' reactions when transparent models limit control. Focusing on interactive topic models, we explore user perceptions of control using a study where 100 participants organize documents with one of three distinct topic modeling approaches. These approaches incorporate input differently, resulting in varied adherence, stability, update speeds, and model quality. Participants disliked slow updates most, followed by lack of adherence. Instability was polarizing: some participants liked it when it surfaced interesting information, while others did not. Across modeling approaches, participants differed only in whether they noticed adherence.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {519–530},
numpages = {12},
keywords = {control, transparency, topic modeling, interactive machine learning, intelligent user interface evaluation},
location = {Cagliari, Italy},
series = {IUI '20}
}

@article{10.1109/TASLP.2020.3012062,
author = {Kesiraju, Santosh and Plchot, Old\v{r}ich and Burget, Luk\'{a}\v{s} and Gangashetty, Suryakanth V.},
title = {Learning Document Embeddings Along With Their Uncertainties},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3012062},
doi = {10.1109/TASLP.2020.3012062},
abstract = {Majority of the text modeling techniques yield only point-estimates of document embeddings and lack in capturing the uncertainty of the estimates. These uncertainties give a notion of how well the embeddings represent a document. We present Bayesian subspace multinomial model (Bayesian SMM), a generative log-linear model that learns to represent documents in the form of Gaussian distributions, thereby encoding the uncertainty in its covariance. Additionally, in the proposed Bayesian SMM, we address a commonly encountered problem of intractability that appears during variational inference in mixed-logit models. We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings. Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the unseen test data better as compared to the state-of-the-art neural variational document model on (Fisher) speech and (20Newsgroups) text corpora. Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data. The topic ID results show that the proposed model outperforms state-of-the-art unsupervised topic models and achieve comparable results to the state-of-the-art fully supervised discriminative models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {2319–2332},
numpages = {14}
}

@inproceedings{10.1145/3308560.3316749,
author = {Bhargava, Preeti and Spasojevic, Nemanja and Ellinger, Sarah and Rao, Adithya and Menon, Abhinand and Fuhrmann, Saul and Hu, Guoning},
title = {Learning to Map Wikidata Entities To Predefined Topics},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316749},
doi = {10.1145/3308560.3316749},
abstract = {Recently much progress has been made in entity disambiguation and linking systems (EDL). Given a piece of text, EDL links words and phrases to entities in a knowledge base, where each entity defines a specific concept. Although extracted entities are informative, they are often too specific to be used directly by many applications. These applications usually require text content to be represented with a smaller set of predefined concepts or topics, belonging to a topical taxonomy, that matches their exact needs. In this study, we aim to build a system that maps Wikidata entities to such predefined topics. We explore a wide range of methods that map entities to topics, including GloVe similarity, Wikidata predicates, Wikipedia entity definitions, and entity-topic co-occurrences. These methods often predict entity-topic mappings that are reliable, i.e., have high precision, but tend to miss most of the mappings, i.e., have low recall. Therefore, we propose an ensemble system that effectively combines individual methods and yields much better performance, comparable with human annotators.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1194–1202},
numpages = {9},
keywords = {wikidata, entity topic mapping, entity topic assignment, wikipedia, natural language processing, knowledge base},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3511095.3531269,
author = {Huang, Xiaolei and Wormley, Alexandra and Cohen, Adam},
title = {Learning to Adapt Domain Shifts of Moral Values via Instance Weighting},
year = {2022},
isbn = {9781450392334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511095.3531269},
doi = {10.1145/3511095.3531269},
abstract = {Classifying moral values in user-generated text from social media is critical in understanding community cultures and interpreting user behaviors of social movements. Moral values and language usage can change across the social movements; however, text classifiers are usually trained in source domains of existing social movements and tested in target domains of new social issues without considering the variations. In this study, we examine domain shifts of moral values and language usage, quantify the effects of domain shifts on the morality classification task, and propose a neural adaptation framework via instance weighting to improve cross-domain classification tasks. The quantification analysis suggests a strong correlation between morality shifts, language usage, and classification performance. We evaluate the neural adaptation framework on a public Twitter data across 7 social movements and gain classification improvements up to 12.1%. Finally, we release a new data of the COVID-19 vaccine labeled with moral values and evaluate our approach on the new target domain. For the case study of the COVID-19 vaccine, our adaptation framework achieves up to 5.26% improvements over neural baselines. This is the first study to quantify impacts of moral shifts, propose adaptive framework to model the shifts, and conduct a case study to model COVID-19 vaccine-related behaviors from moral values.},
booktitle = {Proceedings of the 33rd ACM Conference on Hypertext and Social Media},
pages = {121–131},
numpages = {11},
keywords = {classification, domain variation, adaptation, morality, instant weighting, moral values},
location = {Barcelona, Spain},
series = {HT '22}
}

@inproceedings{10.1145/3097983.3098122,
author = {Wang, Hao and Fu, Yanmei and Wang, Qinyong and Yin, Hongzhi and Du, Changying and Xiong, Hui},
title = {A Location-Sentiment-Aware Recommender System for Both Home-Town and Out-of-Town Users},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098122},
doi = {10.1145/3097983.3098122},
abstract = {Spatial item recommendation has become an important means to help people discover interesting locations, especially when people pay a visit to unfamiliar regions. Some current researches are focusing on modelling individual and collective geographical preferences for spatial item recommendation based on users' check-in records, but they fail to explore the phenomenon of user interest drift across geographical regions, i.e., users would show different interests when they travel to different regions. Besides, they ignore the influence of public comments for subsequent users' check-in behaviors. Specifically, it is intuitive that users would refuse to check in to a spatial item whose historical reviews seem negative overall, even though it might fit their interests. Therefore, it is necessary to recommend the right item to the right user at the right location. In this paper, we propose a latent probabilistic generative model called LSARS to mimic the decision-making process of users' check-in activities both in home-town and out-of-town scenarios by adapting to user interest drift and crowd sentiments, which can learn location-aware and sentiment-aware individual interests from the contents of spatial items and user reviews. Due to the sparsity of user activities in out-of-town regions, LSARS is further designed to incorporate the public preferences learned from local users' check-in behaviors. Finally, we deploy LSARS into two practical application scenes: spatial item recommendation and target user discovery. Extensive experiments on two large-scale location-based social networks (LBSNs) datasets show that LSARS achieves better performance than existing state-of-the-art methods.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1143},
numpages = {9},
keywords = {recommendation, crowd sentiment, check-in behavior, user interest drift},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3531146.3533107,
author = {Laufer, Benjamin and Jain, Sameer and Cooper, A. Feder and Kleinberg, Jon and Heidari, Hoda},
title = {Four Years of FAccT: A Reflexive, Mixed-Methods Analysis of Research Contributions, Shortcomings, and Future Prospects},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533107},
doi = {10.1145/3531146.3533107},
abstract = {Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {401–426},
numpages = {26},
keywords = {FAccT, impact, mixed methods, community perspectives, values, reflexivity, topics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3366424.3382678,
author = {Li, Quanzhi and Zhang, Qiong},
title = {Abstractive Event Summarization on Twitter},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382678},
doi = {10.1145/3366424.3382678},
abstract = {This paper presents a new approach for automatically summarizing a social media event. It utilizes the BERT model as the encoder and a Transformer architecture as the decoder. The framework also includes an event topic prediction component, and the predicted event topic will help the decoder focus more on the specific aspects of the topic category when generating summary. To make the summary more succinct and coherent, the most important messages from an event cluster are selected by a message selection model and encoded by the BERT model. Our preliminary experiment shows that our approach outperforms the baseline methods.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {22–23},
numpages = {2},
keywords = {event summary, event topic, BERT, social media, Twitter},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3340531.3412050,
author = {Aloteibi, Saad and Clark, Stephen},
title = {Learning to Personalize for Web Search Sessions},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412050},
doi = {10.1145/3340531.3412050},
abstract = {The task of session search focuses on using interaction data to improve relevance for the user's next query at the session level. In this paper, we formulate session search as a personalization task under the framework of learning to rank. Personalization approaches re-rank results to match a user model. Such user models are usually accumulated over time based on the user's browsing behaviour. We use a pre-computed and transparent set of user models based on concepts from the social science literature. Interaction data are used to map each session to these user models. Novel features are then estimated based on such models as well as sessions' interaction data. Extensive experiments on test collections from the TREC session track show statistically significant improvements over current session search algorithms.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {15–24},
numpages = {10},
keywords = {user models, session search, retrieval model, personalization},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3366423.3380278,
author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Wang, Zihan and Zhang, Chao and Zhang, Yu and Han, Jiawei},
title = {Discriminative Topic Mining via Category-Name Guided Text Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380278},
doi = {10.1145/3366423.3380278},
abstract = {Mining a set of meaningful and distinctive topics automatically from massive text corpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users’ particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guided text embedding method for discriminative topic mining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatE mines high-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2121–2132},
numpages = {12},
keywords = {Text Classification, Text Embedding, Discriminative Analysis, Topic Mining},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3512900,
author = {Engel, Kristen and Hua, Yiqing and Zeng, Taixiang and Naaman, Mor},
title = {Characterizing Reddit Participation of Users Who Engage in the QAnon Conspiracy Theories},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512900},
doi = {10.1145/3512900},
abstract = {Widespread conspiracy theories may significantly impact our society. This paper focuses on the QAnon conspiracy theory, a consequential conspiracy theory that started on and disseminated successfully through social media. Our work characterizes how Reddit users who have participated in QAnon-focused subreddits engage in activities on the platform, especially outside their own communities. Using a large-scale Reddit moderation action against QAnon-related activities in 2018 as the source, we identified 13,000 users active in the early QAnon communities. We collected the 2.1 million submissions and 10.8 million comments posted by these users across all of Reddit from October 2016 to January 2021. The majority of these users were only active after the emergence of the QAnon conspiracy theory and decreased in activity after Reddit's 2018 QAnon ban. A qualitative analysis of a sample of 915 subreddits where the "QAnon-enthusiastic" users were especially active shows that they participated in a diverse range of subreddits, often of unrelated topics to QAnon. However, most of the users' submissions were concentrated in subreddits that have sympathetic attitudes towards the conspiracy theory, characterized by discussions that were pro-Trump, or emphasized unconstricted behavior (often anti-establishment and anti-interventionist). Further study of a sample of 1,571 of these submissions indicates that most consist of links from low-quality sources, bringing potential harm to the broader Reddit community. These results point to the likelihood that the activities of early QAnon users on Reddit were dedicated and committed to the conspiracy, providing implications on both platform moderation design and future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {53},
numpages = {22},
keywords = {QAnon, content moderation, Reddit ban, online communities, conspiracy theories}
}

@article{10.1145/3511097,
author = {Nasim, Zarmeen and Haider, Sajjad},
title = {Automatic Labeling of Clusters for a Low-Resource Urdu Language},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511097},
doi = {10.1145/3511097},
abstract = {Document clustering techniques often produce clusters that require human intervention to interpret the meaning of such clusters. Automatic cluster labeling refers to the process of assigning a meaningful phrase to a cluster as a label. This article proposes an unsupervised method for cluster labeling that is based on noun phrase chunking. The proposed method is compared with four other statistical-based methods, including Z-Order, M-Order, T-Order, and YAKE. In addition to the statistical measures based labeling schemes, the approach is also compared with two graph-based techniques: TextRank and PositionRank. The experiments were performed on the low-resource Urdu language corpus of News Headlines. The proposed approach's effectiveness was evaluated using cosine similarity, the Jaccard index, and feedback received from human evaluators. The results show that the proposed method outperforms other methods. It was found that the labels produced were more relevant and semantically rich in contrast to other approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {93},
numpages = {22},
keywords = {low-resource language, Cluster labeling, urdu language processing}
}

@inproceedings{10.1145/3209281.3209378,
author = {Deng, Qing and Cai, Guoray and Zhang, Hui and Liu, Yi and Huang, Lida and Sun, Feng},
title = {Enhancing Situation Awareness of Public Safety Events by Visualizing Topic Evolution Using Social Media},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209378},
doi = {10.1145/3209281.3209378},
abstract = {Social media contributes to enhancing transparency and openness for the purpose of innovating public services and policy-making. In disaster management, social media data can be mined to discover public perceptions and concerns on large disaster events. However, converting large data streams into useful information remains a challenge due to the unstructured nature of textual data. This study proposes an interactive topic modeling method to analyze microblog data for understanding the dynamics of public expressions immediately after a major explosion event. First, we extract topics from microblog message data. In order to test the influence of the number of topics, the topics are detected at multiple levels of granularity by varying the number of topics. Second, these topics are used to detect topical compositions of contents at different time slices and assess the topic evolution over time. The topic evolution patterns are visualized by the streamgraph method to discover informative topics to help to take further actions. Third, since the first-level topics are not informative, we conduct a second-level topic (subtopic) analysis to detect key decision elements by choosing "investigation" from the first-level topics, a hot focus in any man-made disaster. The results improve our understanding of the topic composition evolution around major man-made disasters and have implications on officials deciding what and when to release formal investigation information to the public.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {7},
numpages = {10},
keywords = {massive man-made disasters, social media, topic evolution, situation awareness, data analysis},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3184558.3186979,
author = {Schneider, Rudolf and Arnold, Sebastian and Oberhauser, Tom and Klatt, Tobias and Steffek, Thomas and L\"{o}ser, Alexander},
title = {Smart-MD: Neural Paragraph Retrieval of Medical Topics},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186979},
doi = {10.1145/3184558.3186979},
abstract = {We demonstrate Smart-MD, an information retrieval system for medical professionals. The system supports topical queries in the form [disease topic], such as ["lyme disease", treatments]. In contrast to document-oriented retrieval systems, Smart-MD retrieves relevant paragraphs and reduces the reading load of a medical doctor drastically. We recognize diseases and topical aspects with a novel paragraph retrieval method based on bidirectional LSTM neural networks. We demonstrate Smart-MD on a dataset that contains 3,469 diseases from the English language part of Wikipedia and 6,876 distinct medical aspects extracted from Wikipedia headlines.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {203–206},
numpages = {4},
keywords = {neural information classification, paragraph retrieval},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3485447.3512034,
author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
title = {Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512034},
doi = {10.1145/3485447.3512034},
abstract = {Topic models have been the prominent tools for automatic topic discovery from text corpora. Despite their effectiveness, topic models suffer from several limitations including the inability of modeling word ordering information in documents, the difficulty of incorporating external linguistic knowledge, and the lack of both accurate and efficient inference methods for approximating the intractable posterior. Recently, pretrained language models (PLMs) have brought astonishing performance improvements to a wide variety of tasks due to their superior representations of text. Interestingly, there have not been standard approaches to deploy PLMs for topic discovery as better alternatives to topic models. In this paper, we begin by analyzing the challenges of using PLM representations for topic discovery, and then propose a joint latent space learning and clustering framework built upon PLM embeddings. In the latent space, topic-word and document-topic distributions are jointly modeled so that the discovered topics can be interpreted by coherent and distinctive terms and meanwhile serve as meaningful summaries of the documents. Our model effectively leverages the strong representation power and superb linguistic features brought by PLMs for topic discovery, and is conceptually simpler than topic models. On two benchmark datasets in different domains, our model generates significantly more coherent and diverse topics than strong topic models, and offers better topic-wise document representations, based on both automatic and human evaluations.1},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3143–3152},
numpages = {10},
keywords = {Clustering, Topic Discovery, Pretrained Language Models},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3078971.3079004,
author = {Momeni, Elaheh and Rawassizadeh, Reza and Adar, Eytan},
title = {Leveraging Semantic Facets for Adaptive Ranking of Social Comments},
year = {2017},
isbn = {9781450347013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078971.3079004},
doi = {10.1145/3078971.3079004},
abstract = {An essential part of the social media ecosystem is user-generated comments. However, not all comments are useful to all people as both authors of comments and readers have different intentions and perspectives. Consequently, the development of automated approaches for the ranking of comments and the optimization of viewers' interaction experiences are becoming increasingly important. This work proposes an adaptive faceted ranking framework which enriches comments along multiple semantic facets (e.g., subjectivity, informativeness, and topics), thus enabling users to explore different facets and select combinations of facets in order to extract and rank comments that match their interests. A prototype implementation of the framework has been developed which allows us to evaluate different ranking strategies of the proposed framework. We find that adaptive faceted ranking shows significant improvements over prevalent ranking methods which are utilized by many platforms such as YouTube or The Economist. We observe substantial improvements in user experience when enriching each element of a comment along multiple explicit semantic facets rather than in a single topic or subjective facets.},
booktitle = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
pages = {356–364},
numpages = {9},
keywords = {semantic facets, social comment, adaptive ranking},
location = {Bucharest, Romania},
series = {ICMR '17}
}

@inproceedings{10.1145/3406324.3410710,
author = {Leiva, Luis A. and Hota, Asutosh and Oulasvirta, Antti},
title = {Enrico: A Dataset for Topic Modeling of Mobile UI Designs},
year = {2020},
isbn = {9781450380522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406324.3410710},
doi = {10.1145/3406324.3410710},
abstract = {Topic modeling of user interfaces (UIs), also known as layout design categorization, contributes to a better understanding of the UI functionality. Starting from Rico, a large dataset of mobile UIs, we revised a random sample of 10k UIs and concluded to Enrico (shorthand of Enhanced Rico), a human-supervised high-quality dataset comprising 1460 UIs and 20 design topics. As a validation example, we train a deep learning model for three different UI representations (screenshots, wireframes, and embeddings). The screenshot representation provides the highest discriminative power (95% AUC) and a competitive accuracy of 75% (a random classifier achieves 5% accuracy in the same task). We discuss several applications that can be developed with this new public resource, including e.g. semantic UI captioning and tagging, explainable UI designs, smart tutorials, and improved design search capabilities.},
booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {9},
numpages = {4},
keywords = {Machine learning, Neural networks, User interface design, Layout classification},
location = {Oldenburg, Germany},
series = {MobileHCI '20}
}

@inproceedings{10.1145/3485447.3512266,
author = {Solovev, Kirill and Pr\"{o}llochs, Nicolas},
title = {Moral Emotions Shape the Virality of COVID-19 Misinformation on Social Media},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512266},
doi = {10.1145/3485447.3512266},
abstract = {While false rumors pose a threat to the successful overcoming of the COVID-19 pandemic, an understanding of how rumors diffuse in online social networks is – even for non-crisis situations – still in its infancy. Here we analyze a large sample consisting of COVID-19 rumor cascades from Twitter that have been fact-checked by third-party organizations. The data comprises N = 10,610 rumor cascades that have been retweeted more than 24 million times. We investigate whether COVID-19 misinformation spreads more viral than the truth and whether the differences in the diffusion of true vs. false rumors can be explained by the moral emotions they carry. We observe that, on average, COVID-19 misinformation is more likely to go viral than truthful information. However, the veracity effect is moderated by moral emotions: false rumors are more viral than the truth if the source tweets embed a high number of other-condemning emotion words, whereas a higher number of self-conscious emotion words is linked to a less viral spread. The effects are pronounced both for health misinformation and false political rumors. These findings offer insights into how true vs. false rumors spread and highlight the importance of considering emotions from the moral emotion families in social media content.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3706–3717},
numpages = {12},
keywords = {COVID-19, Social media, explanatory modeling, misinformation, moral emotions, virality, computational social science},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3336191.3371863,
author = {Wei, Jiaqi and Han, Shuo and Zou, Lei},
title = {VISION-KG: Topic-Centric Visualization System for Summarizing Knowledge Graph},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371863},
doi = {10.1145/3336191.3371863},
abstract = {Large scale knowledge graph (KG) has attracted wide attentions in both academia and industry recently. However, due to the complexity of SPARQL syntax and massive volume of real KG, it remains difficult for ordinary users to access KG. In this demo, we present VISION-KG, a topic-centric visualization system to help users navigate KG easily via entity summarization and entity clustering. Given a query entity v0, VISION-KG summarizes the induced subgraph of v0's neighbor nodes via our proposed facts ranking method that measures importance, relatedness and diversity. Moreover, to achieve conciseness, we split the summarized graph into several topic-centric summarized subgraph according to semantic and structural similarities among entities. We will demonstrate how VISION-KG provides a user-friendly visualization interface for navigating KG.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {857–860},
numpages = {4},
keywords = {knowledge graph visualization, entity clustering, topic-centric, entity summarization},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1145/3394831,
author = {Al-Ramahi, Mohammad and Noteboom, Cherie},
title = {Mining User-Generated Content of Mobile Patient Portal: Dimensions of User Experience},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2469-7818},
url = {https://doi.org/10.1145/3394831},
doi = {10.1145/3394831},
abstract = {Patient portals are positioned as a central component of patient engagement through the potential to change the physician-patient relationship and enable chronic disease self-management. The incorporation of patient portals provides the promise to deliver excellent quality, at optimized costs, while improving the health of the population. This study extends the existing literature by extracting dimensions related to the Mobile Patient Portal Use. We use a topic modeling approach to systematically analyze users’ feedback from the actual use of a common mobile patient portal, Epic's MyChart. Comparing results of Latent Dirichlet Allocation analysis with those of human analysis validated the extracted topics. Practically, the results provide insights into adopting mobile patient portals, revealing opportunities for improvement and to enhance the design of current basic portals. Theoretically, the findings inform the social-technical systems and Task-Technology Fit theories in the healthcare field and emphasize important healthcare structural and social aspects. Further, findings inform the humanization of healthcare framework, support the results of existing studies, and introduce new important design dimensions (i.e., aspects) that influence patient satisfaction and adherence to patient portal.},
journal = {Trans. Soc. Comput.},
month = {jun},
articleno = {15},
numpages = {24},
keywords = {sentiment analysis, Patient portal, predictive analytics, latent dirichlet allocation (LDA), user-generated contents, explanatory analysis}
}

@inproceedings{10.1145/3477495.3531812,
author = {Zhang, Dake and Vakili Tahami, Amir and Abualsaud, Mustafa and Smucker, Mark D.},
title = {Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531812},
doi = {10.1145/3477495.3531812},
abstract = {When searching the web for answers to health questions, people can make incorrect decisions that have a negative effect on their lives if the search results contain misinformation. To reduce health misinformation in search results, we need to be able to detect documents with correct answers and promote them over documents containing misinformation. Determining the correct answer has been a difficult hurdle to overcome for participants in the TREC Health Misinformation Track. In the 2021 track, automatic runs were not allowed to use the known answer to a topic's health question, and as a result, the top automatic run had a compatibility-difference score of 0.043 while the top manual run, which used the known answer, had a score of 0.259. The compatibility-difference measures the ability of methods to rank correct and credible documents before incorrect and non-credible documents. By using an existing set of health questions and their known answers, we show it is possible to learn which web hosts are trustworthy, from which we can predict the correct answers to the 2021 health questions with an accuracy of 76%. Using our predicted answers, we can promote documents that we predict contain this answer and achieve a compatibility-difference score of 0.129, which is a three-fold increase in performance over the best previous automatic method.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2099–2104},
numpages = {6},
keywords = {health misinformation, stance detection, web search},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3449170,
author = {Adhikary, Rishiraj and Patel, Zeel B. and Srivastava, Tanmay and Batra, Nipun and Singh, Mayank and Bhatia, Udit and Guttikunda, Sarath},
title = {Vartalaap: What Drives #AirQuality Discussions: Politics, Pollution or Pseudo-Science?},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449170},
doi = {10.1145/3449170},
abstract = {Air pollution is a global challenge for cities across the globe. Understanding the public perception of air pollution can help policymakers engage better with the public and appropriately introduce policies. Accurate public perception can also help people to identify the health risks of air pollution and act accordingly. Unfortunately, current techniques for determining perception are not scalable: it involves surveying few hundred people with questionnaire-based surveys. Using the advances in natural language processing (NLP), we propose a more scalable solution called Vartalaap to gauge public perception of air pollution via the microblogging social network Twitter. We curated a dataset of more than 1.2M tweets discussing Delhi-specific air pollution. We find that (unfortunately) the public is supportive of unproven mitigation strategies to reduce pollution, thus risking their health due to a false sense of security. We also find that air quality is a year-long problem, but the discussions are not proportional to the level of pollution and spike up when pollution is more visible. The information required by Vartalaap is publicly available and, as such, it can be immediately applied to study different societal issues across the world.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {96},
numpages = {29},
keywords = {air pollution, social media, perception}
}

@article{10.1145/3458770,
author = {Biester, Laura and Matton, Katie and Rajendran, Janarthanan and Provost, Emily Mower and Mihalcea, Rada},
title = {Understanding the Impact of COVID-19 on Online Mental Health Forums},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3458770},
doi = {10.1145/3458770},
abstract = {Like many of the disasters that have preceded it, the COVID-19 pandemic is likely to have a profound impact on people’s mental health. Understanding its impact can inform strategies for mitigating negative consequences. This work seeks to better understand the impacts of COVID-19 on mental health by examining how discussions on mental health subreddits have changed in the three months following the WHO’s declaration of a global pandemic. First, the rate at which the pandemic is discussed in each community is quantified. Then, volume of activity is measured to determine whether the number of people with mental health concerns has risen, and user interactions are analyzed to determine how they have changed during the pandemic. Finally, the content of the discussions is analyzed. Each of these metrics is considered with respect to a set of control subreddits to better understand if the changes present are specific to mental health subreddits or are representative of Reddit as a whole. There are numerous changes in the three mental health subreddits that we consider, r/Anxiety, r/depression, r/SuicideWatch; there is reduced posting activity in most cases, and there are significant changes in discussion of some topics such as work and anxiety. The results suggest that there is not an overwhelming increase in online mental health support-seeking on Reddit during the pandemic, but that discussion content related to mental health has changed.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
articleno = {31},
numpages = {28},
keywords = {time series, user interaction, COVID-19, topic modeling, Mental health}
}

@inproceedings{10.1145/3316782.3316792,
author = {Montenegro, C. and Santana, R. and Lozano, J. A.},
title = {Data Generation Approaches for Topic Classification in Multilingual Spoken Dialog Systems},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3316792},
doi = {10.1145/3316782.3316792},
abstract = {The conception of spoken-dialog systems (SDS) usually faces the problem of extending or adapting the system to multiple languages. This implies the creation of modules specifically for the new languages, which is a time consuming process. In this paper, we propose two methods to reduce the time needed to extend the SDS to other languages. Our methods are particularly oriented to the topic classification and semantic tagging tasks and we evaluate their effectiveness on topic classification for three languages: English, Spanish, French.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {211–217},
numpages = {7},
keywords = {spoken dialog systems, neural networks, topic classification, bilingual datasets},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@article{10.1145/3449209,
author = {Li, Xuyang and Bahursettiwar, Antara and Kogan, Marina},
title = {Hello? Is There Anybody in There? Analysis of Factors Promoting Response From Authoritative Sources in Crisis},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449209},
doi = {10.1145/3449209},
abstract = {As social media has become more present in people's day-to-day lives, many turn to these platforms in natural disasters to keep abreast of the ever-evolving crisis situation. Facing the increasing amount of crisis-related information available on the social media platforms, users tend to focus on and reach out to authoritative sources---individuals or organizations that provide authoritative and credible crisis-related information due to their official status or position. As they provide high-quality information, response from the authoritative sources can be especially valuable to social media users directly affected in natural disasters. In this study, we aim to extend the reach of credible information during crisis, and direct the attention of authoritative users to the affected users who need their help. Specifically, we investigate what factors differentiate the tweets by regular users that receive responses from authoritative accounts from those that do not. We find that regular users' popularity and official accounts' level of busyness do not seem to affect the likelihood of tweets receiving a response. We thus explore the linguistic features of the tweets' content. Topic modeling and sentiment analysis results suggest that these linguistic aspects of the tweets may affect the response rate from authoritative sources. Our findings suggest crisis-related policy implications, as well as design implications for social media platforms where such exchanges take place, which can potentially increase the reach of credible information in a crisis and help those affected obtain the safety-critical information they need.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {135},
numpages = {21},
keywords = {crisis informatics, information seeking, authoritative sources, social media data}
}

@inproceedings{10.1145/3127526.3127529,
author = {Accuosto, Pablo and Ronzano, Francesco and Ferr\'{e}s, Daniel and Saggion, Horacio},
title = {Multi-Level Mining and Visualization of Scientific Text Collections: Exploring a Bi-Lingual Scientific Repository},
year = {2017},
isbn = {9781450353885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127526.3127529},
doi = {10.1145/3127526.3127529},
abstract = {We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).},
booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
pages = {9–16},
numpages = {8},
keywords = {Big Scientific Data, PDF Conversion, Data Visualization, Language Resources, Scientific Text Mining, Information Extraction},
location = {Toronto, ON, Canada},
series = {WOSP 2017}
}

@inproceedings{10.1145/3132847.3133011,
author = {Saha, Tanay Kumar and Joty, Shafiq and Hassan, Naeemul and Hasan, Mohammad Al},
title = {Regularized and Retrofitted Models for Learning Sentence Representation with Context},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133011},
doi = {10.1145/3132847.3133011},
abstract = {Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences. For solving these tasks, bag-of-word based representation has been used for a long time. In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations. However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context. In this paper, we first characterize two types of contexts depending on their scope and utility. We then propose two approaches to incorporate contextual information into content-based models. We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors. Experimental results demonstrate that our proposed models outshine existing models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {547–556},
numpages = {10},
keywords = {clustering, classification, ranking, feature learning, distributed representation of sentences, discourse, sen2vec, retrofitting},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3466640,
author = {Li, Siqing and Li, Yaliang and Zhao, Wayne Xin and Ding, Bolin and Wen, Ji-Rong},
title = {Interpretable Aspect-Aware Capsule Network for Peer Review Based Citation Count Prediction},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3466640},
doi = {10.1145/3466640},
abstract = {Citation count prediction is an important task for estimating the future impact of research papers. Most of the existing works utilize the information extracted from the paper itself. In this article, we focus on how to utilize another kind of useful data signal (i.e., peer review text) to improve both the performance and interpretability of the prediction models.Specially, we propose a novel aspect-aware capsule network for citation count prediction based on review text. It contains two major capsule layers, namely the feature capsule layer and the aspect capsule layer, with two different routing approaches, respectively. Feature capsules encode the local semantics from review sentences as the input of aspect capsule layer, whereas aspect capsules aim to capture high-level semantic features that will be served as final representations for prediction. Besides the predictive capacity, we also enhance the model interpretability with two strategies. First, we use the topic distribution of the review text to guide the learning of aspect capsules so that each aspect capsule can represent a specific aspect in the review. Then, we use the learned aspect capsules to generate readable text for explaining the predicted citation count. Extensive experiments on two real-world datasets have demonstrated the effectiveness of the proposed model in both performance and interpretability.},
journal = {ACM Trans. Inf. Syst.},
month = {nov},
articleno = {11},
numpages = {29},
keywords = {Citation count prediction, capsule network, peer review}
}

@inproceedings{10.1145/3366423.3380066,
author = {Abebe, Rediet and Giorgi, Salvatore and Tedijanto, Anna and Buffone, Anneke and Schwartz, H. Andrew Andrew},
title = {Quantifying Community Characteristics of Maternal Mortality Using Social Media},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380066},
doi = {10.1145/3366423.3380066},
abstract = {While most mortality rates have decreased in the US, maternal mortality has increased and is among the highest of any OECD nation. Extensive public health research is ongoing to better understand the characteristics of communities with relatively high or low rates. In this work, we explore the role that social media language can play in providing insights into such community characteristics. Analyzing pregnancy-related tweets generated in US counties, we reveal a diverse set of latent topics including Morning Sickness, Celebrity Pregnancies, and Abortion Rights. We find that rates of mentioning these topics on Twitter predicts maternal mortality rates with higher accuracy than standard socioeconomic and risk variables such as income, race, and access to health-care, holding even after reducing the analysis to six topics chosen for their interpretability and connections to known risk factors. We then investigate psychological dimensions of community language, finding the use of less trustful, more stressed, and more negative affective language is significantly associated with higher mortality rates, while trust and negative affect also explain a significant portion of racial disparities in maternal mortality. We discuss the potential for these insights to inform actionable health interventions at the community-level.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2976–2983},
numpages = {8},
keywords = {community characteristics, language, health disparities, maternal mortality, topic modeling},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.5555/3122009.3153018,
author = {Papanikolaou, Yannis and Foulds, James R. and Rubin, Timothy N. and Tsoumakas, Grigorios},
title = {Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multilabel classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2058–2115},
numpages = {58},
keywords = {multi-label classification, latent dirichlet allocation, collapsed Gibbs sampling, unsupervised learning, topic models, CVB0, Bayesian inference, text mining}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing Feature Model by Identifying Variability-Aware Modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {feature modules, product line, feature model recovery, variability-aware modularity, configuration},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3543829.3544529,
author = {Meyer, Selina and Elsweiler, David and Ludwig, Bernd and Fernandez-Pichel, Marcos and Losada, David E.},
title = {Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543829.3544529},
doi = {10.1145/3543829.3544529},
abstract = {Scarcity of user data continues to be a problem in research on conversational user interfaces and often hinders or slows down technical innovation. In the past, different ways of synthetically generating data, such as data augmentation techniques have been explored. With the rise of ever improving pre-trained language models, we ask if we can go beyond such methods by simply providing appropriate prompts to these general purpose models to generate data. We explore the feasibility and cost-benefit trade-offs of using non fine-tuned synthetic data to train classification algorithms for conversational agents. We compare this synthetically generated data with real user data and evaluate the performance of classifiers trained on different combinations of synthetic and real data. We come to the conclusion that, although classifiers trained on such synthetic data perform much better than random baselines, they do not compare to the performance of classifiers trained on even very small amounts of real user data, largely because such data is lacking much of the variability found in user generated data. Nevertheless, we show that in situations where very little data and resources are available, classifiers trained on such synthetically generated data might be preferable to the collection and annotation of naturalistic data.},
booktitle = {Proceedings of the 4th Conference on Conversational User Interfaces},
articleno = {8},
numpages = {6},
keywords = {conversational ai, datasets, text generation, nlp},
location = {Glasgow, United Kingdom},
series = {CUI '22}
}

@inproceedings{10.1145/3127526.3127530,
author = {Botev, Viktor and Marinov, Kaloyan and Sch\"{a}fer, Florian},
title = {Word Importance-Based Similarity of Documents Metric (WISDM): Fast and Scalable Document Similarity Metric for Analysis of Scientific Documents},
year = {2017},
isbn = {9781450353885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127526.3127530},
doi = {10.1145/3127526.3127530},
abstract = {We present the Word importance-based similarity of documents metric (WISDM), a fast and scalable novel method for document similarity/distance computation for analysis of scientific documents. It is based on recent advancements in the area of word embeddings. WISDM combines learned word vectors together with traditional count-based models for document similarity computation, eventually achieving state-of-the-art performance and precision. The novel method first selects from two text documents those words that carry the most information and forms a word set for each document respectively. Then it relies on an existing word embeddings model to get the vector representations of the selected words. In the final step, it computes the closeness of the two sets of word vector representations, fit into a matrix, using a correlation coefficient. The presented metric was evaluated on three tasks, relevant to the analysis of scientific documents, and three data sets of open access scientific research. The results demonstrate that WISDM achieves significant performance speed-up in comparison to state-of-the-art metrics with a very marginal drop in precision.},
booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
pages = {17–23},
numpages = {7},
keywords = {TF-IDF, word embeddings, document metric, text metric, importance-based, document distance, similarity, document similarity, correlation coefficient, word2vec},
location = {Toronto, ON, Canada},
series = {WOSP 2017}
}

@inproceedings{10.1145/3406865.3418568,
author = {Choi, Yoonseo and Monserrat, Toni-Jan Keith and Park, Jeongeon and Shin, Hyungyu and Lee, Nyoungwoo and Kim, Juho},
title = {ProtoChat: Supporting the Conversation Design Process with Crowd Feedback},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418568},
doi = {10.1145/3406865.3418568},
abstract = {Conversation designers use iterative design to create, test, and improve conversation flows. While it is possible to iterate conversation design with existing chatbot prototyping tools, challenges remain such as recruiting participants and collecting structured feedback on specific conversational components, hindering rapid iterations, and making informed design decisions. To address these limitations, we introduce ProtoChat, a crowd-powered design tool built to support the iterative process of conversation design for a chatbot. ProtoChat enables rapid testing with the crowd and guiding the crowd workers to provide granular feedback on specific points of conversation.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {19–23},
numpages = {5},
keywords = {conversational user interface, conversation design, crowdsourcing, iterative design, chatbot design},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3394171.3413582,
author = {Zhang, Huaizheng and Luo, Yong and Ai, Qiming and Wen, Yonggang and Hu, Han},
title = {Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413582},
doi = {10.1145/3394171.3413582},
abstract = {Given the massive market of advertising and the sharply increasing online multimedia content (such as videos), it is now fashionable to promote advertisements (ads) together with the multimedia content. However, manually finding relevant ads to match the provided content is labor-intensive, and hence some automatic advertising techniques are developed. Since ads are usually hard to understand only according to its visual appearance due to the contained visual metaphor, some other modalities, such as the contained texts, should be exploited for understanding. To further improve user experience, it is necessary to understand both the ads' topic and sentiment. This motivates us to develop a novel deep multimodal multitask framework that integrates multiple modalities to achieve effective topic and sentiment prediction simultaneously for ads understanding. In particular, in our framework termed Deep$M^2$Ad, we first extract multimodal information from ads and learn high-level and comparable representations. The visual metaphor of the ad is decoded in an unsupervised manner. The obtained representations are then fed into the proposed hierarchical multimodal attention modules to learn task-specific representations for final prediction. A multitask loss function is also designed to jointly train both the topic and sentiment prediction models in an end-to-end manner, where bottom-layer parameters are shared to alleviate over-fitting. We conduct extensive experiments on a large-scale advertisement dataset and achieve state-of-the-art performance for both prediction tasks. The obtained results could be utilized as a benchmark for ads understanding.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {430–438},
numpages = {9},
keywords = {online advertising, ads understanding, multitask learning, multimodal learning, neural networks},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3419249.3420106,
author = {van Berkel, Niels and Papachristos, Eleftherios and Giachanou, Anastasia and Hosio, Simo and Skov, Mikael B.},
title = {A Systematic Assessment of National Artificial Intelligence Policies: Perspectives from the Nordics and Beyond},
year = {2020},
isbn = {9781450375795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419249.3420106},
doi = {10.1145/3419249.3420106},
abstract = {Echoing the evolving interest and impact of artificial intelligence on society, governments are increasingly looking for ways to strategically position themselves as both innovators and regulators in this new domain. One of the most explicit and accessible ways in which governments outline these plans is through national strategy and policy documents. We follow a systematic search strategy to identify national AI policy documents across twenty-five countries. Through an analysis of these documents, including topic modelling, clustering, and reverse topic-search, we provide an overview of the topics discussed in national AI policies and contrast the differences between countries. Furthermore, we analyse the frequency of eleven ethical principles across our corpus. Our paper outlines implications of the differences between geographical and cultural clusters in relation to the future development of artificial intelligence applications.},
booktitle = {Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society},
articleno = {10},
numpages = {12},
keywords = {ethics, national guidelines, AI, Artificial Intelligence, policy, topic modelling, strategy},
location = {Tallinn, Estonia},
series = {NordiCHI '20}
}

@inproceedings{10.5555/3382225.3382358,
author = {Arag\'{o}n, Pablo and Bermejo, Yago and G\'{o}mez, Vicen\c{c} and Kaltenbrunner, Andreas},
title = {Interactive Discovery System for Direct Democracy},
year = {2018},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Decide Madrid is the civic technology of Madrid City Council which allows users to create and support online petitions. Despite the initial success, the platform is encountering problems with the growth of petition signing because petitions are far from the minimum number of supporting votes they must gather. Previous analyses have suggested that this problem is produced by the interface: a paginated list of petitions which applies a non-optimal ranking algorithm. For this reason, we present an interactive system for the discovery of topics and petitions. This approach leads us to reflect on the usefulness of data visualization techniques to address relevant societal challenges.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {601–604},
numpages = {4},
keywords = {platform design, online petitions, technopolitics, text clustering, civic technology, collective action, decide madrid, data visualization, e-democracy},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3194452.3194474,
author = {Yang, Shanliang and Sun, Qi and Zhou, Huyong and Gong, Zhengjie and Zhou, Yangzhi and Huang, Junhong},
title = {A Topic Detection Method Based on KeyGraph and Community Partition},
year = {2018},
isbn = {9781450364195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194452.3194474},
doi = {10.1145/3194452.3194474},
abstract = {More and more media stream data is created on the Internet every day. It's more difficult for persons to obtain valuable information due to information overload. Topic detection is the method that extracts valuable hot topics from media stream data. It is the tool to help to solve the problem of overload information. The topic positive accuracy of cluster method is very low. In this paper, we proposed one topic detection method based on KeyGraph to improve the positive accuracy, and took experiments compared with baseline method on corpus marked by graduate students. In the result, the positive accuracy of KeyGraph method reaches 88.48% with great improvement. The result verified the effectiveness of our proposed method.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Artificial Intelligence},
pages = {30–34},
numpages = {5},
keywords = {KeyGraph, Topic detection, TF-IDF, Louvain},
location = {Chengdu, China},
series = {ICCAI 2018}
}

@inproceedings{10.1145/3488560.3502190,
author = {Butler, Rhys and Duggirala, Vishnu Dutt and Banaei-Kashani, Farnoush},
title = {ILFQA: A Platform for Efficient and Accurate Long-Form Question Answering},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502190},
doi = {10.1145/3488560.3502190},
abstract = {We present an efficient and accurate long-form question-answering platform, dubbed iLFQA (i.e., short for intelligent Long-Form Question Answering). The purpose of iLFQA is to function as a platform which accepts unscripted questions and efficiently produces semantically meaningful, explanatory, and accurate long-form responses. iLFQA consists of a number of modules for zero-shot classification, text retrieval, and text generation to generate answers to questions based on an open-domain knowledge base. iLFQA is unique in the question answering space because it is an example of a deployable and efficient long-form question answering system. Question answering systems exist in many forms, but long-form question answering remains relatively unexplored, and to the best of our knowledge none of the existing long-form question answering systems are shown to be sufficiently efficient to be deployable. We have made the source code and implementation details of iLFQA available for the benefit of researchers and practitioners in this field. With this demonstration, we present iLFQA as an open-domain, deployable, and accurate open-source long-form question answering platform.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1565–1568},
numpages = {4},
keywords = {generalized language models, natural language processing, text generation, long-form question answering, text retrieval},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3106237.3119875,
author = {Ellmann, Mathias},
title = {On the Similarity of Software Development Documentation},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3119875},
doi = {10.1145/3106237.3119875},
abstract = {Software developers spent 20% of their time on information seeking on Stack Overflow, YouTube or an API reference documentation. Software developers can search within Stack Overflow for duplicates or similar posts. They can also take a look on software development documentations that have similar and additional information included as a Stack Overflow post or a development screencast in order to get new inspirations on how to solve their current development problem. The linkage of same and different types of software development documentation might safe time to evolve new software solutions and might increase the productivity of the developer’s work day. In this paper we will discuss our approach to get a broader understanding of different similarity types (exact, similar and maybe) within and between software documentation as well as an understanding of how different software documentations can be extended.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {1030–1033},
numpages = {4},
keywords = {Software Development Documentation, Software Analytics, Similarity Types},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3173574.3173597,
author = {Niu, Xi and Abbas, Fakhri and Maher, Mary Lou and Grace, Kazjon},
title = {Surprise Me If You Can: Serendipity in Health Information},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173597},
doi = {10.1145/3173574.3173597},
abstract = {Our natural tendency to be curious is increasingly important now that we are exposed to vast amounts of information. We often cope with this overload by focusing on the familiar: information that matches our expectations. In this paper we present a framework for interactive serendipitous information discovery based on a computational model of surprise. This framework delivers information that users were not actively looking for, but which will be valuable to their unexpressed needs. We hypothesize that users will be surprised when presented with information that violates the expectations predicted by our model of them. This surprise model is balanced by a value component which ensures that the information is relevant to the user. Within this framework we have implemented two surprise models, one based on association mining and the other on topic modeling approaches. We evaluate these two models with thirty users in the context of online health news recommendation. Positive user feedback was obtained for both of the computational models of surprise compared to a baseline random method. This research contributes to the understanding of serendipity and how to "engineer" serendipity that is favored by users.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {surprise, value, computational models, information retrieval systems, health news, serendipity},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3387902.3394035,
author = {Wei, Xianglin and Liu, Jianwei and Wang, Junwei and Wang, Yangang and Fan, Jianhua},
title = {Similarity-Aware Popularity-Based Caching in Wireless Edge Computing},
year = {2020},
isbn = {9781450379564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387902.3394035},
doi = {10.1145/3387902.3394035},
abstract = {Mobile edge computing (MEC) can greatly reduce the latency experienced by mobile devices and their energy consumption through bringing data processing, computing, and caching services closer to the source of data generation. However, existing edge caching mechanisms usually focus on predicting the popularity of contents or data chunks based on their request history. This will lead to a slow start problem for the newly arrived contents and fail to fulfill MEC's context-aware requirements. Moreover, the dynamic nature of contents as well as mobile devices has not been fully studied. Both of them hinder the further promotion and application of MEC caching. In this backdrop, this paper aims to tackle the caching problem in wireless edge caching scenarios, and a new dynamic caching architecture is proposed. The mobility of users and the dynamics nature of contents are considered comprehensively in our caching architecture rather than adopting a static assumption as that in many current efforts. Based on this framework, a Similarity-Aware Popularity-based Caching (SAPoC) algorithm is proposed which considers a content's freshness, short-term popularity, and the similarity between contents when making caching decisions. Extensive simulation experiments have been conducted to evaluate SAPoC's performance, and the results have shown that SAPoC outperforms several typical proposals in both cache hit ratio and energy consumption.},
booktitle = {Proceedings of the 17th ACM International Conference on Computing Frontiers},
pages = {257–260},
numpages = {4},
keywords = {mobile edge computing, wireless network, cache algorithm, dynamic caching},
location = {Catania, Sicily, Italy},
series = {CF '20}
}

@inproceedings{10.1145/3269206.3269309,
author = {Hoang, Tuan-Anh and Vo, Khoi Duy and Nejdl, Wolfgang},
title = {W2E: A Worldwide-Event Benchmark Dataset for Topic Detection and Tracking},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269309},
doi = {10.1145/3269206.3269309},
abstract = {Topic detection and tracking in document streams is a critical task in many important applications, hence has been attracting research interest in recent decades. With the large size of data streams, there have been a number of works from different approaches that propose automatic methods for the task. However, there is only a few small benchmark datasets that are publicly available for evaluating the proposed methods. The lack of large datasets with fine-grained groundtruth implicitly restrains the development of more advanced methods. In this work, we address this issue by collecting and publishing W2E - a large dataset consisting of news articles from more than 50 prominent mass media channels worldwide. The articles cover a large set of popular events within a full year. W2E is more than 15 times larger than TREC's TDT2 dataset, which is widely used in prior work. We further conduct exploratory analysis to examine the dynamics and diversity of W2E and propose potential uses of the dataset in other research.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1847–1850},
numpages = {4},
keywords = {benchmark dataset, topic tracking, topic detection},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3485447.3512163,
author = {Min, Erxue and Rong, Yu and Bian, Yatao and Xu, Tingyang and Zhao, Peilin and Huang, Junzhou and Ananiadou, Sophia},
title = {Divide-and-Conquer: Post-User Interaction Network for Fake News Detection on Social Media},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512163},
doi = {10.1145/3485447.3512163},
abstract = {Fake News detection has attracted much attention in recent years. Social context based detection methods attempt to model the spreading patterns of fake news by utilizing the collective wisdom from users on social media. This task is challenging for three reasons: (1) There are multiple types of entities and relations in social context, requiring methods to effectively model the heterogeneity. (2) The emergence of news in novel topics in social media causes distribution shifts, which can significantly degrade the performance of fake news detectors. (3) Existing fake news datasets usually lack of great scale, topic diversity and user social relations, impeding the development of this field. To solve these problems, we formulate social context based fake news detection as a heterogeneous graph classification problem, and propose a fake news detection model named Post-User Interaction Network (PSIN), which adopts a divide-and-conquer strategy to model the post-post, user-user and post-user interactions in social context effectively while maintaining their intrinsic characteristics. Moreover,we adopt an adversarial topic discriminator for topic-agnostic feature learning, in order to improve the generalizability of our method for new-emerging topics. Furthermore, we curate a new dataset for fake news detection, which contains over 27,155 news from 5 topics, 5 million posts, 2 million users and their induced social graph with 0.2 billion edges. It has been published on https://github.com/qwerfdsaplking/MC-Fake. Extensive experiments illustrate that our method outperforms SOTA baselines in both in-topic and out-of-topic settings.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1148–1158},
numpages = {11},
keywords = {Graph Neural Network, Social Media, Fake News Detection},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3410530.3414327,
author = {Adhikary, Rishiraj and Batra, Nipun},
title = {Computational Tools for Understanding Air Pollution},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414327},
doi = {10.1145/3410530.3414327},
abstract = {Ambient fine particulate (PM2.5) is the most significant risk factor for premature death, shortening life expectancy at birth by 1.5 to 1.9 years [2]. 91% of the world's population lives in areas where air pollution exceeds safety limits1. 99% of the people in countries like India, Pakistan, Nepal, and Bangladesh experience ambient exposures of PM2.5 exceeding 75 μg/m3 to 100 μg/m3 [3]. My Ph.D. thesis will be on understanding the perception of air pollution among people using social media data. I also intend to develop a wearable air pollution exposure monitor and design an air pollution visualisation tool to reduce the entry barrier for air pollution research.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {199–203},
numpages = {5},
keywords = {air pollution perception, social media, air pollution wearable, visualisation},
location = {Virtual Event, Mexico},
series = {UbiComp-ISWC '20}
}

@inproceedings{10.1145/3110025.3110102,
author = {Effendy, Suhendry and Yap, Roland H. C.},
title = {Using Community Structure to Categorize Computer Science Conferences: Initial Results},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110102},
doi = {10.1145/3110025.3110102},
abstract = {Research in computer science (CS) is published mainly in conferences. We investigate the possibility of automatically categorizing CS conferences by using exemplars (influential conferences). We propose an automatic exemplars selection method. Our experiments show that categorizing by exemplars matches well with curated topic classification from the Chinese CCF conference list. The results also accord with manual judgement which show promise as a practical and robust method for categorizing CS conferences.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {297–300},
numpages = {4},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3436286.3436291,
author = {Simou, Chen},
title = {Hot Topics of Big Data Research in China},
year = {2020},
isbn = {9781450376457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436286.3436291},
doi = {10.1145/3436286.3436291},
abstract = {This paper aims at exploring the hot topics of big data research in China in the past three years. We choose Big Data, one influential and representative Chinese academic journal as our data source, and download the bibliographic data of 195 articles published in the past three years on this journal from CNKI. In order to gain the general overview of Chinese big data research status quo, we count the high frequency words and distribution of publish years. From the high frequency words list, we can tell that the times of mentions of "data" is far more than that of other frequency words, which verifies the fact that data is the most important concept in big data research. In order to extract the hot topics of big data research, we adopt LDA model to generate topics of the 195 research articles. With several experiments, we set the topic number as seven for the best outcome quality. With expert suggestion, we label the seven topics as "Big data helps government decisions", "Internet and management"," Artificial intelligence helps public security"," Big data promotes information", "Big data governance", "Artificial intelligence and 5G"," Big data prediction" based on the high frequency words in each topic. Most of the topic labels are linked to 5G, artificial intelligence and other modern techniques, which implies that the big data plays a necessary role in empowering the future techniques. We hope this paper could contribute to further big data research in China.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data and Artificial Intelligence},
pages = {20–25},
numpages = {6},
keywords = {Big Data, Hot Topics, Lda Model},
location = {Johannesburg, South Africa},
series = {ISBDAI '20}
}

@inproceedings{10.1145/3442442.3451361,
author = {Hoppe, Fabian and Dess\`{\i}, Danilo and Sack, Harald},
title = {Deep Learning Meets Knowledge Graphs for Scholarly Data Classification},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451361},
doi = {10.1145/3442442.3451361},
abstract = {The amount of scientific literature continuously grows, which poses an increasing challenge for researchers to manage, find and explore research results. Therefore, the classification of scientific work is widely applied to enable the retrieval, support the search of suitable reviewers during the reviewing process, and in general to organize the existing literature according to a given schema. The automation of this classification process not only simplifies the submission process for authors, but also ensures the coherent assignment of classes. However, especially fine-grained classes and new research fields do not provide sufficient training data to automatize the process. Additionally, given the large number of not mutual exclusive classes, it is often difficult and computationally expensive to train models able to deal with multi-class multi-label settings. To overcome these issues, this work presents a preliminary Deep Learning framework as a solution for multi-label text classification for scholarly papers about Computer Science. The proposed model addresses the issue of insufficient data by utilizing the semantics of classes, which is explicitly provided by latent representations of class labels. This study uses Knowledge Graphs as a source of these required external class definitions by identifying corresponding entities in DBpedia to improve the overall classification.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {417–421},
numpages = {5},
keywords = {Multi-Label Classification, Scholarly Data, Deep Learning, Knowledge Graphs},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3313831.3376768,
author = {Tahaei, Mohammad and Vaniea, Kami and Saphra, Naomi},
title = {Understanding Privacy-Related Questions on Stack Overflow},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376768},
doi = {10.1145/3313831.3376768},
abstract = {We analyse Stack Overflow (SO) to understand challenges and confusions developers face while dealing with privacy-related topics. We apply topic modelling techniques to 1,733 privacy-related questions to identify topics and then qualitatively analyse a random sample of 315 privacy-related questions. Identified topics include privacy policies, privacy concerns, access control, and version changes. Results show that developers do ask SO for support on privacy-related issues. We also find that platforms such as Apple and Google are defining privacy requirements for developers by specifying what "sensitive" information is and what types of information developers need to communicate to users (e.g. privacy policies). We also examine the accepted answers in our sample and find that 28% of them link to official documentation and more than half are answered by SO users without references to any external resources.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {software developers, stack overflow, usable privacy},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3109859.3109865,
author = {Pe\~{n}a, Francisco J.},
title = {Unsupervised Context-Driven Recommendations Based On User Reviews},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109865},
doi = {10.1145/3109859.3109865},
abstract = {In this work we present Rich-Context, a context-driven recommender system that extracts contextual information using topic modeling without the need to define keywords. Our system uses the mined context to produce recommendations. We propose a methodology to measure the quality of context topic models along with a novel way to represent context that allows it to be used as side-information in a recommendation engine. Results show that Rich-Context makes more accurate predictions than five well-established recommendation algorithms.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {426–430},
numpages = {5},
keywords = {review-mining, topic modeling, context-aware recommender systems, context-driven recommender system},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3290621.3290629,
author = {Krasnov, Fedor},
title = {Topic Classification Through Topic Modeling with Additive Regularization for Collection of Scientific Papers},
year = {2018},
isbn = {9781450361767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290621.3290629},
doi = {10.1145/3290621.3290629},
abstract = {Any text can be presented as an infinite number of topic models.Any of these topics would not have any attributes that would make it possible to break them up into classes. The Author has suggested additive regularization when creating models to single out topic clusters from the Probabilistic Latent Semantic Analysis, PLSA.The method proposed by the Author allows singling out topic classes based on their density in the document-topic space (Matrix Θ) for a selected collection of documents.Such segmentation is similar to hierarchical variant LDA (HLDA) but has no such drawback as that the LDA models have.},
booktitle = {Proceedings of the 14th Central and Eastern European Software Engineering Conference Russia},
articleno = {5},
numpages = {5},
keywords = {Clustering, Topic modeling, Practical approach, ARTM, Natural Language Processing},
location = {Moscow, Russian Federation},
series = {CEE-SECR '18}
}

@article{10.1145/3462478,
author = {Chauhan, Uttam and Shah, Apurva},
title = {Topic Modeling Using Latent Dirichlet Allocation: A Survey},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3462478},
doi = {10.1145/3462478},
abstract = {We are not able to deal with a mammoth text corpus without summarizing them into a relatively small subset. A computational tool is extremely needed to understand such a gigantic pool of text. Probabilistic Topic Modeling discovers and explains the enormous collection of documents by reducing them in a topical subspace. In this work, we study the background and advancement of topic modeling techniques. We first introduce the preliminaries of the topic modeling techniques and review its extensions and variations, such as topic modeling over various domains, hierarchical topic modeling, word embedded topic models, and topic models in multilingual perspectives. Besides, the research work for topic modeling in a distributed environment, topic visualization approaches also have been explored. We also covered the implementation and evaluation techniques for topic models in brief. Comparison matrices have been shown over the experimental results of the various categories of topic modeling. Diverse technical challenges and future directions have been discussed.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {145},
numpages = {35},
keywords = {probabilistic model, statistical inference, latent dirichlet allocation, gibbs sampling, Topic modeling}
}

@article{10.1145/3418062,
author = {Guangce, Ruan and Lei, Xia},
title = {Knowledge Discovery of News Text Based on Artificial Intelligence},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3418062},
doi = {10.1145/3418062},
abstract = {The explosion of news text and the development of artificial intelligence provide a new opportunity and challenge to provide high-quality media monitoring service. In this article, we propose a semantic analysis approach based on the Latent Dirichlet Allocation (LDA) and Apriori algorithm, and we realize application to improve media monitoring reports by mining large-scale news text. First, we propose to use LDA model to mine news text topic words and reducing news dimensionality. Then, we propose to use Apriori algorithm to discovering the relationship of topic words. Finally, we discovery the relevance of news text topic words and show the intensity and dependency among topic words through drawing. This application can realize to extract the news topics and discover the correlation and dependency among news topics in mass news text. The results show that the method based on LDA and Apriori can help the media monitoring staff to better understand the hidden knowledge in the news text and improve the media analysis report.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {6},
numpages = {18},
keywords = {knowledge discovery, news text, LDA, association rules}
}

@inproceedings{10.1145/3437963.3441693,
author = {Cinus, Federico and Bonchi, Francesco and Monti, Corrado and Panisson, Andr\'{e}},
title = {WoMG: A Library for Word-of-Mouth Cascades Generation},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441693},
doi = {10.1145/3437963.3441693},
abstract = {Studying information propagation in social media is an important task with plenty of applications for business and science. Generating realistic synthetic information cascades can help the research community in developing new methods and applications, testing sociological hypotheses and different what-if scenarios by simply changing few parameters. We demonstrate womg, a synthetic data generator which combines topic modeling and a topic-aware propagation model to create realistic information-rich cascades, whose shape depends on many factors, including the topic of the item and its virality, the homophily of the social network, the interests of its users and their social influence.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1065–1068},
numpages = {4},
keywords = {simulation, diffusion dynamics, information propagation, online social media},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3477495.3531817,
author = {Xia, Jinxiong and Liu, Cao and Chen, Jiansong and Li, Yuchen and Yang, Fan and Cai, Xunliang and Wan, Guanglu and Wang, Houfeng},
title = {Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531817},
doi = {10.1145/3477495.3531817},
abstract = {Dialogue topic segmentation is a challenging task in which dialogues are split into segments with pre-defined topics. Existing works on topic segmentation adopt a two-stage paradigm, including text segmentation and segment labeling. However, such methods tend to focus on the local context in segmentation, and the inter-segment dependency is not well captured. Besides, the ambiguity and labeling noise in dialogue segment bounds bring further challenges to existing models. In this work, we propose the Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to address the above issues. Specifically, we propose the parallel extraction network to perform segment extractions, optimizing the bipartite matching cost of segments to capture inter-segment dependency. Furthermore, we propose neighbor smoothing to handle the segment-bound noise and ambiguity. Experiments on a dialogue-based and a document-based topic segmentation dataset show that PEN-NS outperforms state-the-of-art models significantly.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2126–2131},
numpages = {6},
keywords = {neighbor smoothing., dialogue topic segmentation, data noise, boundary ambiguity, parallel extraction},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3331000,
author = {Gon\c{c}alves, Rodrigo and Dorneles, Carina Friedrich},
title = {Automated Expertise Retrieval: A Taxonomy-Based Survey and Open Issues},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3331000},
doi = {10.1145/3331000},
abstract = {Understanding people’s expertise is not a trivial task since it is time-consuming when manually executed. Automated approaches have become a topic of research in recent years in various scientific fields, such as information retrieval, databases, and machine learning. This article carries out a survey on automated expertise retrieval, i.e., finding data linked to a person that describes the person’s expertise, which allows tasks such as profiling or finding people with a certain expertise. A faceted taxonomy is introduced that covers many of the existing approaches and classifies them on the basis of features chosen from studying the state-of-the-art. A list of open issues, with suggestions for future research topics, is introduced as well. It is hoped that our taxonomy and review of related works on expertise retrieval will be useful when analyzing different proposals and will allow a better understanding of existing work and a systematic classification of future work on the topic.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {96},
numpages = {30},
keywords = {expert finding, Expertise retrieval, expertise profile}
}

@inproceedings{10.1145/3529399.3529438,
author = {Abri, Rayan and Abri, Sara and Cetin, Salih},
title = {Providing A Topic-Based LSTM Model to Re-Rank Search Results},
year = {2022},
isbn = {9781450395748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529399.3529438},
doi = {10.1145/3529399.3529438},
abstract = {The ranking of search results is directly affected by user click preferences and is an effective way to improve the quality of the result of search engines. To tailor the ranking, it is necessary to use the submitted query information by the user, such as click history, user profile, the previous queries history, or query click entropy. There are ranking methods that explore the issue of underlying information using traditional machine learning algorithms. Recently LSTM (Long Short-Term Memory) based models are investigated in this field. As traditional ML models require considering short-term and long-term preferences in predicting, the LSTM based models can improve prediction efficiency by considering both short and long-term. This paper proposes a topic-based LSTM model to re-rank search results on a submitted input quey using the previous queries sequence and user click history. In this model, we use the topic distribution of user documents to the LSTM model. We compare the model with topic-based ranking models with data from an AOL search engine and Session TREC 2013,2014 to show its performance. The result reveals significant improvement in the Topic-based LSTM model using topics in the Mean Reciprocal Rank by 13% compared to the baseline topic-based models.},
booktitle = {2022 7th International Conference on Machine Learning Technologies (ICMLT)},
pages = {249–254},
numpages = {6},
keywords = {Re-ranking algorithms, Topic-based models, LSTM model},
location = {Rome, Italy},
series = {ICMLT 2022}
}

@inproceedings{10.1109/ASONAM49781.2020.9381379,
author = {Duong, Viet and Luo, Jiebo and Pham, Phu and Yang, Tongyu and Wang, Yu},
title = {The Ivory Tower Lost: How College Students Respond Differently than the General Public to the COVID-19 Pandemic},
year = {2020},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381379},
doi = {10.1109/ASONAM49781.2020.9381379},
abstract = {In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been implemented by the President. Following the closure of the University of Washington on March 7th, more than 1000 colleges and universities in the United States have cancelled in-person classes and campus activities, impacting millions of students. This paper aims to discover the social implications of this unprecedented disruption in our interactive society regarding both the general public and higher education populations by mining people's opinions on social media. We discover several topics embedded in a large number of COVID-19 tweets that represent the most central issues related to the pandemic, which are of great concerns for both college students and the general public. Moreover, we find significant differences between these two groups of Twitter users with respect to the sentiments they expressed towards the COVID-19 issues. To our best knowledge, this is the first social media-based study which focuses on the college student community's demographics and responses to prevalent social issues during a major crisis.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {126–130},
numpages = {5},
keywords = {sentiment analysis, college students, classification, COVID-19, Twitter},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3386392.3399559,
author = {Abri, Sara and Abri, Rayan and \c{C}etin, Salih},
title = {Group-Based Personalization Using Topical User Profile},
year = {2020},
isbn = {9781450379502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386392.3399559},
doi = {10.1145/3386392.3399559},
abstract = {Although user profiles are indicative of the user's interests, they can be incomplete to reflect all the user's interests and in more times it is needed to use a group of personalized user profiles to re-rank the returned results by search engines. One of the disadvantages of the personalization based on the user profile is that it is built by considering only the documents that the user has clicked. The set of the clicked documents might be sparse for some users. Data sparsity can be resolved by backing off to the group of users with similar behavior to the user. In this paper, we present a group-based personalization model using topical user-profiles and compare the result of the proposed ranking methods based on the group and user profiles. To cluster the groups of users we use the Kmeans clustering algorithm and the similarity between users is measured by symmetric Kullback-Leibler divergence between their latent topic distributions. Using the proposed group-based personalization model, we can improve the ranking result using group-based profiles and solve the cold start problem of users without history. In the issues related to privacy concerns, group profiles are also more secure than user profiles because both the computation and the storage of the user information are done as a group of users. The result reveals that the group-based personalization using topical user profile improves the Mean Reciprocal Rank and the Normalized Discounted Cumulative Gain by 7% and 6% respectively in all short, long and session term profiles while the short term user profile obtains more effective than the others profiles.},
booktitle = {Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {181–186},
numpages = {6},
keywords = {group based personalization, user search behaviour, topical user model, personalized web search},
location = {Genoa, Italy},
series = {UMAP '20 Adjunct}
}

@inproceedings{10.1145/3172944.3172965,
author = {Smith, Alison and Kumar, Varun and Boyd-Graber, Jordan and Seppi, Kevin and Findlater, Leah},
title = {Closing the Loop: User-Centered Design and Evaluation of a Human-in-the-Loop Topic Modeling System},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172965},
doi = {10.1145/3172944.3172965},
abstract = {Human-in-the-loop topic modeling allows users to guide the creation of topic models and to improve model quality without having to be experts in topic modeling algorithms. Prior work in this area has focused either on algorithmic implementation without understanding how users actually wish to improve the model or on user needs but without the context of a fully interactive system. To address this disconnect, we implemented a set of model refinements requested by users in prior work and conducted a study with twelve non-expert participants to examine how end users are affected by issues that arise with a fully interactive, user-centered system. As these issues mirror those identified in interactive machine learning more broadly, such as unpredictability, latency, and trust, we also examined interactive machine learning challenges with non-expert end users through the lens of human-in-the-loop topic modeling. We found that although users experience unpredictability, their reactions vary from positive to negative, and, surprisingly, we did not find any cases of distrust, but instead noted instances where users perhaps trusted the system too much or had too little confidence in themselves.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {293–304},
numpages = {12},
location = {Tokyo, Japan},
series = {IUI '18}
}

@article{10.1145/3186260,
author = {Blei, David M.},
title = {Technical Perspective: Expressive Probabilistic Models and Scalable Method of Moments},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3186260},
doi = {10.1145/3186260},
journal = {Commun. ACM},
month = {mar},
pages = {84},
numpages = {1}
}

@article{10.5555/3122009.3122021,
author = {Kim, Daeil and Swanson, Benjamin F. and Hughes, Michael C. and Sudderth, Erik B.},
title = {Refinery: An Open Source Topic Modeling Web Platform},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms. The project website http://daeilkim.github.io/refinery/ contains Python code and further documentation.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {382–386},
numpages = {5},
keywords = {software, topic models, visualization}
}

@inproceedings{10.1109/ICSE-C.2017.102,
author = {Wang, Lu and Sun, Xiaobing and Wang, Jingwei and Duan, Yucong and Li, Bin},
title = {Construct Bug Knowledge Graph for Bug Resolution: Poster},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.102},
doi = {10.1109/ICSE-C.2017.102},
abstract = {Software bug issues are unavoidable in software development and maintenance [1--3]. When a bug is reported, developers usually search various software sources, such as bug repositories, software control versions and so on, to get a referred solution [1, 4]. However, the volume of search results is usually large, most of which are irrelevant. What's more, developers need to search different sources, and manually integrate different search results to get a referred bug resolution.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {189–191},
numpages = {3},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1145/3020165.3022141,
author = {Medlar, Alan and Glowacka, Dorota},
title = {Using Topic Models to Assess Document Relevance in Exploratory Search User Studies},
year = {2017},
isbn = {9781450346771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020165.3022141},
doi = {10.1145/3020165.3022141},
abstract = {Evaluation is crucial in assessing the effectiveness of new information retrieval and human computer interaction techniques and systems. Relevance judgements are often performed by humans, which makes obtaining them expensive and time consuming. Consequently, relevance judgements are usually performed only on a subset of a given collection of data or experimental results with a focus on the top ranked documents. However, when assessing the performance of exploratory search systems, the diversity or subjective relevance of documents that the user was presented with over a search session are often of more importance than the relative ranking of top documents. In order to perform these types of assessment, all the documents in a given collection need to be judged for relevance. In this paper, we propose an approach based on topic modeling that can greatly accelerate document relevance judgment of an entire document collection with an expert assessor needing to mark only a small subset of documents from a given collection. Experimental results show a substantial overlap between relevance judgments compared to a human assessor.},
booktitle = {Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval},
pages = {313–316},
numpages = {4},
keywords = {exploratory search, user studies, relevance assessment, topic models},
location = {Oslo, Norway},
series = {CHIIR '17}
}

@inproceedings{10.1145/3461778.3462053,
author = {Troost, Ivar and Tanhaei, Ghazaleh and Hardman, Lynda and H\"{u}rst, Wolfgang},
title = {Exploring Relations in Neuroscientific Literature Using Augmented Reality: A Design Study},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462053},
doi = {10.1145/3461778.3462053},
abstract = {To support scientists in maintaining an overview of disciplinary concepts and their interrelations, we investigate whether Augmented Reality can serve as a platform to make automated methods more accessible and integrated into current literature exploration practices. Building on insights from text and immersive analytics, we identify information and design requirements. We embody these in DatAR, a system design and implementation focussed on analysis of co-occurrences in neuroscientific text collections. We conducted a scenario-based video survey with a sample of neuroscientists and other domain experts, focusing on participants’ willingness to adopt such an AR system in their regular literature review practices. The AR-tailored epistemic and representational designs of our system were generally perceived as suitable for performing complex analytics. We also discuss several fundamental issues with our chosen 3D visualisations, making steps towards understanding in which ways AR is a suitable medium for high-level conceptual literature exploration.},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {266–274},
numpages = {9},
keywords = {linked data, immersive analytics, neuroscience, literature exploration, augmented reality},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3443467.3443751,
author = {Gongda, Qiu and Guixin, Zhang and Hui, Shi and Liqiong, Deng},
title = {A Comprehensive Sentiment Analysis Method Based on Sentiment Multi-Label and Probabilistic Hesitant Fuzzy Decision-Making},
year = {2020},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443751},
doi = {10.1145/3443467.3443751},
abstract = {Aiming at the difficulty of analysis on social text with complex sentiment and lacking of the comprehensive sentiment analysis based on historical social text set, a sentiment portrait analysis method based on multi-sentiment analysis and probabilistic hesitant fuzzy decision-making was proposed. Firstly, the BERT model was appropriately adapted to the hesitant fuzzy problem. Topic class and sentiment label of user text are acquired by topic classification and text sentiment multi-label model based on BERT model, Meanwhile the hesitant and fuzzy sentiment of different topics text are preserved by multi-level and multi-label sentiment. Secondly, the probabilistic hesitation fuzzy set is constructed based on the hesitation fuzzy and probability of text, in which the probabilistic hesitation fuzzy elements of each attribute are integrated by PHFWA operator. Finally, the modified sentiment entropy was proposed based on the statistical distribution of sentiment and information entropy, combined with its score function, deviation degree function and modified sentiment entropy, three types of sentiment distribution are defined and judged. In the experiment, four public Weibo accounts were selected and their status data were collected for analysis, which verified the effectiveness of the method in this paper.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {186–194},
numpages = {9},
keywords = {Sentiment Multi-label, Probability hesitation fuzzy, Topic multi-classification, Complex sentiment text, Modified Sentiment Entropy},
location = {Xiamen, China},
series = {EITCE 2020}
}

@inproceedings{10.1145/3121257.3121260,
author = {Ellmann, Mathias and Oeser, Alexander and Fucci, Davide and Maalej, Walid},
title = {Find, Understand, and Extend Development Screencasts on YouTube},
year = {2017},
isbn = {9781450351577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121257.3121260},
doi = {10.1145/3121257.3121260},
abstract = {A software development screencast is a video that captures the screen of a developer working on a particular task and explaining implementation details. Due to the increased popularity of development screencasts e.g., on YouTube, we study how and to what extent they can be used as additional source of knowledge to answer developers’ questions, for example about the use of a specific API. We first study the difference between development screencasts and other types of screencasts using video frame analysis. When comparing frames with the Cosine algorithm, developers can expect ten development screencasts in the top 20 out of 100 different YouTube videos. We then extracted popular development topics. These were: database operations, system set-up, plug-in development, game development, and testing. We also identified six recurring tasks performed in development screencasts, such as object usage and UI operations. Finally, we conducted a similarity analysis of the screencast transcripts and the Javadoc of the corresponding screencasts.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics},
pages = {1–7},
numpages = {7},
keywords = {API documentation, Similarity Analytics, Development Screencasts},
location = {Paderborn, Germany},
series = {SWAN 2017}
}

@article{10.1145/3464299,
author = {Bahrainian, Seyed Ali and Zerveas, George and Crestani, Fabio and Eickhoff, Carsten},
title = {CATS: Customizable Abstractive Topic-Based Summarization},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3464299},
doi = {10.1145/3464299},
abstract = {Neural sequence-to-sequence models are the state-of-the-art approach used in abstractive summarization of textual documents, useful for producing condensed versions of source text narratives without being restricted to using only words from the original text. Despite the advances in abstractive summarization, custom generation of summaries (e.g., towards a user’s preference) remains unexplored. In this article, we present CATS, an abstractive neural summarization model that summarizes content in a sequence-to-sequence fashion while also introducing a new mechanism to control the underlying latent topic distribution of the produced summaries. We empirically illustrate the efficacy of our model in producing customized summaries and present findings that facilitate the design of such systems. We use the well-known CNN/DailyMail dataset to evaluate our model. Furthermore, we present a transfer-learning method and demonstrate the effectiveness of our approach in a low resource setting, i.e., abstractive summarization of meetings minutes, where combining the main available meetings’ transcripts datasets, AMI and International Computer Science Institute(ICSI), results in merely a few hundred training documents.},
journal = {ACM Trans. Inf. Syst.},
month = {oct},
articleno = {5},
numpages = {24},
keywords = {Sequence-to-sequence neural models, topical customization, abstractive summarization}
}

@inproceedings{10.1145/3083165.3083179,
author = {Zhang, Cong and Liu, Jiangchuan and Ma, Ming and Sun, Lifeng and Li, Bo},
title = {Seeker: Topic-Aware Viewing Pattern Prediction in Crowdsourced Interactive Live Streaming},
year = {2017},
isbn = {9781450350037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083165.3083179},
doi = {10.1145/3083165.3083179},
abstract = {Recently, Crowdsourced Interactive Live Streaming (CILS), such as Twitch.tv and Periscope, has emerged as one of the most popular streaming applications over the Internet. In such applications, a large number of geo-distributed users publish live sources to broadcast their game sessions, personal activities, and other events, while fellow viewers not only watch these live streams, but also contribute interactive messages to influence streaming content. Such explosively increasing popularity has posed significant challenges to predict viewing patterns using traditional time-series approaches, which lack the start/end knowledge of live streams and cannot capture the viewing burst very well.In this paper, we closely examine the characteristics of interactive messages in the real-world datasets, we find that the strong topic relevances exist in the viewers' discussions. Motivated by this observation, we design a crowdsourced framework Seeker to overcome aforementioned challenges. It explores the correlation between three viewing patterns (i.e., start/burst/end of live streams) and viewers' interactive messages (even before a live broadcast) through capturing the key topics. Our trace-driven evaluation and case study show the effectiveness of our solution, which can predict aforementioned patterns in advance and achieve much higher performance than the time-series approaches.},
booktitle = {Proceedings of the 27th Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {25–30},
numpages = {6},
keywords = {Crowdsourced interactive live streaming, Twitch.tv, interactive messages, viewing patterns},
location = {Taipei, Taiwan},
series = {NOSSDAV'17}
}

@inproceedings{10.1145/3341620.3341631,
author = {Liang, Qiancheng and Wu, Ping and Huang, Chaoyi},
title = {An Efficient Method for Text Classification Task},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341631},
doi = {10.1145/3341620.3341631},
abstract = {Text classification has always been a research hot-spot in the field of natural language processing. For the neural network input matrix, only the word vector of the word level is extracted, which ignores the expression of the overall semantic features of the text level, resulting in insufficient representation of text features and affecting accurate classification. In this paper, a text representation matrix combining word2vec and LDA topic models is proposed. Combining word meaning and semantic features, inputting LSTM for text classification, and introducing Attention mechanism to improve LSTM model, LSTM-Attention model is designed. The experimental results show that the LSTM classification model has better classification result than the traditional machine learning model, and the LSTM model with the Attention mechanism has a certain degree of improvement compared with the classical text classification models.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {92–97},
numpages = {6},
keywords = {Word2Vec, LDA, LSTM-Attention Model, Attention mechanism},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3338533.3366583,
author = {Chen, Yiyan and Tao, Li and Wang, Xueting and Yamasaki, Toshihiko},
title = {Weakly Supervised Video Summarization by Hierarchical Reinforcement Learning},
year = {2019},
isbn = {9781450368414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338533.3366583},
doi = {10.1145/3338533.3366583},
abstract = {Conventional video summarization approaches based on reinforcement learning have the problem that the reward can only be received after the whole summary is generated. Such kind of reward is sparse and it makes reinforcement learning hard to converge. Another problem is that labelling each shot is tedious and costly, which usually prohibits the construction of large-scale datasets. To solve these problems, we propose a weakly supervised hierarchical reinforcement learning framework, which decomposes the whole task into several subtasks to enhance the summarization quality. This framework consists of a manager network and a worker network. For each subtask, the manager is trained to set a subgoal only by a task-level binary label, which requires much fewer labels than conventional approaches. With the guide of the subgoal, the worker predicts the importance scores for video shots in the subtask by policy gradient according to both global reward and innovative defined sub-rewards to overcome the sparse problem. Experiments on two benchmark datasets show that our proposal has achieved the best performance, even better than supervised approaches.},
booktitle = {Proceedings of the ACM Multimedia Asia},
articleno = {3},
numpages = {6},
keywords = {hierarchical reinforcement learning, video summarization, sub-reward},
location = {Beijing, China},
series = {MMAsia '19}
}

@inproceedings{10.1145/3239235.3267435,
author = {Mantyla, Mika V. and Claes, Maelick and Farooq, Umar},
title = {Measuring LDA Topic Stability from Clusters of Replicated Runs},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3267435},
doi = {10.1145/3239235.3267435},
abstract = {Background: Unstructured and textual data is increasing rapidly and Latent Dirichlet Allocation (LDA) topic modeling is a popular data analysis methods for it. Past work suggests that instability of LDA topics may lead to systematic errors. Aim: We propose a method that relies on replicated LDA runs, clustering, and providing a stability metric for the topics. Method: We generate k LDA topics and replicate this process n times resulting in n*k topics. Then we use K-medioids to cluster the n*k topics to k clusters. The k clusters now represent the original LDA topics and we present them like normal LDA topics showing the ten most probable words. For the clusters, we try multiple stability metrics, out of which we recommend Rank-Biased Overlap, showing the stability of the topics inside the clusters. Results: We provide an initial validation where our method is used for 270,000 Mozilla Firefox commit messages with k=20 and n=20. We show how our topic stability metrics are related to the contents of the topics. Conclusions: Advances in text mining enable us to analyze large masses of text in software engineering but non-deterministic algorithms, such as LDA, may lead to unreplicable conclusions. Our approach makes LDA stability transparent and is also complementary rather than alternative to many prior works that focus on LDA parameter tuning.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {49},
numpages = {4},
keywords = {rank-biased overlap, similarity, stability, latent dirichlet allocation, replication, commit messages, clustering},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1145/3469886,
author = {Desolda, Giuseppe and Ferro, Lauren S. and Marrella, Andrea and Catarci, Tiziana and Costabile, Maria Francesca},
title = {Human Factors in Phishing Attacks: A Systematic Literature Review},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469886},
doi = {10.1145/3469886},
abstract = {Phishing is the fraudulent attempt to obtain sensitive information by disguising oneself as a trustworthy entity in digital communication. It is a type of cyber attack often successful because users are not aware of their vulnerabilities or are unable to understand the risks. This article presents a systematic literature review conducted to draw a “big picture” of the most important research works performed on human factors and phishing. The analysis of the retrieved publications, framed along the research questions addressed in the systematic literature review, helps in understanding how human factors should be considered to defend against phishing attacks. Future research directions are also highlighted.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {173},
numpages = {35},
keywords = {cybersecurity, human factors, Phishing}
}

@inproceedings{10.1145/3366715.3366739,
author = {Chai, Yue and Zhao, Tongzhou and Jiang, Yiqi and Gao, Peidong and Li, Xuan},
title = {Text Representation Method Combining Multi- Level Semantic Features},
year = {2019},
isbn = {9781450362429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366715.3366739},
doi = {10.1145/3366715.3366739},
abstract = {The text vector representation transforms text from unstructured to structured, from high dimensional to low dimensional, and from sparse to dense, which is the basic task of text analysis. The senLDA model obtains the multinomial distribution of topics on the document based on the sentence, but due to the lack of semantic information for words, there is incomplete coverage of the high-value information and thus affects the effect of text representation. Aiming at this problem, a method that combines senLDA with Word2Vec's word-level features is proposed, which fuses three-level semantic features of words, sentences and documents to realize the text representation. F1 value of three datasets were increased by 11.41%, 17.88%, 17.63% respectively compared to the senLDA method, and increased by 4.65%, 7.73%, 8.62% respectively compared to Word2Vec.},
booktitle = {Proceedings of the 2019 International Conference on Robotics Systems and Vehicle Technology},
pages = {21–26},
numpages = {6},
keywords = {Text classification, Text representation, Word2Vec, senLDA},
location = {Wuhan, China},
series = {RSVT '19}
}

@inproceedings{10.1145/3141880.3141887,
author = {Morgan, Michael and Nyl\'{e}n, Aletta and Butler, Matthew and Eckerdal, Anna and Thota, Neena and Kinnunen, P\"{a}ivi},
title = {Examining Manual and Semi-Automated Methods of Analysing MOOC Data for Computing Education},
year = {2017},
isbn = {9781450353014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141880.3141887},
doi = {10.1145/3141880.3141887},
abstract = {We examine a semi-automated approach to the analysis of data from MOOC discussion forums. Previous research had analysed a sample of discussion forum data and developed a manual analysis framework, however this process can be very time consuming, especially given the class size of some online courses. Therefore it is important to investigate appropriate and automated analysis techniques to improve timeliness of analysis and to reveal the topics that emerge from a semi-automated process. An analysis of a data set from a coding MOOC in 2015 using the automated Structural Topic Modeling (STM) technique in R is described and contrasted against a manual analysis conducted on a segment of data from the same course in 2014. The types of analyses available and the relevance to computing education research is highlighted, with a focus on providing a discussion of the contrasting capabilities of each approach. The aim is to enable computing education researchers to assess the relevance of these techniques for further work.},
booktitle = {Proceedings of the 17th Koli Calling International Conference on Computing Education Research},
pages = {153–157},
numpages = {5},
keywords = {programming, data analysis, MOOC, online discussion},
location = {Koli, Finland},
series = {Koli Calling '17}
}

@inproceedings{10.1145/3383455.3422543,
author = {Fazelnia, Ghazal and Ibrahim, Mark and Modarres, Ceena and Wu, Kevin and Paisley, John},
title = {Mixed Membership Recurrent Neural Networks for Modeling Customer Purchases},
year = {2020},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422543},
doi = {10.1145/3383455.3422543},
abstract = {Models of sequential data such as the recurrent neural network (RNN) often implicitly treat a sequence of data as having a fixed time interval between observations. They also do not account for group-level effects when multiple sequences are observed generated from separate sources. A simple example is user purchasing behavior, where each user generates a unique sequence of purchases, and the time between purchases is variable. We propose a model for such sequential data based on the RNN that accounts for varying time intervals between observations in a sequence. We do this by learning a group-level "base" parameter to which each data-generating object can revert as more time passes before the next observation. This requires modeling assumptions about the data that we argue are typically satisfied by consumer purchasing behavior. Our approach is motivated by the mixed membership framework, with Latent Dirichlet Allocation being the canonical example, which we adapt to our dynamic setting. We demonstrate our approach on two consumer shopping datasets: The Instacart set of 3.4 million online grocery orders made by 206K customers, and a UK retail set consisting of over 500K orders.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {36},
numpages = {8},
keywords = {mixed membership models, recurrent neural networks, bayesian deep learning},
location = {New York, New York},
series = {ICAIF '20}
}

@article{10.1145/3377870,
author = {Wood, Ian D. and Glover, John and Buitelaar, Paul},
title = {Community Topic Usage in Online Social Media},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2469-7818},
url = {https://doi.org/10.1145/3377870},
doi = {10.1145/3377870},
abstract = {Humans have a natural tendency to form social groups, and individual behaviours are thought to be strongly influenced by a salient sense of belonging to one or more such groups. It can be expected, therefore, that there will be behaviours that are specific to the group(s) to which a person currently feels they are interacting with and that some of these behaviours will manifest in topics and patterns of linguistic style associated with those groups. Here we explore this idea by attempting to identify group specific patterns of language usage in social media data from Twitter and Reddit. Topic models are used to infer patterns of language usage and group structures are either provided with the data (Reddit) inferred from the follower network (Twitter). We apply a Bayesian graphical model to infer community-topic associations, finding that substantially more coherent associations can often be identified than with a naive probability-based approach. Strong associations are found between groups and topics with both approaches, indicating that the methods used to (independently) identify groups and topics represent real underlying patterns of social communication and promising fruitful investigation of human social behaviour using these or similar techniques.},
journal = {Trans. Soc. Comput.},
month = {may},
articleno = {14},
numpages = {21},
keywords = {eating disorders, Social networks, community detection, topic models, anorexia, social representation theory}
}

@inproceedings{10.1145/3143699.3143734,
author = {Mitrofanova, Olga and Sedova, Anastasiia},
title = {Topic Modelling in Parallel and Comparable Fiction Texts (the Case Study of English and Russian Prose)},
year = {2017},
isbn = {9781450354370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143699.3143734},
doi = {10.1145/3143699.3143734},
abstract = {The paper is devoted to processing parallel and comparable corpora by means of topic modelling. We focus our attention on Russian and English parallel and comparable texts. We use Latent Dirichlet Allocation (LDA) algorithm for building topic models of fiction texts, evaluation of compatibility for the original text and its translation(s), selection of possible translation equivalents.},
booktitle = {Proceedings of the International Conference IMS-2017},
pages = {175–180},
numpages = {6},
keywords = {Topic Modelling, Text Corpora, Russian, Fiction, Parallel and Comparable Texts, English},
location = {Saint Petersburg, Russian Federation},
series = {IMS2017}
}

@inproceedings{10.1145/3154979.3154987,
author = {Niyogi, Mitodru and Pal, Asim Kumar},
title = {Business: Do You Wanna Sell More? Discovering Topics, Sentiments and Prediction of Ratings},
year = {2017},
isbn = {9781450353243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3154979.3154987},
doi = {10.1145/3154979.3154987},
abstract = {In the era of Social Computing, the role of customer reviews and ratings can be instrumental in predicting the success and sustainability of businesses as customers and even competitors use them to judge the quality of a business. Yelp is one of the most popular websites for users to write such reviews. This rating can be subjective and biased toward user's personality. Business preferences of a user can be decrypted based on his/ her past reviews. In this paper, we deal with (i) uncovering latent topics in Yelp data based on positive and negative reviews using topic modeling to learn which topics are the most frequent among customer reviews, (ii) sentiment analysis of users' reviews to learn how these topics associate to a positive or negative rating which will help businesses improve their offers and services, and (iii) predicting unbiased ratings from user-generated review text alone, using Linear Regression model. We also perform data analysis to get some deeper insights into customer reviews.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communication Technology},
pages = {133–138},
numpages = {6},
keywords = {machine learning, predictive analysis, sentiment analysis, text mining, Topic modeling, Yelp reviews, data visualization},
location = {Allahabad, India},
series = {ICCCT-2017}
}

@inproceedings{10.1145/3148011.3148016,
author = {Maheshwari, Gaurav and Trivedi, Priyansh and Sahijwani, Harshita and Jha, Kunal and Dasgupta, Sourish and Lehmann, Jens},
title = {SimDoc: Topic Sequence Alignment Based Document Similarity Framework},
year = {2017},
isbn = {9781450355537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148011.3148016},
doi = {10.1145/3148011.3148016},
abstract = {Document similarity is the problem of estimating the degree to which a given pair of documents has similar semantic content. An accurate document similarity measure can improve several enterprise relevant tasks such as document clustering, text mining, and question-answering. In this paper, we show that a document's thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their similarity. To this end, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of related words. Then, we use a sequence alignment algorithm to estimate their semantic similarity. We further conceptualize a novel mechanism to compute topic-topic similarity to fine tune our system. In our experiments, we show that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering.},
booktitle = {Proceedings of the Knowledge Capture Conference},
articleno = {16},
numpages = {8},
keywords = {Lexical Semantics, Similarity Measures, Document Topic Models},
location = {Austin, TX, USA},
series = {K-CAP 2017}
}

@article{10.1145/3108238,
author = {Liu, Yining and Liu, Yong and Shen, Yanming and Li, Keqiu},
title = {Recommendation in a Changing World: Exploiting Temporal Dynamics in Ratings and Reviews},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3108238},
doi = {10.1145/3108238},
abstract = {Users’ preferences, and consequently their ratings and reviews to items, change over time. Likewise, characteristics of items are also time-varying. By dividing data into time periods, temporal Recommender Systems (RSs) improve recommendation accuracy by exploring the temporal dynamics in user rating data. However, temporal RSs have to cope with rating sparsity in each time period. Meanwhile, reviews generated by users contain rich information about their preferences, which can be exploited to address rating sparsity and further improve the performance of temporal RSs. In this article, we develop a temporal rating model with topics that jointly mines the temporal dynamics of both user-item ratings and reviews. Studying temporal drifts in reviews helps us understand item rating evolutions and user interest changes over time. Our model also automatically splits the review text in each time period into interim words and intrinsic words. By linking interim words and intrinsic words to short-term and long-term item features, respectively, we jointly mine the temporal changes in user and item latent features together with the associated review text in a single learning stage. Through experiments on 28 real-world datasets collected from Amazon, we show that the rating prediction accuracy of our model significantly outperforms the existing state-of-art RS models. And our model can automatically identify representative interim words in each time period as well as intrinsic words across all time periods. This can be very useful in understanding the time evolution of users’ preferences and items’ characteristics.},
journal = {ACM Trans. Web},
month = {aug},
articleno = {3},
numpages = {20},
keywords = {recommender systems, Topic models, temporal dynamics}
}

@inproceedings{10.1145/3038462.3038464,
author = {Sorkhei, Amin and Ilves, Kalle and Glowacka, Dorota},
title = {Exploring Scientific Literature Search through Topic Models},
year = {2017},
isbn = {9781450349031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3038462.3038464},
doi = {10.1145/3038462.3038464},
abstract = {With the fast growing amount of scientific literature, browsing through it can be a dicult task: formulating a precise query may be problematic as new research areas emerge quickly and different terms are often used to describe the same concept. To tackle some of these issues, we built a system for exploratory scientific search based on topic models. An initial short user study shows that through visualizing the relationship between keyphrases, documents and authors, the system allows the user to better explore the document search space compared to traditional systems based solely on search query.},
booktitle = {Proceedings of the 2017 ACM Workshop on Exploratory Search and Interactive Data Analytics},
pages = {65–68},
numpages = {4},
keywords = {exploratory search, topic models},
location = {Limassol, Cyprus},
series = {ESIDA '17}
}

@inproceedings{10.1145/3038912.3052630,
author = {Chen, Long and Jose, Joemon M. and Yu, Haitao and Yuan, Fajie},
title = {A Semantic Graph-Based Approach for Mining Common Topics from Multiple Asynchronous Text Streams},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052630},
doi = {10.1145/3038912.3052630},
abstract = {In the age of Web 2.0, a substantial amount of unstructured content are distributed through multiple text streams in an asynchronous fashion, which makes it increasingly difficult to glean and distill useful information. An effective way to explore the information in text streams is topic modelling, which can further facilitate other applications such as search, information browsing, and pattern mining. In this paper, we propose a semantic graph based topic modelling approach for structuring asynchronous text streams. Our model integrates topic mining and time synchronization, two core modules for addressing the problem, into a unified model. Specifically, for handling the lexical gap issues, we use global semantic graphs of each timestamp for capturing the hidden interaction among entities from all the text streams. For dealing with the sources asynchronism problem, local semantic graphs are employed to discover similar topics of different entities that can be potentially separated by time gaps. Our experiment on two real-world datasets shows that the proposed model significantly outperforms the existing ones.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1201–1209},
numpages = {9},
keywords = {knowledge repository, language modelling, topic modelling},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3486622.3493968,
author = {Kawamae, Noriaki},
title = {A Text Generation Model That Maintains the Order of Words, Topics, and Parts of Speech via Their Embedding Representations and Neural Language Models},
year = {2021},
isbn = {9781450391153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486622.3493968},
doi = {10.1145/3486622.3493968},
abstract = {Our goal is to generate coherent text accurately in terms of their semantic information and syntactic structure. Embedding methods and neural language models are indispensable in generating coherent text as they learn semantic information, and syntactic structure, respectively, and they are indispensable methods for generating coherent text. We focus here on parts of speech (POS) (e.g. noun, verb, preposition, etc.) so as to enhance these models, and allow us to generate truly coherent text more efficiently than is possible by using any of them in isolation. This leads us to derive Words and Topics and POS 2 Vec (WTP2Vec) as an embedding method, and Structure Aware Unified Language Model (SAUL) as a neural language model. Experiments show that our approach enhances previous models and generates coherent and semantically valid text with natural syntactic structure.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {262–269},
numpages = {8},
keywords = {embedding method, neural language modeling, text generation},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1145/3186262,
author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
title = {Learning Topic Models -- Provably and Efficiently},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3186262},
doi = {10.1145/3186262},
journal = {Commun. ACM},
month = {mar},
pages = {85–93},
numpages = {9}
}

@inproceedings{10.1145/3510362.3510368,
author = {Abri, Sara and Abri, Rayan and Cetin, Salih},
title = {User Click Preference Prediction Using Attention-Based LSTM Model},
year = {2021},
isbn = {9781450389006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510362.3510368},
doi = {10.1145/3510362.3510368},
abstract = {Predicting user clicking preferences is a critical issue in the ranking process reflected by the returned results from search engines. A few studies have explored the extracted sequen- tial information in the queries while this underlying information can be focused on as a valuable source for predicting user click preferences. This paper proposes an attention-based LSTM model to predict user click preferences on a submitted input quey using the previous queries sequence and user click history. In this model, we use the topic distribution of user documents as attention to the LSTM model. The feature information of the content extracted is used as the attention information of the LSTM network during the training process. We compare the model with topic-based ranking models with data from an AOL search engine and Session TREC 2013,2014 to show its performance. The result reveals significant improvement in the attention-based LSTM model using topics in the Mean Reciprocal Rank by 13% compared to the baseline topic-based models.},
booktitle = {2021 6th International Conference on Systems, Control and Communications (ICSCC)},
pages = {30–34},
numpages = {5},
keywords = {Topic-based models, LSTM model, User click preference},
location = {Chongqing, China},
series = {ICSCC 2021}
}

@inproceedings{10.1145/3337722.3337727,
author = {Lu, Chien and Peltonen, Jaakko and Nummenmaa, Timo},
title = {Game Postmortems vs. Developer Reddit AMAs: Computational Analysis of Developer Communication},
year = {2019},
isbn = {9781450372176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337722.3337727},
doi = {10.1145/3337722.3337727},
abstract = {Postmortems and Reddit Ask Me Anything (AMA) threads represent communications of game developers through two different channels about their game development experiences, culture, processes, and practices. We carry out a quantitative text mining based comprehensive analysis of online available postmortems and AMA threads from game developers over multiple years. We find and analyze underlying topics from the postmortems and AMAs as well as their variation among the data sources and over time. The analysis is done based on structural topic modeling, a probabilistic modeling technique for text mining. The extracted topics reveal differing and common interests as well as their evolution of prevalence over time in the two text sources. We have found that postmortems put more emphasis on detail-oriented development aspects as well as technically-oriented game design problems whereas AMAs feature a wider variety of discussion topics that are related to a more general game development process, game-play and game-play experience related game design. The prevalences of the topics also evolve differently over time in postmortems versus AMAs.},
booktitle = {Proceedings of the 14th International Conference on the Foundations of Digital Games},
articleno = {22},
numpages = {7},
keywords = {Reddit, text mining, literature analysis, postmortem analysis, game development},
location = {San Luis Obispo, California, USA},
series = {FDG '19}
}

@inproceedings{10.1145/3183399.3183410,
author = {Watanabe, Yasuhiro and Washizaki, Hironori and Honda, Kiyoshi and Fukazawa, Yoshiaki and Taga, Masahiro and Matsuzaki, Akira and Suzuki, Takayoshi},
title = {Retrospective Based on Data-Driven Persona Significance in B-to-B Software Development},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183410},
doi = {10.1145/3183399.3183410},
abstract = {A Business-to-Business (B-to-B) software development company develops services to satisfy their customers' requirements. Developers should prioritize customer satisfaction because customers greatly influence on agile software development. However, it is possible that a B-to-B software development company has following issues: 1) failure to understand actual users because the requirements are not often derived from actual users and 2) failure to satisfy the future customers' requirements when only satisfying current customers. Although many previous works proposed methods to elicit the requirements based on actual quantitative data, these works had not considered customers and end-users simultaneously. Herein we proposed Retrospective based on Data-Driven Persona Significance (ReD2PS) to help developers to plan future releases. ReD2PS includes Persona Significance Index (PerSil) to reflect the correspondence between target users, which developers assume based on requirements in releases, and end-users' personas. A case study involving a Japanese cloud application shows that PerSil reflects the relationship between target users and end-users to discuss about the validity and effectiveness of ReD2PS.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {89–92},
numpages = {4},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.1145/3316615.3316682,
author = {Rodzman, Shaiful Bakhtiar bin and Suhaili, Siti Suhaima binti and Ismail, Normaly Kamal and Rahman, Nurazzah Abd and Aljunid, Syed Ahmad and Omar, Aslida binti},
title = {Domain Specific Classification of Malay Based Complaints Using the Complaint Concept Ontologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316682},
doi = {10.1145/3316615.3316682},
abstract = {The complaint from users is an effective method to identify the quality of services and facilities provided by an organization. The efficiency to respond to users' complaint also depends on an effective workflow. By having an effective method and workflow, the action taken by the management to improve the quality of services and facilities can be done immediately and effectively. One of the ways is by classifying the complaints that will isolate related complaints. This paper presents the implementation of the classification system that combines the application of Complaint Concept Ontologies in Malay language as classifier rules with the BM25 model of Information Retrieval system. Experiments showed the semantic based elements such as Malay ontology may bring the improvement of the classification of the Malay Complaint. The result yielded showed that the proposed classifier produced better result in four category compared to BM25 original score that only produced better result in one category. OBMCS also outperformed the LDA model in all eight categories on the Recall, Precision and F-measure metrics. The finding proven the proposed system is very useful, especially to the Malay complaint in regards of classification for documents in the domain area.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {481–486},
numpages = {6},
keywords = {ontology based classification, Malay complaint, bm25 model, semantic classification},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/3148011.3154465,
author = {Hingmire, Swapnil and Chakraborti, Sutanu and Palshikar, Girish and Sodani, Abhay},
title = {WikiLDA: Towards More Effective Knowledge Acquisition in Topic Models Using Wikipedia},
year = {2017},
isbn = {9781450355537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148011.3154465},
doi = {10.1145/3148011.3154465},
abstract = {Towards the goal of enhancing interpretability of Latent Dirichlet Allocation (LDA) topics, we propose WikiLDA, an enhancement to LDA using Wikipedia concepts. In WikiLDA, initially, for each document in a corpus we "sprinkle" (append) its most relevant Wikipedia concepts. We then use Generalized P\'{o}lya Urn (GPU) to incorporate word-word, word-concept, and concept-concept semantic relatedness into the generative process of LDA. As the most probable concepts from inferred topics can be referred on Wikipedia, the topics are likely to become more interpretable and hence more usable in acquiring domain knowledge from humans for various text mining tasks (e.g. eliciting topic labels for text classification). Empirical results show that a projection of documents by WikiLDA in a semantically enriched and coherent topic space leads to improved performance in text classification like tasks, especially in domains where the classes are hard to separate.},
booktitle = {Proceedings of the Knowledge Capture Conference},
articleno = {37},
numpages = {4},
keywords = {knowledge acquisition, topic models, interpretability},
location = {Austin, TX, USA},
series = {K-CAP 2017}
}

@inproceedings{10.1145/3341105.3373997,
author = {Park, Heesoo and Lee, Jongwuk},
title = {Decoupled Word Embeddings Using Latent Topics},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373997},
doi = {10.1145/3341105.3373997},
abstract = {In this paper, we propose decoupled word embeddings (DWE) as a universal word representation that covers multiple senses of words. Toward this goal, our model represents each word as a combination of multiple word vectors that are associated with latent topics. Specifically, we decompose a word vector into multiple word vectors for multiple senses, according to the topic weight obtained from pre-trained topic models. Although this dynamic word representation is simple, the proposed model can leverage both local and global contexts. Through extensive experiments, including qualitative and quantitative analyses, we demonstrate that the proposed model is comparable to or better than state-of-the-art word embedding models. The code is publicly available at https://github.com/righ120/DWE.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {875–882},
numpages = {8},
keywords = {multi-sense word embedding, topic modeling, contextualized word embedding},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3394332.3402838,
author = {E. Middleton, Stuart and Lavorgna, Anita and Neumann, Geoff and Whitehead, David},
title = {Information Extraction from the Long Tail: A Socio-Technical AI Approach for Criminology Investigations into the Online Illegal Plant Trade},
year = {2020},
isbn = {9781450379946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394332.3402838},
doi = {10.1145/3394332.3402838},
abstract = {In today's online forums and marketplaces cybercrime activity can often be found lurking in plain sight behind legitimate posts. Most popular criminology techniques are either manually intensive, and so do not scale well, or focus on statistical summaries across websites and can miss infrequent behaviour patterns. We present an inter-disciplinary (computer science, criminology and conservation science) socio-technical artificial intelligence (AI) approach to information extraction from the long tail of online forums around internet-facilitated illegal trades of endangered species. Our methodology is highly iterative, taking entities of interest (e.g. endangered plant species, suspects, locations) identified by a criminologist and using them to direct computer science tools including crawling, searching and information extraction over many steps until an acceptable resulting intelligence package is achieved. We evaluate our approach using two case study experiments, each based on a one-week duration criminology investigation (aided by conservation science experts) and evaluate both named entity (NE) directed graph visualization and Latent Dirichlet Allocation (LDA) topic modelling. NE directed graph visualization consistently outperforms topic modelling for discovering connected entities in the long tail of online forums and marketplaces.},
booktitle = {12th ACM Conference on Web Science Companion},
pages = {82–88},
numpages = {7},
keywords = {Artificial Intelligence, CITES, Socio-technical, Information Extraction, Criminology, Natural Language Processing, Illegal Wildlife Trade},
location = {Southampton, United Kingdom},
series = {WebSci '20}
}

@inproceedings{10.1145/3328433.3328455,
author = {Mattis, Toni and D\"{u}rsch, Falco and Hirschfeld, Robert},
title = {Faster Feedback through Lexical Test Prioritization},
year = {2019},
isbn = {9781450362573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328433.3328455},
doi = {10.1145/3328433.3328455},
abstract = {Immediacy and continuity of feedback are desirable properties during programming. Automated tests are a widely used practice to gain feedback on whether test authors' expectations are consistent with an implementation. With growing test suites, feedback becomes less immediate and is obtained less frequently because of that. The objective of test prioritization is to choose an order of tests that catches errors as early as possible, ideally within a time frame that we can consider live.Research in test prioritization often relies on dynamic analysis, which is expensive to obtain. Newer approaches focus on most recently edited source code locations and propose IR (information retrieval) approaches that regard a change to the software as query against a collection of tests.We study the capability of the IR approach to reduce testing time in the presence of faults using the example of open-source Python projects, identify trade-offs in classical TF-IDF-based IR frameworks, and propose different approaches that consider lexical and semantic context of a change, including topic modeling.We conclude that even simple IR strategies achieve immediate error detection, especially when tests themselves were edited alongside program code. We further discuss applications of this approach in live programming environments, where change granularity does not leave sufficient time to run a test suite entirely.},
booktitle = {Proceedings of the Conference Companion of the 3rd International Conference on Art, Science, and Engineering of Programming},
articleno = {21},
numpages = {10},
keywords = {feedback, topic models, testing, information retrieval},
location = {Genova, Italy},
series = {Programming '19}
}

@inproceedings{10.1145/3132847.3132968,
author = {Zheng, Xin and Sun, Aixin and Wang, Sibo and Han, Jialong},
title = {Semi-Supervised Event-Related Tweet Identification with Dynamic Keyword Generation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132968},
doi = {10.1145/3132847.3132968},
abstract = {Twitter provides us a convenient channel to get access to the immediate information about major events. However, it is challenging to acquire a clean and complete set of event-related data due to the characteristics of tweets, eg short and noisy. In this paper, we propose a semi-supervised method to obtain high quality event-related tweets from Twitter stream, in terms of precision and recall. Specifically, candidate event-related tweets are selected based on a set of keywords. We propose to generate and update these keywords dynamically along the event development. To be included in this keyword set, words are evaluated based on single word properties, property based on co-occurred words, and changes of word importance over time. Our solution is capable of capturing keywords of emerging aspects or aspects with increasing importance along event evolvement. By leveraging keyword importance information and a few labeled tweets, we propose a semi-supervised expectation maximization process to identify event-related tweets. This process significantly reduces human effort in acquiring high quality tweets. Experiments on three real world datasets show that our solution outperforms state-of-the-art approaches by up to 10% in F1 measure.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1619–1628},
numpages = {10},
keywords = {event-related tweet identification, dynamic keyword generation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3106426.3106478,
author = {Jing, Xia and Tang, Jie},
title = {Guess You like: Course Recommendation in MOOCs},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106478},
doi = {10.1145/3106426.3106478},
abstract = {Recommending courses to online students is a fundamental and also challenging issue in MOOCs. Not exactly like recommendation in traditional online systems, students who enrolled the same course may have very different purposes and with very different backgrounds. For example, one may want to study "data mining" after studying the course of "big data analytics" because the former is a prerequisite course of the latter, while some other may choose "data mining" simply because of curiosity.Employing the complete data from XuetangX1, one of the largest MOOCs in China, we conduct a systematic investigation on the problem of student behavior modeling for course recommendation. We design a content-aware algorithm framework using content based users' access behaviors to extract user-specific latent information to represent students' interest profile. We also leverage the demographics and course prerequisite relation to better reveal users' potential choice. Finally, we develop a course recommendation algorithm based on the user interest, demographic profiles and course prerequisite relation using collaborative filtering strategy. Experiment results demonstrate that the proposed algorithm performs much better than several baselines (over 2X by MRR). We have deployed the recommendation algorithm onto the platform XuetangX as a new feature, which significantly helps improve the course recommendation performance (+24.6% by click rate) comparing with the recommendation strategy previously used in the system.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {783–789},
numpages = {7},
keywords = {MOOCs, personalization, course recommendation},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3341069.3342989,
author = {Liu, Pan and Liu, Jie and Ma, Xiaoli and Zhou, Jianshe},
title = {Off-Topic Detection Model Based on Biterm-LDA and Doc2vec},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3342989},
doi = {10.1145/3341069.3342989},
abstract = {Chinese writing in primary and secondary schools occupies an extremely important position in Chinese education. With the advent of natural language processing, the automatic e ssay review system has gradually matured, which has greatly promoted the development of composition writing. Especially the off-topic detection plays a key role in the automatic essay review system. We propose effective methods for off-topic detection. Firstly, we use Biterm-LDA combined with Doc2vec to inspect the topic and semantics of composition. Secondly, we propose a threshold calculation method based on the topic composition class center under different topic compositions. Finally, the ROC curve is employed to find the optimal threshold for each type of topic composition, then according to the optimal threshold, the off topic essay is judged. Experiments of the five types of topic composition show the average F1-score value of the off-topic detection reach about 65%.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {157–161},
numpages = {5},
keywords = {Doc2vec, Off-topic Detection, Automatic Essay Review, Biterm-LDA},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.1145/3172944.3173011,
author = {Derezinski, Michal and Rohanimanesh, Khashayar and Hydrie, Aamer},
title = {Discovering Surprising Documents with Context-Aware Word Representations},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173011},
doi = {10.1145/3172944.3173011},
abstract = {User experiences can be made more engaging by incorporating surprise. For example, online shoppers may like to view unique products. In this paper we propose an approach for detecting surprising documents, such as product titles. As the concept of surprise is subjective, there is currently no principled method for measuring the surprisingness score of a document. We present such a method; an unsupervised approach for automatically discovering surprising documents in an unlabeled corpus. Our approach is based on a probabilistic model of surprise, and a construction of effective distributional word embeddings, which can be adapted to the semantic context in which the word appears. As the performance of our model does not degrade with the length of the document, it is particularly well suited for very short documents (even a single sentence). We evaluate our model both in supervised and unsupervised settings, demonstrating its state-of-the-art performance on two real-world data sets: a collection of e-commerce products from eBay, and a corpus of NSF proposals. These experiments show that our surprisingness score exhibits high correlation with human annotated labels.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {31–35},
numpages = {5},
keywords = {information theory, recommender systems, text surprisingness, product discovery, topic modeling},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3018661.3018678,
author = {Liu, Huiwen and Xu, Jiajie and Zheng, Kai and Liu, Chengfei and Du, Lan and Wu, Xian},
title = {Semantic-Aware Query Processing for Activity Trajectories},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018678},
doi = {10.1145/3018661.3018678},
abstract = {Nowadays, users of social networks like tweets and weibo have generated massive geo-tagged records, and these records reveal their activities in the physical world together with spatio-temporal dynamics. Existing trajectory data management studies mainly focus on analyzing the spatio-temporal properties of trajectories, while leaving the understanding of their activities largely untouched. In this paper, we incorporate the semantic analysis of the activity information embedded in trajectories into query modelling and processing, with the aim of providing end users more accurate and meaningful trip recommendations. To this end, we propose a novel trajectory query that not only considers the spatio-temporal closeness but also, more importantly, leverages probabilistic topic modelling to capture the semantic relevance of the activities between data and query. To support efficient query processing, we design a novel hybrid index structure, namely ST-tree, to organize the trajectory points hierarchically, which enables us to prune the search space in spatial and topic dimensions simultaneously. The experimental results on real datasets demonstrate the efficiency and scalability of the proposed index structure and search algorithms.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {283–292},
numpages = {10},
keywords = {activity trajectories query, semantic relevance, spatial keywords},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@article{10.1145/3094786,
author = {Gao, Yang and Li, Yuefeng and Lau, Raymond Y. K. and Xu, Yue and Bashar, Md Abul},
title = {Finding Semantically Valid and Relevant Topics by Association-Based Topic Selection Model},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3094786},
doi = {10.1145/3094786},
abstract = {Topic modelling methods such as Latent Dirichlet Allocation (LDA) have been successfully applied to various fields, since these methods can effectively characterize document collections by using a mixture of semantically rich topics. So far, many models have been proposed. However, the existing models typically outperform on full analysis on the whole collection to find all topics but difficult to capture coherent and specifically meaningful topic representations. Furthermore, it is very challenging to incorporate user preferences into existing topic modelling methods to extract relevant topics. To address these problems, we develop a novel personalized Association-based Topic Selection (ATS) model, which can identify semantically valid and relevant topics from a set of raw topics based on the semantical relatedness between users’ preferences and the structured patterns captured in topics. The advantage of the proposed ATS model is that it enables an interactive topic modelling process driven by users’ specific interests. Based on three benchmark datasets, namely, RCV1, R8, and WT10G under the context of information filtering (IF) and information retrieval (IR), our rigorous experiments show that the proposed ATS model can effectively identify relevant topics with respect to users’ specific interests, and hence to improve the performance of IF and IR.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {3},
numpages = {22},
keywords = {topic components, topic evaluation, Topic selection, information filtering}
}

@inproceedings{10.1145/3477495.3532042,
author = {Wu, Likang and Wang, Hao and Chen, Enhong and Li, Zhi and Zhao, Hongke and Ma, Jianhui},
title = {Preference Enhanced Social Influence Modeling for Network-Aware Cascade Prediction},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532042},
doi = {10.1145/3477495.3532042},
abstract = {Network-aware cascade size prediction aims to predict the final reposted number of user-generated information via modeling the propagation process in social networks. Estimating the user's reposting probability by social influence, namely state activation plays an important role in the information diffusion process. Therefore, Graph Neural Networks (GNN), which can simulate the information interaction between nodes, has been proved as an effective scheme to handle this prediction task. However, existing studies including GNN-based models usually neglect a vital factor of user's preference which influences the state activation deeply. To that end, we propose a novel framework to promote cascade size prediction by enhancing the user preference modeling according to three stages, i.e., preference topics generation, preference shift modeling, and social influence activation. Our end-to-end method makes the user activating process of information diffusion more adaptive and accurate. Extensive experiments on two large-scale real-world datasets have clearly demonstrated the effectiveness of our proposed model compared to state-of-the-art baselines.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2704–2708},
numpages = {5},
keywords = {cascade prediction, network-aware diffusion, text semantics, graph neural networks, preference modeling},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3487553.3524721,
author = {Ommi, Yassaman and Yousefabadi, Matin and Faez, Faezeh and Sabour, Amirmojtaba and Soleymani Baghshah, Mahdieh and Rabiee, Hamid R.},
title = {CCGG: A Deep Autoregressive Model for Class-Conditional Graph Generation},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524721},
doi = {10.1145/3487553.3524721},
abstract = {Graph data structures are fundamental for studying connected entities. With an increase in the number of applications where data is represented as graphs, the problem of graph generation has recently become a hot topic. However, despite its significance, conditional graph generation that creates graphs with desired features is relatively less explored in previous studies. This paper addresses the problem of class-conditional graph generation that uses class labels as generation constraints by introducing the Class Conditioned Graph Generator (CCGG). We built CCGG by injecting the class information as an additional input into a graph generator model and including a classification loss in its total loss along with a gradient passing trick. Our experiments show that CCGG outperforms existing conditional graph generation methods on various datasets. It also manages to maintain the quality of the generated graphs in terms of distribution-based evaluation metrics.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1092–1098},
numpages = {7},
keywords = {Graph Generation, Generative Models, Conditional Generative Models},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3191697.3213797,
author = {Mattis, Toni},
title = {Mining Concepts from Code Using Community Detection in Co-Occurrence Graphs},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3213797},
doi = {10.1145/3191697.3213797},
abstract = {Software modularity is a quality that determines how fluently individual parts (modules) of a system can vary and be understood if taken by themselves. However, modularity tends to degrade during program evolution – old concepts may get entangled with code introduced into their modules, while new concepts can be scattered over many existing modules. In this work, we propose to infer high-level concepts and relations between them independently from the current module decomposition by exploiting the vocabulary used by programmers. Our approach uses an extensible graph-based vocabulary representation in which we detect latent communities representing our concepts. Inferred concepts can be used to support program comprehension, track architectural drift over time, and provide recommendations for related code or refactorings.},
booktitle = {Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming},
pages = {232–233},
numpages = {2},
keywords = {graphs, topic models, modularity},
location = {Nice, France},
series = {Programming'18 Companion}
}

@article{10.1145/3451167,
author = {Wang, Yashen and Zhang, Huanhuan and Liu, Zhirun and Zhou, Qiang},
title = {Hierarchical Concept-Driven Language Model},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3451167},
doi = {10.1145/3451167},
abstract = {For guiding natural language generation, many semantic-driven methods have been proposed. While clearly improving the performance of the end-to-end training task, these existing semantic-driven methods still have clear limitations: for example, (i) they only utilize shallow semantic signals (e.g., from topic models) with only a single stochastic hidden layer in their data generation process, which suffer easily from noise (especially adapted for short-text etc.) and lack of interpretation; (ii) they ignore the sentence order and document context, as they treat each document as a bag of sentences, and fail to capture the long-distance dependencies and global semantic meaning of a document. To overcome these problems, we propose a novel semantic-driven language modeling framework, which is a method to learn a Hierarchical Language Model and a Recurrent Conceptualization-enhanced Gamma Belief Network, simultaneously. For scalable inference, we develop the auto-encoding Variational Recurrent Inference, allowing efficient end-to-end training and simultaneously capturing global semantics from a text corpus. Especially, this article introduces concept information derived from high-quality lexical knowledge graph Probase, which leverages strong interpretability and anti-nose capability for the proposed model. Moreover, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence concept dependence. Experiments conducted on several NLP tasks validate the superiority of the proposed approach, which could effectively infer meaningful hierarchical concept structure of document and hierarchical multi-scale structures of sequences, even compared with latest state-of-the-art Transformer-based models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {may},
articleno = {104},
numpages = {22},
keywords = {representation learning, Language modeling, interpretation, concept semantic information, text generation, recurrent conceptualization-enhanced gamma belief network, hierarchical language modeling}
}

@inproceedings{10.1145/3498366.3505806,
author = {Tanhaei, Ghazaleh},
title = {Exploring Neuroscience Literature and Understanding Relations Between Brain-Related Topics - Using Augmented Reality},
year = {2022},
isbn = {9781450391863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498366.3505806},
doi = {10.1145/3498366.3505806},
abstract = {Neuroscience researchers are interested in understanding relation between anatomical regions of the brain and disorders that affect them, for example. Using the topics themselves, rather than individual articles, to examine relation in a large body of literature, can provide a higher-level approach. I investigate the use of 3D representations in Augmented Reality to aid neuroscientists to explore literature and understand relations between brain-related topics, given the three-dimensional nature of the brain. Distant reading refers to comprehending the results of studies of a large number of articles, as opposed to the more common ”close reading” of individual publications. For distant reading of neuroscience literature, I identify visualization and interaction design requirements. My assumption is that by providing overviews of the correlations among topics through the use of literature, these will allow neuroscientists to better understand the gaps in the literature and more quickly identify the most suitable experiments to carry out. The DatAR team at Utrecht University has created a prototype 3D AR implementation using which I have carried out two studies of a literature exploration interface. These studies showed that visualizing topics and their relation in an immersive AR environment is clear, understandable and helpful for exploring neuroscience literature. In the following, I will carry out a study that participants can make parallel query and compare the results. I will further investigate in finding indirect relations between brain regions and brain diseases. The last study will support neuroscience students to understand course material. Interface improvements are considering where necessary.},
booktitle = {ACM SIGIR Conference on Human Information Interaction and Retrieval},
pages = {387–390},
numpages = {4},
keywords = {virtual reality, measures of user experience and performance, neuroscience literature search, information visualization and visual analytics, exploratory search, augmented reality},
location = {Regensburg, Germany},
series = {CHIIR '22}
}

@inproceedings{10.1145/3025171.3025172,
author = {Jiang, Biye and Canny, John},
title = {Interactive Machine Learning via a GPU-Accelerated Toolkit},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025172},
doi = {10.1145/3025171.3025172},
abstract = {Machine learning is growing in importance in industry, sciences, and many other fields. In many and perhaps most of these applications, users need to trade off competing goals. Machine learning, however, has evolved around the optimization of a single, usually narrowly-defined criterion. In most cases, an expert makes (or should be making) trade-offs between these criteria which requires high-level (human) intelligence. With interactive customization and optimization the expert can incorporate secondary criteria into the model-generation process in an interactive way. In this paper we develop the techniques to perform customized and interactive model optimization, and demonstrate the approach on several examples. The keys to our approach are (i) a machine learning architecture which is modular and supports primary and secondary loss functions, while users can directly manipulate its parameters during training (ii) high-performance training so that non-trivial models can be trained in real-time (using roofline design and GPU hardware), and (iii) highly-interactive visualization tools that support dynamic creation of visualizations and controls to match various optimization criteria.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {535–546},
numpages = {12},
keywords = {machine learning, hyper-parameters tuning, multiple objective optimization, GPU, interactive},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3491101.3519674,
author = {Liao, Mengqi and Sundar, S. Shyam},
title = {#facebookdown: Time to Panic or Detox? Understanding Users’ Reactions to Social Media Outage},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519674},
doi = {10.1145/3491101.3519674},
abstract = {Non-use, particularly involuntary non-use, is an under-researched topic in HCI research, even though it has become quite common nowadays due to frequent digital outages. How do users react to social media outage? Do they become anxious? Or, do they enjoy these brief episodes of social media detox? To answer these questions, we conducted a topic modeling analysis of 223,815 tweets that used the hashtag #facebookdown during the major Facebook outage on 10/4/2021. We uncovered 10 major themes of users’ reactions towards social media outage. Results showed that most users complained, mocked and showed desperation about the outage situation, and during the outage period, increased their quest for other social-media alternatives. Also, surprisingly, many users celebrated the detox from Facebook rather than wishing it to come back as soon as possible. Results offer design implications for practitioners who would like to better respond to future outages.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {357},
numpages = {8},
keywords = {involuntary non-use, Social media outage, Topic Modeling},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3393527.3393550,
author = {Yin, Shengjun and Yang, Kailai and Wang, Hongzhi},
title = {A MOOC Courses Recommendation System Based on Learning Behaviours},
year = {2020},
isbn = {9781450375344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393527.3393550},
doi = {10.1145/3393527.3393550},
abstract = {MOOC1 courses recommendation is an important and challenging task, especially in an era with a quick development of Internet which consists of gigantic and diverse education resources. Its challenge is due to its massive amount of education information in almost all academic fields and as a result, the inevitable negligence of personalized needs for certain knowledge. Therefore, the research on timely capturing of the learners' behaviours and then personalized guidance of their learning process becomes increasingly essential. In this paper, we analyse online learning behaviours to improve personalized recommendations in MOOC courses. Our main contribution is to utilize information from different sources and design a centralized framework to combine them, thus making superior recommendation. We propose two different models based on the above sources and a combined model, and then contrast the models with other traditional models to prove the superior performance of our models.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
pages = {133–137},
numpages = {5},
keywords = {Learning Behaviours, Recommendation System, Mooc},
location = {Hefei, China},
series = {ACM TURC'20}
}

@inproceedings{10.1145/3378936.3378971,
author = {Kim, Jaekwang},
title = {A Novel Approach for Blog Feeds Recommendation Based on Meta-Data},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378971},
doi = {10.1145/3378936.3378971},
abstract = {As the blogosphere continues to grow, finding good quality blog feeds has been very time consuming and requires much effort. So, recommending blog feeds, which handle topics close to user interests, can be useful. Recently, the number of bloggers who use the subscription services has been increasing. Subscription is a service using protocols like RSS and ATOM, which notify users when new entries are posted on the blogs that the users register for subscription. In this paper, we present an effective and efficient approach to recommending log feeds based on the subscription lists and meta-data of blogs. In order to find blogs that handle topics close to the blogs in subscription lists, we first model the topic of blogs by collecting and expanding tags in the blogs, and we then compare the topic models of blogs for recommendation. Also, the usefulness of blogs is an important factor. For choosing useful blogs, we adopt the update frequency and number of subscribers because useful blogs are the ones in which new entries will be frequently posted and to which many users will subscribe. In order to validate the proposed blog recommendation algorithm, experiments on real blog data have been conducted, and the results of experiments show that our blog feed recommendation can satisfy users who want to subscribe to blog feeds.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {15–20},
numpages = {6},
keywords = {Recommendation, Subscription, Blog, Meta-data},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/3217804.3217946,
author = {Sachdeva, Sonya and McCaffrey, Sarah},
title = {Using Social Media to Predict Air Pollution during California Wildfires},
year = {2018},
isbn = {9781450363341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3217804.3217946},
doi = {10.1145/3217804.3217946},
abstract = {Wildfires have significant effects on human populations worldwide. Smoke pollution, in particular, from either prescribed burns or uncontrolled wildfires, can have profound health impacts, such as reducing birth weight in children and aggravating respiratory and cardiovascular conditions. Scarcity in the measurements of particulate matter responsible for these public health issues makes addressing the problem of smoke dispersion challenging, especially when fires occur in remote regions. Previous research has shown that in the case of the 2014 King fire in California, crowdsourced data can be useful in estimating particulate pollution from wildfire smoke. In this paper, we show that the previous model continues to provide good estimates when extended statewide to cover several wildfires over an entire season in California. Moreover, adding the semantic information contained in the social media data to the predictive model significantly increases model accuracy, indicating a confluence of social and spatio-temporal data.},
booktitle = {Proceedings of the 9th International Conference on Social Media and Society},
pages = {365–369},
numpages = {5},
keywords = {automated text analysis, air pollution, social media, wildfire, smoke},
location = {Copenhagen, Denmark},
series = {SMSociety '18}
}

@inproceedings{10.1145/3018661.3018666,
author = {He, Yuan and Wang, Cheng and Jiang, Changjun},
title = {Modeling Document Networks with Tree-Averaged Copula Regularization},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018666},
doi = {10.1145/3018661.3018666},
abstract = {Document network is a kind of intriguing dataset which provides both topical (texts) and topological (links) information. Most previous work assumes that documents closely linked with each other share common topics. However, the associations among documents are usually complex, which are not limited to the homophily (i.e., tendency to link to similar others). Actually, the heterophily (i.e., tendency to link to different others) is another pervasive phenomenon in social networks. In this paper, we introduce a new tool, called copula, to separately model the documents and links, so that different copula functions can be applied to capture different correlation patterns. In statistics, a copula is a powerful framework for explicitly modeling the dependence of random variables by separating the marginals and their correlations. Though widely used in Economics, copulas have not been paid enough attention to by researchers in machine learning field. Besides, to further capture the potential associations among the unconnected documents, we apply the tree-averaged copula instead of a single copula function. This improvement makes our model achieve better expressive power, and also more elegant in algebra. We derive efficient EM algorithms to estimate the model parameters, and evaluate the performance of our model on three different datasets. Experimental results show that our approach achieves significant improvements on both topic and link modeling compared with the current state of the art.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {691–699},
numpages = {9},
keywords = {topic models, plsa, document networks, copula},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@article{10.1145/3051127,
author = {Yang, Yang and Tang, Jie and Li, Juanzi},
title = {Learning to Infer Competitive Relationships in Heterogeneous Networks},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3051127},
doi = {10.1145/3051127},
abstract = {Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this article, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Overall, we find that similar entities tend to be competitors, with a probability of 4 times higher than chance. On the other hand, in social network, we also find a 10 minutes phenomenon: when two entities are mentioned by the same user within 10 minutes, the likelihood of them being competitors is 25 times higher than chance. Based on the discovered patterns, we propose a novel Topical Factor Graph Model. Generally, our model defines a latent topic layer to bridge the Twitter network and patent network. It then employs a semi-supervised learning algorithm to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46% improvement over alternative methods. Besides, we further demonstrate the competitive relationships inferred by our proposed model can be applied in the job-hopping prediction problem by achieving an average of +10.7% improvement.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {12},
numpages = {23},
keywords = {heterogeneous network, competitive relationship, Social network}
}

@inproceedings{10.1145/3297280.3297648,
author = {Koupaee, Mahnaz},
title = {Mortality Prediction Using Medical Notes: Student Research Abstract},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297648},
doi = {10.1145/3297280.3297648},
abstract = {Mortality prediction is a critical task for assessing patients' conditions in Intensive Care Units (ICU) of hospitals to improve decision-making and quality of care. Measurements taken and recorded at different time points are the main source of information to be used for tasks related to healthcare. However, the notes written by medical service providers during patients' stay in hospital as a rich source of detailed information is not sufficiently exploited. In this work, we propose a Convolutional Neural Network (CNN) architecture to utilize the unstructured texts to predict the pre-discharge and post-discharge mortality of ICU patients. Evaluations show high performance of the proposed method in terms of precision and recall. Moreover, our method outperforms the state of the art method by achieving a higher AUC.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {778–781},
numpages = {4},
keywords = {medical notes, convolutional neural network, mortality prediction, MIMIC},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3404687.3404691,
author = {Kuang, Guangsheng and Guo, Yan and Liu, Yue and Pang, Liang and Yu, Zhihua and Cheng, Xueqi and Liu, Jinlong and Yu, Xiaoming},
title = {Bursty Event Detection via Multichannel Feature Alignment},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404691},
doi = {10.1145/3404687.3404691},
abstract = {To quickly identify bursty events that are emerging and developing in their early stages is important for our emergency response and public security. Daily news and social media are two major channels for people to contact with the world, thus become the main sources for the bursty event detection. However, recent works either use daily news only which is authoritative and well-organized, but easily out of date, or use social media only which is real-time and abundant, but contain a lot of noise. In this paper, to construct an efficient and effective bursty event detection system, we propose to combine the data from daily news and social media channels. Firstly, bursty features are extracted from social media and initially grouped into bursty events. Then, the data from two channels are aligned by using supervised learning at the level of events. Finally, we use the news to verify the detected result from social media by alignment algorithms. Experimental results show that our framework outperforms baselines.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {39–45},
numpages = {7},
keywords = {multichannel, bursty detection, data fusion, Event detection},
location = {Chengdu, China},
series = {ICBDC '20}
}

@article{10.5555/3122009.3242026,
author = {Morstatter, Fred and Liu, Huan},
title = {In Search of Coherence and Consensus: Measuring the Interpretability of Statistical Topics},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (e.g., their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence--model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories of topics. Good topics should lead to good categories, thus, high topic consensus. Therefore, if there is low topic consensus in terms of categories, topics could be of low interpretability. We then further discuss how topic coherence and topic consensus assess different aspects of topic interpretability and hope that this work can pave way for comprehensive measures of topic interpretability.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {6177–6208},
numpages = {32}
}

@inproceedings{10.1145/3019612.3019673,
author = {Bahrainian, Seyed Ali and Mele, Ida and Crestani, Fabio},
title = {Modeling Discrete Dynamic Topics},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019673},
doi = {10.1145/3019612.3019673},
abstract = {Topic modeling is an important area which aims at indexing and exploring massive data streams. In this paper we introduce a discrete Dynamic Topic Modeling (dDTM) algorithm, which is able to model a dynamic topic that is not necessarily present over all time slices in a stream of documents. Our proposed model has applications in modeling dynamic topics of rapidly changing and less structured data, such as online microblogs and news streams.Our results show that the topical chains (i.e., evolution of topics) computed by our algorithm is more representative of the contents of documents than the original Dynamic Topic Modeling (DTM) in terms of likelihood on held-out data. Furthermore, we show that our method is effective in identifying emerging trends in streaming data.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {858–865},
numpages = {8},
keywords = {stream mining, dynamic topic modeling, news mining},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3447548.3467426,
author = {Gupta, Nilesh and Bohra, Sakina and Prabhu, Yashoteja and Purohit, Saurabh and Varma, Manik},
title = {Generalized Zero-Shot Extreme Multi-Label Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467426},
doi = {10.1145/3447548.3467426},
abstract = {Extreme Multi-label Learning (XML) involves assigning the subset of most relevant labels to a data point from millions of label choices. A hitherto unaddressed challenge in XML is that of predicting unseen labels with no training points. These form a significant fraction of total labels and contain fresh and personalized information desired by end users. Most existing extreme classifiers are not equipped for zero-shot label prediction and hence fail to leverage unseen labels. As a remedy, this paper proposes a novel approach called ZestXML for the task of Generalized Zero-shot XML (GZXML) where relevant labels have to be chosen from all available seen and unseen labels. ZestXML learns to project a data point's features close to the features of its relevant labels through a highly sparsified linear transform. This L0-constrained linear map between the two high-dimensional feature vectors is tractably recovered through a novel optimizer based on Hard Thresholding. By effectively leveraging the sparsities in features, labels and the learnt model, ZestXML achieves higher accuracy and smaller model size than existing XML approaches while also promoting efficient training &amp; prediction, real-time label update as well as explainable prediction.Experiments on large-scale GZXML datasets demonstrated that ZestXML can be up to 14% and 10% more accurate than state-of-the-art extreme classifiers and leading BERT-based dense retrievers respectively, while having 10x smaller model size. ZestXML trains on largest dataset with 31M labels in just 30 hours on a single core of a commodity desktop. When added to an large ensemble of existing models in Bing Sponsored Search Advertising, ZestXML significantly improved click yield of IR based system by 17% and unseen query coverage by 3.4% respectively. ZestXML's source code and benchmark datasets for GZXML will be publically released for research purposes here.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {527–535},
numpages = {9},
keywords = {label metadata, zero-shot learning, extreme multi-label classification, sponsored search advertising},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3027385.3029486,
author = {Vytasek, Jovita M. and Wise, Alyssa F. and Woloshen, Sonya},
title = {Topic Models to Support Instructors in MOOC Forums},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029486},
doi = {10.1145/3027385.3029486},
abstract = {This paper explores the potential of using na\"{\i}ve topic modeling to support instructors in navigating MOOC discussion forums. Categorizing discussion threads into topics can provide an overview of the discussion, improve navigation of the forum, and support replying to a representative sample of content related posts. We investigate four different approaches to using topic models to organize and present discussion posts, highlighting the strength and weaknesses of each approach to support instructors.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {610–611},
numpages = {2},
keywords = {topic modeling, MOOC, discussion forums},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3396743.3396756,
author = {Yang, Mingyi and Xu, Yang},
title = {The Promotion Mechanism Development for User's Information Need Understanding Based on Knowledge Structure Precise Matching},
year = {2020},
isbn = {9781450377065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396743.3396756},
doi = {10.1145/3396743.3396756},
abstract = {Understanding user's information needs is the premise and basis to improve the quality of information services and systems. When engaging in various activities, people have run into problems that are followed by the realization of insufficient knowledge structure to resolve these problems, and the need for information has arisen. After information needs are generated, they have experienced an expression process from the internal to the external and from the abstract to the concrete. The process of information need expression is influenced by the users themselves and their surrounding environment. Ambiguities will appear during this process. The current systems mainly promote understanding through (1) information extraction and knowledge acquisition, (2) ontology, semantic web, and knowledge graphs, (3) semantic disambiguation, and (4) topic modeling. This research proposes a new promotion mechanism for the understanding of user information needs. Based on collaborative filtering and folksonomy, the mechanism can precisely match the user with other most related users and get the user's knowledge structure.},
booktitle = {Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering},
pages = {159–163},
numpages = {5},
keywords = {folksonomy, Information need, ontology},
location = {Osaka, Japan},
series = {MSIE 2020}
}

@inproceedings{10.1145/3184066.3184084,
author = {Hong, Tham Vo Thi and Do, Phuc},
title = {Developing a Graph-Based System for Storing, Exploiting and Visualizing Text Stream},
year = {2018},
isbn = {9781450363365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184066.3184084},
doi = {10.1145/3184066.3184084},
abstract = {In an era of information explosion, collecting and exploiting information automatically is very essential so that many studies have proposed models for solving storage problems and supporting efficient data processing. In this paper, we propose a system based on graph that can store, exploit and visualize text streams. This model first gathers daily articles automatically from Vietnamese online journals. After articles are collected, keywords' frequency of existence is calculated to rank the importance of keywords, finding worthy topics and tracking the changes in pertinent topics. In addition, our system supports users to search and create statistical maps to visually display their data. We also perform the system testing and evaluation to show its performance, estimate its responding time and find out how to improve it in the future.},
booktitle = {Proceedings of the 2nd International Conference on Machine Learning and Soft Computing},
pages = {82–86},
numpages = {5},
keywords = {data stream, graph database, topic models, documents, crawler, text stream, text processing, visualization},
location = {Phu Quoc Island, Viet Nam},
series = {ICMLSC '18}
}

@inproceedings{10.1145/3091478.3091499,
author = {Ashktorab, Zahra and Haber, Eben and Golbeck, Jennifer and Vitak, Jessica},
title = {Beyond Cyberbullying: Self-Disclosure, Harm and Social Support on ASKfm},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091499},
doi = {10.1145/3091478.3091499},
abstract = {ASKfm is a social media platform popular among teens and young adults where users can interact anonymously or semi-anonymously. In this paper, we identify the modes of disclosure and interaction that occur on the site, and evaluate why users are motivated to post and interact on the site, despite its reputation for facilitating cyberbullying. Through topic modeling - supplemented with manual annotation - of a large dataset of ASKfm posts, we identify and classify the rich variety of discourse posted on ASKfm, including both positive and negative forms, providing insights into the why individuals continue to engage with the site. These findings are complemented by a survey of young adults (aged 18-20) ASKfm users, which provides additional insights into users' motivations and interaction patterns. We discuss how the affordances specific to platforms like ASKfm, including anonymity and visibility, might enable users to respond to cyberbullying in novel ways, engage in positive forms of self-disclosure, and gain social support on sensitive topics. We conclude with design recommendations that would highlight the positive interactions on the website and help diminish the repurcussions of the negative interactions.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {3–12},
numpages = {10},
keywords = {self-disclosure, topic modeling, askfm, cyberbullying},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@inproceedings{10.1145/3386415.3386976,
author = {Li, Danyang and Li, Xinlai},
title = {Analyzing The Development of Technology from The Perspective of Patent: Taking Ships Diesel Engine as An Example},
year = {2019},
isbn = {9781450372930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386415.3386976},
doi = {10.1145/3386415.3386976},
abstract = {It is an important strategic issue for enterprises and countries to analyze and study the patent of ships combination of diesel and diesel (CODAD), understand the process of technological development and identify potential research and development hotspots. This paper discusses the development status of the patent of ships combination of diesel and diesel (CODAD) from the aspects of language distribution, patentee and organization distribution, patent classification number distribution, etc. On the basis of using the LDA model to explore the core technology, the technical development level in this field is understood. The research on relevant patents shows that although the technical level of ships diesel engine in China has been greatly improved, there is still a certain gap compared with the leading countries.},
booktitle = {Proceedings of the 2nd International Conference on Information Technologies and Electrical Engineering},
articleno = {29},
numpages = {6},
keywords = {Diesel Engine, LDA model, Ships, Patent},
location = {Zhuzhou, Hunan, China},
series = {ICITEE-2019}
}

@inproceedings{10.1145/3350546.3352506,
author = {Fromm, Michael and Faerman, Evgeniy and Seidl, Thomas},
title = {TACAM: Topic And Context Aware Argument Mining},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352506},
doi = {10.1145/3350546.3352506},
abstract = {In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {99–106},
numpages = {8},
keywords = {argument search, transfer learning, argument mining, natural language processing},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1145/3309129.3309146,
author = {Khan, M. Taimoor and Khalid, Shehzad and Aziz, Furqan},
title = {Graph Clustering Based Size Varying Rules for Lifelong Topic Modeling},
year = {2018},
isbn = {9781450366113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309129.3309146},
doi = {10.1145/3309129.3309146},
abstract = {Lifelong learning topic models identify the hidden concepts discussed in the collection of documents. The concepts are represented as topics having groups of ordered words based on their relevance to the topic. Lifelong learning models have an automatic learning mechanism which allows continuous learning without external support. In the process, the model gets more knowledgeable with experience as it learns from the past in the form of rules. It is carries rules to the future and utilize them when a similar scenario arises. The existing lifelong learning topic models heavily rely on statistical measures to learn rules that leads to two limitations. The rules are evaluated for fixed number of words while ignoring the natural arrangement of words within the documents. Moreover, the rules have arbitrary orientation that causes repeated patterns of transferring the impact of a rule into a topic during the early iterations of the inference technique. In this research work, we introduce complex networks analysis for learning rules which addresses both of the limitations discussed. The rules are obtained through hierarchical clustering of the complex network that have different number of words within a rule and have directed orientation. The proposed approach improves the utilization of rules for improved quality of topics at higher performance with unidirectional rules on the standard lifelong learning dataset.},
booktitle = {Proceedings of the 2018 5th International Conference on Bioinformatics Research and Applications},
pages = {73–77},
numpages = {5},
keywords = {community extraction, complex network analysis, Topic modeling, lifelong machine learning},
location = {Hong Kong, Hong Kong},
series = {ICBRA '18}
}

@article{10.1145/3332932,
author = {Yang, Wenmain and Wang, Kun and Ruan, Na and Gao, Wenyuan and Jia, Weijia and Zhao, Wei and Liu, Nan and Zhang, Yunyong},
title = {Time-Sync Video Tag Extraction Using Semantic Association Graph},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332932},
doi = {10.1145/3332932},
abstract = {Time-sync comments (TSCs) reveal a new way of extracting the online video tags. However, such TSCs have lots of noises due to users’ diverse comments, introducing great challenges for accurate and fast video tag extractions. In this article, we propose an unsupervised video tag extraction algorithm named Semantic Weight-Inverse Document Frequency (SW-IDF). Specifically, we first generate corresponding semantic association graph (SAG) using semantic similarities and timestamps of the TSCs. Second, we propose two graph cluster algorithms, i.e., dialogue-based algorithm and topic center-based algorithm, to deal with the videos with different density of comments. Third, we design a graph iteration algorithm to assign the weight to each comment based on the degrees of the clustered subgraphs, which can differentiate the meaningful comments from the noises. Finally, we gain the weight of each word by combining Semantic Weight (SW) and Inverse Document Frequency (IDF). In this way, the video tags are extracted automatically in an unsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based algorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in high-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments; while SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122 MAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density comments. It has a better performance than the state-of-the-art unsupervised algorithms in both F1-score and MAP.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {37},
numpages = {24},
keywords = {video tagging, crowdsourced time-sync comments, semantic association graph, Multimedia retrieval, keywords extraction}
}

@inproceedings{10.1145/3491102.3517742,
author = {Saxena, Devansh and Moon, Seh Young and Shehata, Dahlia and Guha, Shion},
title = {Unpacking Invisible Work Practices, Constraints, and Latent Power Relationships in Child Welfare through Casenote Analysis},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517742},
doi = {10.1145/3491102.3517742},
abstract = {Caseworkers are trained to write detailed narratives about families in Child-Welfare (CW) which informs collaborative high-stakes decision-making. Unlike other administrative data, these narratives offer a more credible source of information with respect to workers’ interactions with families as well as underscore the role of systemic factors in decision-making. SIGCHI researchers have emphasized the need to understand human discretion at the street-level to be able to design human-centered algorithms for the public sector. In this study, we conducted computational text analysis of casenotes at a child-welfare agency in the midwestern United States and highlight patterns of invisible street-level discretionary work and latent power structures that have direct implications for algorithm design. Casenotes offer a unique lens for policymakers and CW leadership towards understanding the experiences of on-the-ground caseworkers. As a result of this study, we highlight how street-level discretionary work needs to be supported by sociotechnical systems developed through worker-centered design. This study offers the first computational inspection of casenotes and introduces them to the SIGCHI community as a critical data source for studying complex sociotechnical systems.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {120},
numpages = {22},
keywords = {topic modeling, human discretion, casenotes, computational text analysis, invisible labor, child-welfare system, worker-centered design},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3289600.3290957,
author = {Lin, Tianyi and Hu, Zhiyue and Guo, Xin},
title = {Sparsemax and Relaxed Wasserstein for Topic Sparsity},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290957},
doi = {10.1145/3289600.3290957},
abstract = {Topic sparsity refers to the observation that individual documents usually focus on several salient topics instead of covering a wide variety of topics, and a real topic adopts a narrow range of terms instead of a wide coverage of the vocabulary. Understanding this topic sparsity is especially important for analyzing user-generated web content and social media, which are featured in the form of extremely short posts and discussions. As topic sparsity of individual documents in online social media increases, so does the difficulty of analyzing the online text sources using traditional methods. In this paper, we propose two novel neural models by providing sparse posterior distributions over topics based on the Gaussian sparsemax construction, enabling efficient training by stochastic backpropagation. We construct an inference network conditioned on the input data and infer the variational distribution with the relaxed Wasserstein (RW) divergence. Unlike existing works based on Gaussian softmax construction and Kullback-Leibler (KL) divergence, our approaches can identify latent topic sparsity with training stability, predictive performance, and topic coherence. Experiments on different genres of large text corpora have demonstrated the effectiveness of our models as they outperform both probabilistic and neural methods.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {141–149},
numpages = {9},
keywords = {neural topic modeling, relaxed wasserstein divergence, topic sparsity, sparsemax, stochastic gradient backpropagation},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3292522.3326042,
author = {Shao, Jialin and Uchendu, Adaku and Lee, Dongwon},
title = {A Reverse Turing Test for Detecting Machine-Made Texts},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326042},
doi = {10.1145/3292522.3326042},
abstract = {As AI technologies rapidly advance, the artifacts created by machines will become prevalent. As recent incidents by the Deepfake illustrate, then, being able to differentiate man-made vs. machine-made artifacts, especially in social media space, becomes more important. In this preliminary work, in this regard, we formulate such a classification task as the Reverse Turing Test (RTT) and investigate on the contemporary status to be able to classify man-made vs. machine-made texts. Studying real-life machine-made texts in three domains of financial earning reports, research articles, and chatbot dialogues, we found that the classification of man-made vs. machine-made texts can be done at least as accurate as 0.84 in F1 score. We also found some differences between man-made and machine-made in sentiment, readability, and textual features, which can help differentiate them.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {275–279},
numpages = {5},
keywords = {machine-made text, reverse turing test, supervised learning},
location = {Boston, Massachusetts, USA},
series = {WebSci '19}
}

@inproceedings{10.1145/3477314.3507226,
author = {Zahrah, Fatima and Nurse, Jason R. C. and Goldsmith, Michael},
title = {A Comparison of Online Hate on Reddit and 4chan: A Case Study of the 2020 US Election},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507226},
doi = {10.1145/3477314.3507226},
abstract = {The rapid integration of the Internet into our daily lives has led to many benefits but also to a number of new, wide-spread threats such as online hate, trolling, bullying, and generally aggressive behaviours. While research has traditionally explored online hate, in particular, on one platform, the reality is that such hate is a phenomenon that often makes use of multiple online networks. In this article, we seek to advance the discussion into online hate by harnessing a comparative approach, where we make use of various Natural Language Processing (NLP) techniques to computationally analyse hateful content from Reddit and 4chan relating to the 2020 US Presidential Elections. Our findings show how content and posting activity can differ depending on the platform being used. Through this, we provide initial comparison into the platform-specific behaviours of online hate, and how different platforms can serve specific purposes. We further provide several avenues for future research utilising a cross-platform approach so as to gain a more comprehensive understanding of the global hate ecosystem.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1797–1800},
numpages = {4},
keywords = {natural language processing, US elections, cross-platform analysis, social network analysis, online behavior, online hate},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3474124.3474169,
author = {Agarwal, Neha and Sikka, Geeta and Awasthi, Lalit Kumar},
title = {Comparative Study of Topic Modeling and Word Embedding Approaches for Web Service Clustering},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474169},
doi = {10.1145/3474124.3474169},
abstract = {Vector space representation of web services plays a prominent role in enhancing the performance of different web service-based processes like clustering, recommendation, ranking, discovery, etc. Generally, Term Frequency - Inverse Document Frequency (TF-IDF) and topic modeling methods are widely used for service representation. In recent years, word embedding techniques have attracted researchers a lot because they can map services or documents based on semantic similarity. This paper provides a comparative analysis of two topic modeling techniques, i.e., Latent Dirichlet Allocation (LDA) and Gibbs Sampling algorithm for Dirichlet Multinomial Mixture (GSDMM) &amp; two word embedding techniques, i.e., word2vec and fastText. These topic modeling and word embedding techniques are applied to a dataset of web service documents for vector space representation. K-Means clustering is used to analyze the performance, and results are evaluated based on standard evaluation criteria. Results demonstrate that word2vec model outperforms other techniques and provides a satisfactory improvement on clustering.},
booktitle = {2021 Thirteenth International Conference on Contemporary Computing (IC3-2021)},
pages = {309–313},
numpages = {5},
keywords = {Web Services, K-Means Clustering, Topic Models, Word Embedding},
location = {Noida, India},
series = {IC3 '21}
}

@article{10.1145/3485187,
author = {Huang, Shi Ming and Yen, David C. and Yan, Ting Jyun and Yang, Yi Ting},
title = {An Intelligent Mechanism to Automatically Discover Emerging Technology Trends: Exploring Regulatory Technology},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3485187},
doi = {10.1145/3485187},
abstract = {Technology trend analysis uses data relevant to historical performance and extrapolates it to estimate and assess the future potential of technology. Such analysis is used to analyze emerging technologies or predict the growing markets that influence the resulting social or economic development to assist in effective decision-making. Traditional trend analysis methods are time-consuming and require considerable labor. Moreover, the implemented processes may largely rely on the specific knowledge of the domain experts. With the advancement in the areas of science and technology, emerging cross-domain trends have received growing attention for its considerable influence on society and the economy. Consequently, emerging cross-domain predictions that combine or complement various technologies or integrate with diverse disciplines may be more critical than other tools and applications in the same domain. This study uses a design science research methodology, a text mining technique, and social network analysis (SNA) to analyze the development trends concerning the presentation of the product or service information on a company's website. This study applies regulatory technology (RegTech) as a case to analyze and justify the emerging cross-disciplinary trend. Furthermore, an experimental study is conducted using the Google search engine to verify and validate the proposed research mechanism at the end of this study. The study results reveal that, compared with Google Trends and Google Correlate, the research mechanism proposed in this study is more illustrative, feasible, and promising because it reduces noise and avoids the additional time and effort required to perform a further in-depth exploration to obtain the information.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {dec},
articleno = {17},
numpages = {29},
keywords = {technology trend analysis, social network analysis, Text mining, regtech, fintech, computer auditing}
}

@inproceedings{10.1145/3297001.3297026,
author = {GM, Sushravya and Sengupta, Shubhashis},
title = {Unsupervised Multi-Task Learning Dialogue Management},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297026},
doi = {10.1145/3297001.3297026},
abstract = {Traditional dialog management systems used in task / goal-oriented scenario require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog management systems, in which all components are trained from the dialogs transcripts, overcome this limitation. This paper proposes a novel unsupervised dialogue-manager for goal-oriented dialogue applications. Set in the context of Corporate Information Organization (CIO) domain, our tasks require assessing user-initiated dialogues to understand the nature of the tickets, issue DB and API calls, and use the output of such calls to assist users. We show that an unsupervised dialog state-tracking and management system based on jointly trained LSTM-RNNs for utterance understanding and response generation can learn to perform non-trivial dialog management.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {196–202},
numpages = {7},
keywords = {topic modeling, Multi-task learning, Unsupervised learning, State tracking and Dialogue management},
location = {Kolkata, India},
series = {CoDS-COMAD '19}
}

@inproceedings{10.1145/3148011.3148019,
author = {Badenes-Olmedo, Carlos and Redondo-Garc\'{\i}a, Jos\'{e} Luis and Corcho, Oscar},
title = {Efficient Clustering from Distributions over Topics},
year = {2017},
isbn = {9781450355537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148011.3148019},
doi = {10.1145/3148011.3148019},
abstract = {There are many scenarios where we may want to find pairs of textually similar documents in a large corpus (e.g. a researcher doing literature review, or an R&amp;D project manager analyzing project proposals). To programmatically discover those connections can help experts to achieve those goals, but brute-force pairwise comparisons are not computationally adequate when the size of the document corpus is too large. Some algorithms in the literature divide the search space into regions containing potentially similar documents, which are later processed separately from the rest in order to reduce the number of pairs compared. However, this kind of unsupervised methods still incur in high temporal costs. In this paper, we present an approach that relies on the results of a topic modeling algorithm over the documents in a collection, as a means to identify smaller subsets of documents where the similarity function can then be computed. This approach has proved to obtain promising results when identifying similar documents in the domain of scientific publications. We have compared our approach against state of the art clustering techniques and with different configurations for the topic modeling algorithm. Results suggest that our approach outperforms (&gt; 0.5) the other analyzed techniques in terms of efficiency.},
booktitle = {Proceedings of the Knowledge Capture Conference},
articleno = {17},
numpages = {8},
keywords = {large-scale text analysis, semantic similarity, scholarly data, topic models},
location = {Austin, TX, USA},
series = {K-CAP 2017}
}

@inproceedings{10.1145/3240323.3240353,
author = {Deng, Dong and Jing, Liping and Yu, Jian and Sun, Shaolong and Zhou, Haofei},
title = {Neural Gaussian Mixture Model for Review-Based Rating Prediction},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240353},
doi = {10.1145/3240323.3240353},
abstract = {Review has been proven to be an important information in recommendation. Different from the overall user-item rating matrix, it can provide textual information that exhibits why a user likes an item or not. Recently, more and more researchers have paid attention on review-based rating prediction. There are two challenging issues: how to extract representative features to characterize users / items from reviews and how to leverage them for recommendation system. In this paper, we propose a Neural Gaussian Mixture Model (NGMM) for review-based rating prediction task. Among it, the review textual information is used to construct two parallel neural networks for users and items respectively, so that the users' preferences and items' properties can be sufficiently extracted and represented as two latent vectors. A shared layer is introduced on the top to couple these two networks together and model user-item rating based on the features learned from reviews. Specifically, each rating is modeled via a Gaussian mixture model, where each Gaussian component has zero variance, the mean described by the corresponding component in user's latent vector and the weight indicated by the corresponding component in item's latent vector. Extensive experiments are conducted on five real-world Amazon review datasets. The experimental results have demonstrated that our proposed NGMM model achieves the state-of-the-art performance in review-based rating prediction task.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {113–121},
numpages = {9},
keywords = {deep learning, review-based rating prediction, recommendation, gaussian mixture model},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.1145/3460231.3473324,
author = {Truong, Quoc-Tuan and Salah, Aghiles and Lauw, Hady},
title = {Multi-Modal Recommender Systems: Hands-On Exploration},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3473324},
doi = {10.1145/3460231.3473324},
abstract = {Recommender systems typically learn from user-item preference data such as ratings and clicks. This information is sparse in nature, i.e., observed user-item preferences often represent less than 5% of possible interactions. One promising direction to alleviate data sparsity is to leverage auxiliary information that may encode additional clues on how users consume items. Examples of such data (referred to as modalities) are social networks, item’s descriptive text, product images. The objective of this tutorial is to offer a comprehensive review of recent advances to represent, transform and incorporate the different modalities into recommendation models. Moreover, through practical hands-on sessions, we consider cross model/modality comparisons to investigate the importance of different methods and modalities. The hands-on exercises are conducted with Cornac (https://cornac.preferred.ai ), a comparative framework for multimodal recommender systems. The materials are made available on https://preferred.ai/recsys21-tutorial/.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {834–837},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3341161.3345024,
author = {Hauffa, Jan and Br\"{a}u, Wolfgang and Groh, Georg},
title = {Detection of Topical Influence in Social Networks via Granger-Causal Inference: A Twitter Case Study},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3345024},
doi = {10.1145/3341161.3345024},
abstract = {With the ever-increasing importance of computer-mediated communication in our everyday life, understanding the effects of social influence in online social networks has become a necessity. In this work, we argue that cascade models of information diffusion do not adequately capture attitude change, which we consider to be an essential element of social influence. To address this concern, we propose a topical model of social influence and attempt to establish a connection between influence and Granger-causal effects on a theoretical and empirical level. While our analysis of a social media dataset finds effects that are consistent with our model of social influence, evidence suggests that these effects can be attributed largely to external confounders. The dominance of external influencers, including mass media, over peer influence raises new questions about the correspondence between objectively measurable information diffusion and social influence as perceived by human observers.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {969–977},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3438872.3439096,
author = {Zhou, Qiaoyu and Du, Yajun and Liu, Taiao},
title = {Discovery of Topic Derivative Relationship in Social Networks},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439096},
doi = {10.1145/3438872.3439096},
abstract = {Detecting social topics and discovering emergencies are necessary for the detection and control of public opinion. One social topic may derive one and more new topics as information spreads in social networks. This paper proposes the concept of derivative topics to describe the trend of topic change in the process of information dissemination, which benefits to discover public opinion and its evolutionary direction. We aggregate the posts into pseudo-documents and construct subgraphs of pseudo-documents with words as nodes. By extracting the topic words to determine whether there is a derivative relationship between documents, and form a visual derivative relationship graph. First, we group the original dataset into time slices and use paragraph2Vec to train each Microblog post as paragraph vectors. Second, we calculate the similarity between the posts in the same group through their paragraph vectors. The posts with high similarity are aggregated into a pseudo-document. Finally, we extract topic words in each pseudo-document and describe the derivation relationship between the topics by constructing the derivative relationship graph. The experimental results show that the concept of derivative topics we proposed has validity. The structure of the graph shows the derivative relationship between derivative topics and makes the derivative relationship visualization.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {290–295},
numpages = {6},
keywords = {short texts, derivative topic, social networks},
location = {Shanghai, China},
series = {RICAI 2020}
}

@inproceedings{10.1145/3201064.3201076,
author = {Quraishi, Mainul and Fafalios, Pavlos and Herder, Eelco},
title = {Viewpoint Discovery and Understanding in Social Networks},
year = {2018},
isbn = {9781450355636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3201064.3201076},
doi = {10.1145/3201064.3201076},
abstract = {The Web has evolved to a dominant platform where everyone has the opportunity to express their opinions, to interact with other users, and to debate on emerging events happening around the world. On the one hand, this has enabled the presence of different viewpoints and opinions about a - usually controversial - topic (like Brexit), but at the same time, it has led to phenomena like media bias, echo chambers and filter bubbles, where users are exposed to only one point of view on the same topic. Therefore, there is the need for methods that are able to detect and explain the different viewpoints. In this paper, we propose a graph partitioning method that exploits social interactions to enable the discovery of different communities (representing different viewpoints) discussing about a controversial topic in a social network like Twitter. To explain the discovered viewpoints, we describe a method, called Iterative Rank Difference (IRD), which allows detecting descriptive terms that characterize the different viewpoints as well as understanding how a specific term is related to a viewpoint (by detecting other related descriptive terms). The results of an experimental evaluation showed that our approach outperforms state-of-the-art methods on viewpoint discovery, while a qualitative analysis of the proposed IRD method on three different controversial topics showed that IRD provides comprehensive and deep representations of the different viewpoints.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {47–56},
numpages = {10},
keywords = {viewpoint discovery, social networks, viewpoint understanding},
location = {Amsterdam, Netherlands},
series = {WebSci '18}
}

@inproceedings{10.1109/ICSE-C.2017.139,
author = {Yan, Meng and Zhang, Xiaohong and Liu, Chao and Zou, Jie and Xu, Ling and Xia, Xin},
title = {Learning to Aggregate: An Automated Aggregation Method for Software Quality Model},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.139},
doi = {10.1109/ICSE-C.2017.139},
abstract = {Quality models are regarded as a well-accepted approach for assessing high-level abstract quality characteristics (e.g., maintainability) by aggregation from low-level metrics. However, most of the existing quality models adopt the weighted linear aggregation method which suffers from a lack of consensus in how to decide the correct weights. To address this issue, we present an automated aggregation method which adopts a kind of probabilistic weight instead of the subjective weight in previous aggregation methods. In particular, we utilize a topic modeling technique to estimate the probabilistic weight by learning from a software benchmark. In this manner, our approach can enable automated quality assessment by using the learned knowledge without manual effort. In addition, we conduct an application on the maintainability assessment of the systems in our benchmark. The result shows that our approach can reveal the maintainability well through a correlation analysis with the changed lines of code.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {268–270},
numpages = {3},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@article{10.1145/3053381,
author = {Liu, Xumin and Shi, Weishi and Kale, Arpeet and Ding, Chen and Yu, Qi},
title = {Statistical Learning of Domain-Specific Quality-of-Service Features from User Reviews},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3053381},
doi = {10.1145/3053381},
abstract = {With the fast increase of online services of all kinds, users start to care more about the Quality of Service (QoS) that a service provider can offer besides the functionalities of the services. As a result, QoS-based service selection and recommendation have received significant attention since the mid-2000s. However, existing approaches primarily consider a small number of standard QoS parameters, most of which relate to the response time, fee, availability of services, and so on. As online services start to diversify significantly over different domains, these small set of QoS parameters will not be able to capture the different quality aspects that users truly care about over different domains. Most existing approaches for QoS data collection depend on the information from service providers, which are sensitive to the trustworthiness of the providers. Some service monitoring mechanisms collect QoS data through actual service invocations but may be affected by actual hardware/software configurations. In either case, domain-specific QoS data that capture what users truly care about have not been successfully collected or analyzed by existing works in service computing. To address this demanding issue, we develop a statistical learning approach to extract domain-specific QoS features from user-provided service reviews. In particular, we aim to classify user reviews based on their sentiment orientations into either a positive or negative category. Meanwhile, statistical feature selection is performed to identify statistically nontrivial terms from review text, which can serve as candidate QoS features. We also develop a topic models-based approach that automatically groups relevant terms and returns the term groups to users, where each term group corresponds to one high-level quality aspect of services. We have conducted extensive experiments on three real-world datasets to demonstrates the effectiveness of our approach.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {22},
numpages = {24},
keywords = {statistical learning, Quality of service}
}

@inproceedings{10.1145/3289600.3291032,
author = {Viegas, Felipe and Canuto, S\'{e}rgio and Gomes, Christian and Luiz, Washington and Rosa, Thierson and Ribas, Sabir and Rocha, Leonardo and Gon\c{c}alves, Marcos Andr\'{e}},
title = {CluWords: Exploiting Semantic Word Clustering Representation for Enhanced Topic Modeling},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291032},
doi = {10.1145/3289600.3291032},
abstract = {In this paper, we advance the state-of-the-art in topic modeling by means of a new document representation based on pre-trained word embeddings for non-probabilistic matrix factorization. Specifically, our strategy, called CluWords, exploits the nearest words of a given pre-trained word embedding to generate meta-words capable of enhancing the document representation, in terms of both, syntactic and semantic information. The novel contributions of our solution include: (i)the introduction of a novel data representation for topic modeling based on syntactic and semantic relationships derived from distances calculated within a pre-trained word embedding space and (ii)the proposal of a new TF-IDF-based strategy, particularly developed to weight the CluWords. In our extensive experimentation evaluation, covering 12 datasets and 8 state-of-the-art baselines, we exceed (with a few ties) in almost cases, with gains of more than 50% against the best baselines (achieving up to 80% against some runner-ups). Finally, we show that our method is able to improve document representation for the task of automatic text classification.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {753–761},
numpages = {9},
keywords = {topic modeling, data representation, word embedding},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3306618.3314292,
author = {Baumer, Eric P. S. and McGee, Micki},
title = {Speaking on Behalf of: Representation, Delegation, and Authority in Computational Text Analysis},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314292},
doi = {10.1145/3306618.3314292},
abstract = {Computational tools can often facilitate human work by rapidly summarizing large amounts of data, especially text. Doing so delegates to such models some measure of authority to speak on behalf of those people whose data are being analyzed. This paper considers the consequences of such delegation. It draws on sociological accounts of representation and translation to examine one particular case: the application of topic modeling to blogs written by parents of children on the autism spectrum. In doing so, the paper illustrates the kinds of statements that topic models, and other computational techniques, can make on behalf of people. It also articulates some of the potential consequences of such statements. The paper concludes by offering several suggestions about how to address potential harms that can occur when computational models speak on behalf of someone.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {163–169},
numpages = {7},
keywords = {ethics, sociology, topic modeling, delegation},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3051457.3054005,
author = {Dowell, Nia M.M. and Brooks, Christopher and Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and Ga\v{s}evi\'{c}, Dragan},
title = {The Changing Patterns of MOOC Discourse},
year = {2017},
isbn = {9781450344500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3051457.3054005},
doi = {10.1145/3051457.3054005},
abstract = {There is an emerging trend in higher education for the adoption of massive open online courses (MOOCs). However, despite this interest in learning at scale, there has been limited work investigating how MOOC participants have changed over time. In this study, we explore the temporal changes in MOOC learners' language and discourse characteristics. In particular, we demonstrate that there is a clear trend within a course for language in discussion forums to be of both more on-topic and reflective of deep learning in subsequent offerings of a course. We measure this in two ways, and demonstrate this trend through several repeated analyses of different courses in different domains. While not all courses show an increase beyond statistical significance, the majority do, providing evidence that MOOC learner populations are changing as the educational phenomena matures.},
booktitle = {Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale},
pages = {283–286},
numpages = {4},
keywords = {moocs, discourse complexity, discussion forums, on-topic discussion, learning at scale},
location = {Cambridge, Massachusetts, USA},
series = {L@S '17}
}

@inproceedings{10.1145/3394486.3403150,
author = {Long, Qingqing and Jin, Yilun and Song, Guojie and Li, Yi and Lin, Wei},
title = {Graph Structural-Topic Neural Network},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403150},
doi = {10.1145/3394486.3403150},
abstract = {Graph Convolutional Networks (GCNs) achieved tremendous success by effectively gathering local features for nodes. However, commonly do GCNs focus more on node features but less on graph structures within the neighborhood, especially higher-order structural patterns. However, such local structural patterns are shown to be indicative of node properties in numerous fields. In addition, it is not just single patterns, but the distribution over all these patterns matter, because networks are complex and the neighborhood of each node consists of a mixture of various nodes and structural patterns. Correspondingly, in this paper, we propose Graph Structural topic Neural Network, abbreviated GraphSTONE 1, a GCN model that utilizes topic models of graphs, such that the structural topics capture indicative graph structures broadly from a probabilistic aspect rather than merely a few structures. Specifically, we build topic models upon graphs using anonymous walks and Graph Anchor LDA, an LDA variant that selects significant structural patterns first, so as to alleviate the complexity and generate structural topics efficiently. In addition, we design multi-view GCNs to unify node features and structural topic features and utilize structural topics to guide the aggregation. We evaluate our model through both quantitative and qualitative experiments, where our model exhibits promising performance, high efficiency, and clear interpretability.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1065–1073},
numpages = {9},
keywords = {topic modeling, structural GCN, local structural patterns, graph convolutional network},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1109/TCBB.2020.3016355,
author = {Kim, Seonho and Yoon, Juntae},
title = {Dual Triggered Correspondence Topic (DTCT)Model for MeSH Annotation},
year = {2022},
issue_date = {March-April 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3016355},
doi = {10.1109/TCBB.2020.3016355},
abstract = {Accurate Medical Subject Headings (MeSH)annotation is an important issue for researchers in terms of effective information retrieval and knowledge discovery in the biomedical literature. We have developed a powerful dual triggered correspondence topic (DTCT)model for MeSH annotated articles. In our model, two types of data are assumed to be generated by the same latent topic factors and words in abstracts and titles serve as descriptions of the other type, MeSH terms. Our model allows the generation of MeSHs in abstracts to be triggered either by general document topics or by document-specific “special” word distributions in a probabilistic manner, allowing for a trade-off between the benefits of topic-based abstraction and specific word matching. In order to relax the topic influences of non-topical words or domain-frequent words in text description, we integrated the discriminative feature of Okapi BM25 into word sampling probability. This allows the model to choose keywords, which stand out from others, in order to generate MeSH terms. We further incorporate prior knowledge about relations between word and MeSH in DTCT with <italic>phi</italic>-coefficient to improve topic coherence. We demonstrated the model's usefulness in automatic MeSH annotation. Our model obtained 0.62 F-score 150,00 MEDLINE test set and showed a strength in recall rate. Specially, it yielded competitive performances in an integrated probabilistic environment without additional post-processing for filtering MeSHs.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {899–911},
numpages = {13}
}

@inproceedings{10.1145/3035918.3035937,
author = {Gao, Zekai J. and Luo, Shangyu and Perez, Luis L. and Jermaine, Chris},
title = {The BUDS Language for Distributed Bayesian Machine Learning},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3035937},
doi = {10.1145/3035918.3035937},
abstract = {We describe BUDS, a declarative language for succinctly and simply specifying the implementation of large-scale machine learning algorithms on a distributed computing platform. The types supported in BUDS--vectors, arrays, etc.--are simply logical abstractions useful for programming, and do not correspond to the actual implementation. In fact, BUDS automatically chooses the physical realization of these abstractions in a distributed system, by taking into account the characteristics of the data. Likewise, there are many available implementations of the abstract operations offered by BUDS (matrix multiplies, transposes, Hadamard products, etc.). These are tightly coupled with the physical representation. In BUDS, these implementations are co-optimized along with the representation. All of this allows for the BUDS compiler to automatically perform deep optimizations of the user's program, and automatically generate efficient implementations.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {961–976},
numpages = {16},
keywords = {distributed system, machine learning, declarative language},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3132847.3132988,
author = {Mele, Ida and Bahrainian, Seyed Ali and Crestani, Fabio},
title = {Linking News across Multiple Streams for Timeliness Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132988},
doi = {10.1145/3132847.3132988},
abstract = {Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams.Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {767–776},
numpages = {10},
keywords = {dynamic topic modeling, news streams, event mining, temporal analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3476039,
author = {Gong, Crystal and Saha, Koustuv and Chancellor, Stevie},
title = {"The Smartest Decision for My Future": Social Media Reveals Challenges and Stress During Post-College Life Transition},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476039},
doi = {10.1145/3476039},
abstract = {The post-college transition is a critical period where individuals experience unique challenges and stress before, during, and after graduation. Individuals often use social media to discuss and share information, advice, and support related to post-college challenges in online communities. These communities are important as they fill gaps in institutional support between college and post-college plans. We empirically study the challenges and stress expressed on social media around this transition as students graduate college and move into emerging adulthood. We assembled a dataset of about 299,000 Reddit posts between 2008 and 2020 about the post-college transition from 10 subreddits. We extracted top concerns, challenges, and conversation points using unsupervised Latent Dirichlet Allocation (LDA). Then, we combined the results of LDA with binary transfer learning to identify stress expressions in the dataset (classifier performance at F1=0.94). Finally, we explore temporal patterns in stress expressions, and the variance of per-topic stress levels throughout the year. Our work highlights more deliberate and focused understanding of the post-college transition, as well as useful research and design impacts to study transient cohorts in need of support.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {298},
numpages = {29},
keywords = {language, disclosure, life transitions, machine learning, social media, stress, reddit, social support}
}

@inproceedings{10.1145/3459930.3469550,
author = {Whitfield, Christopher and Liu, Yang and Anwar, Mohd},
title = {Surveillance of COVID-19 Pandemic Using Social Media: A Reddit Study in North Carolina},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469550},
doi = {10.1145/3459930.3469550},
abstract = {Coronavirus disease (COVID-19) pandemic has changed various aspects of people's lives and behaviors. At this stage, there are no other ways to control the natural progression of the disease than adopting mitigation strategies such as wearing masks, watching distance, and washing hands. Moreover, at this time of social distancing, social media plays a key role in connecting people and providing a platform for expressing their feelings. In this study, we tap into social media to surveil the uptake of mitigation and detection strategies, and capture issues and concerns about the pandemic. In particular, we explore the research question, "how much can be learned regarding the public uptake of mitigation strategies and concerns about COVID-19 pandemic by using natural language processing on Reddit posts?" After extracting COVID-related posts from the four largest subreddit communities of North Carolina over six months, we performed NLP-based preprocessing to clean the noisy data. We employed a custom Named-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA) method for topic modeling on a Reddit corpus. We observed that mask, flu, and testing are the most prevalent named-entities for "Personal Protective Equipment", "symptoms", and "testing" categories, respectively. We also observed that the most discussed topics are related to testing, masks, and employment. The mitigation measures are the most prevalent theme of discussion across all subreddits.},
booktitle = {Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {43},
numpages = {8},
keywords = {reddit, social media, COVID-19 surveillance, LDA, named-entity recognition, natural language processing, topic modeling},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.1145/3414274.3414276,
author = {Qiu, Qizhi and Qiu, Junan and Chen, Hui},
title = {A Topic Extraction Method for Network Public Opinion},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414276},
doi = {10.1145/3414274.3414276},
abstract = {Network public opinion has played a vital role during the government's decision-making process. There have been many existing topic extraction methods on processing network public opinion, while most of them have paid attention to its short text characteristics and have underutilized its evolutionary characteristics over time. This paper intends to hybrid the textual and evolutionary characteristics during the topic extraction and proposes a phase-based topic extraction (P-TE) method. Firstly, a novel idea about combining qualitative and quantitative methods is developed, and the evolutionary characteristics are used to divide the process into several phases by time series analysis. The number of phases depends on the certain event and differs from each other. Then, based on the textual characteristics of network public opinion, feature extraction is described. Finally, topics are extracted for every single phase separately. The experimental results show that P-TE can reveal more details and careful thoughts about the event than other methods. Furthermore, the rationality of P-TE is verified.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {6–11},
numpages = {6},
keywords = {Network public opinion, Topic extraction, K-means, Phase},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3507524.3507536,
author = {Jia, Shichao and Chen, Qi and Wang, Wei},
title = {Covid-19 Tweets Analysis with Topic Modeling},
year = {2021},
isbn = {9781450387194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507524.3507536},
doi = {10.1145/3507524.3507536},
abstract = {Social media has become an important data resource for knowledge discovery and data mining in multiple disciplines. With the exploding amount of social media data, how to efficiently and effectively exploit values and insights from such overwhelming amount of data has become an emerging area. Recently, various natural language processing techniques have been developed, e.g., word embedding, deep neural network and Latent Dirichlet Allocation (LDA), for studies such as sentiment analysis, traffic event detection, nature disaster assessment and COVID-19 tweet analysis. In this paper, topic modeling through LDA was used to conduct text mining on a large real-world COVID-19 tweet dataset, which contains more than 524 million multilingual tweets and covers 218 countries over a period of 3 months. We conducted extensive experiments and visualise insights discovered through this unsupervised process.},
booktitle = {2021 4th International Conference on Computing and Big Data},
pages = {68–74},
numpages = {7},
keywords = {Covid-19, LDA, Social media analysis},
location = {Wuhan, China},
series = {ICCBD 2021}
}

@inproceedings{10.1145/3308558.3313669,
author = {Umar, Prasanna and Squicciarini, Anna and Rajtmajer, Sarah},
title = {Detection and Analysis of Self-Disclosure in Online News Commentaries},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313669},
doi = {10.1145/3308558.3313669},
abstract = {Online users engage in self-disclosure - revealing personal information to others - in pursuit of social rewards. However, there are associated costs of disclosure to users' privacy. User profiling techniques support the use of contributed content for a number of purposes, e.g., micro-targeting advertisements. In this paper, we study self-disclosure as it occurs in newspaper comment forums. We explore a longitudinal dataset of about 60,000 comments on 2202 news articles from four major English news websites. We start with detection of language indicative of various types of self-disclosure, leveraging both syntactic and semantic information present in texts. Specifically, we use dependency parsing for subject, verb, and object extraction from sentences, in conjunction with named entity recognition to extract linguistic indicators of self-disclosure. We then use these indicators to examine the effects of anonymity and topic of discussion on self-disclosure. We find that anonymous users are more likely to self-disclose than identifiable users, and that self-disclosure varies across topics of discussion. Finally, we discuss the implications of our findings for user privacy.},
booktitle = {The World Wide Web Conference},
pages = {3272–3278},
numpages = {7},
keywords = {anonymity, Online self-disclosure, public platforms, privacy},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3132847.3132881,
author = {He, Yuan and Wang, Cheng and Jiang, Changjun},
title = {Incorporating the Latent Link Categories in Relational Topic Modeling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132881},
doi = {10.1145/3132847.3132881},
abstract = {The soaring of social media services has greatly propelled the prevalence of document networks. Rather than a set of plain texts, documents are nodes in graphs. An observable link connects the documents at its two ends, thus it implicitly reflects the semantic association between the document pair. Previous work assumes that only similar documents tend to be connected, which neglects the rich connective patterns in the topological structure. In this paper, we introduce a latent correlation factor to categorize the links into several categories, and each category corresponds to a unique kind of association. By fitting the data, the relational information (e.g., homophily and heterophily) can be comprehensively captured. By resorting to Canonical Correlation Analysis (CCA), we maximize the correlation between all pairs of linked documents. We propose a pure generative model and derive efficient learning algorithms based on the variational EM methods. Experiments on three different datasets demonstrate that the proposed model is competitive and usually better than the state-of-the-art baselines on both topic modeling and link prediction.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1877–1886},
numpages = {10},
keywords = {link prediction, canonical correlation analysis, relational topic modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3149485.3149522,
author = {Borg, Markus and Bjarnason, Elizabeth and Unterkalmsteiner, Michael and Yu, Tingting and Gay, Gregory and Felderer, Michael},
title = {Summary of the 4th International Workshop on Requirements Engineering and Testing (RET 2017)},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3149485.3149522},
doi = {10.1145/3149485.3149522},
abstract = {The RET (Requirements Engineering and Testing) workshop series provides a meeting point for researchers and practitioners from the two separate fields of Requirements Engineering (RE) and Testing. The long term aim is to build a community and a body of knowledge within the intersection of RE and Testing, i.e., RET. The 4th workshop was co-located with the 25th International Requirements Engineering Conference (RE'17) in Lisbon, Portugal and attracted about 20 participants. In line with the previous workshop instances, RET 2017 o ered an interactive setting with a keynote, an invited talk, paper presentations, and a concluding hands-on exercise.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {28–31},
numpages = {4}
}

@inproceedings{10.1145/3340482.3342746,
author = {Viuginov, Nickolay and Filchenkov, Andrey},
title = {A Machine Learning Based Automatic Folding of Dynamically Typed Languages},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342746},
doi = {10.1145/3340482.3342746},
abstract = {The popularity of dynamically typed languages has been growing strongly lately. Elegant syntax of such languages like javascript, python, PHP and ruby pays back when it comes to finding bugs in large codebases. The analysis is hindered by specific capabilities of dynamically typed languages, such as defining methods dynamically and evaluating string expressions. For finding bugs or investigating unfamiliar classes and libraries in modern IDEs and text editors features for folding unimportant code blocks are implemented. In this work, data on user foldings from real projects were collected and two classifiers were trained on their basis. The input to the classifier is a set of parameters describing the structure and syntax of the code block. These classifiers were subsequently used to identify unimportant code fragments. The implemented approach was tested on JavaScript and Python programs and compared with the best existing algorithm for automatic code folding.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Dynamically typed languages, Source code analysis, Python, JavaScript, Automatic Folding, Abstract Syntax tree},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3232829.3232833,
author = {Sun, Shaohua and Wang, Kuisheng and Zhang, Tiantian},
title = {The Definition, Current Situation and Development Trend of Latent Aspect Rating Analysis in Text Mining},
year = {2018},
isbn = {9781450364713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232829.3232833},
doi = {10.1145/3232829.3232833},
abstract = {The field of latent aspect rating analysis has been developed in the last few years. Firstly, we introduce the background and definition of latent aspect rating analysis in text mining. Secondly, we have collected literature on the latent aspect rating analysis of the research in recent years and summarized the development status of this field. Finally, the future development trend and expectation of this field are put forward according to relevant literature. Furthermore, the main contribution of this paper is to describe the field and analyze its development trend according to the author's research work.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Pattern Recognition},
pages = {21–26},
numpages = {6},
keywords = {Latent Aspect Rating Analysis, Aspect, Text Mining},
location = {Shenzhen, China},
series = {ICCPR '18}
}

@article{10.1145/3447270,
author = {Cui, Wanqiu and Du, Junping and Wang, Dawei and Kou, Feifei and Xue, Zhe},
title = {MVGAN: Multi-View Graph Attention Network for Social Event Detection},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3447270},
doi = {10.1145/3447270},
abstract = {Social networks are critical sources for event detection thanks to the characteristics of publicity and dissemination. Unfortunately, the randomness and semantic sparsity of the social network text bring significant challenges to the event detection task. In addition to text, time is another vital element in reflecting events since events are often followed for a while. Therefore, in this article, we propose a novel method named Multi-View Graph Attention Network (MVGAN) for event detection in social networks. It enriches event semantics through both neighbor aggregation and multi-view fusion in a heterogeneous social event graph. Specifically, we first construct a heterogeneous graph by adding the hashtag to associate the isolated short texts and describe events comprehensively. Then, we learn view-specific representations of events through graph convolutional networks from the perspectives of text semantics and time distribution, respectively. Finally, we design a hashtag-based multi-view graph attention mechanism to capture the intrinsic interaction across different views and integrate the feature representations to discover events. Extensive experiments on public benchmark datasets demonstrate that MVGAN performs favorably against many state-of-the-art social network event detection algorithms. It also proves that more meaningful signals can contribute to improving the event detection effect in social networks, such as published time and hashtags.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
articleno = {27},
numpages = {24},
keywords = {multi-view, heterogeneous graph, Event detection, hashtag attention}
}

@article{10.5555/3455716.3455893,
author = {Bing, Xin and Bunea, Florentina and Wegkamp, Marten},
title = {Optimal Estimation of Sparse Topic Models},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Topic models have become popular tools for dimension reduction and exploratory analysis of text data which consists in observed frequencies of a vocabulary of p words in n documents, stored in a p \texttimes{} n matrix. The main premise is that the mean of this data matrix can be factorized into a product of two non-negative matrices: a p \texttimes{} K word-topic matrix A and a K \texttimes{} n topic-document matrix W.This paper studies the estimation of A that is possibly element-wise sparse, and the number of topics K is unknown. In this under-explored context, we derive a new minimax lower bound for the estimation of such A and propose a new computationally efficient algorithm for its recovery. We derive a finite sample upper bound for our estimator, and show that it matches the minimax lower bound in many scenarios. Our estimate adapts to the unknown sparsity of A and our analysis is valid for any finite n, p, K and document lengths.Empirical results on both synthetic data and semi-synthetic data show that our proposed estimator is a strong competitor of the existing state-of-the-art algorithms for both non-sparse A and sparse A, and has superior performance is many scenarios of interest.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {177},
numpages = {45},
keywords = {minimax estimation, adaptive estimation, topic models, separability, non-negative matrix factorization, sparse estimation, high dimensional estimation, anchor words}
}

@inproceedings{10.1145/3209978.3210189,
author = {Zhai, ChengXiang and Geigle, Chase},
title = {A Tutorial on Probabilistic Topic Models for Text Data Retrieval and Analysis},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210189},
doi = {10.1145/3209978.3210189},
abstract = {As text data continues to grow quickly, it is increasingly important to develop intelligent systems to help people manage and make use of vast amounts of text data ("big text data''). As a new family of effective general approaches to text data retrieval and analysis, probabilistic topic models---notably Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocations (LDA), and their many extensions---have been studied actively in the past decade with widespread applications. These topic models are powerful tools for extracting and analyzing latent topics contained in text data; they also provide a general and robust latent semantic representation of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can be applied to text data in any natural language and about any topics. This tutorial systematically reviews the major research progress in probabilistic topic models and discuss their applications in text retrieval and text mining. The tutorial provides (1) an in-depth explanation of the basic concepts, underlying principles, and the two basic topic models (i.e., PLSA and LDA) that have widespread applications, (2) an introduction to EM algorithms and Bayesian inference algorithms for topic models, (3) a hands-on exercise to allow the tutorial attendants to learn how to use the topic models implemented in the MeTA Open Source Toolkit and experiment with provided data sets, (4) a broad overview of all the major representative topic models that extend PLSA or LDA, and (5) a discussion of major challenges and future research directions.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1395–1398},
numpages = {4},
keywords = {data mining, text mining, probabilistic topic models, language models, information retrieval},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3159652.3160588,
author = {Tepper, Naama and Hashavit, Anat and Barnea, Maya and Ronen, Inbal and Leiba, Lior},
title = {Collabot: Personalized Group Chat Summarization},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3160588},
doi = {10.1145/3159652.3160588},
abstract = {In recent years, enterprise group chat collaboration tools, such as Slack, IBM»s Watson Workspace and Microsoft Teams, have presented unprecedented growth. With all the potential benefits of these tools - productivity increase and improved group communication - come significant challenges. Specifically, the 'always on' feature that makes it hard for users to cope with the load of conversational content and get up to speed after logging off for a while. In this demo, we present Collabot - a chat assistant service that implicitly learns users interests and social ties within a chat group and provides a personalized digest of missed content. Collabot assists users in coping with chat information overload by helping them understand the main topics discussed, collaborators, links and resources. This demo has two main contributions. First, we present a novel personalized group chat summarization algorithm; second the demonstration depicts a working implementation applied on different chat groups from different domains within IBM. A video, describing the demo can be found at https://www.youtube.com/watch?v=6cVsstiJ9vk.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {771–774},
numpages = {4},
keywords = {personalization, social networks, summarization, group chat},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{10.1145/3326467.3326488,
author = {Yukselen, Murat and Mutlu, Alev and Karagoz, Pinar},
title = {Infuencee Oriented Topic Prediction: Investigating the Effect of Influence on the Author},
year = {2019},
isbn = {9781450361903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326467.3326488},
doi = {10.1145/3326467.3326488},
abstract = {In this paper, we study the problem of topic adoption prediction for an author within a social academic network. The previous efforts on the problem use topic similarity and topic adoption of co-authors. We model the problem with an influence detection point of view, and propose that the influence on the author is an important factor. Hence, we define a novel influencee prediction based feature. To this aim, in this work, an algorithm is proposed to calculate the influence propagated towards the author. The effect of this feature is explored together with and in comparison to other features used in the literature for the problem. The experiments conducted on Arnet Miner data set show that accumulated influence on author is effective for predicting topic adoption.},
booktitle = {Proceedings of the 9th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {9},
keywords = {link prediction, information flow, social networks},
location = {Seoul, Republic of Korea},
series = {WIMS2019}
}

@article{10.1145/3432924,
author = {Choi, Yoonseo and Monserrat, Toni-Jan Keith Palma and Park, Jeongeon and Shin, Hyungyu and Lee, Nyoungwoo and Kim, Juho},
title = {ProtoChat: Supporting the Conversation Design Process with Crowd Feedback},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432924},
doi = {10.1145/3432924},
abstract = {Similar to a design process for designing graphical user interfaces, conversation designers often apply an iterative design process by defining a conversation flow, testing with users, reviewing user data, and improving the design. While it is possible to iterate on conversation design with existing chatbot prototyping tools, there still remain challenges in recruiting participants on-demand and collecting structured feedback on specific conversational components. These limitations hinder designers from running rapid iterations and making informed design decisions. We posit that involving a crowd in the conversation design process can address these challenges, and introduce ProtoChat, a crowd-powered chatbot design tool built to support the iterative process of conversation design. ProtoChat makes it easy to recruit crowd workers to test the current conversation within the design tool. ProtoChat's crowd-testing tool allows crowd workers to provide concrete and practical feedback and suggest improvements on specific parts of the conversation. With the data collected from crowd-testing, ProtoChat provides multiple types of visualizations to help designers analyze and revise their design. Through a three-day study with eight designers, we found that ProtoChat enabled an iterative design process for designing a chatbot. Designers improved their design by not only modifying the conversation design itself, but also adjusting the persona and getting UI design implications beyond the conversation design itself. The crowd responses were helpful for designers to explore user needs, contexts, and diverse response formats. With ProtoChat, designers can successfully collect concrete evidence from the crowd and make decisions to iteratively improve their conversation design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {225},
numpages = {27},
keywords = {conversation design, crowd testing, design iteration, design process, conversational user interface, crowdsourcing, crowd feedback, chatbot design}
}

@inproceedings{10.1145/3397271.3401245,
author = {Lin, Lihui and Jiang, Hongyu and Rao, Yanghui},
title = {Copula Guided Neural Topic Modelling for Short Texts},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401245},
doi = {10.1145/3397271.3401245},
abstract = {Extracting the topical information from documents is important for public opinion analysis, text classification, and information retrieval tasks. Compared with identifying a wide variety of topics from long documents, it is challenging to generate a concentrated topic distribution for each short message. Although this problem can be tackled by adjusting the hyper-parameters in traditional topic models such as Latent Dirichlet Allocation, it remains an open problem in neural topic modelling. In this paper, we focus on adapting the popular Auto-Encoding Variational Bayes based neural topic models to short texts, by exploring the Archimedean copulas to guide the estimated topic distributions derived from linear projected samples of re-parameterized posterior distributions. Experimental results show the superiority of our method when compared with existing neural topic models in terms of perplexity, topic coherence, and classification accuracy.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1773–1776},
numpages = {4},
keywords = {neural topic modelling, auto-encoding variational Bayes, Archimedean copulas, short text modelling},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3462204.3481752,
author = {Lin, Kung-Pai and Lee, Hao-Ping (Hank) and Chou, Yu-Ling and Shih, Faye and Chang, Yung-Ju},
title = {Distinguishing IM Communication Patterns with Relationship and Conversation Topics},
year = {2021},
isbn = {9781450384797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462204.3481752},
doi = {10.1145/3462204.3481752},
abstract = {Despite being characterized as constantly on and connected, IM users’ responsiveness varies across different contacts. While research has shown that the relationship between conversation partners plays an important role in influencing their communication patterns, characterization of such patterns simply by using relationship information is limited [7, 8]. In this paper, we identify five distinct clusters of IM patterns using unsupervised learning derived from 46 users’ conversation history. We show that the relationship category sufficed to characterize three clusters of communication patterns, but failed for the most active one. However, considering both relationship and topics would distinguish most communication patterns, including the most active one. This result suggests that future research on IM communication patterns should pay more attention to the topics in users’ conversations.},
booktitle = {Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {121–125},
numpages = {5},
location = {Virtual Event, USA},
series = {CSCW '21}
}

@inproceedings{10.1145/3287921.3287925,
author = {Tung, Nguyen Trong and Dieu, Vu Hoang and Than, Khoat and Van Linh, Ngo},
title = {Reducing Class Overlapping in Supervised Dimension Reduction},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287925},
doi = {10.1145/3287921.3287925},
abstract = {Dimension reduction is to find a low-dimensional subspace to project high-dimensional data on, such that the discriminative property of the original higher-dimensional data is preserved. In supervised dimension reduction, class labels are integrated into the lower-dimensional representation, to produce better results on classification tasks. The supervised dimension reduction (SDR) framework by [17] is one of the state-of-the-art methods that takes into account not only the class labels but also the neighborhood graphs of the data, and have some advantages in preserving the within-class local structure and widening the between-class margin. However, the reduced-dimensional representation produced by the SDR framework suffers from the class overlapping problem - in which, data points lie closer to a different class rather than the class they belong to. The class overlapping problem can hurt the quality on the classification task. In this paper, we propose a new method to reduce the overlap for the SDR framework in [17]. The experimental results show that our method reduces the size of the overlapping set by an order of magnitude. As a result, our method outperforms the pre-existing framework on the classification task significantly. Moreover, visualization plots show that the reduced-dimensional representation learned by our method is more scattered for within-class data and more separated for between-class data, as compared to the pre-existing SDR framework.},
booktitle = {Proceedings of the Ninth International Symposium on Information and Communication Technology},
pages = {8–15},
numpages = {8},
keywords = {Class overlapping, Supervised dimension reduction, Probabilistic topic models},
location = {Danang City, Viet Nam},
series = {SoICT 2018}
}

@inproceedings{10.1145/3404835.3462982,
author = {Fan, Wentao and Guo, Zhiyan and Bouguila, Nizar and Hou, Wenjuan},
title = {Clustering-Based Online News Topic Detection and Tracking Through Hierarchical Bayesian Nonparametric Models},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462982},
doi = {10.1145/3404835.3462982},
abstract = {In this paper, we propose a clustering-based online news topic detection and tracking (TDT) approach based on hierarchical Bayesian nonparametric framework that allows topics to be shared across different news stories in a corpus. Our approach is formulated using the hierarchical Pitman-Yor process mixture model with the inverted Beta-Liouville (IBL) distribution as its component density, which has shown superior performance in modeling text data than the widely used Gaussian distribution. Moreover, we theoretically develop a convergence-guaranteed online learning algorithm that can effectively learn the proposed TDT model from a stream of news stories based on varational Bayes. The merits of our TDT approach are illustrated by comparing it with other well-defined clustering-based TDT approaches on different news data sets.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2126–2130},
numpages = {5},
keywords = {clustering, topic detection and tracking, hierarchical Bayesian model},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3180374.3181354,
author = {Zhao, Guifen and Liu, Yanjun and Zhang, Wei and Wang, Yiou},
title = {TFIDF Based Feature Words Extraction and Topic Modeling for Short Text},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181354},
doi = {10.1145/3180374.3181354},
abstract = {In this paper, feature words extraction and topic modeling based on Term Frequency times In-verse Document Frequency (TFIDF) and Latent Dirichlet Allocation (LDA) is achieved aiming at short titles text of The National Institutes of Health (NIH) supported research. After preprocess of raw text, distinct terms extracted from titles of NIH supported research compose the Bag of Words. And then TFIDF is used in order to re-weight the count features for reducing the influence of more frequent yet less valuable terms and enhancing the influence of rarer yet more valuable terms. Via topic modeling with Latent Dirichlet Allocation, ten topics and corresponding three feature words are extracted, and each topic is characterized by the three special words. As a result, it can achieve topic extraction of NIH supported research according to titles and reveal the most probable research area.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {188–191},
numpages = {4},
keywords = {Term-frequency, Topic modeling, Inverse Document-frequency, Latent Dirichlet Allocation, Feature Words Extraction},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@inproceedings{10.1145/3387940.3392245,
author = {Uras, Nicola and Vacca, Stefano and Destefanis, Giuseppe},
title = {Investigation of Mutual-Influence among Blockchain Development Communities and Cryptocurrency Price Changes},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392245},
doi = {10.1145/3387940.3392245},
abstract = {This paper aims to identify and model relationships between cryptocurrencies market price changes and topic discussion occurrences on social media. The considered cryptocurrencies are the two highest in value at the moment, Bitcoin and Ethereum. At the same time, topics were realized through a classification of the comments gained from the Reddit social media platform, implementing a Hawkes model. The results highlight that it is possible to identify some interactions among the considered features, and it appears that some topics are indicative of certain types of price movements. Specifically, the discussions concerning issues about government, trading and Ethereum cryptocurrency as an exchange currency, appear to affect Bitcoin and Ethereum prices negatively. The discussions of investment appear to be indicative of price rises, while the discussions related to new decentralized realities and technological applications is indicative of price falls.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {779–782},
numpages = {4},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3341105.3374041,
author = {Calisir, Emre and Brambilla, Marco},
title = {Wide-Spectrum Characterization of Long-Running Political Phenomena on Social Media: The Brexit Case},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374041},
doi = {10.1145/3341105.3374041},
abstract = {In this study, we propose a wide-spectrum analysis of long-running political events on social media, with reference to an interesting real-world international case: the so-called Brexit, the process through which the United Kingdom activated the option of leaving the European Union. In this study, we model the users participating in 33 months of Twitter debate, covering their behaviour and demographics. By using publicly shared tweets, we developed a stance classification model to evaluate the change of stance over time. We also extracted the key topics of the long-running debate, studying which political side have discussed them most and what is the general sentiment on each. We also revealed the participation of bot accounts, and we found that the higher the bot score, the more likely the account is in a pro-Leave position. We conclude our study with a temporal and comparative analysis of politicians' social media accounts.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1869–1876},
numpages = {8},
keywords = {political stance classification, topic discovery, automated political accounts, brexit referendum},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3041021.3054252,
author = {Savov, Pavel and Jatowt, Adam and Nielek, Radoslaw},
title = {Towards Understanding the Evolution of the WWW Conference},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054252},
doi = {10.1145/3041021.3054252},
abstract = {The World Wide Web conference is a well-established and mature venue with an already long history. Over the years it has been attracting papers reporting many important research achievements centered around the Web. In this work we aim at understanding the evolution of WWW conference series by detecting crucial years and important topics. We propose a simple yet novel approach based on tracking the classification errors of the conference papers according to their predicted publication years.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {835–836},
numpages = {2},
keywords = {www, www research, evolution},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3357384.3357939,
author = {Bi, Keping and Ai, Qingyao and Zhang, Yongfeng and Croft, W. Bruce},
title = {Conversational Product Search Based on Negative Feedback},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357939},
doi = {10.1145/3357384.3357939},
abstract = {Intelligent assistants change the way people interact with computers and make it possible for people to search for products through conversations when they have purchase needs. During the interactions, the system could ask questions on certain aspects of the ideal products to clarify the users' needs. For example, previous work proposed to ask users the exact characteristics of their ideal items before showing results. However, users may not have clear ideas about what an ideal item looks like, especially when they have not seen any item. So it is more feasible to facilitate the conversational search by showing example items and asking for feedback instead. In addition, when the users provide negative feedback for the presented items, it is easier to collect their detailed feedback on certain properties (aspect-value pairs) of the non-relevant items. By breaking down the item-level negative feedback to fine-grained feedback on aspect-value pairs, more information is available to help clarify users' intents. So in this paper, we propose a conversational paradigm for product search driven by non-relevant items, based on which fine-grained feedback is collected and utilized to show better results in the next iteration. We then propose an aspect-value likelihood model to incorporate both positive and negative feedback on fine-grained aspect-value pairs of the non-relevant items. Experimental results show that our model is significantly better than state-of-the-art product search baselines without using feedback and those baselines using item-level negative feedback.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {359–368},
numpages = {10},
keywords = {personalized agent, negative feedback, conversational search, product search, dialogue system},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3219819.3220043,
author = {Liang, Shangsong and Zhang, Xiangliang and Ren, Zhaochun and Kanoulas, Evangelos},
title = {Dynamic Embeddings for User Profiling in Twitter},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220043},
doi = {10.1145/3219819.3220043},
abstract = {In this paper, we study the problem of dynamic user profiling in Twitter. We address the problem by proposing a dynamic user and word embedding model (DUWE), a scalable black-box variational inference algorithm, and a streaming keyword diversification model (SKDM). DUWE dynamically tracks the semantic representations of users and words over time and models their embeddings in the same space so that their similarities can be effectively measured. Our inference algorithm works with a convex objective function that ensures the robustness of the learnt embeddings. SKDM aims at retrieving top-K relevant and diversified keywords to profile users' dynamic interests. Experiments on a Twitter dataset demonstrate that our proposed embedding algorithms outperform state-of-the-art non-dynamic and dynamic embedding and topic models.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1764–1773},
numpages = {10},
keywords = {profiling, dynamic model, word embeddings},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3472720.3483622,
author = {Amati, Giambattista and Angelini, Simone and Cruciani, Antonio and Fusco, Gianmarco and Gaudino, Giancarlo and Pasquini, Daniele and Vocca, Paola},
title = {Topic Modeling by Community Detection Algorithms},
year = {2021},
isbn = {9781450386326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472720.3483622},
doi = {10.1145/3472720.3483622},
abstract = {We first estimate the number of Italian users active on Twitter in the last year by filtering the Italian flow of Twitter. We show that our filter misses about the 6.86% of the Italian flow, while 86.80% of the selected tweets belongs to the Italian language. Given this accuracy of the Italian Twitter's Firehose filter, we are able to assess the actual number of the Italian active users (AUs) of this platform. We then introduce a massive text document clustering algorithm that is easily applicable and scalable to the Twitter social network. Instead of a topic modeling approach based on features selection and any conventional clustering algorithm, such as LDA, we apply community detection algorithms on the weighted hashtag graph . In order to scale with the graph size, we apply two linear community detection algorithms, CoDA and Louvain. Once the hashtags have been assigned to clusters, both the most numerous clusters and hashtags were associated with topics of general interest, such as sports, politics, health etc. In this way we are able to provide significant statistics of the topics covered on Twitter in the past year.},
booktitle = {Proceedings of the 2021 Workshop on Open Challenges in Online Social Networks},
pages = {15–20},
numpages = {6},
keywords = {data analysis, topic modelling, louvain, community detection, lda, twitter},
location = {Virtual Event, Ireland},
series = {OASIS '21}
}

@inproceedings{10.1145/3282373.3282390,
author = {Sanjaya, Ngurah Agus and Ba, Mouhamadou Lamine and Abdessalem, Talel and Bressan, St\'{e}phane},
title = {Harnessing Truth Discovery Algorithms On The Topic Labelling Problem},
year = {2018},
isbn = {9781450364799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282373.3282390},
doi = {10.1145/3282373.3282390},
abstract = {Topics in topic modelling approaches are represented as a collection of weighted words. The labels for the topics, however, are not clearly defined and must be interpreted manually. Topic labelling proposes to automatically label the topics by leveraging a knowledge base or applying data mining and machine learning algorithms. We propose a naive topic labelling approach where we transform the labeling problem into selecting the best label for each word in the topic. The candidate labels are generated by querying a knowledge base using the top-N words of each topic. We construct a heterogeneous graph of topics, words, articles and candidate labels. To rank the candidate labels, we apply truth discovery algorithms on the graph. The performance evaluation using popular topic modelling datasets shows that the approach receives satisfactory accuracy.},
booktitle = {Proceedings of the 20th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {8–14},
numpages = {7},
keywords = {evaluation, topic labelling, ranking, truth discovery, truth finding},
location = {Yogyakarta, Indonesia},
series = {iiWAS2018}
}

@inproceedings{10.1145/3290621.3290630,
author = {Shtekh, Gennady and Kazakova, Polina and Nikitinsky, Nikita and Skachkov, Nikolay},
title = {Applying Topic Segmentation to Document-Level Information Retrieval},
year = {2018},
isbn = {9781450361767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290621.3290630},
doi = {10.1145/3290621.3290630},
abstract = {In the present paper we discuss how text segmentation could be applied in the information retrieval domain. We assume that topic text segmentation allows one to better model text structure and therefore language itself, which influences the quality of text representation. We test the initial hypothesis by conducting experiments with several baseline models on the arXiv dataset comparing their quality on whole texts and on segmented texts. The experiments demonstrated that, indeed, the quality of retrieval is generally slightly improved.},
booktitle = {Proceedings of the 14th Central and Eastern European Software Engineering Conference Russia},
articleno = {6},
numpages = {6},
keywords = {Topic Modeling, Topic Segmentation, Information Retrieval},
location = {Moscow, Russian Federation},
series = {CEE-SECR '18}
}

@inproceedings{10.1145/3430895.3460137,
author = {Ntourmas, Anastasios and Dimitriadis, Yannis and Daskalaki, Sophia and Avouris, Nikolaos},
title = {Classification of Discussions in MOOC Forums: An Incremental Modeling Approach},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460137},
doi = {10.1145/3430895.3460137},
abstract = {Supervised classification models are commonly used for classifying discussions in a MOOC forum. In most cases these models require a tedious process for manual labeling the forum messages as training data. So, new methods are needed to reduce the human effort necessary for the preparation of such training datasets. In this study we follow an incremental approach in order to examine how soon after the beginning of a new course, we have collected enough data for training a supervised classification model. We show that by employing features that derive from a seeded topic modeling method, we achieve classifiers with reliable performance early enough in the course life, thus reducing significantly the human effort. The content of the MOOC platform is used to bias the topic extraction towards discussions related to (a) course content, (b) logistics, or (c) social interactions. Then, we develop a supervised model at the start of each week based on the topic features of all previous weeks and evaluate its performance in classifying the discussions for the rest of the course. Our approach was implemented in three different MOOCs of different subjects and different sizes. The findings reveal that supervised models are able to perform reliably quite early in a MOOC's life and retain a steady overall accuracy across the remaining weeks, without requiring to be trained with the entire forum dataset.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {183–194},
numpages = {12},
keywords = {corex, discussion forum, MOOC, supervised classification, topic modeling},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{10.1145/3430895.3460150,
author = {Shashkov, Alexander and Gold, Robert and Hemberg, Erik and Kong, ByeongJo and Bell, Ana and O'Reilly, Una-May},
title = {Analyzing Student Reflection Sentiments and Problem-Solving Procedures in MOOCs},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460150},
doi = {10.1145/3430895.3460150},
abstract = {Student reflection is thought to be an important part of retaining and understanding knowledge gained in a course. Using natural language processing, we analyze and interpret student reflections from Massive Open Online Courses (MOOCs) to understand the students' sentiments and problem-solving procedures. The reflections are free text responses to questions from MIT 6.00.1x, an introductory programming MOOC. We compare different sentiment analysis methods, and conclude that the best-performing methods can robustly classify sentiment of student responses. In addition, we develop methods to analyze student problem-solving procedures using sentence parsing and topic modeling. We find our method can distinguish some common problem-solving procedures such as utilizing course resources.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {247–250},
numpages = {4},
keywords = {natural language processing, reflective learning, topic modeling, MOOCs, sentiment analysis, sentence parsing},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{10.1145/3372938.3372943,
author = {Mifrah, Sara and El Habib, Ben Lahmar},
title = {Semantic Relationship Study between Citing and Cited Scientific Articles Using Topic Modeling},
year = {2019},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372943},
doi = {10.1145/3372938.3372943},
abstract = {In the evaluation of scientific research, it is clear that 'scientific quality' is proving to be one of the most important criteria, and in the case of research for development, it is on an equal footing with another key dimension: relevance for development. The evaluation criteria and procedures are very similar to those applied in the evaluation of research in general: peer reviews, panels, bibliometric indices, etc ... The practical application of these tools in the case of research for development can sometimes be quite difficult. There is, however, a general consensus that quality should be expressed identically for any scientific research, with the same problems and constraints. In this article and to evaluate a scientific article, we thought to study the semantic relationship between the citing article and the other cited, we based on the Topic modeling theme with the choice of LDA and LDA2VEC algorithms applied to the corpus Nips (1987-2017) then on the citations of each article of the corpus.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {5},
numpages = {8},
keywords = {Natural language processing, Machine Learning, Cosine-Similarity, Topic Modeling},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3299771.3299787,
author = {Link, Daniel and Behnamghader, Pooyan and Moazeni, Ramin and Boehm, Barry},
title = {The Value of Software Architecture Recovery for Maintenance},
year = {2019},
isbn = {9781450362153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299771.3299787},
doi = {10.1145/3299771.3299787},
abstract = {In order to maintain a system, it is beneficial to know its software architecture. In the common case that this architecture is unavailable, architecture recovery provides a way to recover an architectural view of the system. Many different methods and tools exist to provide such a view. While there have been taxonomies of different recovery methods and surveys of their results along with measurements of how these results conform to expert's opinions on the systems, there has not been a survey that goes beyond a simple automatic comparison. Instead, this paper seeks to answer questions about the viability of individual methods in given situations, the quality of their results and whether these results can be used to indicate and measure the quality and quantity of architectural changes. For our case study, we look at the results of recoveries of versions of Android, Apache Hadoop and Apache Chukwa obtained by running PKG, ACDC and ARC.},
booktitle = {Proceedings of the 12th Innovations on Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {17},
numpages = {10},
keywords = {architecture recovery, Software architecture, software maintenance, incremental development},
location = {Pune, India},
series = {ISEC'19}
}

@inproceedings{10.1145/3442442.3452311,
author = {Panchendrarajan, Rrubaa and Hsu, Wynne and Li Lee, Mong},
title = {Emotion-Aware Event Summarization in Microblogs},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452311},
doi = {10.1145/3442442.3452311},
abstract = {Microblogs have become the preferred means of communication for people to share information and feelings, especially for fast evolving events. Understanding the emotional reactions of people allows decision makers to formulate policies that are likely to be more well-received by the public and hence better accepted especially during policy implementation. However, uncovering the topics and emotions related to an event over time is a challenge due to the short and noisy nature of microblogs. This work proposes a weakly supervised learning approach to learn coherent topics and the corresponding emotional reactions as an event unfolds. We summarize the event by giving the representative microblogs and the emotion distributions associated with the topics over time. Experiments on multiple real-world event datasets demonstrate the effectiveness of the proposed approach over existing solutions.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {486–494},
numpages = {9},
keywords = {Topic-Emotion Learning, Event Summary, Microblogs},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3341161.3345332,
author = {Kim, Do Yeon and Li, Xiaohang and Wang, Sheng and Zhuo, Yunying and Lee, Roy Ka-Wei},
title = {Topic Enhanced Word Embedding for Toxic Content Detection in Q&amp;A Sites},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3345332},
doi = {10.1145/3341161.3345332},
abstract = {Increasingly, users are adopting community question-and-answer (Q&amp;A) sites to exchange information. Detecting and eliminating toxic and divisive content in these Q&amp;A sites are paramount tasks to ensure a safe and constructive environment for the users. Insincere question, which is founded upon false premises, is one type of toxic content in Q&amp;A sites. In this paper, we proposed a novel deep learning framework enhanced pre-trained word embeddings with topical information for insincere question classification. We evaluated our proposed framework on a large real-world dataset from Quora Q&amp;A site and showed that the topically enhanced word embedding is able to achieve better results in toxic content classification. An empirical study was also conducted to analyze the topics of the insincere questions on Quora, and we found that topics on "religion", "gender" and "politics" has a higher proportion of insincere questions.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1064–1071},
numpages = {8},
keywords = {sequence model, NLP, toxic content, text classification, word embedding},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3459637.3482253,
author = {Liu, Chunyu and Yang, Yongjian and Yao, Zijun and Xu, Yuanbo and Chen, Weitong and Yue, Lin and Wu, Haomeng},
title = {Discovering Urban Functions of High-Definition Zoning with Continuous Human Traces},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482253},
doi = {10.1145/3459637.3482253},
abstract = {Identifying the dynamic functions of different urban zones enables a variety of smart city applications, such as intelligent urban planning, real-time traffic scheduling, and community precision management. Traditional urban function research using government administrative zoning systems is often conducted in a coarse resolution with fixed split, and ignore the reshaping of zones by city growth. To solve this problem, we propose a two-stage framework in order to represent the high-definition distribution of urban function across the city, by analyzing continuous human traces extracted from the dense, widespread, and full-time cellular data. At the representation stage, we embed the locations of base stations by modeling the user movements with staying and transfer events, along with the consideration of dynamic trip purposes in continuous human traces. At the annotation stage, we first divide the city into the finest unit zones and each covers at least one base station. By clustering the base stations, we further group the unit zones into functional zones. Last, we annotate functional zones based on the local point-of-interest (POI) information. In experiments, we evaluate the proposed high-definition function study in two tasks: (i) in-zone crowd flow prediction, and (ii) zone-enhanced POI recommendation. The results demonstrate the advantage of the proposed method with both the effectiveness of city split and the high-quality function annotation.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1048–1057},
numpages = {10},
keywords = {mobile trajectory, signaling data, fine-grained functional zone, zone embedding, urban computing},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3197026.3203891,
author = {Zuo, Zhiya and Zhao, Kang},
title = {A Graphical Model for Topical Impact over Time},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3203891},
doi = {10.1145/3197026.3203891},
abstract = {After being published, a document, whether it is a research paper or an online post, can make an impact when readers cite, share, or endorse it. A document may not make its greatest impact right after its publication, and some documents' impact can last a long period of time. This study develops a graphical model to capture the temporal dynamics in the impact of latent topics from a corpus of documents. Specifically, we modeled citation counts using Poisson distributions with Gamma priors. We conducted experiments on papers published in (i) D-Lib Magazine and (ii) The Library Quarterly from 2007 to 2017. Comparing with ToT, we found that our model produced more robust results on topical trends over time. The results also showed that prevalence and impact of the same topic are not correlated. Enabling better understanding and modeling of topical impact over time, this model can be used for the design of digital libraries and social media platforms, as well as evaluation of scientific contributions and policies.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {405–406},
numpages = {2},
keywords = {temporal impact, text mining, topic models},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3404835.3462975,
author = {Xie, Yi and Sun, Yuqing and Bertino, Elisa},
title = {Learning Domain Semantics and Cross-Domain Correlations for Paper Recommendation},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462975},
doi = {10.1145/3404835.3462975},
abstract = {Understanding how knowledge is technically transferred across academic disciplines is very relevant for understanding and facilitating innovation. There are two challenges for this purpose, namely the semantic ambiguity and the asymmetric influence across disciplines. In this paper we investigate knowledge propagation and characterize semantic correlations for cross discipline paper recommendation. We adopt a generative model to represent a paper content as the probabilistic association with an existing hierarchically classified discipline to reduce the ambiguity of word semantics. The semantic correlation across disciplines is represented by an influence function, a correlation metric and a ranking mechanism. Then a user interest is represented as a probabilistic distribution over the target domain semantics and the correlated papers are recommended. Experimental results on real datasets show the effectiveness of our methods. We also discuss the intrinsic factors of results in an interpretable way. Compared with traditional word embedding based methods, our approach supports the evolution of domain semantics that accordingly lead to the update of semantic correlation. Another advantage of our approach is its flexibility and uniformity in supporting user interest specifications by either a list of papers or a query of key words, which is suited for practical scenarios.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {706–715},
numpages = {10},
keywords = {recommendation, semantic correlation, academic paper},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.5555/3535850.3536112,
author = {Hadfi, Rafik and Ito, Takayuki},
title = {Augmented Democratic Deliberation: Can Conversational Agents Boost Deliberation in Social Media?},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Online social media are currently perceived as the new means of providing a platform for participation among citizens. This comes at a time when people are led to alternative spaces of political expression as traditional channels become strained. New technologies appear to have a transformational potential that could lead to social change and achieve deeper and wider mobilisation in political processes. However, people still face challenges when seeking information, disseminating information, or engaging in online deliberation. Allowing every citizen to participate in discussions and thus influence the final decision requires countless interactions that take considerable amounts of time and energy. This process is cognitively demanding due to linguistic barriers or when the problems on the table are multidisciplinary. We envision in this blue sky paper the development of autonomous and intelligent conversational agents that can augment the deliberative capacities of citizens in social media. To implement our vision, we start by proposing an approach that quantifies deliberation in online argumentative discussions. Then, we propose a methodology to optimise deliberation across discussion threads. The proposed concept is expected to pave the way to a form of augmented democratic deliberation built on the cooperation of humans and conversational agents.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1794–1798},
numpages = {5},
keywords = {collective intelligence, online discussions, digital democracy, augmented democracy, deliberation, natural language processing, conversational agents, social media, multiagent systems},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3397271.3401231,
author = {Jain, Arnav Kumar and Arora, Gundeep and Agrawal, Rahul},
title = {Graph Regularization for Multi-Lingual Topic Models},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401231},
doi = {10.1145/3397271.3401231},
abstract = {Unsupervised multi-lingual language modeling has gained attraction in the last few years and poly-lingual topic models provide a mechanism to learn aligned document representations. However, training such models require translation-aligned data across languages, which is not always available. Also, in case of short texts like tweets, search queries, etc, the training of topic models continues to be a challenge. In this work, we present a novel strategy of creating a pseudo-parallel dataset followed by training topic models for sponsored search retrieval, that also mitigates the short text challenge. Our data augmentation strategy leverages easily available bipartite click-though graph that allows us to draw similar documents in different languages. The proposed methodology is evaluated on sponsored search system whose performance is measured on correctly matching the user intent, presented via the query, with ads provided by the advertiser. Our experiments substantiate the goodness of the method on EuroParl dataset and live search-engine traffic.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1741–1744},
numpages = {4},
keywords = {cross-lingual information retrieval, topic models, graph regularization},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3275219.3275239,
author = {Qin, Hanmin and Sun, Xin},
title = {Classifying Bug Reports into Bugs and Non-Bugs Using LSTM},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275239},
doi = {10.1145/3275219.3275239},
abstract = {Studies have found that significant amount of bug reports are misclassified between bugs and non-bugs, which inevitably affects relevant studies, e.g., bug prediction. Manually classifying bug reports helps reduce the noise but is often time-consuming. To ease the problem, we propose a bug classification method based on Long Short-Term Memory (LSTM), a typical recurrent neural network which is widely used in text classification tasks. Our method outperforms existing topic-based method and n-gram IDF-based method on four datasets from three popular JAVA open source projects. We believe our work can assist developers and researches to classify bug reports and identify misclassified bug reports. Datasets and scripts used in this work are provided on GitHub1 for others to reproduce and further improve our study.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {20},
numpages = {4},
keywords = {bug classification, LSTM},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3077136.3084151,
author = {Vuong, Tung and Jacucci, Giulio and Ruotsalo, Tuukka},
title = {Proactive Information Retrieval via Screen Surveillance},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3084151},
doi = {10.1145/3077136.3084151},
abstract = {We demonstrate proactive information retrieval via screen surveillance. A user's digital activities are continuously monitored by capturing all content on a user's screen using optical character recognition. This includes all applications and services being exploited and relies on each individual user's computer usage, such as their Web browsing, emails, instant messaging, and word processing. Topic modeling is then applied to detect the user's topical activity context to retrieve information. We demonstrate a system that proactively retrieves information from a user's activity history being observed on the screen when the user is performing unseen activities on a personal computer. We report an evaluation with ten participants that shows high user satisfaction and retrieval effectiveness. Our demonstration and experimental results show that surveillance of a user's screen can be used to build an extremely rich model of a user's digital activities across application boundaries and enable effective proactive information retrieval.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1313–1316},
numpages = {4},
keywords = {proactive information retrieval, screen surveillance, user modeling},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3084226.3084243,
author = {Ros, Rasmus and Bjarnason, Elizabeth and Runeson, Per},
title = {A Machine Learning Approach for Semi-Automated Search and Selection in Literature Studies},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084243},
doi = {10.1145/3084226.3084243},
abstract = {Background. Search and selection of primary studies in Systematic Literature Reviews (SLR) is labour intensive, and hard to replicate and update. Aims. We explore a machine learning approach to support semi-automated search and selection in SLRs to address these weaknesses. Method. We 1) train a classifier on an initial set of papers, 2) extend this set of papers by automated search and snowballing, 3) have the researcher validate the top paper, selected by the classifier, and 4) update the set of papers and iterate the process until a stopping criterion is met. Results. We demonstrate with a proof-of-concept tool that the proposed automated search and selection approach generates valid search strings and that the performance for subsets of primary studies can reduce the manual work by half. Conclusions. The approach is promising and the demonstrated advantages include cost savings and replicability. The next steps include further tool development and evaluate the approach on a complete SLR.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {118–127},
numpages = {10},
keywords = {Automation, Reinforcement learning, Study selection, Machine learning, Research identification, Systematic literature review},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@inproceedings{10.1145/3397271.3401179,
author = {Zhuang, Honglei and Guo, Fang and Zhang, Chao and Liu, Liyuan and Han, Jiawei},
title = {Joint Aspect-Sentiment Analysis with Minimal User Guidance},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401179},
doi = {10.1145/3397271.3401179},
abstract = {Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1241–1250},
numpages = {10},
keywords = {autoencoder, aspect-based sentiment analysis, weakly-supervised},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3132847.3133071,
author = {Liu, Shen and Liu, Hongyan},
title = {Exploiting User Consuming Behavior for Effective Item Tagging},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133071},
doi = {10.1145/3132847.3133071},
abstract = {Automatic tagging techniques are important for many applications such as searching and recommendation, which has attracted many researchers' attention in recent years. Existing methods mainly rely on users' tagging behavior or items' content information for tagging, yet users' consuming behavior is ignored. In this paper, we propose to leverage such information and introduce a probabilistic model called joint-tagging LDA to improve tagging accuracy. An effective algorithm based on Zero-Order Collapsed Variational Bayes is developed. Experiments conducted on a real dataset demonstrate that joint-tagging LDA outperforms existing competing methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2175–2178},
numpages = {4},
keywords = {tag recommendation, user behavior modeling, generative model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3077240.3077246,
author = {Pujara, Jay},
title = {Extracting Knowledge Graphs from Financial Filings: Extended Abstract},
year = {2017},
isbn = {9781450350310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077240.3077246},
doi = {10.1145/3077240.3077246},
abstract = {Textual corpora, such as financial documents, contain a wealth of knowledge. Recently, knowledge graphs have become a popular approach to capturing structured knowledge of entities and their interrelationships. In this paper, we evaluate open information extraction (IE) and knowledge graph construction techniques for assessing the relevance of textual segments in the Financial Entity Identification and Information Integration Challenge. Our approach is to extract several textual signals, including topics and open IE triples, and combine these in a probabilistic framework to predict the relevance of each potential relationship.},
booktitle = {Proceedings of the 3rd International Workshop on Data Science for Macro--Modeling with Financial and Economic Datasets},
articleno = {5},
numpages = {2},
location = {Chicago, IL, USA},
series = {DSMM'17}
}

@inproceedings{10.1145/3200947.3201013,
author = {Potha, Nektaria and Stamatatos, Efstathios},
title = {Intrinsic Author Verification Using Topic Modeling},
year = {2018},
isbn = {9781450364331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3200947.3201013},
doi = {10.1145/3200947.3201013},
abstract = {Author verification is a fundamental task in authorship analysis and associated with important applications in humanities and forensics. In this paper, we propose the use of an intrinsic profile-based verification method that is based on latent semantic indexing (LSI). Our proposed approach is easy-to-follow and language independent. Based on experiments using benchmark corpora from the PAN shared task in author verification, we demonstrate that LSI is both more effective and more stable than latent Dirichlet allocation in this task. Moreover, LSI models are able to outperform existing approaches especially when multiple texts of known authorship are available per verification instance and all documents belong to the same thematic area and genre. We also study several feature types and similarity measures to be combined with the proposed topic models.},
booktitle = {Proceedings of the 10th Hellenic Conference on Artificial Intelligence},
articleno = {20},
numpages = {7},
location = {Patras, Greece},
series = {SETN '18}
}

@inproceedings{10.1145/3336191.3371870,
author = {Chien, Jen-Tzung},
title = {Deep Bayesian Data Mining},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371870},
doi = {10.1145/3336191.3371870},
abstract = {This tutorial addresses the fundamentals and advances in deep Bayesian mining and learning for natural language with ubiquitous applications ranging from speech recognition to document summarization, text classification, text segmentation, information extraction, image caption generation, sentence generation, dialogue control, sentiment classification, recommendation system, question answering and machine translation, to name a few. Traditionally, "deep learning" is taken to be a learning process where the inference or optimization is based on the real-valued deterministic model. The "semantic structure" in words, sentences, entities, actions and documents drawn from a large vocabulary may not be well expressed or correctly optimized in mathematical logic or computer programs. The "distribution function" in discrete or continuous latent variable model for natural language may not be properly decomposed or estimated. This tutorial addresses the fundamentals of statistical models and neural networks, and focus on a series of advanced Bayesian models and deep models including hierarchical Dirichlet process, Chinese restaurant process, hierarchical Pitman-Yor process, Indian buffet process, recurrent neural network (RNN), long short-term memory, sequence-to-sequence model, variational auto-encoder (VAE), generative adversarial network (GAN), attention mechanism, memory-augmented neural network, skip neural network, temporal difference VAE, stochastic neural network, stochastic temporal convolutional network, predictive state neural network, and policy neural network. Enhancing the prior/posterior representation is addressed. We present how these models are connected and why they work for a variety of applications on symbolic and complex patterns in natural language. The variational inference and sampling method are formulated to tackle the optimization for complicated models. The word and sentence embeddings, clustering and co-clustering are merged with linguistic and semantic constraints. A series of case studies, tasks and applications are presented to tackle different issues in deep Bayesian mining, searching, learning and understanding. At last, we will point out a number of directions and outlooks for future studies. This tutorial serves the objectives to introduce novices to major topics within deep Bayesian learning, motivate and explain a topic of emerging importance for data mining and natural language understanding, and present a novel synthesis combining distinct lines of machine learning work.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {865–868},
numpages = {4},
keywords = {information retrieval, data mining, bayesian learning, natural language processing, deep learning},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3097983.3098074,
author = {He, Junxian and Hu, Zhiting and Berg-Kirkpatrick, Taylor and Huang, Ying and Xing, Eric P.},
title = {Efficient Correlated Topic Modeling with Topic Embedding},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098074},
doi = {10.1145/3097983.3098074},
abstract = {Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {225–233},
numpages = {9},
keywords = {scalability, correlated topic models, topic embedding},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3219819.3219987,
author = {Zhang, Bang and Zhang, Lelin and Guo, Ting and Wang, Yang and Chen, Fang},
title = {Simultaneous Urban Region Function Discovery and Popularity Estimation via an Infinite Urbanization Process Model},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219987},
doi = {10.1145/3219819.3219987},
abstract = {Urbanization is a global trend that we have all witnessed in the past decades. It brings us both opportunities and challenges. On the one hand, urban system is one of the most sophisticated social-economic systems that is responsible for efficiently providing supplies meeting the demand of residents in various of domains, e.g., dwelling, education, entertainment, healthcare, etc. On the other hand, significant diversity and inequality exist in the development patterns of urban systems, which makes urban data analysis difficult. Different urban regions often exhibit diverse urbanization patterns and provide distinct urban functions, e.g., commercial and residential areas offer significantly different urban functions. It is desired to develop the data analytic capabilities for discovering the underlying cross-domain urbanization patterns, clustering urban regions based on their function similarity and predicting region popularity in specified domains. Previous studies in the urban data analysis area often just focus on individual domains and rarely consider cross-domain urban development patterns hidden in different urban regions. In this paper, we propose the infinite urbanization process (IUP) model for simultaneous urban region function discovery and region popularity prediction. The IUP model is a generative Bayesian nonparametric process that is capable of describing a potentially infinite number of urbanization patterns. It is developed within the supervised topic modelling framework and is supported by a novel hierarchical spatial distance dependent Bayesian nonparametric prior over the spatial region partition space. The empirical study conducted on the real-world datasets shows promising outcome compared with the state-of-the-art techniques.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2692–2700},
numpages = {9},
keywords = {topic modelling, urban computing, urban function discovery, bayesian nonparametric},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3025171.3025193,
author = {Gil, Yolanda and Garijo, Daniel},
title = {Towards Automating Data Narratives},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025193},
doi = {10.1145/3025171.3025193},
abstract = {We propose a new area of research on automating data narratives. Data narratives are containers of information about computationally generated research findings. They have three major components: 1) A record of events, that describe a new result through a workflow and/or provenance of all the computations executed; 2) Persistent entries for key entities involved for data, software versions, and workflows; 3) A set of narrative accounts that are automatically generated human-consumable renderings of the record and entities and can be included in a paper. Different narrative accounts can be used for different audiences with different content and details, based on the level of interest or expertise of the reader. Data narratives can make science more transparent and reproducible, because they ensure that the text description of the computational experiment reflects with high fidelity what was actually done. Data narratives can be incorporated in papers, either in the methods section or as supplementary materials. We introduce DANA, a prototype that illustrates how to generate data narratives automatically, and describe the information it uses from the computational records. We also present a formative evaluation of our approach and discuss potential uses of automated data narratives.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {565–576},
numpages = {12},
keywords = {data narratives, computational workflows, explanation, wings, semantic workflows, reproducibility},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.5555/3091125.3091466,
author = {Padmanabhan, Divya},
title = {Theoretical Models for Learning from Multiple, Heterogenous and Strategic Agents},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {With the advent of internet enabled hand-held mobile devices, there is a proliferation of user generated data. Often there is a wealth of useful knowledge embedded within this data and machine learning techniques can be used to extract the information. However, as much of this data is user generated, it suffers from subjectivity. Any machine learning techniques used in this context should address the subjectivity in a principled way. We broadly study three problems in the context of learning from multiple agents, (1) Multi-label classification (2) Active Linear Regression (3) Sponsored Search Auctions.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1847–1848},
numpages = {2},
keywords = {sponsored search auctions, crowdsourcing, incentives, multi-label classification},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/3404709.3404754,
author = {Lao, Yingying and Wei, Yilun and Han, Dongli},
title = {A Method for Discovering Local Travel Information from Travel-Blogs},
year = {2020},
isbn = {9781450375337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404709.3404754},
doi = {10.1145/3404709.3404754},
abstract = {Information mining for travel has become a popular research domain in the development of tourism industry nowadays. Currently, studies focusing on the extraction of available travel information from actual tourists' evaluations and reviews on social media sites have been conducted under numerous occasions. In this study, we have proposed another effective yet simple method of discovering local travel-information from traveler's blogs by locating region-sensitive information for the travelers. In this process, regionally restricted words are first extracted based on their frequency of appearances inside the blog. Then, a region-restrictedness score of each blog is calculated through the analysis of the previously extracted regionally restricted phrases. From there, we begin analyzing the content of each blog and classify them into pre-defined categories, using LDA model and word embedding-representations. Through this process, we are able to generate blog recommendations based on our method and see some successful examples as a proof for the effectiveness of our approach.},
booktitle = {Proceedings of the 6th International Conference on Frontiers of Educational Technologies},
pages = {212–216},
numpages = {5},
keywords = {Region restrictedness, LDA model, Travel information, Region-restrictedness score, Feature words},
location = {Tokyo, Japan},
series = {ICFET '20}
}

@inproceedings{10.1109/ICSE-C.2017.175,
author = {Rau, Andreas},
title = {Topic-Driven Testing},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.175},
doi = {10.1109/ICSE-C.2017.175},
abstract = {When manually testing Web sites humans can go with vague, yet general instructions, such as "add the product to shopping cart and proceed to checkout". Can we teach a robot to follow such instructions as well?In this paper I present a novel model, called semantic usage patterns which allows us to capture the general topics behind the individual steps of interactions. These models can be extracted from existing test descriptions be they in natural language or in form of system tests. Those usage patterns can be applied even on applications they were not designed for. They allow to test applications automatically in order to identify behavioral anomalies in the application model or detect missing functionalities.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {409–412},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1145/3167110,
author = {Mattis, Toni and Rein, Patrick and Ramson, Stefan and Lincke, Jens and Hirschfeld, Robert},
title = {Towards Concept-Aware Programming Environments for Guiding Software Modularity},
year = {2017},
isbn = {9781450355223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167110},
doi = {10.1145/3167110},
abstract = {To design and implement a program, programmers choose analogies and metaphors to explain and understand programmatic concepts. In source code, they manifest themselves as a particular choice of names. During program comprehension, reading such names is an important starting point to understand the meaning of modules and guide the exploration process. On the one hand, understanding a program in depth by looking for names that suggest a particular analogy can be a time-consuming process. On the other hand, a lack of awareness which concepts are present and which analogies have been chosen can lead to modularity issues, such as redundancy and architectural drift if concepts are misaligned with respect to the current module decomposition. In this work-in-progress paper, we propose to integrate first-class concepts into the programming environment. We assign meaning to names by labeling them with a color corresponding to the metaphor or analogy this name was derived from. We hypothesize that aggregating labels upwards along the module hierarchy helps to understand how concepts are distributed across the program, collecting names belonging to a specific concept helps programmers to recognize which metaphor has been chosen, and presenting relations between concepts can summarize complex interactions between program parts. We argue that continuous feedback and awareness of how names are grouped into concepts and where they are located can help preventing modularity issues and ease program comprehension. As a first step towards an implementation, we define criteria that help to detect names belonging to the same concept. We then investigate how techniques from natural language processing can be re-used and modified to compute an initial concept allocation with respect to these criteria. Eventually, we show design sketches how we plan to arrange and present concepts to programmers through tools, and what kind of information they can provide to help programmers make informed implementation decisions.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Programming Experience},
pages = {36–45},
numpages = {10},
keywords = {program comprehension, analogies, integrated development environments, topic models, first-class concepts},
location = {Vancouver, BC, Canada},
series = {PX/17.2}
}

@inproceedings{10.1145/3490354.3494388,
author = {Pagliaro, Cynthia and Mehta, Dhagash and Shiao, Han-Tai and Wang, Shaofei and Xiong, Luwei},
title = {Investor Behavior Modeling by Analyzing Financial Advisor Notes: A Machine Learning Perspective},
year = {2021},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494388},
doi = {10.1145/3490354.3494388},
abstract = {Modeling investor behavior is crucial to identifying behavioral coaching opportunities for financial advisors. With the help of natural language processing (NLP) we analyze an unstructured (textual) dataset of financial advisors' summary notes, taken after every investor conversation, to gain first ever insights into advisor-investor interactions. These insights are used to predict investor needs during adverse market conditions; thus allowing advisors to coach investors and help avoid inappropriate financial decision-making. First, we perform topic modeling to gain insight into the emerging topics and trends. Based on this insight, we construct a supervised classification model to predict the probability that an advised investor will move assets to cash during volatile market periods. To the best of our knowledge, ours is the first work on exploring the advisor-investor relationship using unstructured data. This work may have far-reaching implications for both traditional and emerging financial advisory service models like robo-advising.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {23},
numpages = {8},
keywords = {machine learning, financial advice, natural language processing, investor's modeling},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3493700.3493727,
author = {Dixit, Vini},
title = {GCFI++: Embedding and Frequent Itemset Based Incremental Hierarchical Clustering with Labels and Outliers},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493727},
doi = {10.1145/3493700.3493727},
abstract = {Customer support is an essential part of any company to ensure customer satisfaction and business growth. The responsibility and scale get even higher for enterprise companies like Freshworks that build customer support platforms for its enterprise customers from multiple and previously unseen domains. A ticket may carry issues never seen before, therefore static, and predefined problem labels will not always work. These predefined problem labels are often created manually by admins. For a large customer account, roughly 10-35% of the problem labels could get discovered by this manual process and only 5-9% of the available open tickets could get linked to a problem label. So, there is a need for a system for a new incoming ticket. The system should either automatically detect an existing problem topic with its possible suggestions or systematically generate new problem topic(s) for its assignment. Our system is based on an incremental approach to mine ticket data streams and assigns them to hierarchical soft clusters with auto-generated problem/issue topics. Our system achieves an improvement in ticket assignments up to 97%. To compare with recent clustering advancements, we’ve also evaluated our approach on a public dataset [9] and achieved comparable results. Source code and results of the evaluation are available in detail at: https://github.com/vinidixit/hierarchical-labelled-clustering-evaluation.},
booktitle = {5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {135–143},
numpages = {9},
keywords = {Hierarchical Labeling, Microtext, Outlier Detection, Frequent Itemset Embeddings, Hierarchical Clustering, Incremental Systems, Customer Support Systems, Topic Detection},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.1145/3459637.3482485,
author = {Xu, Siyong and Yang, Cheng and Shi, Chuan and Fang, Yuan and Guo, Yuxin and Yang, Tianchi and Zhang, Luhao and Hu, Maodi},
title = {Topic-Aware Heterogeneous Graph Neural Network for Link Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482485},
doi = {10.1145/3459637.3482485},
abstract = {Heterogeneous graphs (HGs), consisting of multiple types of nodes and links, can characterize a variety of real-world complex systems. Recently, heterogeneous graph neural networks (HGNNs), as a powerful graph embedding method to aggregate heterogeneous structure and attribute information, has earned a lot of attention. Despite the ability of HGNNs in capturing rich semantics which reveal different aspects of nodes, they still stay at a coarse-grained level which simply exploits structural characteristics. In fact, rich unstructured text content of nodes also carries latent but more fine-grained semantics arising from multi-facet topic-aware factors, which fundamentally manifest why nodes of different types would connect and form a specific heterogeneous structure. However, little effort has been devoted to factorizing them. In this paper, we propose a Topic-aware Heterogeneous Graph Neural Network, named THGNN, to hierarchically mine topic-aware semantics for learning multi-facet node representations for link prediction in HGs. Specifically, our model mainly applies an alternating two-step aggregation mechanism including intra-metapath decomposition and inter-metapath mergence, which can distinctively aggregate rich heterogeneous information according to the inferential topic-aware factors and preserve hierarchical semantics. Furthermore, a topic prior guidance module is also designed to keep the quality of multi-facet topic-aware embeddings relying on the global knowledge from unstructured text content in HGs. It helps to simultaneously improve both performance and interpretability. Experimental results on three real-world HGs demonstrate that our proposed model can effectively outperform the state-of-the-art methods in the link prediction task, and show the potential interpretability of learnt multi-facet topic-aware representations.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2261–2270},
numpages = {10},
keywords = {graph neural networks, heterogeneous graph, link prediction, representation learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3083187.3083194,
author = {Yu, Chenguang and Ding, Hao and Cao, Houwei and Liu, Yong and Yang, Can},
title = {Follow Me: Personalized IPTV Channel Switching Guide},
year = {2017},
isbn = {9781450350020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083187.3083194},
doi = {10.1145/3083187.3083194},
abstract = {Compared with the traditional television services, Internet Protocol TV (IPTV) can provide far more TV channels to end users. However, it may also make users feel confused even painful to find channels of their interests from a large number of them. In this paper, using a large IPTV trace, we analyze user channel-switching behaviors to understand when, why and how they switch channels. Based on user behavior analysis, we develop several base and fusion recommender systems that generate in real-time a short list of channels for users to consider whenever they want to switch channels. Evaluation on the IPTV trace demonstrates that our recommender systems can achieve up to 45 percent hit ratio with only three candidate channels. Our recommender systems only need access to user channel watching sequences, and can be easily adopted by IPTV systems with low data and computation overheads.},
booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
pages = {147–157},
numpages = {11},
keywords = {Fusion Method, IPTV, Realtime Recommendation, Recommender System, Channel Switching},
location = {Taipei, Taiwan},
series = {MMSys'17}
}

@inproceedings{10.1145/3340531.3412878,
author = {Hofst\"{a}tter, Sebastian and Zlabinger, Markus and Sertkan, Mete and Schr\"{o}der, Michael and Hanbury, Allan},
title = {Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412878},
doi = {10.1145/3340531.3412878},
abstract = {There are many existing retrieval and question answering datasets. However, most of them either focus on ranked list evaluation or single-candidate question answering. This divide makes it challenging to properly evaluate approaches concerned with ranking documents and providing snippets or answers for a given query. In this work, we present FiRA: a novel dataset of Fine-Grained Relevance Annotations. We extend the ranked retrieval annotations of the Deep Learning track of TREC 2019 with passage and word level graded relevance annotations for all relevant documents. We use our newly created data to study the distribution of relevance in long documents, as well as the attention of annotators to specific positions of the text. As an example, we evaluate the recently introduced TKL document ranking model. We find that although TKL exhibits state-of-the-art retrieval results for long documents, it misses many relevant passages.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3031–3038},
numpages = {8},
keywords = {fine-grained annotations, position bias, relevance distribution, word-level relevance},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.5555/3280489.3280512,
author = {Glass, Michael and Jagvaral, Yesukhei and Bouman, Nathaniel and Graham, Emily and Kalafatis, Stamatina and Arndt, Lindsey and Desjarlais, Melissa and Kim, Jung Hee and Bryant, Kelvin},
title = {Problem-Independent Text Analytics for Real-Time Judgment of CSCL Typed-Chat Dialogues},
year = {2018},
issue_date = {October 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {1},
issn = {1937-4771},
abstract = {This experiment trained text classifiers to categorize some of the conversational behaviors that might be indicative of productive online student collaborative exercises. COMPS project exercises have students working together via typed chat, solving problems in small groups. Instructors oversee these conversations. Toward the goal of aiding the instructor in locating which conversations could benefit from intervention, this experiment applies text analytics to recognize when students are using substantive vocabulary, and when they are agreeing or disagreeing with other students. The text classifiers were built from student conversations from two different schools solving problems in two different subject areas, using a vocabulary of only the more common English words.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {145–154},
numpages = {10}
}

@inproceedings{10.1145/3290605.3300807,
author = {Chandrasegaran, Senthil and Bryan, Chris and Shidara, Hidekazu and Chuang, Tung-Yen and Ma, Kwan-Liu},
title = {TalkTraces: Real-Time Capture and Visualization of Verbal Content in Meetings},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300807},
doi = {10.1145/3290605.3300807},
abstract = {Group Support Systems provide ways to review and edit shared content during meetings, but typically require participants to explicitly generate the content. Recent advances in speech-to-text conversion and language processing now make it possible to automatically record and review spoken information. We present the iterative design and evaluation of TalkTraces, a real-time visualization that helps teams identify themes in their discussions and obtain a sense of agenda items covered. We use topic modeling to identify themes within the discussions and word embeddings to compute the discussion "relatedness" to items in the meeting agenda. We evaluate TalkTraces iteratively: we first conduct a comparative between-groups study between two teams using TalkTraces and two teams using traditional notes, over four sessions. We translate the findings into changes in the interface, further evaluated by one team over four sessions. Based on our findings, we discuss design implications for real-time displays of discussion content.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {collaboration, real-time visualization, streaming data},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/3241741,
author = {Smirnova, Alisa and Cudr\'{e}-Mauroux, Philippe},
title = {Relation Extraction Using Distant Supervision: A Survey},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3241741},
doi = {10.1145/3241741},
abstract = {Relation extraction is a subtask of information extraction where semantic relationships are extracted from natural language text and then classified. In essence, it allows us to acquire structured knowledge from unstructured text. In this article, we present a survey of relation extraction methods that leverage pre-existing structured or semi-structured data to guide the extraction process. We introduce a taxonomy of existing methods and describe distant supervision approaches in detail. We describe, in addition, the evaluation methodologies and the datasets commonly used for quality assessment. Finally, we give a high-level outlook on the field, highlighting open problems as well as the most promising research directions.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {106},
numpages = {35},
keywords = {knowledge graph, Relation extraction, distant supervision}
}

@inproceedings{10.1145/3387940.3392207,
author = {Peirs, Tom and Westra, Freark and Molenaar, Sabine and Jansen, Slinger},
title = {The Influence of Technical Variety in Software Ecosystems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392207},
doi = {10.1145/3387940.3392207},
abstract = {There is a lack of empirical evidence on software ecosystem health metrics, and a need for operationalizable metrics that describe software ecosystem characteristics. This study unveils a new approach for measuring technical variety concisely. Studies show that a high variety opens up new opportunities and thus, better niche creation, and ultimately, improves software ecosystem health. Four different ecosystems are evaluated, and compared. Variety is measured in relation to robustness, and productivity metrics of the ecosystem to uncover the influence of technical variety on software ecosystems. Technical variety indicates a positive correlation with robustness, however acceptance of this statement is not confirmed with certainty due to a weak relation. Furthermore, significant relations indicate differences between ecosystem types.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {725–732},
numpages = {8},
keywords = {Software Ecosystems, Technical Variety, Niche Creation, Software Ecosystem Health},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3318464.3386126,
author = {Liberty, Edo and Karnin, Zohar and Xiang, Bing and Rouesnel, Laurence and Coskun, Baris and Nallapati, Ramesh and Delgado, Julio and Sadoughi, Amir and Astashonok, Yury and Das, Piali and Balioglu, Can and Chakravarty, Saswata and Jha, Madhav and Gautier, Philip and Arpin, David and Januschowski, Tim and Flunkert, Valentin and Wang, Yuyang and Gasthaus, Jan and Stella, Lorenzo and Rangapuram, Syama and Salinas, David and Schelter, Sebastian and Smola, Alex},
title = {Elastic Machine Learning Algorithms in Amazon SageMaker},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386126},
doi = {10.1145/3318464.3386126},
abstract = {There is a large body of research on scalable machine learning (ML). Nevertheless, training ML models on large, continuously evolving datasets is still a difficult and costly undertaking for many companies and institutions. We discuss such challenges and derive requirements for an industrial-scale ML platform. Next, we describe the computational model behind Amazon SageMaker, which is designed to meet such challenges. SageMaker is an ML platform provided as part of Amazon Web Services (AWS), and supports incremental training, resumable and elastic learning as well as automatic hyperparameter optimization. We detail how to adapt several popular ML algorithms to its computational model. Finally, we present an experimental evaluation on large datasets, comparing SageMaker to several scalable, JVM-based implementations of ML algorithms, which we significantly outperform with regard to computation time and cost.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {731–737},
numpages = {7},
keywords = {elastic machine learning, scalable machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/3491260,
author = {Chen, Chung-Kuan and Lin, Si-Chen and Huang, Szu-Chun and Chu, Yung-Tien and Lei, Chin-Laung and Huang, Chun-Ying},
title = {Building Machine Learning-Based Threat Hunting System from Scratch},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2692-1626},
url = {https://doi.org/10.1145/3491260},
doi = {10.1145/3491260},
abstract = {Machine learning has been widely used for solving challenging problems in diverse areas. However, to the best of our knowledge, seldom literature has discussed in-depth how machine learning approaches can be used effectively to “hunt” (identify) threats, especially advanced persistent threats (APTs), in a monitored environment. In this study, we share our past experiences in building machine learning-based threat-hunting models. Several challenges must be considered when a security team attempts to build such models. These challenges include (1)&nbsp;weak signal, (2)&nbsp;imbalanced data sets, (3)&nbsp;lack of high-quality labels, and (4)&nbsp;no storyline. In this study, we propose Fuchikoma and APTEmu to demonstrate how we tackle the above-mentioned challenges. The former is a proof of concept system for demonstrating the ideas behind autonomous threat-hunting. It is a machine learning-based anomaly detection and threat hunting system which leveragesnatural language processing (NLP) and graph algorithms. The latter is an APT emulator, which emulates the behavior of a well-known APT called APT3, which is the target used in the first round of MITRE ATT&amp;CK Evaluations. APTEmu generates attacks on Windows machines in a virtualized environment, and the captured system events can be further used to train and enhance Fuchikoma’s capabilities. We illustrate the steps and experiments we used to build the models, discuss each model’s effectiveness and limitations of each model, and propose countermeasures and solutions to improve the models. Our evaluation results show that machine learning algorithms can effectively assist threat hunting processes and significantly reduce security analysts’ efforts. Fuchikoma correctly identifies malicious commands and achieves high performance in terms of over 80% True Positive Rate and True Negative Rate and over 60% F3. We believe our proposed approaches provide valuable experiences in the area and shed light on automated threat-hunting research.},
journal = {Digital Threats},
month = {sep},
articleno = {20},
numpages = {21},
keywords = {APT3, machine learning, threat hunting}
}

@article{10.1145/3375192,
author = {Freeman, Cole and Alhoori, Hamed and Shahzad, Murtuza},
title = {Measuring the Diversity of Facebook Reactions to Research},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {GROUP},
url = {https://doi.org/10.1145/3375192},
doi = {10.1145/3375192},
abstract = {Online and in the real world, communities are bonded together by emotional consensus around core issues. Emotional responses to scientific findings often play a pivotal role in these core issues. When there is too much diversity of opinion on topics of science, emotions flare up and give rise to conflict. This conflict threatens positive outcomes for research. Emotions have the power to shape how people process new information. They can color the public's understanding of science, motivate policy positions, even change lives. And yet little work has been done to evaluate the public's emotional response to science using quantitative methods. In this paper, we use a dataset of responses to scholarly articles on Facebook to analyze the dynamics of emotional valence, intensity, and diversity. We present a novel way of weighting click-based reactions that increases their comprehensibility, and use these weighted reactions to develop new metrics of aggregate emotional responses. We use our metrics along with LDA topic models and statistical testing to investigate how users' emotional responses differ from one scientific topic to another. We find that research articles related to gender, genetics, or agricultural/environmental sciences elicit significantly different emotional responses from users than other research topics. We also find that there is generally a positive response to scientific research on Facebook, and that articles generating a positive emotional response are more likely to be widely shared---a conclusion that contradicts previous studies of other social media platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {12},
numpages = {17},
keywords = {facebook reactions, social computing, text analytics, social media, web mining, altmetrics, click-based reactions, emotion detection, emotions}
}

@inproceedings{10.1109/ICPC.2017.28,
author = {Zhang, Tao and Chen, Jiachi and Jiang, He and Luo, Xiapu and Xia, Xin},
title = {Bug Report Enrichment with Application of Automated Fixer Recommendation},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.28},
doi = {10.1109/ICPC.2017.28},
abstract = {For large open source projects (e.g., Eclipse, Mozilla), developers usually utilize bug reports to facilitate software maintenance tasks such as fixer assignment. However, there are a large portion of short reports in bug repositories. We find that 78.1% of bug reports only include less than 100 words in Eclipse and require bug fixers to spend more time on resolving them due to limited informative contents. To address this problem, in this paper, we propose a novel approach to enrich bug reports. Concretely, we design a sentence ranking algorithm based on a new textual similarity metric to select the proper contents for bug report enrichment. For the enriched bug reports, we conduct a user study to assess whether the additional sentences can provide further help to fixer assignment. Moreover, we assess whether the enriched versions can improve the performance of automated fixer recommendation. In particular, we perform three popular automated fixer recommendation approaches on the enriched bug reports of Eclipse, Mozilla, and GNU Compiler Collection (GCC). The experimental results show that enriched bug reports improve the average F-measure scores of the automated fixer recommendation approaches by up to 10% for DREX, 13.37% for DRETOM, and 8% for DevRec when top-10 bug fixers are recommended.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {230–240},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3328778.3366817,
author = {Karbasian, Habib and Johri, Aditya},
title = {Insights for Curriculum Development: Identifying Emerging Data Science Topics through Analysis of Q&amp;A Communities},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366817},
doi = {10.1145/3328778.3366817},
abstract = {Updating curricula in new computer science domains is a critical challenge faced by many instructors and programs. In this paper we present an approach for identifying emerging topics and issues in Data Science by using Question and Answer (Q&amp;A) sites as a resource. Q&amp;A sites provide a useful online platform for discussion of topics and through the sharing of information they become a valuable corpus of knowledge. We applied latent Dirichlet allocation (LDA), a statistical topic modeling technique, to analyze data science related threads from from two popular Q&amp;A communities "Stack Exchange and Reddit". We uncovered both important topics as well as useful examples that can be incorporated into teaching. In addition to technical topics, our analysis also identified topics related to professional development. We believe that approaches such as these are critical in order to update curriculum and bridge the workplace-school divide in teaching of newer topics such as data science. Given the pace of technical development and frequent changes in the field, this is an inventive and effective method to keep teaching up to date. We also discuss the limitations of this approach whereby topics of importance such as data ethics are largely missing from online discussions.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {192–198},
numpages = {7},
keywords = {curriculum development, text mining, topic modeling, online q&amp;a platforms, reddit, stackexchange},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@article{10.1007/s00779-018-01192-y,
author = {Feng, Zhang},
title = {Hot News Mining and Public Opinion Guidance Analysis Based on Sentiment Computing in Network Social Media},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-018-01192-y},
doi = {10.1007/s00779-018-01192-y},
abstract = {The texts of social media event have feathers of massive-sparse, dynamic-heterogeneous, and obscure-vague, which increase the difficulty of event emotion computing. Aiming at the problem, we construct the dictionary supervised emotion computing model, which can be applied in hot news mining and public opinion guidance analysis based on sentiment computing in network social media. The text words and labels are used as the input of the models, and the profile distribution and emotion distribution of the texts, the word distribution of the profiles, and emotions are output by the models. In addition, the words with definite emotion are used as the constraint condition of the model to enhance the accuracy of text emotion calculation. Our proposed algorithm can express the emotion of the text by using the words and labels from labeled texts, and the emotion words value is calculated through a finite iteration of the network. We also make use of the word emotion in the basic word emotion dictionary to modify the network and then recompute the word emotion, which effectively overcomes the problem of emotion uncertainty of the traditional methods. Experiments show that the accuracy of our model is generally higher than that of ETM, MSTM, and SLTM. Therefore, our proposed method is effective and feasible.},
journal = {Personal Ubiquitous Comput.},
month = {jul},
pages = {373–381},
numpages = {9},
keywords = {Hot news mining, Sentiment computing, Social media, Opinion guidance, Profile number, ESmotion dictionary}
}

@inproceedings{10.1145/3097983.3098200,
author = {Ahmed, Amr and Long, James and Silva, Daniel and Wang, Yuan},
title = {A Practical Algorithm for Solving the Incoherence Problem of Topic Models In Industrial Applications},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098200},
doi = {10.1145/3097983.3098200},
abstract = {Topic models are often applied in industrial settings to discover user profiles from activity logs where documents correspond to users and words to complex objects such as web sites and installed apps. Standard topic models ignore the content-based similarity structure between these objects largely because of the inability of the Dirichlet prior to capture such side information of word-word correlation. Several approaches were proposed to replace the Dirichlet prior with more expressive alternatives. However, this added expressivity comes with a heavy premium: inference becomes intractable and sparsity is lost which renders these alternatives not suitable for industrial scale applications. In this paper we take a radically different approach to incorporating word-word correlation in topic models by applying this side information at the posterior level rather than at the prior level. We show that this choice preserves sparsity and results in a graph-based sampler for LDA whose computational complexity is asymptotically on bar with the state of the art Alias base sampler for LDA cite{aliasLDA}. We illustrate the efficacy of our approach over real industrial datasets that span up to billion of users, tens of millions of words and thousands of topics. To the best of our knowledge, our approach provides the first practical and scalable solution to this important problem.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1713–1721},
numpages = {9},
keywords = {user modeling, topic models, interpretable models, knowledge representation, latent variable models, big data},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3396452.3396460,
author = {Zhang, Yong and Chen, Fen and Zhang, Wufeng and Zuo, Haoyang and Yu, Fangyuan},
title = {Keywords Extraction Based on Word2Vec and TextRank},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396460},
doi = {10.1145/3396452.3396460},
abstract = {In order to improve the performance of keyword extraction by enhancing the semantic representations of documents, we propose a method of keyword extraction which exploits the document's internal semantic information and the semantic representations of words pre-trained by massive external documents. Firstly, we utilize the deep learning tool Word2Vec to characterize the external document information, and evaluate the similarity between the words by the cosine distance, thus we obtain the semantic information between words in the external documents. Then, the word-to-word similarity is used to replace the probability transfer matrix in the TextRank of word graph of the target document. At the same time, the information of the title and the abstract of the internal document are exploited to construct the words' semantic graph for keyword extraction. The experiments select the related academic paper data from AMiner as experimental data set. The experimental results show that our method outperforms the TextRank algorithm and the precision, recall and F-score of the five keywords are increased by 28.60%, 10.70% and 12.90% respectively compared to the single TextRank algorithm.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {37–42},
numpages = {6},
keywords = {word map, TextRank, word2vec, keyword extraction},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1145/3020165.3022172,
author = {Mitsui, Matthew Ryan},
title = {A Generative Framework to Query Recommendation and Evaluation},
year = {2017},
isbn = {9781450346771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020165.3022172},
doi = {10.1145/3020165.3022172},
abstract = {In practice, query recommenders in Web search typically recommend queries directly from a query log or iteratively refine a user's current context to make recommendations. These approaches either limit themselves to queries in the log or do not take necessary exploratory leaps in their recommendations. Moreover, they do not directly incorporate the encompassing, driving information needs and tasks. The author first shows that user queries may not necessarily be the best to use for recommendations and moreover proposes a framework for generating novel queries for query recommendation, using an approximation of need.},
booktitle = {Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval},
pages = {407–409},
numpages = {3},
keywords = {diversity, user simulations, search session analysis, query recommendation},
location = {Oslo, Norway},
series = {CHIIR '17}
}

@article{10.1007/s00778-019-00545-0,
author = {Chen, Xinhong and Li, Qing},
title = {Event Modeling and Mining: A Long Journey toward Explainable Events},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00545-0},
doi = {10.1007/s00778-019-00545-0},
abstract = {Recently, research on event management has redrawn much attention and made great progress. As the core tasks of event management, event modeling and mining are essential for accessing and utilizing events effectively. In this survey, we provide a detailed review of event modeling and event mining. Based on a general definition, different characteristics of events are described, along with the associated challenges. Then, we define four forms of events in order to better classify currently available but somewhat confusing event types; we also compare different event representation and relationship analysis techniques used for different forms of events. Finally, we discuss several pending issues and application-specific challenges which also shed light on future research directions.},
journal = {The VLDB Journal},
month = {jan},
pages = {459–482},
numpages = {24},
keywords = {Event modeling, Event management, Event relation analysis, Event representation, Event mining}
}

@inproceedings{10.1145/3510003.3510201,
author = {Tushev, Miroslav and Ebrahimi, Fahimeh and Mahmoud, Anas},
title = {Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510201},
doi = {10.1145/3510003.3510201},
abstract = {Mobile application (app) reviews contain valuable information for app developers. A plethora of supervised and unsupervised techniques have been proposed in the literature to synthesize useful user feedback from app reviews. However, traditional supervised classification algorithms require extensive manual effort to label ground truth data, while unsupervised text mining techniques, such as topic models, often produce suboptimal results due to the sparsity of useful information in the reviews. To overcome these limitations, in this paper, we propose a fully automatic and unsupervised approach for extracting useful information from mobile app reviews. The proposed approach is based on keyATM, a keyword-assisted approach for generating topic models. keyATM overcomes the problem of data sparsity by using seeding keywords extracted directly from the review corpus. These keywords are then used to generate meaningful domain-specific topics. Our approach is evaluated over two datasets of mobile app reviews sampled from the domains of Investing and Food Delivery apps. The results show that our approach produces significantly more coherent topics than traditional topic modeling techniques.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {762–773},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3361217,
author = {Lv, Pengtao and Meng, Xiangwu and Zhang, Yujie},
title = {BoRe: Adapting to Reader Consumption Behavior Instability for News Recommendation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361217},
doi = {10.1145/3361217},
abstract = {News recommendation has become an essential way to help readers discover interesting stories. While a growing line of research has focused on modeling reading preferences for news recommendation, they neglect the instability of reader consumption behaviors, i.e., consumption behaviors of readers may be influenced by other factors in addition to user interests, which degrades the recommendation effectiveness of existing methods. In this article, we propose a probabilistic generative model, BoRe, where user interests and crowd effects are used to adapt to the instability of reader consumption behaviors, and reading sequences are utilized to adapt user interests evolving over time. Further, the extreme sparsity problem in the domain of news severely hinders accurately modeling user interests and reading sequences, which discounts BoRe’s ability to adapt to the instability. Accordingly, we leverage domain-specific features to model user interests in the situation of extreme sparsity. Meanwhile, we consider groups of users instead of individuals to capture reading sequences. Besides, we study how to reduce the computation to allow online application. Extensive experiments have been conducted to evaluate the effectiveness and efficiency of BoRe on real-world datasets. The experimental results show the superiority of BoRe, compared with the state-of-the-art competing methods.},
journal = {ACM Trans. Inf. Syst.},
month = {oct},
articleno = {3},
numpages = {33},
keywords = {reader consumption behavior, domain-specific feature, probabilistic generative model, News recommendation, instability}
}

@inproceedings{10.1145/3387940.3391463,
author = {Omri, Safa and Sinz, Carsten},
title = {Deep Learning for Software Defect Prediction: A Survey},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391463},
doi = {10.1145/3387940.3391463},
abstract = {Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {209–214},
numpages = {6},
keywords = {deep learning, software quality assurance, machine learning, software testing, software defect prediction},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3546157.3546164,
author = {Mohseni, Maryam and Maher, Mary Lou},
title = {A Framework for Exploring Computational Models of Novelty in Unstructured Text},
year = {2022},
isbn = {9781450396257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546157.3546164},
doi = {10.1145/3546157.3546164},
abstract = {Novelty modeling in unstructured text data is a research topic within the Natural Language Processing (NLP) Community. Effective novelty models can play a key role in providing relevant and interesting content to the users which is the central goal in many applications including education and recommender systems. This paper presents a framework for comparing different approaches and applications of computational models of novelty in unstructured text data. We focus on computational models that apply methods such as natural language processing and information theory. The framework provides an ontology for computational novelty with respect to the source of text data, methods for representing the data, and models for measuring novelty. We explore the value of the framework by applying it to research on computational novelty in news articles, research publications, books, and recipes. This framework is independent of the type of data in the items and can be used as a tool for researchers to study, compare, and extend existing computational novelty models and applications.},
booktitle = {Proceedings of the 6th International Conference on Information System and Data Mining},
pages = {36–45},
numpages = {10},
keywords = {Unstructured text, NLP, Recommender systems, Surprise, Computational models of novelty},
location = {Silicon Valley, CA, USA},
series = {ICISDM '22}
}

@inproceedings{10.1145/3018661.3018686,
author = {Ren, Zhaochun and Liang, Shangsong and Li, Piji and Wang, Shuaiqiang and de Rijke, Maarten},
title = {Social Collaborative Viewpoint Regression with Explainable Recommendations},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018686},
doi = {10.1145/3018661.3018686},
abstract = {A recommendation is called explainable if it not only predicts a numerical rating for an item, but also generates explanations for users' preferences. Most existing methods for explainable recommendation apply topic models to analyze user reviews to provide descriptions along with the recommendations they produce. So far, such methods have neglected user opinions and influences from social relations as a source of information for recommendations, even though these are known to improve the rating prediction.In this paper, we propose a latent variable model, called social collaborative viewpoint regression (sCVR), for predicting item ratings based on user opinions and social relations. To this end, we use so-called viewpoints, represented as tuples of a concept, topic, and a sentiment label from both user reviews and trusted social relations. In addition, such viewpoints can be used as explanations. We apply a Gibbs EM sampler to infer posterior distributions of sCVR. Experiments conducted on three large benchmark datasets show the effectiveness of our proposed method for predicting item ratings and for generating explanations.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {485–494},
numpages = {10},
keywords = {trusted social relations, recommender systems, user comment analysis, topic modeling},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3175587.3175600,
author = {Zuorba, Hussain D. and Olan, Celine Louise O. and Cantara, Anthonette D.},
title = {A Framework for Identifying Excessive Sadness in Students through Twitter and Facebook in the Philippines},
year = {2017},
isbn = {9781450353823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175587.3175600},
doi = {10.1145/3175587.3175600},
abstract = {Natural Language Processing (NLP) can be used to identify a person's sentiments or emotions. Depression is one sentiment that researchers have tried to identify through Natural Language Processing with little success. Depression is an episode of sadness or apathy, along with other symptoms, that lasts for at least two consecutive weeks. Depression is especially bad with students due to the amount of stress and anxiety they have to go through. While depression is very difficult to identify and treat, excessive sadness, one of the symptoms that may lead to depression can be identified early and appropriate action can be taken. The Philippines is known to have the highest depression count in Southeast Asia. Data Mining was performed on Twitter and Facebook, and with the use of Natural Language Processing (NLP) and Sentiment Analysis, a logistics regression model was devised with the use of emotion Lexicons to identify the user's state. The Latent Dirichlet Allocation (LDA) was then used to identify important topics of each user and cluster the data and make sense out of each user's excessive sadness.},
booktitle = {Proceedings of the International Conference on Bioinformatics Research and Applications 2017},
pages = {52–56},
numpages = {5},
keywords = {logistics regression, sentiment analysis, latent dirichlet allocation, depression, Natural language processing},
location = {Barcelona, Spain},
series = {ICBRA 2017}
}

@inproceedings{10.1145/3206025.3206036,
author = {Xue, Jianfei and Eguchi, Koji},
title = {Supervised Nonparametric Multimodal Topic Modeling Methods for Multi-Class Video Classification},
year = {2018},
isbn = {9781450350464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206025.3206036},
doi = {10.1145/3206025.3206036},
abstract = {Nonparametric topic models such as hierarchical Dirichlet processes (HDP) have been attracting more and more attentions for multimedia data analysis. However, the existing models for multimedia data are unsupervised ones that purely cluster semantically or characteristically related features into a specific latent topic without considering side information such as class information. In this paper, we present a novel supervised sequential symmetric correspondence HDP (Sup-SSC-HDP) model for multi-class video classification, where the empirical topic frequencies learned from multimodal video data are modeled as a predictor of video class. Qualitative and quantitative assessments demonstrate the effectiveness of Sup-SSC-HDP.},
booktitle = {Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval},
pages = {370–378},
numpages = {9},
keywords = {bayesian nonparametric methods, hierarchical dirichlet processes, multimedia content classification, topic models},
location = {Yokohama, Japan},
series = {ICMR '18}
}

@inproceedings{10.1145/3308558.3313757,
author = {Kawamae, Noriaki},
title = {Topic Structure-Aware Neural Language Model: Unified Language Model That Maintains Word and Topic Ordering by Their Embedded Representations},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313757},
doi = {10.1145/3308558.3313757},
abstract = {Our goal is to exploit a unified language model so as to explain the generative process of documents precisely in view of their semantic and topic structures. Because various methods model documents in disparate ways, we are motivated by the expectation that coordinating these methods will allow us to achieve this goal more efficiently than using them in isolation; we combine topic models, embedding models, and neural language models. As we focus on the fact that topic models can be shared among, and indeed complement embedding models and neural language models, we propose Word and topic 2 vec (Wat2vec), and Topic Structure-Aware Neural Language Model (TSANL). Wat2vec uses topics as global semantic information and local semantic information as embedding representations of topics and words, and embeds both words and topics in the same space. TSANL uses recurrent neural networks to capture long-range dependencies over topics and words. Since existing topic models demand time consuming learning and have poor scalability, both due to breaking the document?s structure such as order of words and topics, TSANL maintains the orders of words and topics as phrases and segments, respectively. TSANL reduces the calculation cost and required memory by feeding topic recurrent neural networks, and topic specific word networks with these embedding representations. Experiments show that TSANL maintains both segments and topical phrases, and so enhances previous models.},
booktitle = {The World Wide Web Conference},
pages = {2900–2906},
numpages = {7},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.1145/3173044,
author = {Peng, Min and Zhu, Jiahui and Wang, Hua and Li, Xuhui and Zhang, Yanchun and Zhang, Xiuzhen and Tian, Gang},
title = {Mining Event-Oriented Topics in Microblog Stream with Unsupervised Multi-View Hierarchical Embedding},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3173044},
doi = {10.1145/3173044},
abstract = {This article presents an unsupervised multi-view hierarchical embedding (UMHE) framework to sufficiently reveal the intrinsic topical knowledge in social events. Event-oriented topics are highly related to such events as it can provide explicit descriptions of what have happened in social community. In many real-world cases, however, it is difficult to include all attributes of microblogs, more often, textual aspects only are available. Traditional topic modelling methods have failed to generate event-oriented topics with the textual aspects, since the inherent relations between topics are often overlooked in these methods. Meanwhile, the metrics in original word vocabulary space might not effectively capture semantic distances. Our UMHE framework overcomes the severe information deficiency and poor feature representation. The UMHE first develops a multi-view Bayesian rose tree to preliminarily generate prior knowledge for latent topics and their relations. With such prior knowledge, we design an unsupervised translation-based hierarchical embedding method to make a better representation of these latent topics. By applying self-adaptive spectral clustering on the embedding space and the original space concomitantly, we eventually extract event-oriented topics in word distributions to express social events. Our framework is purely data-driven and unsupervised, without any external knowledge. Experimental results on TREC Tweets2011 dataset and Sina Weibo dataset demonstrate that the UMHE framework can construct hierarchical structure with high fitness, but also yield topic embeddings with salient semantics; therefore, it can derive event-oriented topics with meaningful descriptions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {38},
numpages = {26},
keywords = {unsupervised learning, Event-oriented topic, multi-view hierarchical embedding, Bayesian rose tree}
}

@article{10.1145/3389684,
author = {Alagheband, Mahdi R. and Mashatan, Atefeh and Zihayat, Morteza},
title = {Time-Based Gap Analysis of Cybersecurity Trends in Academic and Digital Media},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3389684},
doi = {10.1145/3389684},
abstract = {This study analyzes cybersecurity trends and proposes a conceptual framework to identify cybersecurity topics of social interest and emerging topics that need to be addressed by researchers in the field. The insights drawn from this framework allow for a more proactive approach to identifying cybersecurity patterns and emerging threats that will ultimately improve the collective cybersecurity posture of the modern society. To achieve this, cybersecurity-oriented content in both media and academic corpora, disseminated between 2008 and 2018, were morphologically analyzed via text mining. A total of 3,556 academic papers obtained from the top-10 highly reputable cybersecurity academic conferences, and 4,163 news articles collected from the New York Times were processed. The LDA topic modeling followed optimal perplexity and coherence scores resulted in 12 trendy topics. Next, the time-based gap between these trendy topics was analyzed to measure the correlation between media and trendy academic topics. Both convergences and divergences between the two cybersecurity corpora were identified, suggesting a strong time-based correlation between these resources. This framework demonstrates the effective use of automated techniques to provide insights about cybersecurity topics of social interest and emerging trends and informs the direction of future academic research in this field.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {oct},
articleno = {20},
numpages = {20},
keywords = {Cybersecurity trends, trend analysis, academic context, digital media, topic modeling}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00020,
author = {Chen, Junjie and He, Xiaoting and Lin, Qingwei and Xu, Yong and Zhang, Hongyu and Hao, Dan and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei},
title = {An Empirical Investigation of Incident Triage for Online Service Systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00020},
doi = {10.1109/ICSE-SEIP.2019.00020},
abstract = {Online service systems have become increasingly popular. During operation of an online service system, incidents (unplanned interruptions or outages of the service) are inevitable. As an initial step of incident management, it is important to be able to automatically assign an incident report to a suitable team. We call this step incident triage, which can significantly affect the efficiency and accuracy of overall incident management. To better understand the incident-triage practice in industry, we perform an empirical study of incident triage on 20 large-scale online service systems in Microsoft. We find that incorrect assignment of incident reports occurs frequently and incurs unnecessary cost, especially for the incidents with high severity. For example, about 4.11% to 91.58% of incident reports are reassigned at least once and the average increment in incident-triage time caused by the reassignments is up to 10.16X. Considering the similarity between bug triage (automatically assigning bug reports to software developers) and incident triage, we then explore the applicability of typical bug-triage techniques to incident triage for online service systems. The results demonstrate that these bug-triage techniques are able to correctly assign incident reports to a certain extent, but still need to be further improved, especially for the incident reports that are assigned incorrectly at the first time. We further discuss possible ways to improve the accuracy of incident triage based on the empirical study. To our best knowledge, we are the first to investigate incident triage in industrial practice. Our results are useful for both practitioners and researchers to develop methods and tools to improve the current incident-triage practice for online service systems.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {111–120},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/3176349.3176399,
author = {Bahrainian, Seyed Ali and Crestani, Fabio},
title = {Augmentation of Human Memory: Anticipating Topics That Continue in the Next Meeting},
year = {2018},
isbn = {9781450349253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176349.3176399},
doi = {10.1145/3176349.3176399},
abstract = {Memory augmentation is the process of providing human memory with information that facilitates and complements the recall of an event in a person»s past. Recently, there has been a lot of attention on processing the content of meetings for later reuse, such as reviewing a meeting for supporting failing memories, keeping in mind key issues, verification, etc. That is due to the fact that meetings are essential for sharing knowledge in organizations. In this paper, we propose four novel time-series methods for predicting the topics that one should review in preparation for a next meeting. The predicted/recommended topics can be reviewed by a user as a memory augmentation process to facilitate recall of key points of a previous meeting. With the growing number of meetings at an organization that one may attend weekly and with the growing number of topics discussed, forgetting past meetings becomes eminent, hence recommending certain topics to the user in order to prepare the user for a future meeting is beneficial and important. Our experimental results on real-world data, demonstrate that our methods significantly outperform a state-of-the-art Hidden Markov Model baseline. This indicates the efficacy of our proposed methods for modeling semantics in temporal data.},
booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
pages = {150–159},
numpages = {10},
keywords = {meetings analysis, topic prediction, human memory augmentation},
location = {New Brunswick, NJ, USA},
series = {CHIIR '18}
}

@article{10.1145/3369873,
author = {Hua, Ting and Lu, Chang-Tien and Choo, Jaegul and Reddy, Chandan K.},
title = {Probabilistic Topic Modeling for Comparative Analysis of Document Collections},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3369873},
doi = {10.1145/3369873},
abstract = {Probabilistic topic models, which can discover hidden patterns in documents, have been extensively studied. However, rather than learning from a single document collection, numerous real-world applications demand a comprehensive understanding of the relationships among various document sets. To address such needs, this article proposes a new model that can identify the common and discriminative aspects of multiple datasets. Specifically, our proposed method is a Bayesian approach that represents each document as a combination of common topics (shared across all document sets) and distinctive topics (distributions over words that are exclusive to a particular dataset). Through extensive experiments, we demonstrate the effectiveness of our method compared with state-of-the-art models. The proposed model can be useful for “comparative thinking” analysis in real-world document collections.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {24},
numpages = {27},
keywords = {Probabilistic topic modeling, text mining}
}

@inproceedings{10.1145/3481357.3481517,
author = {Bhagavatula, Sruti and Bauer, Lujo and Kapadia, Apu},
title = {What Breach? Measuring Online Awareness of Security Incidents by Studying Real-World Browsing Behavior},
year = {2021},
isbn = {9781450384230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3481357.3481517},
doi = {10.1145/3481357.3481517},
abstract = {Learning about real-world security incidents and data breaches can inform people how their information is vulnerable online and thus encourage safer security behavior. This paper examines 1)&nbsp;how often people read about security incidents online, 2)&nbsp;of those people, whether and to what extent they follow up with an action (e.g., trying to read more about the incident), and 3)&nbsp;what influences the likelihood that they will read about an incident and take some action. Our quantitative study of the real-world internet-browsing behavior of 303 participants finds a low level of awareness. Only 16% of participants visited any web page related to six widely publicized large-scale security incidents; few read about an incident even when it was likely to have affected them. We also found that more severe incidents and articles that constructively spoke about the incident were associated with more action. Our findings highlight two issues: 1)&nbsp;security awareness needs to be increased; and 2)&nbsp;current awareness is so low that expecting users to be aware and take remedial action may not be effective.},
booktitle = {Proceedings of the 2021 European Symposium on Usable Security},
pages = {180–199},
numpages = {20},
keywords = {security incidents, security awareness, data breaches},
location = {Karlsruhe, Germany},
series = {EuroUSEC '21}
}

@inproceedings{10.1145/3303772.3303836,
author = {Yan, Wenfei and Dowell, Nia and Holman, Caitlin and Welsh, Stephen S. and Choi, Heeryung and Brooks, Christopher},
title = {Exploring Learner Engagement Patterns in Teach-Outs Using Topic, Sentiment and On-Topicness to Reflect on Pedagogy},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303836},
doi = {10.1145/3303772.3303836},
abstract = {MOOCs have developed into multiple learning design models with a wide range of objectives. Teach-Outs are one such example, aiming to drive meaningful discussions around topics of pressing social urgency without the use of formal assessments. Given this approach, it is crucial to evaluate learners' engagement in the discussion forum to understand their experiences. This paper presents a pilot study that applied unsupervised natural language processing techniques to understand what and how students engage in dialogue in a Teach-Out. We used topic modeling to discover the emerging topics in the discussion forums and evaluated the on-topicness of the discussions (i.e. the degree to which discussions were relevant to the Teach-Out content). We also applied content analysis to investigate the sentiments associated with the discussions. We have taken a step toward extracting structure from students' discussions to understand learning behaviors happen in the discussion forum. This is the first study to analyze discussion forums in a Teach-Out.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {180–184},
numpages = {5},
keywords = {semantic similarity, Teach-Out, sentiment analysis, learning experience, online discussion forum, MOOC, topic modeling},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1109/ESEM.2017.25,
author = {Alqahtani, Sultan S. and Rilling, Juergen},
title = {An Ontology-Based Approach to Automate Tagging of Software Artifacts},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.25},
doi = {10.1109/ESEM.2017.25},
abstract = {Context: Software engineering repositories contain a wealth of textual information such as source code comments, developers' discussions, commit messages and bug reports. These free form text descriptions can contain both direct and implicit references to security concerns. Goal: Derive an approach to extract security concerns from textual information that can yield several benefits, such as bug management (e.g., prioritization), bug triage or capturing zero-day attack. Method: Propose a fully automated classification and tagging approach that can extract security tags from these texts without the need for manual training data. Results: We introduce an ontology based Software Security Tagger Framework that can automatically identify and classify cybersecurity-related entities, and concepts in text of software artifacts. Conclusion: Our preliminary results indicate that the framework can successfully extract and classify cybersecurity knowledge captured in unstructured text found in software artifacts.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {169–174},
numpages = {6},
keywords = {automated security concern classification, tagging, topic modeling, bug reports},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@article{10.1145/3333030,
author = {Qiang, Jipeng and Chen, Ping and Ding, Wei and Wang, Tong and Xie, Fei and Wu, Xindong},
title = {Heterogeneous-Length Text Topic Modeling for Reader-Aware Multi-Document Summarization},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3333030},
doi = {10.1145/3333030},
abstract = {More and more user comments like Tweets are available, which often contain user concerns. In order to meet the demands of users, a good summary generating from multiple documents should consider reader interests as reflected in reader comments. In this article, we focus on how to generate a summary from multi-document documents by considering reader comments, named as reader-aware multi-document summarization (RA-MDS). We present an innovative topic-based method for RA-MDA, which exploits latent topics to obtain the most salient and lessen redundancy summary from multiple documents. Since finding latent topics for RA-MDS is a crucial step, we also present a Heterogeneous-length Text Topic Modeling (HTTM) to extract topics from the corpus that includes both news reports and user comments, denoted as heterogeneous-length texts. In this case, the latent topics extract by HTTM cover not only important aspects of the event, but also aspects that attract reader interests. Comparisons on summary benchmark datasets also confirm that the proposed RA-MDS method is effective in improving the quality of extracted summaries. In addition, experimental results demonstrate that the proposed topic modeling method outperforms existing topic modeling algorithms.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {aug},
articleno = {42},
numpages = {21},
keywords = {Topic modeling, multi-document summarization, heterogeneous-length text, LDA}
}

@article{10.1145/2990506,
author = {Kang, Jeon-Hyung and Lerman, Kristina},
title = {Effort Mediates Access to Information in Online Social Networks},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/2990506},
doi = {10.1145/2990506},
abstract = {Individuals’ access to information in a social network depends on how it is distributed and where in the network individuals position themselves. In addition, individuals vary in how much effort they invest in managing their social connections. Using data from a social media site, we study how the interplay between effort and network position affects social media users’ access to diverse and novel information. Previous studies of the role of networks in information access were limited in their ability to measure the diversity of information. We address this problem by learning the topics of interest to social media users from the messages they share online with followers. We use the learned topics to measure the diversity of information users receive from the people they follow online. We confirm that users in structurally diverse network positions, which bridge otherwise disconnected regions of the follower network, tend to be exposed to more diverse and novel information. We also show that users who invest more effort in their activity on the site are not only located in more structurally diverse positions within the network than the less engaged users but also receive more novel and diverse information when in similar network positions. These findings indicate that the relationship between network structure and access to information in networks is more nuanced than previously thought.},
journal = {ACM Trans. Web},
month = {mar},
articleno = {3},
numpages = {19},
keywords = {information in networks, social media, probabilistic topic models, Social networks}
}

@inproceedings{10.5555/3200334.3200367,
author = {Murdock, Jaimie and Jett, Jacob and Cole, Tim and Ma, Yu and Downie, J. Stephen and Plale, Beth},
title = {Towards Publishing Secure Capsule-Based Analysis},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Computational engagement with the HathiTrust Digital Library (HTDL) is confounded by the in-copyright status and licensing restrictions on the majority of the content. Because of these limitations, computational analysis on the HTDL must either be carried out in a secure environment or on derivative datasets. The HathiTrust Research Center (HTRC) Data Capsule service provides researchers with a secure environment through which they invoke tools that create, analyze, and export non-consumptive datasets. These derivative datasets, so long as they do not reproduce the full-text of the original work, are a transformative work protected by Fair Use provisions of United States Copyright Law, and can be published for reuse by other researchers, as the HTRC Extracted Features Dataset has been. Secure environments and derivative datasets enable researchers to engage with restricted data from focused studies of a few dozen volumes to large-scale experiments on millions of volumes. This paper describes advances in the Capsule service through a case study of how the HTRC Data Capsule service has advanced our activities on provenance, workflows, worksets, and non-consumptive exports through a topic modeling example. We also discuss the potential applications of this Capsule-based model to other digital libraries wrestling with research access and copyright restrictions.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {261–264},
numpages = {4},
keywords = {metadata management, data provenance, digital libraries, text processing, research workflows, semantic web},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.1145/3025171.3025180,
author = {Kumar, Arun and Schrater, Paul},
title = {Novelty Learning via Collaborative Proximity Filtering},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025180},
doi = {10.1145/3025171.3025180},
abstract = {The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for spontaneous changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {601–610},
numpages = {10},
keywords = {implicit preferences, novelty, latent tastes, user behaviors, boredom, recommender systems, user preferences},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3415958.3433076,
author = {Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
title = {LEOnto: New Approach for Ontology Enrichment Using LDA},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433076},
doi = {10.1145/3415958.3433076},
abstract = {The Latent Dirichlet Allocation (LDA) model [18] was originally developed and utilised for document modeling and topic extraction in Information Retrieval. To design high quality domain ontologies, effective and usable methodologies are needed to facilitate their building process. In this paper, we propose a new approach for semi-automatic ontology enriching from textual corpus based on LDA model. In our approach, LDA is adopted to provide efficient dimension reduction, able to capture semantic relationships between word-topic and topic-document in terms of probability distributions with minimum human intervention. We conducted several experiments with different model parameters and the corresponding behavior of the enriching technique was evaluated by domain experts. We also compared the results of our method with two existing learning methods using the same dataset. The study showed that our method outperforms the other methods in terms of recall and precision measures.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {132–139},
numpages = {8},
keywords = {LDA, Ontology enrichment, Knowledge acquisition, Probabilistic topic models, Ontology learning},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3123266.3123420,
author = {Chen, Shizhe and Chen, Jia and Jin, Qin and Hauptmann, Alexander},
title = {Video Captioning with Guidance of Multimodal Latent Topics},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123420},
doi = {10.1145/3123266.3123420},
abstract = {The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&amp;M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. The topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model. M&amp;M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves better generalization ability.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {multi-task, video captioning, latent topics, multimodal},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3289600.3290963,
author = {Rakesh, Vineeth and Wang, Suhang and Shu, Kai and Liu, Huan},
title = {Linked Variational AutoEncoders for Inferring Substitutable and Supplementary Items},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290963},
doi = {10.1145/3289600.3290963},
abstract = {Recommendation in the modern world is not only about capturing the interaction between users and items, but also about understanding the relationship between items. Besides improving the quality of recommendation, it enables the generation of candidate items that can serve as substitutes and supplements of another item. For example, when recommending Xbox, PS4 could be a logical substitute and the supplements could be items such as game controllers, surround system, and travel case. Therefore, given a network of items, our objective is to learn their content features such that they explain the relationship between items in terms of substitutes and supplements. To achieve this, we propose a generative deep learning model that links two variational autoencoders using a connector neural network to create Linked Variational Autoencoder (LVA). LVA learns the latent features of items by conditioning on the observed relationship between items. Using a rigorous series of experiments, we show that LVA significantly outperforms other representative and state-of-the-art baseline methods in terms of prediction accuracy. We then extend LVA by incorporating collaborative filtering (CF) to create CLVA that captures the implicit relationship between users and items. By comparing CLVA with LVA we show that inducing CF-based features greatly improve the recommendation quality of substitutable and supplementary items on a user level.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {438–446},
numpages = {9},
keywords = {link prediction, deep learning, recommendation, variational autoencoder, graphical model},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3357384.3357828,
author = {Shi, Tian and Rakesh, Vineeth and Wang, Suhang and Reddy, Chandan K.},
title = {Document-Level Multi-Aspect Sentiment Classification for Online Reviews of Medical Experts},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357828},
doi = {10.1145/3357384.3357828},
abstract = {In the era of big data, online doctor review platforms, which enable patients to give feedback to their doctors, have become one of the most important components in healthcare systems. On one hand, they help patients to choose their doctors based on the experience of others. On the other hand, they help doctors to improve the quality of their service. Moreover, they provide important sources for us to discover common concerns of patients and existing problems in clinics, which potentially improve current healthcare systems. In this paper, we systematically investigate the dataset from one of such review platform, namely, ratemds.com, where each review for a doctor comes with an overall rating and ratings of four different aspects. A comprehensive statistical analysis is conducted first for reviews, ratings, and doctors. Then, we explore the content of reviews by extracting latent topics related to different aspects with unsupervised topic modeling techniques. As the core component of this paper, we propose a multi-task learning framework for the document-level multi-aspect sentiment classification. This task helps us to not only recover missing aspect-level ratings and detect inconsistent rating scores but also identify aspect-keywords for a given review based on ratings. The proposed model takes both features of doctors and aspect-keywords into consideration. Extensive experiments have been conducted on two subsets of ratemds dataset to demonstrate the effectiveness of the proposed model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2723–2731},
numpages = {9},
keywords = {attention mechanism, sentiment classification, multi-aspect, multi-task learning, online reviews},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1145/3432049,
author = {Liu, Peng and Zhang, Lemei and Gulla, Jon Atle},
title = {Multilingual Review-Aware Deep Recommender System via Aspect-Based Sentiment Analysis},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3432049},
doi = {10.1145/3432049},
abstract = {With the dramatic expansion of international markets, consumers write reviews in different languages, which poses a new challenge for Recommender Systems (RSs) dealing with this increasing amount of multilingual information. Recent studies that leverage deep-learning techniques for review-aware RSs have demonstrated their effectiveness in modelling fine-grained user-item interactions through the aspects of reviews. However, most of these models can neither take full advantage of the contextual information from multilingual reviews nor discriminate the inherent ambiguity of words originated from the user’s different tendency in writing. To this end, we propose a novel Multilingual Review-aware Deep Recommendation Model (MrRec) for rating prediction tasks. MrRec mainly consists of two parts: (1) Multilingual aspect-based sentiment analysis module (MABSA), which aims to jointly extract aligned aspects and their associated sentiments in different languages simultaneously with only requiring overall review ratings. (2) Multilingual recommendation module that learns aspect importances of both the user and item with considering different contributions of multiple languages and estimates aspect utility via a dual interactive attention mechanism integrated with aspect-specific sentiments from MABSA. Finally, overall ratings can be inferred by a prediction layer adopting the aspect utility value and aspect importance as inputs. Extensive experimental results on nine real-world datasets demonstrate the superior performance and interpretability of our model.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {15},
numpages = {33},
keywords = {Recommender systems, deep learning, neural attention, multilingual aspect-based sentiment analysis, co-attention}
}

@inproceedings{10.1145/3269206.3271797,
author = {Viegas, Felipe and Luiz, Washington and Gomes, Christian and Khatibi, Amir and Canuto, S\'{e}rgio and Mour\~{a}o, Fernando and Salles, Thiago and Rocha, Leonardo and Gon\c{c}alves, Marcos Andr\'{e}},
title = {Semantically-Enhanced Topic Modeling},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271797},
doi = {10.1145/3269206.3271797},
abstract = {In this paper, we advance the state-of-the-art in topic modeling by means of the design and development of a novel (semi-formal) general topic modeling framework. The novel contributions of our solution include: (i) the introduction of new semantically-enhanced data representations for topic modeling based on pooling, and (ii) the proposal of a novel topic extraction strategy - ASToC - that solves the difficulty in representing topics in our semantically-enhanced information space. In our extensive experimentation evaluation, covering 12 datasets and 12 state-of-the-art baselines, totalizing 108 tests, we exceed (with a few ties) in almost 100 cases, with gains of more than 50% against the best baselines (achieving up to 80% against some runner-ups). We provide qualitative and quantitative statistical analyses of why our solutions work so well. Finally, we show that our method is able to improve document representation in automatic text classification.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {893–902},
numpages = {10},
keywords = {bag of words, topic modeling, word embeddings},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3491101.3519810,
author = {Adenuga, Iyadunni J. and Hanrahan, Benjamin V. and Wu, Chen and Mitra, Prasenjit},
title = {Living Documents: Designing for User Agency over Automated Text Summarization},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519810},
doi = {10.1145/3491101.3519810},
abstract = {Text summarization is an example of a complex algorithm with a single direct input to output user operation. As different types of users e.g. students, journalists, etc. adopt these algorithms, it is imperative that user experience with text summarization AI systems is modified to allow agency. To explore this, we designed an interactive multi-document summarization system, called Living Documents, that allows agency over the summarization process. We evaluated this system in a preliminary study with 25 students identified from Prolific. The students perform summarization tasks using the control functions in our system. Results from this study show that these functions contributed to the user’s understanding of how summarization works and this understanding slightly impacts user trust in our system positively. We discuss our experience designing for user control of a normally automated, complex process that produces immutable output and the future implications for other applications.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {394},
numpages = {6},
keywords = {User Agency, Text Summarization System, Interaction Design},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3383455.3422529,
author = {Chen, Qian and Liu, Xiao-Yang},
title = {Quantifying ESG Alpha Using Scholar Big Data: An Automated Machine Learning Approach},
year = {2020},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422529},
doi = {10.1145/3383455.3422529},
abstract = {ESG (Environmental, social and governance) alpha strategy that makes sustainable investment has gained popularity among investors. The ESG fields of study in scholar big data is a valuable alternative data that reflects a company's long-term ESG commitment. However, it is considered a difficulty to quantitatively measure a company's ESG premium and its impact to the company's stock price using scholar big data. In this paper, we utilize ESG scholar data as alternative data to develop an automatic trading strategy and propose a practical machine learning approach to quantify the ESG premium of a company and capture the ESG alpha. First, we construct our ESG investment universe and apply feature engineering on the companies' ESG scholar data from the Microsoft Academic Graph database. Then, we train six complementary machine learning models using a combination of financial indicators and ESG scholar data features and employ an ensemble method to predict stock prices and automatically set up portfolio allocation. Finally, we manage our portfolio, trade and rebalance the portfolio allocation monthly using predicted stock prices. We backtest our ESG alpha strategy and compare its performance with benchmarks. The proposed ESG alpha strategy achieves a cumulative return of 2,154.4% during the backtesting period of ten years, which significantly outperforms the NASDAQ-100 index's 397.4% and S&amp;P 500's 226.9%. The traditional financial indicators results in only 1,443.7%, thus our scholar data-based ESG alpha strategy is better at capturing ESG premium than traditional financial indicators.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {41},
numpages = {8},
keywords = {alternative data, scholar data, AI in finance, ESG alpha, quantitative investment},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{10.1145/3308560.3316582,
author = {Peng, Jinhua and Ma, Zongyang and Jiang, Di and Wu, Hua},
title = {Integrating Bayesian and Neural Networks for Discourse Coherence},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316582},
doi = {10.1145/3308560.3316582},
abstract = {In dialogue systems, discourse coherence is an important concept that measures semantic relevance between an utterance and its context. It plays a critical role in determining the inappropriate reply of dialogue systems with regard to a given dialogue context. In this paper, we present a novel framework for evaluating discourse coherence by seamlessly integrating Bayesian and neural networks. The Bayesian network corresponds to Coherence-Pivoted Latent Dirichlet Allocation (cpLDA). cpLDA concentrates on generating the fine-grained topics from dialogue data and takes both local and global semantics into account. The neural network corresponds to Multi-Hierarchical Coherence Network (MHCN). Coupled with cpLDA, MHCN quantifies discourse coherence between an utterance and its context by comprehensively utilizing original texts, topic distribution and topic embedding. Extensive experiments show that the proposed framework yields superior performance comparing with the state-of-the-art methods.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {294–300},
numpages = {7},
keywords = {Bayesian Network, Discourse Coherence, Multi-Hierarchical Coherence Network, Dialogue systems, Global Semantics},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{10.1145/3143402,
author = {Park, Souneil and Matic, Aleksandar and Garg, Kamini and Oliver, Nuria},
title = {When Simpler Data Does Not Imply Less Information: A Study of User Profiling Scenarios With Constrained View of Mobile HTTP(S) Traffic},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3143402},
doi = {10.1145/3143402},
abstract = {The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services that are often free in exchange for the use of its customers’ data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this article, we unfold profiling scenarios around mobile HTTP(S) traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in the wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights.},
journal = {ACM Trans. Web},
month = {jan},
articleno = {9},
numpages = {23},
keywords = {user modeling, personalized services, Mobile computing}
}

@inproceedings{10.1145/3469877.3490604,
author = {Fan, Ruichao and Wang, Hanli and Gu, Jinjing and Liu, Xianhui},
title = {Visual Storytelling with Hierarchical BERT Semantic Guidance},
year = {2021},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3490604},
doi = {10.1145/3469877.3490604},
abstract = {Visual storytelling, which aims at automatically producing a narrative paragraph for photo album, remains quite challenging due to the complexity and diversity of photo album content. In addition, open-domain photo albums cover a broad range of topics and this results in highly variable vocabularies and expression styles to describe photo albums. In this work, a novel teacher-student visual storytelling framework with hierarchical BERT semantic guidance&nbsp;(HBSG) is proposed to address the above-mentioned challenges. The proposed teacher module consists of two joint tasks, namely, word-level latent topic generation and semantic-guided sentence generation. The first task aims to predict the latent topic of the story. As there is no ground-truth topic information, a pre-trained BERT model based on visual contents and annotated stories is utilized to mine topics. Then the topic vector is distilled to a designed image-topic prediction model. In the semantic-guided sentence generation task, HBSG is introduced for two purposes. The first is to narrow down the language complexity across topics, where the co-attention decoder with vision and semantic is designed to leverage the latent topics to induce topic-related language models. The second is to employ sentence semantic as an online external linguistic knowledge teacher module. Finally, an auxiliary loss is devised to transform linguistic knowledge into the language generation model. Extensive experiments are performed to demonstrate the effectiveness of HBSG framework, which surpasses the state-of-the-art approaches evaluated on the VIST test set.},
booktitle = {ACM Multimedia Asia},
articleno = {24},
numpages = {7},
keywords = {teacher-student neural network, auxiliary loss., co-attention mechanism, BERT, Visual storytelling},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

@article{10.1145/3412847,
author = {Zhang, Chengyuan and Song, Jiayu and Zhu, Xiaofeng and Zhu, Lei and Zhang, Shichao},
title = {HCMSL: Hybrid Cross-Modal Similarity Learning for Cross-Modal Retrieval},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3412847},
doi = {10.1145/3412847},
abstract = {The purpose of cross-modal retrieval is to find the relationship between different modal samples and to retrieve other modal samples with similar semantics by using a certain modal sample. As the data of different modalities presents heterogeneous low-level feature and semantic-related high-level features, the main problem of cross-modal retrieval is how to measure the similarity between different modalities. In this article, we present a novel cross-modal retrieval method, named Hybrid Cross-Modal Similarity Learning model (HCMSL for short). It aims to capture sufficient semantic information from both labeled and unlabeled cross-modal pairs and intra-modal pairs with same classification label. Specifically, a coupled deep fully connected networks are used to map cross-modal feature representations into a common subspace. Weight-sharing strategy is utilized between two branches of networks to diminish cross-modal heterogeneity. Furthermore, two Siamese CNN models are employed to learn intra-modal similarity from samples of same modality. Comprehensive experiments on real datasets clearly demonstrate that our proposed technique achieves substantial improvements over the state-of-the-art cross-modal retrieval techniques.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
articleno = {2},
numpages = {22},
keywords = {hybrid cross-modal similarity, deep learning, intra-modal semantic correlation, Cross-modal retrieval}
}

@article{10.1145/3078849,
author = {Wang, Pengfei and Liu, Guannan and Fu, Yanjie and Zhou, Yuanchun and Li, Jianhui},
title = {Spotting Trip Purposes from Taxi Trajectories: A General Probabilistic Model},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078849},
doi = {10.1145/3078849},
abstract = {What is the purpose of a trip? What are the unique human mobility patterns and spatial contexts in or near the pickup points and delivery points of trajectories for a specific trip purpose? Many prior studies have modeled human mobility patterns in urban regions; however, these analytics mainly focus on interpreting the semantic meanings of geographic topics at an aggregate level. Given the lack of information about human activities at pick-up and dropoff points, it is challenging to convert the prior studies into effective tools for inferring trip purposes. To address this challenge, in this article, we study large-scale taxi trajectories from an unsupervised perspective in light of the following observations. First, the POI configurations of origin and destination regions closely relate to the urban functionality of these regions and further indicate various human activities. Second, with respect to the functionality of neighborhood environments, trip purposes can be discerned from the transitions between regions with different functionality at particular time periods.Along these lines, we develop a general probabilistic framework for spotting trip purposes from massive taxi GPS trajectories. Specifically, we first augment the origin and destination regions of trajectories by attaching neighborhood POIs. Then, we introduce a latent factor, POI Topic, to represent the mixed functionality of the regions, such that each origin or destination point in the city can be modeled as a mixture over POI Topics. In addition, considering the transitions from origins to destinations at specific time periods, the trip time is generated collaboratively from the pairwise POI Topics at both ends of the O-D pairs, constituting POI Links, and hence the trip purpose can be explained semantically by the POI Links. Finally, we present extensive experiments with the real-world data of New York City to demonstrate the effectiveness of our proposed method for spotting trip purposes, and moreover, the model is validated to perform well in predicting the destinations and trip time among all the baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {29},
numpages = {26},
keywords = {taxi trajectories, trip purposes, Human mobility, probabilistic model}
}

@article{10.1109/TASLP.2016.2635445,
author = {Lee, Hung-Yi and Tseng, Bo-Hsiang and Wen, Tsung-Hsien and Tsao, Yu and Hung-Yi Lee and Bo-Hsiang Tseng and Tsung-Hsien Wen and Yu Tsao},
title = {Personalizing Recurrent-Neural-Network-Based Language Model by Social Network},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2635445},
doi = {10.1109/TASLP.2016.2635445},
abstract = {With the popularity of mobile devices, personalized speech recognizers have become more attainable and are highly attractive. Since each mobile device is used primarily by a single user, it is possible to have a personalized recognizer that well matches the characteristics of the individual user. Although acoustic model personalization has been investigated for decades, much less work has been reported on personalizing language models, presumably because of the difficulties in collecting sufficient personalized corpora. In this paper, we propose a general framework for personalizing recurrent-neural-network-based language models RNNLMs using data collected from social networks, including the posts of many individual users and friend relationships among the users. Two major directions for this are model-based and feature-based RNNLM personalization. In model-based RNNLM personalization, the RNNLM parameters are fine-tuned to an individual user's wording patterns by incorporating social texts posted by the target user and his or her friends. For the feature-based approach, the RNNLM model parameters are fixed across users, but the RNNLM input features are instead augmented with personalized information. Both approaches not only drastically reduce the model perplexity, but also moderately reduce word error rates in n-best rescoring tests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {519–530},
numpages = {12}
}

@inproceedings{10.1145/3489088.3489096,
author = {Heryawan, Lukman and Subiantoro, Ardacandra},
title = {Medical Subject Headings (MeSH) Indexing Using Unsupervised Learningfor Low-Resource Controlled Vocabulary},
year = {2021},
isbn = {9781450385244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489088.3489096},
doi = {10.1145/3489088.3489096},
abstract = {Medical subject heading indexing is a process to assign Medical Subject Headings (MeSH) to the biomedical literatures, such as abstracts or articles. The indexing process was done manually by human MeSH indexer. There is also indexing that was done by machine learning algorithms like supervised learning. Both of processes requires MeSH labels or terms to be assigned manually to an articles, which is a tedious task and requires a lot of hand-crafted labels from domain experts. In order to do MeSH indexing without manually assigned labels, the unsupervised learning algorithm like LDA (Latent Dirichlet allocation) was performed and analyzed. LDA is kind of unsupervised learning, which can generate topics in a collection of texts, and then automatically classify any individual texts within the collection into a relevant generated topics. In term of MeSH indexing, topic in LDA is a MeSH label or term. LDA was able to generate topics from biomedical literatures with high similarity, which is 74% similar to MeSH terms provided by domain experts. We confirmed that unsupervised learning algorithm like LDA is a promising approach for MeSH indexing with a lack of MeSH labels provided by a domain experts.},
booktitle = {The 2021 International Conference on Computer, Control, Informatics and Its Applications},
pages = {31–34},
numpages = {4},
keywords = {Automatic MeSH indexing, Latent Dirichlet allocation, Unsupervised learning, Machine learning, Medical subject headings},
location = {Virtual/online conference, Indonesia},
series = {IC3INA 2021}
}

@inproceedings{10.1145/3469213.3470401,
author = {Haotian, Zheng and Tao, Li and Song, Yan},
title = {Design and Implementation of Data Middle Platform},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470401},
doi = {10.1145/3469213.3470401},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {194},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3239438.3239440,
author = {Master, Lawrence},
title = {Generated Document Trees: Latent Dirichlet Allocation},
year = {2018},
isbn = {9781450363891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239438.3239440},
doi = {10.1145/3239438.3239440},
abstract = {Creating Structure or building hierarchy is a very frequently performed task. We are familiar with this act when organizing our documents in folders and subfolders on our personal computer hard drives and attempting to determine which documents are more "general" or should be in "parent" folders. When dealing with a handful of documents, the task is usually trivial. However, what about when a physician needs to diagnose a complicated disease and explore alternative treatments by attempting to sort through thousands of related article documents? In the past several years, machine learning and information retrieval techniques have been used to develop many topic models based on the extremely popular Latent Dirichlet Allocation algorithm. However, in the space of unsupervised dynamically generated document structures, this specific area of machine learning has been lacking. We propose a new method, which we call Generated Document Trees Latent Derelict Allocation. Our method does not rely on being given manually created document structures apriori and was developed to dynamically generate trees of documents in an unsupervised fashion. We show an example of its application on the TREC clinical decision support dataset and explore its performance.},
booktitle = {Proceedings of the 2nd International Conference on Medical and Health Informatics},
pages = {101–104},
numpages = {4},
keywords = {Set Theory, Statistical Hypothesis Testing, Information Retrieval, Document Structure, TREC Clinical Decision Support, Latent Derelict Allocation, Dynamic Hierarchy Generation},
location = {Tsukuba, Japan},
series = {ICMHI '18}
}

@inproceedings{10.1145/3077286.3077291,
author = {Dai, Xiangfeng and Spasic, Irena and Andres, Frederic},
title = {A Framework for Automated Rating of Online Reviews Against the Underlying Topics},
year = {2017},
isbn = {9781450350242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077286.3077291},
doi = {10.1145/3077286.3077291},
abstract = {Even though the most online review systems offer star rating in addition to free text reviews, this only applies to the overall review. However, different users may have different preferences in relation to different aspects of a product or a service and may struggle to extract relevant information from a massive amount of consumer reviews available online. In this paper, we present a framework for extracting prevalent topics from online reviews and automatically rating them on a 5-star scale. It consists of five modules, including linguistic pre-processing, topic modelling, text classification, sentiment analysis, and rating. Topic modelling is used to extract prevalent topics, which are then used to classify individual sentences against these topics. A state-of-the-art word embedding method is used to measure the sentiment of each sentence. The two types of information associated with each sentence -- its topic and sentiment -- are combined to aggregate the sentiment associated with each topic. The overall topic sentiment is then projected onto the 5-star rating scale. We use a dataset of Airbnb online reviews to demonstrate a proof of concept. The proposed framework is simple and fully unsupervised. It is also domain independent, and, therefore, applicable to any other domains of products and services.},
booktitle = {Proceedings of the SouthEast Conference},
pages = {164–167},
numpages = {4},
keywords = {Machine Learning, Topic Modelling, Natural Language Processing, Weighted Word Embeddings, Sentiment Analysis, Data Mining, Latent Dirichlet Allocation},
location = {Kennesaw, GA, USA},
series = {ACM SE '17}
}

@inproceedings{10.1145/3471158.3472258,
author = {Xu, Zhichao and Zeng, Hansi and Ai, Qingyao},
title = {Understanding the Effectiveness of Reviews in E-Commerce Top-N Recommendation},
year = {2021},
isbn = {9781450386111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471158.3472258},
doi = {10.1145/3471158.3472258},
abstract = {Modern E-commerce websites contain heterogeneous sources of information, such as numerical ratings, textual reviews and images. These information can be utilized to assist recommendation. Through textual reviews, a user explicitly express her affinity towards the item. Previous researchers found that by using the information extracted from these reviews, we can better profile the users' explicit preferences as well as the item features, leading to the improvement of recommendation performance. However, most of the previous algorithms were only utilizing the review information for explicit-feedback problem i.e. rating prediction, and when it comes to implicit-feedback ranking problem such as top-N recommendation, the usage of review information has not been fully explored. Seeing this gap, in this work, we investigate the effectiveness of textual review information for top-N recommendation under E-commerce settings. We adapt several SOTA review-based rating prediction models for top-N recommendation tasks and compare them to existing top-N recommendation models from both performance and efficiency. We find that models utilizing only review information can not achieve better performances than vanilla implicit-feedback matrix factorization method. When utilizing review information as a regularizer or auxiliary information, the performance of implicit-feedback matrix factorization method can be further improved. However, the optimal model structure to utilize textual reviews for E-commerce top-N recommendation is yet to be determined.},
booktitle = {Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {149–155},
numpages = {7},
keywords = {implicit feedback, reproducibility, top-N recommendation, recommender system},
location = {Virtual Event, Canada},
series = {ICTIR '21}
}

@inproceedings{10.1145/3524273.3532891,
author = {Lommatzsch, Andreas and Kille, Benjamin and \"{O}zg\"{o}bek, \"{O}zlem and Zhou, Yuxiao and Te\v{s}i\'{c}, Jelena and Bartolomeu, Cl\'{a}udio and Semedo, David and Pivovarova, Lidia and Liang, Mingliang and Larson, Martha},
title = {NewsImages: Addressing the Depiction Gap with an Online News Dataset for Text-Image Rematching},
year = {2022},
isbn = {9781450392839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524273.3532891},
doi = {10.1145/3524273.3532891},
abstract = {We present NewsImages, a dataset of online news items, and the related NewsImages rematching task. The goal of NewsImages is to provide researchers with a means of studying the depiction gap, which we define to be the difference between what an image literally depicts and the way in which it is connected to the text that it accompanies. Online news is a domain in which the image-text connection is known to be indirect: The news article does not describe what is literally depicted in the image. We validate NewsImages with experiments that show the dataset's and the task's use for studying occurring connections between image and text, as well as addressing the depiction gap, which include sparse data, diversity of content, and importance of background knowledge.},
booktitle = {Proceedings of the 13th ACM Multimedia Systems Conference},
pages = {227–233},
numpages = {7},
keywords = {multi-modal matching, embeddings, depiction gap, datasets},
location = {Athlone, Ireland},
series = {MMSys '22}
}

@inproceedings{10.1145/3397271.3401185,
author = {Alokaili, Areej and Aletras, Nikolaos and Stevenson, Mark},
title = {Automatic Generation of Topic Labels},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401185},
doi = {10.1145/3397271.3401185},
abstract = {Topic modelling is a popular unsupervised method for identifying the underlying themes in document collections that has many applications in information retrieval. A topic is usually represented by a list of terms ranked by their probability but, since these can be difficult to interpret, various approaches have been developed to assign descriptive labels to topics. Previous work on the automatic assignment of labels to topics has relied on a two-stage approach: (1) candidate labels are retrieved from a large pool (e.g. Wikipedia article titles); and then (2) re-ranked based on their semantic similarity to the topic terms. However, these extractive approaches can only assign candidate labels from a restricted set that may not include any suitable ones. This paper proposes using a sequence-to-sequence neural-based approach to generate labels that does not suffer from this limitation. The model is trained over a new large synthetic dataset created using distant supervision. The method is evaluated by comparing the labels it generates to ones rated by humans.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1965–1968},
numpages = {4},
keywords = {topic representation, neural network, topic modeling},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3447568.3448518,
author = {Gasmi, Sara and Bouhadada, Tahar and Benmachiche, Abdelmadjid},
title = {Survey on Recommendation Systems},
year = {2020},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448518},
doi = {10.1145/3447568.3448518},
abstract = {In recent decade's recommendation systems (RSs) plays an essential role in many applications as World Wide Web. Also recommendation system is one of the most important research area in machine learning. Recommendation system functions as a helper to find the interest of users by making relevant suggestions to users. The RSs mainly use four filtering methods to provide personalized recommendations to users, the most popular ones are: Collaborative filtering (CF), Content-based filtering, Demographic filtering and hybrid filtering. Data mining is one of the important analysis techniques used in RSs to predict user interest in information, products and services among the vast amount of available items. The data mining techniques that are most commonly used in RSs are: classification, clustering and association rule discovery. This paper performs a survey on recommendation systems, techniques, challenges and issues and lists some research papers solve these obstacles, also data mining methods used in recommender systems.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {10},
numpages = {7},
keywords = {Demographic filtering, Content based filtering, Collaborative filtering, Recommendation system, Data mining, Hybrid filtering},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3446132.3446192,
author = {Turenne, Nicolas and Xu, Bokai and Li, Xinyue and Xu, Xindi and Liu, Hongyu and Zhu, Xiaolin},
title = {Exploration of a Balanced Reference Corpus with a Wide Variety of Text Mining Tools},
year = {2020},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446192},
doi = {10.1145/3446132.3446192},
abstract = {To compare various techniques, the same platform is generally used into which the user will import a text dataset. Another approach uses an evaluation based on a gold standard for a specific task, but a balanced common language corpus is not often used. We choose the Corpus of Contemporary American English Corpus (COCA) as a balanced reference corpus, and split this corpus into categories, such as topics and genres, to apply families of feature extraction and machine learning algorithms. We found that the Stanford CoreNLP method was faster and more accurate than the NLTK method, and was more reliable and easier to understand. The results of clustering show that a higher modularity influences interpretation. For genre and topic classification, all techniques achieved a relatively high score, though these were below the state-of-the-art scores from challenge text datasets. Na\"{\i}ve Bayes outperformed the other alternatives. We hope that balanced corpora from a variety of different vernacular (or low-resource) languages can be used as references to determine the efficiency of the wide diversity of state-of-the-art text mining tools.},
booktitle = {2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {60},
numpages = {9},
keywords = {natural language processing, text mining},
location = {Sanya, China},
series = {ACAI 2020}
}

@inproceedings{10.1145/3097983.3098017,
author = {Chen, Yu and Zaki, Mohammed J.},
title = {KATE: K-Competitive Autoencoder for Text},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098017},
doi = {10.1145/3097983.3098017},
abstract = {Autoencoders have been successful in learning meaningful representations from image datasets. However, their performance on text datasets has not been widely studied. Traditional autoencoders tend to learn possibly trivial representations of text documents due to their confoundin properties such as high-dimensionality, sparsity and power-law word distributions. In this paper, we propose a novel k-competitive autoencoder, called KATE, for text documents. Due to the competition between the neurons in the hidden layer, each neuron becomes specialized in recognizing specific data patterns, and overall the model can learn meaningful representations of textual data. A comprehensive set of experiments show that KATE can learn better representations than traditional autoencoders including denoising, contractive, variational, and k-sparse autoencoders. Our model also outperforms deep generative models, probabilistic topic models, and even word representation models (e.g., Word2Vec) in terms of several downstream tasks such as document classification, regression, and retrieval.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {85–94},
numpages = {10},
keywords = {text analytics, autoencoders, competitive learning, representation learning},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3051457.3054010,
author = {Boughoula, Assma and Geigle, Chase and Zhai, ChengXiang},
title = {A Probabilistic Approach for Discovering Difficult Course Topics Using Clickstream Data},
year = {2017},
isbn = {9781450344500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3051457.3054010},
doi = {10.1145/3051457.3054010},
abstract = {One of the main factors affecting the success and effectiveness of Massive Open Online Courses is the ability of the instructor to acquire and incorporate student feedback in a timely manner, and preferably before assigning grades to student assessments. This research uses raw clickstream data from video watching sessions of the Coursera MOOC: "Text Retrieval and Search Engines" to discover which topics are difficult for the students. We introduce a measure for topic difficulty based on these clickstream events, and rank the topics according to this measure. The validity of our ranking is evaluated by comparing it with the ranking of topics based on student votes and find that our method agrees with the ranking based on student votes with &gt; 63% accuracy.},
booktitle = {Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale},
pages = {303–306},
numpages = {4},
keywords = {topic difficulty, probabilistic clustering, student feedback},
location = {Cambridge, Massachusetts, USA},
series = {L@S '17}
}

@inproceedings{10.1145/3041021.3054151,
author = {Jin, Peiquan and Mu, Lin and Zheng, Lizhou and Zhao, Jie and Yue, Lihua},
title = {News Feature Extraction for Events on Social Network Platforms},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054151},
doi = {10.1145/3041021.3054151},
abstract = {Microblog-based social network platforms like Twitter and Sina Weibo have been important sources for news event extraction. However, existing works on microblog event extraction, which usually use keywords, entities, or selected microblogs to represent events, are not able to extract details of an event. Based on the view of news report, an event should present detailed news features, i.e., when, where, who, whom, and what. Such news features are helpful for conducting deeply data analysis on microblogs, e.g., competitor monitoring and public crisis discovery. However, the challenge is that the news features of an event on microblogs are usually distributed among different posts because of the short-text property of microblogs. This is much different from extracting news events from Web news pages that usually contain most details of an event. In this paper, we propose a new framework to extract events together with their news features from microblogs. We first extract a set of events from microblogs. Each event is represented as a distribution over four kinds of named entities including location, person name, organization, and time. In addition, the type of each event, i.e., location-related, person-related, or organization-related, is determined by a machine-learning method. In order to obtain the news features of an event, we propose an event-clustering approach that puts together all the relevant events into a cluster. For each cluster, we propose different algorithms to extract the news features of the event reported in the cluster. We conduct experiments on two microblog datasets crawled from a commercial microblogging platform to evaluate the performance of the proposed framework. The results suggest the effectiveness of our proposal.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {69–78},
numpages = {10},
keywords = {microblog, event extraction, news features},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3219819.3219929,
author = {Zhou, Xiao and Noulas, Anastasios and Mascolo, Cecilia and Zhao, Zhongxiang},
title = {Discovering Latent Patterns of Urban Cultural Interactions in WeChat for Modern City Planning},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219929},
doi = {10.1145/3219819.3219929},
abstract = {Cultural activity is an inherent aspect of urban life and the success of a modern city is largely determined by its capacity to offer generous cultural entertainment to its citizens. To this end, the optimal allocation of cultural establishments and related resources across urban regions becomes of vital importance, as it can reduce financial costs in terms of planning and improve quality of life in the city, more generally. In this paper, we make use of a large longitudinal dataset of user location check-ins from the online social network WeChat to develop a data-driven framework for cultural planning in the city of Beijing. We exploit rich spatio-temporal representations on user activity at cultural venues and use a novel extended version of the traditional latent Dirichlet allocation model that incorporates temporal information to identify latent patterns of urban cultural interactions. Using the characteristic typologies of mobile user cultural activities emitted by the model, we determine the levels of demand for different types of cultural resources across urban areas. We then compare those with the corresponding levels of supply as driven by the presence and spatial reach of cultural venues in local areas to obtain high resolution maps that indicate urban regions with lack of cultural resources, and thus give suggestions for further urban cultural planning and investment optimisation.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1069–1078},
numpages = {10},
keywords = {pattern mining, topic modeling, urban computing, spatial accessibility, spatio-temporal analysis},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3477495.3531949,
author = {Xin, Haoran and Lu, Xinjiang and Zhu, Nengjun and Xu, Tong and Dou, Dejing and Xiong, Hui},
title = {CAPTOR: A Crowd-Aware Pre-Travel Recommender System for Out-of-Town Users},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531949},
doi = {10.1145/3477495.3531949},
abstract = {Pre-travel out-of-town recommendation aims to recommend Point-of-Interests (POIs) to the users who plan to travel out of their hometown in the near future yet have not decided where to go, i.e., their destination regions and POIs both remain unknown. It is a non-trivial task since the searching space is vast, which may lead to distinct travel experiences in different out-of-town regions and eventually confuse decision-making. Besides, users' out-of-town travel behaviors are affected not only by their personalized preferences but heavily by others' travel behaviors. To this end, we propose a Crowd-Aware Pre-Travel Out-of-town Recommendation framework (CAPTOR) consisting of two major modules: spatial-affined conditional random field (SA-CRF) and crowd behavior memory network (CBMN). Specifically, SA-CRF captures the spatial affinity among POIs while preserving the inherent information of POIs. Then, CBMN is proposed to maintain the crowd travel behaviors w.r.t. each region through three affiliated blocks reading and writing the memory adaptively. We devise the elaborated metric space with a dynamic mapping mechanism, where the users and POIs are distinguishable both inherently and geographically. Extensive experiments on two real-world nationwide datasets validate the effectiveness of CAPTOR against the pre-travel out-of-town recommendation task.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1174–1184},
numpages = {11},
keywords = {memory network, pre-travel recommendation, crf, metric learning},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3386723.3387884,
author = {Remaida, Ahmed and Moumen, Aniss and El Idrissi, Younes El Bouzekri and Sabri, Zineb},
title = {Handwriting Recognition with Artificial Neural Networks a Decade Literature Review},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387884},
doi = {10.1145/3386723.3387884},
abstract = {Deep Learning Artificial Neural Networks has pushed forward researches in the field of pattern recognition, furthermore in human handwriting recognition. From online to offline approach, signature verification, writing or writer identification, segmentation or features extraction, a multitude of Artificial Neural Networks (ANNs) models are applied in the process. This paper focuses on the literature review of human handwriting recognition with ANN's over the last decade. We propose an exploratory analysis of 294 research papers collected from five indexed research engines: ACM Digital Library, IEEE digital library, Science Direct, Scopus and Web of Science. Our aim is to provide a research papers distribution across years and journals, a Keywords frequency analysis using cloud visualization, and a Natural Language Processing Topic Modeling using Non-Negative Matrix Factorization (NMF). The results of this study show that the number of research papers reached noticeably a peak in the 2010 with 44 published papers; also Pattern Recognition was the top publishing journal with 12 published papers. As for the topic modeling using NMF we obtained 3 topics listed as follows: 1) Feature Extraction and segmentation techniques for Handwritten Texts Recognition; 2) Signature Verification in Biometric security for Off-line Authentication; 3) Assessment Systems for Student Identification},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {65},
numpages = {5},
keywords = {Nonnegative Matrix Factorization, Human Handwriting Recognition, Topic Modeling, Natural Language Processing, Artificial Neural Networks},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@inproceedings{10.1145/3170427.3188448,
author = {Fruchter, Nathaniel and Liccardi, Ilaria},
title = {Consumer Attitudes Towards Privacy and Security in Home Assistants},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188448},
doi = {10.1145/3170427.3188448},
abstract = {Home assistants such as Amazon's Echo and Google's Home have become a common household item. In this paper we investigate if and what consumers have reported online (in the form of reviews) related to privacy and security after purchasing or using these devices.We use natural language processing to first identify privacy and security related reviews, and then to investigate the topics consumers discuss within the reviews. We were interested in understanding consumers' major concerns.Issues and/or concerns related to security and privacy have have been reported within reviews; however, these topics only account for 2% of the total reviews given for these devices. Three major concerns were highlighted in our findings: data collection and scope, "creepy" device behavior, and violations of personal privacy thresholds.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {home assistants, privacy, consumer attitudes, internet of things, security},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3219819.3220094,
author = {Yin, Jianhua and Chao, Daren and Liu, Zhongkun and Zhang, Wei and Yu, Xiaohui and Wang, Jianyong},
title = {Model-Based Clustering of Short Text Streams},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220094},
doi = {10.1145/3219819.3220094},
abstract = {Short text stream clustering has become an increasingly important problem due to the explosive growth of short text in diverse social medias. In this paper, we propose a model-based short text stream clustering algorithm (MStream) which can deal with the concept drift problem and sparsity problem naturally. The MStream algorithm can achieve state-of-the-art performance with only one pass of the stream, and can have even better performance when we allow multiple iterations of each batch. We further propose an improved algorithm of MStream with forgetting rules called MStreamF, which can efficiently delete outdated documents by deleting clusters of outdated batches. Our extensive experimental study shows that MStream and MStreamF can achieve better performance than three baselines on several real datasets.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2634–2642},
numpages = {9},
keywords = {mixture model, text stream clustering, dirichlet process},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1109/TASLP.2017.2651361,
author = {Sheikh, Imran and Fohr, Dominique and Illina, Irina and Linares, Georges and Sheikh, Imran and Fohr, Dominique and Illina, Irina and Linares, Georges},
title = {Modelling Semantic Context of OOV Words in Large Vocabulary Continuous Speech Recognition},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651361},
doi = {10.1109/TASLP.2017.2651361},
abstract = {The diachronic nature of broadcast news data leads to the problem of out-of-vocabulary OOV words in large vocabulary continuous speech recognition LVCSR systems. Analysis of OOV words reveals that a majority of them are proper names PNs. However, PNs are important for automatic indexing of audio-video content and for obtaining reliable automatic transcriptions. In this paper, we focus on the problem of OOV PNs in diachronic audio documents. To enable the recovery of the PNs missed by the LVCSR system, relevant OOV PNs are retrieved by exploiting the semantic context of the LVCSR transcriptions. For retrieval of OOV PNs, we explore topic and semantic context derived from latent Dirichlet allocation LDA topic models, continuous word vector representations and the neural bag-of-words NBOW model which is capable of learning task specific word and context representations. We propose a neural bag-of-weighted words NBOW2 model which learns to assign higher weights to words that are important for retrieval of an OOV PN. With experiments on French broadcast news videos, we show that the NBOW and NBOW2 models outperform the methods based on raw embeddings from LDA and Skip-gram models. Combining the NBOW and NBOW2 models gives a faster convergence during training. Second pass speech recognition experiments, in which the LVCSR vocabulary and language model are updated with the retrieved OOV PNs, demonstrate the effectiveness of the proposed context models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {598–610},
numpages = {13}
}

@article{10.1145/3169795,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Lau, Jey Han and Abebe, Ermyas and Ruan, Wenjie},
title = {Duplicate Detection in Programming Question Answering Communities},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3169795},
doi = {10.1145/3169795},
abstract = {Community-based Question Answering (CQA) websites are attracting increasing numbers of users and contributors in recent years. However, duplicate questions frequently occur in CQA websites and are currently manually identified by the moderators. Automatic duplicate detection, on one hand, alleviates this laborious effort for moderators before taking close actions, and, on the other hand, helps question issuers quickly find answers. A number of studies have looked into related problems, but very limited works target Duplicate Detection in Programming CQA (PCQA), a branch of CQA that is dedicated to programmers. Existing works framed the task as a supervised learning problem on the question pairs and relied on only textual features. Moreover, the issue of selecting candidate duplicates from large volumes of historical questions is often un-addressed. To tackle these issues, we model duplicate detection as a two-stage “ranking-classification” problem over question pairs. In the first stage, we rank the historical questions according to their similarities to the newly issued question and select the top ranked ones as candidates to reduce the search space. In the second stage, we develop novel features that capture both textual similarity and latent semantics on question pairs, leveraging techniques in deep learning and information retrieval literature. Experiments on real-world questions about multiple programming languages demonstrate that our method works very well; in some cases, up to 25% improvement compared to the state-of-the-art benchmarks.},
journal = {ACM Trans. Internet Technol.},
month = {apr},
articleno = {37},
numpages = {21},
keywords = {Community-based question answering, question quality, classification, latent semantics, association rules}
}

@inproceedings{10.1145/3340531.3411932,
author = {Lee, Dongho and Oh, Byungkook and Seo, Seungmin and Lee, Kyong-Ho},
title = {News Recommendation with Topic-Enriched Knowledge Graphs},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411932},
doi = {10.1145/3340531.3411932},
abstract = {News recommendation systems? purpose is to tackle the immense amount of news and offer personalized recommendations to users. A major issue in news recommendation is to capture the precise news representations for the efficacy of recommended items. Commonly, news contents are filled with well-known entities of different types. However, existing recommendation systems overlook exploiting external knowledge about entities and topical relatedness among the news. To cope with the above problem, in this paper, we propose Topic-Enriched Knowledge Graph Recommendation System(TEKGR). Three encoders in TEKGR handle news titles in two perspectives to obtain news representation embedding: (1) to extract meaning of news words without considering latent knowledge features in the news and (2) to extract semantic knowledge of news through topic information and contextual information from a knowledge graph. After obtaining news representation vectors, an attention network compares clicked news to the candidate news in order to get the user's final embedding. Our TEKGR model is superior to existing news recommendation methods by manipulating topical relations among entities and contextual features of entities. Experimental results on two public datasets show that our approach outperforms state-of-the-art deep recommendation approaches.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {695–704},
numpages = {10},
keywords = {recommendation system, news recommendation, knowledge graphs, neural networks},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3097286.3097288,
author = {Addawood, Aseel and Schneider, Jodi and Bashir, Masooda},
title = {Stance Classification of Twitter Debates: The Encryption Debate as A Use Case},
year = {2017},
isbn = {9781450348478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097286.3097288},
doi = {10.1145/3097286.3097288},
abstract = {Social media have enabled a revolution in user-generated content. They allow users to connect, build community, produce and share content, and publish opinions. To better understand online users' attitudes and opinions, we use stance classification. Stance classification is a relatively new and challenging approach to deepen opinion mining by classifying a user's stance in a debate. Our stance classification use case is tweets that were related to the spring 2016 debate over the FBI's request that Apple decrypt a user's iPhone. In this "encryption debate," public opinion was polarized between advocates for individual privacy and advocates for national security. We propose a machine learning approach to classify stance in the debate, and a topic classification that uses lexical, syntactic, Twitter-specific, and argumentative features as a predictor for classifications. Models trained on these feature sets showed significant increases in accuracy relative to the unigram baseline.},
booktitle = {Proceedings of the 8th International Conference on Social Media &amp; Society},
articleno = {2},
numpages = {10},
keywords = {Stance Classification, Supervised Machine Learning, Argumentative Features, Natural Language Processing},
location = {Toronto, ON, Canada},
series = {#SMSociety17}
}

@article{10.1145/3361141,
author = {Joshi, Aditya and Karimi, Sarvnaz and Sparks, Ross and Paris, C\'{e}cile and Macintyre, C. Raina},
title = {Survey of Text-Based Epidemic Intelligence: A Computational Linguistics Perspective},
year = {2019},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3361141},
doi = {10.1145/3361141},
abstract = {Epidemic intelligence deals with the detection of outbreaks using formal (such as hospital records) and informal sources (such as user-generated text on the web) of information. In this survey, we discuss approaches for epidemic intelligence that use textual datasets, referring to it as “text-based epidemic intelligence.” We view past work in terms of two broad categories: health mention classification (selecting relevant text from a large volume) and health event detection (predicting epidemic events from a collection of relevant text). The focus of our discussion is the underlying computational linguistic techniques in the two categories. The survey also provides details of the state of the art in annotation techniques, resources, and evaluation strategies for epidemic intelligence.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {119},
numpages = {19},
keywords = {natural language processing, Epidemic intelligence}
}

@article{10.1145/3451530,
author = {Guo, Jinjin and Cao, Longbing and Gong, Zhiguo},
title = {Recurrent Coupled Topic Modeling over Sequential Documents},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3451530},
doi = {10.1145/3451530},
abstract = {The abundant sequential documents such as online archival, social media, and news feeds are streamingly updated, where each chunk of documents is incorporated with smoothly evolving yet dependent topics. Such digital texts have attracted extensive research on dynamic topic modeling to infer hidden evolving topics and their temporal dependencies. However, most of the existing approaches focus on single-topic-thread evolution and ignore the fact that a current topic may be coupled with multiple relevant prior topics. In addition, these approaches also incur the intractable inference problem when inferring latent parameters, resulting in a high computational cost and performance degradation. In this work, we assume that a current topic evolves from all prior topics with corresponding coupling weights, forming the multi-topic-thread evolution. Our method models the dependencies between evolving topics and thoroughly encodes their complex multi-couplings across time steps. To conquer the intractable inference challenge, a new solution with a set of novel data augmentation techniques is proposed, which successfully discomposes the multi-couplings between evolving topics. A fully conjugate model is thus obtained to guarantee the effectiveness and efficiency of the inference technique. A novel Gibbs sampler with a backward–forward filter algorithm efficiently learns latent time-evolving parameters in a closed-form. In addition, the latent Indian Buffet Process compound distribution is exploited to automatically infer the overall topic number and customize the sparse topic proportions for each sequential document without bias. The proposed method is evaluated on both synthetic and real-world datasets against the competitive baselines, demonstrating its superiority over the baselines in terms of the low per-word perplexity, high coherent topics, and better document time prediction.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {8},
numpages = {32},
keywords = {Topic modeling, dropout, data augmentation, bayesian network, topic coupling, gibbs sampling, multiple dependency, topic evolution}
}

@inproceedings{10.1145/3209542.3209564,
author = {Khazaei, Taraneh and Xiao, Lu and Mercer, Robert E. and Khan, Atif},
title = {Understanding Privacy Dichotomy in Twitter},
year = {2018},
isbn = {9781450354271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209542.3209564},
doi = {10.1145/3209542.3209564},
abstract = {Balancing personalization and privacy is one of the challenges marketers commonly face. The privacy dilemmas associated with personalized services are particularly concerning in the context of social networking websites, wherein the privacy dichotomy problem is widely observed. To prevent potential privacy violations, businesses need to employ multiple safeguards beyond the current privacy settings of users. As a possible solution, companies can utilize user social footprints to detect user privacy preferences. To take a step towards this goal, we first ran a series of experiments to examine if the privacy preference attribute is homophilous in social media. As a result, we found a set of clues that users' privacy preferences are similar to the privacy behaviour of their social contacts, signaling that privacy homophily exists in social networks. We further studied users located in different neighbourhoods with varying degrees of privacy and found a set of characteristics that are specific to public users located in private neighbourhoods. These identified features can be used in a predictive model to identify public user accounts that are intended to be private, supporting companies to make an informed decision whether or not to exploit one's publicly available data for personalization purposes.},
booktitle = {Proceedings of the 29th on Hypertext and Social Media},
pages = {156–164},
numpages = {9},
keywords = {preference detection, social network analysis, social privacy},
location = {Baltimore, MD, USA},
series = {HT '18}
}

@inproceedings{10.1145/3287560.3287583,
author = {Bountouridis, Dimitrios and Harambam, Jaron and Makhortykh, Mykola and Marrero, M\'{o}nica and Tintarev, Nava and Hauff, Claudia},
title = {SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287583},
doi = {10.1145/3287560.3287583},
abstract = {The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as "Matthew effects", "filter bubbles", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {150–159},
numpages = {10},
keywords = {simulation, diversity, recommender systems, news media},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{10.1145/3524458.3547240,
author = {Ba, Cheick Tidiane and Choquet, Chlo\'{e} and Interdonato, Roberto and Roche, Mathieu},
title = {Explaining Food Security Warning Signals with YouTube Transcriptions and Local News Articles},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547240},
doi = {10.1145/3524458.3547240},
abstract = {Food security is a major concern in many countries all over the world. After a relatively long period characterized by a positive trend, the number and severity of food insecurity situations has been growing again in recent years, with alarming projections for the near future. While several Early Warning Systems (EWS) exist to monitor this phenomenon and guide the interventions of governments and ONGs, such systems rely on a narrow set of data types, i.e., mainly satellite imagery and survey data. These data can explain just a limited number of the multiple factors that impact on food security, thus producing an incomplete picture of the real scenario. In this work, we propose a spatio-temporal analysis of unconventional textual data (i.e., YouTube transcriptions and articles from local news papers) to support the explanatory process of food insecurity situations. This data, being completely exogenous to the one used in currently active EWS, can offer a different and complementary perspective on the causes of such crises. We focus on the area of West Africa, which has been at the center of many humanitarian crisis since the beginning of this century. By exploiting state of the art text mining techniques on a corpus of textual documents in French (including video transcriptions extracted from the YouTube channels of four West African news broadcasters and news articles obtained from the online versions of two local newspapers of Burkina Faso) we will analyze food security situations in different regions of the study area in recent years, by also proposing a food security indicator based on textual data, namely TXT-FS.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {315–322},
numpages = {8},
keywords = {topic modeling, spatiotemporal analysis, food security, text mining, social media},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inproceedings{10.1145/3269206.3271737,
author = {Meng, Yu and Shen, Jiaming and Zhang, Chao and Han, Jiawei},
title = {Weakly-Supervised Neural Text Classification},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271737},
doi = {10.1145/3269206.3271737},
abstract = {Deep neural networks are gaining increasing popularity for the classic text classification task, due to their strong expressive power and less requirement for feature engineering. Despite such attractiveness, neural text classification models suffer from the lack of training data in many real-world applications. Although many semi-supervised and weakly-supervised text classification models exist, they cannot be easily applied to deep neural models and meanwhile support limited supervision types. In this paper, we propose a weakly-supervised method that addresses the lack of training data in neural text classification. Our method consists of two modules: (1) a pseudo-document generator that leverages seed information to generate pseudo-labeled documents for model pre-training, and (2) a self-training module that bootstraps on real unlabeled data for model refinement. Our method has the flexibility to handle different types of weak supervision and can be easily integrated into existing deep neural models for text classification. We have performed extensive experiments on three real-world datasets from different domains. The results demonstrate that our proposed method achieves inspiring performance without requiring excessive training data and outperforms baseline methods significantly.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {983–992},
numpages = {10},
keywords = {text classification, pseudo document generation, weakly-supervised learning, neural classification model},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1109/MSR.2019.00040,
author = {Rahman, Akond and Farhana, Effat and Imtiaz, Nasif},
title = {Snakes in Paradise? Insecure Python-Related Coding Practices in Stack Overflow},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00040},
doi = {10.1109/MSR.2019.00040},
abstract = {Despite being the most popular question and answer website for software developers, answers posted on Stack Overflow (SO) are susceptible to contain Python-related insecure coding practices. A systematic analysis on how frequently insecure coding practices appear in SO answers can help the SO community assess the prevalence of insecure Python code blocks in SO. An insecure coding practice is recurrent use of insecure coding patterns in Python. We conduct an empirical study using 529,054 code blocks collected from Python-related 44,966 answers posted on SO. We observe 7.1% of the 44,966 Python-related answers to include at least one insecure coding practice. The most frequently occurring insecure coding practice is code injection. We observe 9.8% of the 7,444 accepted answers to include at least one insecure code block. We also find user reputation not to relate with the presence of insecure code blocks, suggesting that both high and low-reputed users are likely to introduce insecure code blocks.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {200–204},
numpages = {5},
keywords = {stack overflow, reputation, python},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3268891.3268897,
author = {Soriano, Lorna T. and Palaoag, Thelma D.},
title = {A Machine Learning-Based Topic Extraction and Categorization of State Universities and Colleges (SUC) Customer Feedbacks},
year = {2018},
isbn = {9781450365024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268891.3268897},
doi = {10.1145/3268891.3268897},
abstract = {Academic institutions collect an increasing amount of data through surveys. Aside from the usual numeric ratings obtained from the survey, the hierarchical concerns and sentiments can also be identified through the text-based customer feedbacks. These feedbacks contain text about customer experiences with the products offered and services delivered by an institution. A challenge in analyzing unstructured customer feedback is in making sense of the topics that are expressed in words used to describe these experiences.This study develops a model for text analysis of the customer feedbacks that exploits machine learning algorithms such as topic modeling. This further described the text mining process steps undergone in extracting useful information from the customer survey feedbacks of one of the SUCs in the Philippines, the Bicol State College of Applied Sciences and Technology (BISCAST). Moreover, the Latent Dirichlet Allocation (LDA), a topic modeling method, was used for automatic text summarization and topic extraction from these text-based data. The topmost concerns extracted from the feedbacks were identified. This information provides useful insights for management analysis as well as inputs for policy making.},
booktitle = {Proceedings of the 8th International Conference on Information Communication and Management},
pages = {1–6},
numpages = {6},
keywords = {topic modeling, Machine learning, Latent Dirichlet Allocation (LDA)},
location = {Edinburgh, United Kingdom},
series = {ICICM '18}
}

@inproceedings{10.1145/3018661.3018711,
author = {Rakesh, Vineeth and Jadhav, Niranjan and Kotov, Alexander and Reddy, Chandan K.},
title = {Probabilistic Social Sequential Model for Tour Recommendation},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018711},
doi = {10.1145/3018661.3018711},
abstract = {The pervasive growth of location-based services such as Foursquare and Yelp has enabled researchers to incorpo- rate better personalization into recommendation models by leveraging the geo-temporal breadcrumbs left by a plethora of travelers. In this paper, we explore Travel path recommendation, which is one of the applications of intelligent urban navigation that aims in recommending sequence of point of interest (POIs) to tourists. Currently, travelers rely on a tedious and time-consuming process of searching the web, browsing through websites such as Trip Advisor, and reading travel blogs to compile an itinerary. On the other hand, people who do not plan ahead of their trip find it extremely difficult to do this in real-time since there are no automated systems that can provide personalized itinerary for travelers. To tackle this problem, we propose a tour recommendation model that uses a probabilistic generative framework to incorporate user's categorical preference, influence from their social circle, the dynamic travel transitions (or patterns) and the popularity of venues to recommend sequence of POIs for tourists. Through comprehensive experiments over a rich dataset of travel patterns from Foursquare, we show that our model is capable of outperforming the state-of-the-art probabilistic tour recommendation model by providing contextual and meaningful recommendation for travelers.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {631–640},
numpages = {10},
keywords = {recommender systems, social media, probabilistic generative models, foursquare, geo-location, topic models},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3469213.3470396,
author = {Wen, Kai and Wang, Hanlin and Lu, Jiali and Zhang, Zhenkang},
title = {Irony Recognition Combined with LDA and Improved One-Dimensional Intra-Attention Model},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470396},
doi = {10.1145/3469213.3470396},
abstract = {For short texts with ironic emotions, because the data is sparse and irony features are difficult to predict and extract, which causes the problem of substitution of irony text recognition accuracy, a more accurate irony recognition model is proposed. On the basis of the internal attention model, focus on the unique contradictory emotional vocabulary of ironic sentences; then, simultaneously input the sentences into the LDA model to obtain the maximum probability topic of the short text and use the Bi-LSTM model to obtain the two-way semantic dependence of the text; Finally, before the prediction layer, the above three are spliced for softmax classification. Compared with traditional irony recognition models such as LSTM, it has achieved better results on the Weibo comment data set and Ptacek data set.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {189},
numpages = {5},
keywords = {LDA, Bi-LSTM, Single-dimensional intra-attention model, Irony text},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1145/3308774.3308795,
author = {Zhang, Yongfeng and Zhang, Yi and Zhang, Min},
title = {Report on EARS'18: 1st International Workshop on ExplainAble Recommendation and Search},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3308774.3308795},
doi = {10.1145/3308774.3308795},
abstract = {This is a report on the first edition of the International Workshop on ExplainAble Recommendation and Search (EARS 2018), co-located with the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2018) held in Ann Arbor, Michigan, USA on July 8-12, 2018. The workshop was held on July 12, 2018, co-chaired by Yongfeng Zhang, Yi Zhang, and Min Zhang, with invited keynote speeches delivered by Prof. Paul Resnick from University of Michigan, and Dr. Qingsong Hua from Alibaba Inc. An invited panelist committee including Professors Matthew Lease, Paul Resnick, Mark Sanderson, Wlodek Zadrozny and Dr. Qingsong Hua made fruitful discussions about the past, present, and future on the research of explainable recommendation and search. The workshop accepted six contributed papers, and attracted over 80 registered participants. The scope of the workshop spans from technical approaches to explainable recommendation and search, to policy debates on the "principle of transparency" and the "right to explanation" of algorithmic decisions implied in recent EU General Data Protection Regulation (GDPR) and other similar regulations such as The California Consumer Privacy Act of 2018.},
journal = {SIGIR Forum},
month = {jan},
pages = {125–131},
numpages = {7}
}

@inproceedings{10.1145/3366423.3380141,
author = {Fang, Zhihan and Wang, Guang and Wang, Shuai and Zuo, Chaoji and Zhang, Fan and Zhang, Desheng},
title = {CellRep: Usage Representativeness Modeling and Correction Based on Multiple City-Scale Cellular Networks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380141},
doi = {10.1145/3366423.3380141},
abstract = {Understanding representativeness in cellular web logs at city scale is essential for web applications. Most of the existing work on cellular web analyses or applications is built upon data from a single network in a city, which may not be representative of the overall usage patterns since multiple cellular networks coexist in most cities in the world. In this paper, we conduct the first comprehensive investigation of multiple cellular networks in a city with a 100% user penetration rate. We study web usage pattern (e.g., internet access services) correlation and difference between diverse cellular networks in terms of spatial and temporal dimensions to quantify the representativeness of web usage from a single network in usage patterns of all users in the same city. Moreover, relying on three external datasets, we study the correlation between the representativeness and contextual factors (e.g., Point-of-Interest, population, and mobility) to explain the potential causalities for the representativeness difference. We found that contextual diversity is a key reason for representativeness difference, and representativeness has a significant impact on the performance of real-world applications. Based on the analysis results, we further design a correction model to address the bias of single cellphone networks and improve representativeness by 45.8%.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {584–595},
numpages = {12},
keywords = {Cellular Networks, Representativeness, Correction},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3299869.3314036,
author = {Bach, Stephen H. and Rodriguez, Daniel and Liu, Yintao and Luo, Chong and Shao, Haidong and Xia, Cassandra and Sen, Souvik and Ratner, Alex and Hancock, Braden and Alborzi, Houman and Kuchhal, Rahul and R\'{e}, Chris and Malkin, Rob},
title = {Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314036},
doi = {10.1145/3299869.3314036},
abstract = {Labeling training data is one of the most costly bottlenecks in developing machine learning-based applications. We present a first-of-its-kind study showing how existing knowledge resources from across an organization can be used as weak supervision in order to bring development time and cost down by an order of magnitude, and introduce Snorkel DryBell, a new weak supervision management system for this setting. Snorkel DryBell builds on the Snorkel framework, extending it in three critical aspects: flexible, template-based ingestion of diverse organizational knowledge, cross-feature production serving, and scalable, sampling-free execution. On three classification tasks at Google, we find that Snorkel DryBell creates classifiers of comparable quality to ones trained with tens of thousands of hand-labeled examples, converts non-servable organizational resources to servable models for an average 52% performance improvement, and executes over millions of data points in tens of minutes.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {362–375},
numpages = {14},
keywords = {systems for machine learning, weak supervision},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/3173574.3173859,
author = {Wang, Xu and Lafreniere, Benjamin and Grossman, Tovi},
title = {Leveraging Community-Generated Videos and Command Logs to Classify and Recommend Software Workflows},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173859},
doi = {10.1145/3173574.3173859},
abstract = {Users of complex software applications often rely on inefficient or suboptimal workflows because they are not aware that better methods exist. In this paper, we develop and validate a hierarchical approach combining topic modeling and frequent pattern mining to classify the workflows offered by an application, based on a corpus of community-generated videos and command logs. We then propose and evaluate a design space of four different workflow recommender algorithms, which can be used to recommend new workflows and their associated videos to software users. An expert validation of the task classification approach found that 82% of the time, experts agreed with the classifications. We also evaluate our workflow recommender algorithms, demonstrating their potential and suggesting avenues for future work.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {workflow recommendation, topic modeling, software learning, application logs, community-generated videos},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3308558.3313623,
author = {Jin, Di and Huang, Jiantao and Jiao, Pengfei and Yang, Liang and He, Dongxiao and Soulie-Fogelman, Fran\c{c}oise and Huang, Yuxiao},
title = {A Novel Generative Topic Embedding Model by Introducing Network Communities},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313623},
doi = {10.1145/3308558.3313623},
abstract = {Topic models have many important applications in fields such as Natural Language Processing. Topic embedding modelling aims at introducing word and topic embeddings into topic models to describe correlations between topics. Existing topic embedding methods use documents alone, which suffer from the topical fuzziness problem brought by the introduction of embeddings of semantic fuzzy words, e.g. polysemous words or some misleading academic terms. Links often exist between documents which form document networks. The use of links may alleviate this semantic fuzziness, but they are sparse and noisy which may meanwhile mislead topics. In this paper, we utilize community structure to solve these problems. It can not only alleviate the topical fuzziness of topic embeddings since communities are often believed to be topic related, but also can overcome the drawbacks brought by the sparsity and noise of networks (because community is a high-order network information). We give a new generative topic embedding model which incorporates documents (with topics) and network (with communities) together, and uses probability transition to describe the relationship between topics and communities to make it robust when topics and communities do not match. An efficient variational inference algorithm is then proposed to learn the model. We validate the superiority of our new approach on two tasks, document classifications and visualization of topic embeddings, respectively.},
booktitle = {The World Wide Web Conference},
pages = {2886–2892},
numpages = {7},
keywords = {Community structure, Document networks, Topic embedding},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3323503.3360644,
author = {Bertalan, Vithor Gomes and Ruiz, Evandro Eduardo Seron},
title = {Using Topic Modeling to Find Main Discussion Topics in Brazilian Political Websites},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360644},
doi = {10.1145/3323503.3360644},
abstract = {Knowing the main discussion topics debated by the general public is a valuable asset to politicians and professionals involved with politics. Lately, alternative media websites became popular venues in which political ideas are debated without the influence of mainstream media. In this article, we propose the construction of a topic modeling framework, using LSI, LDA, and HDP, to identify main discussion issues in political websites. Experiments show that these models presented results similar to state of the art, offering a viable solution to track political discourse in left-wing and right-wing websites.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {245–248},
numpages = {4},
keywords = {latent semantic indexing, political texts, topic coherence, topic modeling, hierarchial dirichlet process, latent dirichlet allocation},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3132847.3133063,
author = {Zhu, Endong and Rao, Yanghui and Xie, Haoran and Liu, Yuwei and Yin, Jian and Wang, Fu Lee},
title = {Cluster-Level Emotion Pattern Matching for Cross-Domain Social Emotion Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133063},
doi = {10.1145/3132847.3133063},
abstract = {This paper addresses the task of cross-domain social emotion classification of online documents. The cross-domain task is formulated as using abundant labeled documents from a source domain and a small amount of labeled documents from a target domain, to predict the emotion of unlabeled documents in the target domain. Although several cross-domain emotion classification algorithms have been proposed, they require that feature distributions of different domains share a sufficient overlapping, which is hard to meet in practical applications. This paper proposes a novel framework, which uses the emotion distribution of training documents at the cluster level, to alleviate the aforementioned issue. Experimental results on two datasets show the effectiveness of our proposed model on cross-domain social emotion classification.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2435–2438},
numpages = {4},
keywords = {clustering, emotion detection, cross-domain classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3077136.3084135,
author = {Jagerman, Rolf and Eickhoff, Carsten and de Rijke, Maarten},
title = {Computing Web-Scale Topic Models Using an Asynchronous Parameter Server},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3084135},
doi = {10.1145/3077136.3084135},
abstract = {Topic models such as Latent Dirichlet Allocation (LDA) have been widely used in information retrieval for tasks ranging from smoothing and feedback methods to tools for exploratory search and discovery. However, classical methods for inferring topic models do not scale up to the massive size of today's publicly available Web-scale data sets. The state-of-the-art approaches rely on custom strategies, implementations and hardware to facilitate their asynchronous, communication-intensive workloads. We present APS-LDA, which integrates state-of-the-art topic modeling with cluster computing frameworks such as Spark using a novel asynchronous parameter server. Advantages of this integration include convenient usage of existing data processing pipelines and eliminating the need for disk writes as data can be kept in memory from start to finish. Our goal is not to outperform highly customized implementations, but to propose a general high-performance topic modeling framework that can easily be used in today's data processing pipelines. We compare APS-LDA to the existing Spark LDA implementations and show that our system can, on a 480-core cluster, process up to 135\texttimes{} more data and 10\texttimes{} more topics without sacricing model quality.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1337–1340},
numpages = {4},
keywords = {topic modeling, parameter server, latent dirichlet allocation},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3524383.3524404,
author = {Liang, Yi and Shi, Yanlan},
title = {Research on Interactive Conceptual Network of Historical Vocabulary for International Chinese Learning and Teaching},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524404},
doi = {10.1145/3524383.3524404},
abstract = {The combination of the Internet and education brings us a series of new technologies, which transcends the boundaries between schools and classes, provides a new educational model for individual learners and forms a brand-new educational environment. International Chinese Education is now going through a major transformation from face-to-face teaching to smart online teaching. From the student's point of view, Chinese History is an attractive but difficult course, there are overtly abundant contents of Chinese history, and made even more so if the course looks closer into dynasties. It is necessary to extract the essentials and systematically display it to students. For this reason, based on the Connectivism learning theory and the schema theory, we made an experimental corpus, collecting Chinese history textbooks and history reading materials for foreign students. We use K-means to cluster the text topics quickly and accurately, synthesize the word co-occurrence relationship in the corpus, and use Networkx to build the word relationship network diagram. Then, we generate a JSON file to facilitate the production of interactive web pages. In the web page, we use d3.js to display the network. By using Ajax, we dynamically update the data to achieve sentence retrieval. Finally, we use the VUE framework to update the data list synchronously on the web page.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {108–113},
numpages = {6},
keywords = {E-learning, International Chinese teaching, Computer assisted instruction, Concept Network, Knowledge graph},
location = {Shanghai, China},
series = {ICBDE '22}
}

@inproceedings{10.1145/3377170.3377232,
author = {Sun, Siyu and Gai, Yingjie and Zhou, Yingying and Xu, Aiting},
title = {Research on User Comments of Douban Animation Made in China Based on Text Mining Technology},
year = {2019},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377232},
doi = {10.1145/3377170.3377232},
abstract = {With the advent of the "Internet plus" era, the online film criticism has springing up rapidly. The success of "NeZha" has brought the domestic animation film to an unprecedented high tide and brought a lot of information about the film reviews. Based on Chinese natural language processing technology, this paper takes the Douban movie review of domestic animation series as the research object, uses Python to crawl the movie review data, on the basis of the preprocessing of data cleaning, data normalization, Chinese word segmentation and removal of stop words, etc., carries out machine learning based emotional tendency analysis, and the visual analysis of word cloud and the analysis of sentiment orientation based on machine learning, At last, uses LDA theme model to mine movie reviews in depth. The research shows that: the audience pays more attention to the plot setting, character shaping, picture presentation, production ability, line performance and other aspects of domestic animation series films; Douban users tend to be positive attitude of domestic animation, but there is still a large proportion of negative emotion;the praise and popularity of domestic animated films continue to rise, but the uneven quality of domestic animation films still needs to be improved.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {89–93},
numpages = {5},
keywords = {Domestic animation, Machine learning, Emotion analysis, LDA theme analysis, NLP},
location = {Shanghai, China},
series = {ICIT 2019}
}

@inproceedings{10.1145/3366030.3366090,
author = {Khan, Muhammad Haseeb UR Rehman and Wakabayashi, Kei and Fukuyama, Satoshi},
title = {Events Insights Extraction from Twitter Using LDA and Day-Hashtag Pooling},
year = {2019},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366090},
doi = {10.1145/3366030.3366090},
abstract = {News extraction from Twitter data is a hot topic. But can we extract much more than just news? The purpose of this research is to find, either news is the only information which can be extracted from Twitter data or it contains much more insights about real life events. So, we introduce a technique for analysis of Twitter's raw content. After pre-processing of tweets data, we apply hashtag pooling and extract topics using available topic modeling algorithm Latent Dirichlet Allocation (LDA) without modifying its core machinery. In the second part, estimated number of tweets per day and correlated top hashtags for each topic are calculated using day-hashtag pooling. Finally, the continues time series graph is constructed for topic analysis. Our findings show interesting results of bursty news detection, topic popularity, people's way to perceiving an event, real-life event's transition over time and before &amp; after affects of a specific event.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {240–244},
numpages = {5},
keywords = {LDA, Hashtag Pooling, Topic Modeling, Time Series Analysis},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3132847.3133150,
author = {Patwari, Ayush and Goldwasser, Dan and Bagchi, Saurabh},
title = {TATHYA: A Multi-Classifier System for Detecting Check-Worthy Statements in Political Debates},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133150},
doi = {10.1145/3132847.3133150},
abstract = {Fact-checking political discussions has become an essential clog in computational journalism. This task encompasses an important sub-task---identifying the set of statements with 'check-worthy' claims. Previous work has treated this as a simple text classification problem discounting the nuances involved in determining what makes statements check-worthy. We introduce a dataset of political debates from the 2016 US Presidential election campaign annotated using all major fact-checking media outlets and show that there is a need to model conversation context, debate dynamics and implicit world knowledge. We design a multi-classifier system TATHYA, that models latent groupings in data and improves state-of-art systems in detecting check-worthy statements by 19.5% in F1-score on a held-out test set, gaining primarily gaining in Recall.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2259–2262},
numpages = {4},
keywords = {clustering, natural language processing, computational journalism},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3514094.3534203,
author = {Adam, Hammaad and Yang, Ming Ying and Cato, Kenrick and Baldini, Ioana and Senteio, Charles and Celi, Leo Anthony and Zeng, Jiaming and Singh, Moninder and Ghassemi, Marzyeh},
title = {Write It Like You See It: Detectable Differences in Clinical Notes by Race Lead to Differential Model Recommendations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534203},
doi = {10.1145/3514094.3534203},
abstract = {Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {7–21},
numpages = {15},
keywords = {clinical notes, natural language processing, health equity},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@article{10.1145/3487580,
author = {Xu, Jia and Zhou, Yuanhang and Chen, Gongyu and Ding, Yuqing and Yang, Dejun and Liu, Linfeng},
title = {Topic-Aware Incentive Mechanism for Task Diffusion in Mobile Crowdsourcing through Social Network},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3487580},
doi = {10.1145/3487580},
abstract = {Crowdsourcing has become an efficient paradigm to utilize human intelligence to perform tasks that are challenging for machines. Many incentive mechanisms for crowdsourcing systems have been proposed. However, most of existing incentive mechanisms assume that there are sufficient participants to perform crowdsourcing tasks. In large-scale crowdsourcing scenarios, this assumption may be not applicable. To address this issue, we diffuse the crowdsourcing tasks in social network to increase the number of participants. To make the task diffusion more applicable to crowdsourcing system, we enhance the classic Independent Cascade model so the influence is strongly connected with both the types and topics of tasks. Based on the tailored task diffusion model, we formulate the Budget Feasible Task Diffusion (BFTD) problem for maximizing the value function of platform with constrained budget. We design a parameter estimation algorithm based on Expectation Maximization algorithm to estimate the parameters in proposed task diffusion model. Benefitting from the submodular property of the objective function, we apply the budget-feasible incentive mechanism, which satisfies desirable properties of computational efficiency, individual rationality, budget-feasible, truthfulness, and guaranteed approximation, to stimulate the task diffusers. The simulation results based on two real-world datasets show that our incentive mechanism can improve the number of active users and the task completion rate by 9.8% and 11%, on average.},
journal = {ACM Trans. Internet Technol.},
month = {dec},
articleno = {23},
numpages = {23},
keywords = {incentive mechanism, Mobile crowdsourcing, reverse auction, social network, EM algorithm}
}

@inproceedings{10.1145/3382494.3410694,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Hu, Yuanzhe and Wang, Qing},
title = {Quest for the Golden Approach: An Experimental Evaluation of Duplicate Crowdtesting Reports Detection},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410694},
doi = {10.1145/3382494.3410694},
abstract = {Background: Given the invisibility and unpredictability of distributed crowdtesting processes, there is a large number of duplicate reports, and detecting these duplicate reports is an important task to help save testing effort. Although, many approaches have been proposed to automatically detect the duplicates, the comparison among them and the practical guidelines to adopt these approaches in crowdtesting remain vague.Aims: We aim at conducting the first experimental evaluation of the commonly-used and state-of-the-art approaches for duplicate detection in crowdtesting reports, and exploring which is the golden approach.Method: We begin with a systematic review of approaches for duplicate detection, and select ten state-of-the-art approaches for our experimental evaluation. We conduct duplicate detection with each approach on 414 crowdtesting projects with 59,289 reports collected from one of the largest crowdtesting platforms.Results: Machine learning based approach, i.e., ML-REP, and deep learning based approach, i.e., DL-BiMPM, are the best two approaches for duplicate reports detection in crowdtesting, while the later one is more sensitive to the size of training data and more time-consuming for model training and prediction.Conclusions: This paper provides new insights and guidelines to select appropriate duplicate detection techniques for duplicate crowdtesting reports detection.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {17},
numpages = {12},
keywords = {machine learning, information retrieval, duplicate detection, Crowdtesting, deep learning},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1145/3402179,
author = {Uprety, Sagar and Gkoumas, Dimitris and Song, Dawei},
title = {A Survey of Quantum Theory Inspired Approaches to Information Retrieval},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3402179},
doi = {10.1145/3402179},
abstract = {Since 2004, researchers have been using the mathematical framework of quantum theory in information retrieval (IR). Quantum theory offers a generalized probability and logic framework. Such a framework has been shown to be capable of unifying the representation, ranking, and user cognitive aspects of IR, and helpful in developing more dynamic, adaptive, and context-aware IR systems. Although quantum-inspired IR is still a growing area, a wide array of work in different aspects of IR has been done and produced promising results. This article presents a survey of the research done in this area, aiming to show the landscape of the field and draw a road map of future directions.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {98},
numpages = {39},
keywords = {Information retrieval, quantum-inspired models, quantum theory}
}

@inproceedings{10.1145/3357384.3358102,
author = {Liu, Yuanxin and Lin, Zheng and Liu, Fenglin and Dai, Qinyun and Wang, Weiping},
title = {Generating Paraphrase with Topic as Prior Knowledge},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358102},
doi = {10.1145/3357384.3358102},
abstract = {Paraphrase generation can be modeled as a sequence-to-sequence (Seq2Seq) learning problem. Nonetheless, a typical Seq2Seq model is liable to convey the original meaning incorrectly, as the vectorial representation of the given sentence is sometimes inadequate in recapitulating complicated semantic. Naturally, paraphrases concern the same topic, which can serve as an auxiliary guidance to promote the preservation of source semantic. Moreover, some interesting words for restatements can be derived from the topical information. To exploit topic in paraphrase generation, we incorporate topic words into the Seq2Seq framework through a topic-aware input and a topic-biased generation distribution. Direct supervision signals are also introduced to help dealing with the topic information more accurately. Empirical studies on two benchmark datasets show that the proposed method significantly improves the basic Seq2Seq model, and it is comparable with the state-of-the-art systems.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2381–2384},
numpages = {4},
keywords = {paraphrase generation, seq2seq framework, topic modeling},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3486622.3493935,
author = {Li, Zongxi and Li, Xianming and Xie, Haoran and Li, Qing and Tao, Xiaohui},
title = {A Label Extension Schema for Improved Text Emotion Classification},
year = {2021},
isbn = {9781450391153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486622.3493935},
doi = {10.1145/3486622.3493935},
abstract = {Due to the subjectiveness and fuzziness of emotions in texts, researchers have been aware that it is ubiquitous to observe multiple emotions in a sentence, and the one-hot label approach is not informative enough in emotion-relevant text classification tasks. Therefore, to facilitate the classification task, recent works focus on generating and employing a coarse-grained emotion distribution, which is based on coarse-grained labels provided by the underlying dataset. Although such methods can alleviate the problem of overfitting and improve robustness, they may cause inter-class confusion between similar emotion categories and introduce undesirable noise during training. Meanwhile, current studies neglect the fine-grained emotions associated with these coarse-grained labels. To address the issue caused by utilizing a coarse-grained distribution, we propose in this paper a general and novel emotion label extension method based on fine-grained emotions. Specifically, we first identify a mapping function between coarse-grained emotions and fine-grained emotion concepts, and extend the original label space with specific fine-grained emotions. Then, we generate a fine-grained emotion distribution by employing a rule-based method, and utilize it as a model constraint to incorporate the dependencies among fine-grained emotions to predict the original coarse-grained emotion labels. We conduct extensive experiments to demonstrate the effectiveness of our proposed label extension method. The results indicate that our proposed method can produce notable improvements over baseline models on the applied datasets.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {32–39},
numpages = {8},
keywords = {emotion classification, sentiment analysis, label extension},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1162/coli_a_00369,
author = {Hao, Shudong and Paul, Michael J.},
title = {An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models},
year = {2020},
issue_date = {March 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00369},
doi = {10.1162/coli_a_00369},
abstract = {Probabilistic topic modeling is a common first step in crosslingual tasks to enable knowledge transfer and extract multilingual features. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different models can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four models on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.},
journal = {Comput. Linguist.},
month = {mar},
pages = {95–134},
numpages = {40}
}

@article{10.14778/3551793.3551864,
author = {Zhou, Xiangmin and Chen, Lei},
title = {Migrating Social Event Recommendation over Microblogs},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551864},
doi = {10.14778/3551793.3551864},
abstract = {Real applications like crisis management require the real time awareness of critical situations. However, the services using traditional methods like phone calls can be easily delayed due to busy lines, transfer delays or limited communication ability in disaster areas. Existing social event analysis solutions enhanced the situation awareness of systems. Unfortunately, they cannot recognize the complex migrating social events that are first observed in social media at a specific time, place and state, but have further moved in space and time, which may affect the system comprehension. While the discussion on events appears in microblogs, their movement over different contexts is unavoidable. So far, the problem of migrating social event analysis from big media is not well investigated yet. To address this issue, we propose a novel framework to monitor and deliver the migrating events in big social media data, which fully exploits the social media information over multiple attributes and their inherent interactions among events. Specifically, we first propose a Concept TF/IDF model to capture the content that is constrained by the time and location of media without costly learning process. Then, we construct a novel Maximal User Influence Graph (MUIG) to extract the social interactions. With MUIG, the event migrations over space and time are well identified. Finally, we design efficient query strategies over Apache Spark for recommending events in real time. Extensive tests over big media are conducted to prove the high effectiveness and efficiency of our approach.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3213–3225},
numpages = {13}
}

@inproceedings{10.1145/3133264.3133305,
author = {Zhi, Shuting and Li, Xiaoge and Zhang, Jinan and Fan, Xian and Du, Liping and Li, Zhongyang},
title = {Aspects Opinion Mining Based on Word Embedding and Dependency Parsing},
year = {2017},
isbn = {9781450352956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133264.3133305},
doi = {10.1145/3133264.3133305},
abstract = {Aspects opinion mining is one of the most important issues in the field of Natural Language Processing. This paper proposes an algorithm of aspects opinion mining by combining word embedding and dependency parsing. Firstly, training word embedding and constructing sentiment and aspect lexicon by word embedding. Secondly, using dependency parser to discover the phrases that have dependencies. Thirdly, filtering these phrases according to the sentiment and aspect lexicon, and obtaining language patterns of aspects. Lastly, using these language patterns of aspects to discover all emotion words of every aspect, and computing the sentimental orientation of every aspect. The experimental results on a reviews corpus of a video software show that the precision, recall and F-score of our algorithm achieves to 73.17%, 76.60%, and 74.85% respectively.},
booktitle = {Proceedings of the International Conference on Advances in Image Processing},
pages = {210–215},
numpages = {6},
keywords = {Aspect lexicon, Sentiment lexicon, Language pattern, Sentiment analysis},
location = {Bangkok, Thailand},
series = {ICAIP 2017}
}

@inproceedings{10.1145/3078564.3078570,
author = {Wang, Jingya and Wang, Peng and Fan, Yaqin and Sun, Feiqiang},
title = {Research on User Influence in Microblog Based on Interest Graph},
year = {2017},
isbn = {9781450352109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078564.3078570},
doi = {10.1145/3078564.3078570},
abstract = {Microblog1 is currently China's largest social networking platform. In recent years, as a social media, microblog influence continues to expand. The users who have large influence play a guiding role in the spread of microblog, and even lead to public opinion tendency. Therefore, we can use the method of influence analysis to find microblog users who are with great influence, which is of great significance for the research and mining of microblog. User influence analysis in microblog has great difficulties due to the short microblog information, update quickly and nonstandard microblog language. Firstly, according to the social relationship of microblog users and the microblog content that users generate, we use label propagation algorithm combined with LDA (Latent Dirichlet Allocation) algorithm to divide users by user interest graph. Then, on the basis of different interest areas, an improved PageRank algorithm based on user interaction behavior is proposed to calculate the user's influence. Finally, the experimental results are analyzed and compared. The results showed that the proposed method can obtain better results.},
booktitle = {Proceedings of the 6th International Conference on Information Engineering},
articleno = {7},
numpages = {6},
keywords = {Interest graph, Influence calculation, Label propagation algorithm, Microblogs, LDA},
location = {Dalian Liaoning, China},
series = {ICIE '17}
}

@inproceedings{10.1145/3478586.3478629,
author = {Nag, Prashant Kumar and Priya R, Vishnu},
title = {Contextual BI-Directional Attention Flow With Embeddings From Language Models: A Generative Approach to Emotion Detection},
year = {2021},
isbn = {9781450389716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478586.3478629},
doi = {10.1145/3478586.3478629},
abstract = {Detection of Emotions from the text is a tedious task. Presently, existing models failed to detect the emotion in absence of the emotional word in the text. The cause phrase selection which gives a deep insight into emotions is considered to be a tough task. The proposed model for detecting emotions is developed through seven layers. Initially, the dataset is represented in the Topical documents using Adversarial Topic Modelling (ATM). Convolutional Neural Network (CNN) maps each phrase in the topical document to Higher-dimensional vectors, followed by the ELMo Model to obtain the fixed word Embeddings vectors. LSTM is responsible for making the interaction between the words in word embeddings and produces the context and query vectors. The bi-directional Attention flow layer determines the most relevant similarity between Context and Query. Finally, Robustly Optimized BERT (RoBERT) architecture is used to detect the Emotion. It is noted that the proposed multi-stage model detects better emotions than all the existing state-of-art models for detecting emotions.},
booktitle = {Advances in Robotics - 5th International Conference of The Robotics Society},
articleno = {51},
numpages = {6},
keywords = {RoBERT, BiDAF, Phrase Mining, Word Embeddings, ELMo, Deep Learning},
location = {Kanpur, India},
series = {AIR2021}
}

@inproceedings{10.1145/3103010.3121040,
author = {Badenes-Olmedo, Carlos and Redondo-Garcia, Jos\'{e} Luis and Corcho, Oscar},
title = {Distributing Text Mining Tasks with LibrAIry},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121040},
doi = {10.1145/3103010.3121040},
abstract = {We present librAIry, a novel architecture to store, process and analyze large collections of textual resources, integrating existing algorithms and tools into a common, distributed, high-performance workflow. Available text mining techniques can be incorporated as independent plug&amp;play modules working in a collaborative manner into the framework. In the absence of a pre-defined flow, librAIry leverages on the aggregation of operations executed by different components in response to an emergent chain of events. Extensive use of Linked Data (LD) and Representational State Transfer (REST) principles are made to provide individually addressable resources from textual documents. We have described the architecture design and its implementation and tested its effectiveness in real-world scenarios such as collections of research papers, patents or ICT aids, with the objective of providing solutions for decision makers and experts in those domains. Major advantages of the framework and lessons-learned from these experiments are reported.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {63–66},
numpages = {4},
keywords = {scholarly data, nlp, large-scale text analysis, text mining, data integration},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3442442.3451371,
author = {Deligiannis, Panagiotis and Vergoulis, Thanasis and Chatzopoulos, Serafeim and Tryfonopoulos, Christos},
title = {Visualising Scientific Topic Evolution},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451371},
doi = {10.1145/3442442.3451371},
abstract = {The automatic extraction of topics is a standard technique for summarizing text corpora from various domains (e.g., news articles, transport or logistic reports, scientific publications) that has several applications. Since, in many cases, topics are subject to continuous change there is the need to monitor the evolution of a set of topics of interest, as the corresponding corpora are updated. The evolution of scientific topics, in particular, is of great interest for researchers, policy makers, fund managers, and other professionals/engineers in the research and academic community. In this work, we demonstrate a prototype that provides intuitive visualisations for the evolution of scientific topics providing insights about topic transformation, merging, and splitting during the recent years. Although the prototype works on top of a scientific text corpus, its implementation is generic and can be easily applied on texts from other domains, as well.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {468–472},
numpages = {5},
keywords = {topic evolution, visualisation, Topic modeling},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3511095.3531289,
author = {Martins, Emanuelle Azevedo and Salles, Isadora and Benevenuto, Fabricio and Goussevskaia, Olga},
title = {Characterizing Sponsored Content in Facebook and Instagram},
year = {2022},
isbn = {9781450392334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511095.3531289},
doi = {10.1145/3511095.3531289},
abstract = {In this work we present a comparative analysis of influencer marketing evolution on Facebook and Instagram, spanning the pre and post Covid-19 pandemic onset periods. We collected and characterized a large-scale cross-platform dataset, comprised of 9.5 million sponsored posts. We analyzed the relative growth rates of the number of ads and of user engagement within different topics of interest, such as sports, retail, travel, and politics. We discuss which topics have been most impacted by the onset of the pandemic, both in terms of sponsored content supply and demand. With this work we hope to expand the understanding of influence dynamics on social networks and provide support for the development of more contextualized and effective branding strategies.},
booktitle = {Proceedings of the 33rd ACM Conference on Hypertext and Social Media},
pages = {52–63},
numpages = {12},
keywords = {Instagram, Social network analysis, influencer marketing, Facebook., sponsored content},
location = {Barcelona, Spain},
series = {HT '22}
}

@inproceedings{10.1145/3462757.3466085,
author = {Aumiller, Dennis and Almasian, Satya and Lackner, Sebastian and Gertz, Michael},
title = {Structural Text Segmentation of Legal Documents},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466085},
doi = {10.1145/3462757.3466085},
abstract = {The growing complexity of legal cases has lead to an increasing interest in legal information retrieval systems that can effectively satisfy user-specific information needs. However, such downstream systems typically require documents to be properly formatted and segmented, which is often done with relatively simple pre-processing steps, disregarding topical coherence of segments. Systems generally rely on representations of individual sentences or paragraphs, which may lack crucial context, or document-level representations, which are too long for meaningful search results. To address this issue, we propose a segmentation system that can predict topical coherence of sequential text segments spanning several paragraphs, effectively segmenting a document and providing a more balanced representation for downstream applications. We build our model on top of popular transformer networks and formulate structural text segmentation as topical change detection, by performing a series of independent classifications that allow for efficient fine-tuning on task-specific data. We crawl a novel dataset consisting of roughly 74,000 online Terms-of-Service documents, including hierarchical topic annotations, which we use for training. Results show that our proposed system significantly outperforms baselines, and adapts well to structural peculiarities of legal documents. We release both data and trained models to the research community for future work.1},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {2–11},
numpages = {10},
keywords = {document understanding, text segmentation, outline generation},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.1145/3443467.3443838,
author = {Duan, Li and Feng, Haojun and Liu, Shukan},
title = {Keywords Extraction in Vertical Domain Based on Global Knowledge and Local Semantic Relations},
year = {2020},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443838},
doi = {10.1145/3443467.3443838},
abstract = {Because most keywords extraction algorithms do not combine domain expertise and semantic information effectively, a vertical domain keywords extraction method is proposed and verified in some fields. Based on the construction of domain dictionary database, the algorithm uses the Label Latent Dirichlet Allocation (Label-LDA) model to learn the global knowledge to obtain the importance evaluation of each words under vertical domain, and uses the Bidirectional Encoder Representation from Transformers (BERT) model to obtain the local semantic relationship evaluation at the same time. The two models are weighted by the gradient descent method to construct a new word graph. Finally, an improved K-shell algorithm is proposed for ranking the importance of words and extract keywords. Experiments show that, compared with several commonly used domain wide keywords extraction algorithms, the effect of this algorithm on military field is improved by 18% to 52%. At the same time, the algorithm can be effectively generalized to other different vertical fields such as economy, sports, science and technology.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {697–702},
numpages = {6},
keywords = {BERT, Label-LDA, Domain dictionary, Improved K-shell, Keywords extraction},
location = {Xiamen, China},
series = {EITCE 2020}
}

@inproceedings{10.1145/3025453.3025893,
author = {Musabirov, Ilya and Bulygin, Denis and Okopny, Paul and Sirotkin, Alexander},
title = {Deconstructing Cosmetic Virtual Goods Experiences in Dota 2},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025893},
doi = {10.1145/3025453.3025893},
abstract = {Cosmetic items do not provide functional advantages in games, but, nevertheless, they play an important role in the overall player experience. Possessing predominantly socially-constructed dimensions of value, cosmetic items are chosen, discussed, assessed, and valuated in an ongoing iterative collaborative process by communities of players. In our study, we explore the case of Dota 2 and apply Topic Modeling to community-discussions data gathered from Reddit.com. We describe social experiences related to the valuation of cosmetic items in interaction and collision of various logics, including artificial scarcity, decomposition of visual effects, and connectedness to the game lore. Our findings connect the collective experience of players in the game and on online community platforms, suggesting that non-utility-based social value construction becomes an important part of game experience.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {2054–2058},
numpages = {5},
keywords = {social media/online communities, cosmetic items, decorative items, virtual goods, games/play},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3487553.3524933,
author = {Sarkar, Souvika and Karmaker, Shubhra Kanti (Santu)},
title = {Concept Annotation from Users Perspective: A New Challenge},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524933},
doi = {10.1145/3487553.3524933},
abstract = {Text data is highly unstructured and can often be viewed as a complex representation of different concepts, entities, events, sentiments etc. For a wide variety of computational tasks, it is thus very important to annotate text data with the associated concepts / entities, which can put some initial structure / index on raw text data. However, It is not feasible to manually annotate a large amount of text, raising the need for automatic text annotation. In this paper, we focus on concept annotation in text data from the perspective of real world users. Concept annotation is not a trivial task and its utility often highly relies on the preference of the user. Despite significant progress in natural language processing research, we still lack a general purpose concept annotation tool which can effectively serve users from a wide range of application domains. Thus, further investigation is needed from a user-centric point of view to design an automated concept annotation tool that will ensure maximum utility to its users. To achieve this goal, we created a benchmark corpus of two real world data-sets, i.e., “News Concept Data-set” and “Medical Concept Data-set”, to introduce the notion of user-oriented concept annotation and provide a way to evaluate this task. The term “user-centric” means that the desired concepts are defined as well as characterized by the users themselves. Throughout the paper, we describe the details about how we created the data-sets, what are the unique characteristics of each data-set, how these data-sets reflect real users perspective for the concept annotation task, and finally, how they can serve as a great resource for future research on user-centric concept annotation.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1180–1188},
numpages = {9},
keywords = {Data-set collection, Human computer collaboration, Information retrieval, Concept annotation, Text annotation, Data Mining},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3139295.3139296,
author = {Cao, Ying and Chan, Antoni B. and Lau, Rynson W. H.},
title = {Mining Probabilistic Color Palettes for Summarizing Color Use in Artwork Collections},
year = {2017},
isbn = {9781450354110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139295.3139296},
doi = {10.1145/3139295.3139296},
abstract = {Artists and designers often use examples to find inspirational ideas for using colors. While growing public art repositories provide more examples to choose from, understanding the color use in such large artwork collections can be challenging. In this paper, we present a novel technique for summarizing the color use in large artwork collections. Our technique is based on a novel representation, probabilistic color palettes, which can intuitively summarize the contextual and stylistic use of colors in a collection of artworks. Unlike traditional color palettes that only encapsulate what colors are used using a compact set of representative colors, probabilistic color palettes encode the knowledge of how the colors are used in terms of frequencies, positions, and sizes, using an intuitive set of probability distributions. Given a collection of artworks organized by artist, we learn the probabilistic color palettes using a probabilistic colorization model, which describes the colorization process in a probabilistic framework and considers the impact of both spatial and semantic factors upon the colorization process. The learned probabilistic color palettes allows users to quickly understand the color use within the collection. We present results on a large collection of artworks by different artists, and evaluate the effectiveness of our probabilistic color palettes in a user study.},
booktitle = {SIGGRAPH Asia 2017 Symposium on Visualization},
articleno = {1},
numpages = {8},
keywords = {probabilistic modeling, visual analytics, color palettes},
location = {Bangkok, Thailand},
series = {SA '17}
}

@inproceedings{10.1145/3123266.3123442,
author = {Gao, Yuqi and Sang, Jitao and Ren, Tongwei and Xu, Changsheng},
title = {Hashtag-Centric Immersive Search on Social Media},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123442},
doi = {10.1145/3123266.3123442},
abstract = {Social media information distributes in different Online Social Networks (OSNs). This paper addresses the problem integrating the cross-OSN information to facilitate an immersive social media search experience. We exploit hashtag, which is widely used to annotate and organize multi-modal items in different OSNs, as the bridge for information aggregation and organization. A three-stage solution framework is proposed for hashtag representation, clustering and demonstration. Given an event query, the related items from three OSNs, Twitter, Flickr and YouTube, are organized in cluster-hashtag-item hierarchy for display. The effectiveness of the proposed solution is validated by qualitative and quantitative experiments on hundreds of trending event queries.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1924–1932},
numpages = {9},
keywords = {social multimedia, cross-osn application, social media search},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3106426.3106440,
author = {Alharbi, Abdullah Semran and Li, Yuefeng and Xu, Yue},
title = {Topical Term Weighting Based on Extended Random Sets for Relevance Feature Selection},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106440},
doi = {10.1145/3106426.3106440},
abstract = {It is challenging to discover relevant features from long documents that describe user information needs due to the nature of text where synonymy, polysemy noise, and high dimensionality are inherited problems. Traditional feature selection methods could not effectively deal with these problems, because they assume that documents describe one topic only. Topic-based techniques, such as Latent Dirichlet Allocation (LDA), relax this assumption. They have been developed on the basis that a document can exhibit multiple hidden topics. However, LDA does not show encouraging results in selecting relevant features, because LDA calculates the weight of terms based on their local documents and does not generalise it globally at the collection level. So as to address this problem, we propose an innovative and effective extended random set model to generalise LDA weight for local document terms. The model is used as a weighting scheme for topical terms. It can assign a more discriminately accurate weight to these terms based on their appearance in LDA topics and relevant documents. The experimental results, based on the standard RCV1 dataset, TREC topics, and five standard performance measures, show that the proposed model significantly outperforms eight state-of-the-art baseline models in information filtering.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {654–661},
numpages = {8},
keywords = {latent dirichlet allocation, text mining, feature selection, term weighting, extended random set},
location = {Leipzig, Germany},
series = {WI '17}
}

@article{10.1145/3405843,
author = {Ma, Tinghuai and Al-Sabri, Raeed and Zhang, Lejun and Marah, Bockarie and Al-Nabhan, Najla},
title = {The Impact of Weighting Schemes and Stemming Process on Topic Modeling of Arabic Long and Short Texts},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3405843},
doi = {10.1145/3405843},
abstract = {In this article, first a comprehensive study of the impact of term weighting schemes on the topic modeling performance (i.e., LDA and DMM) on Arabic long and short texts is presented. We investigate six term weighting methods including Word count method (standard topic models), TFIDF, PMI, BDC, CLPB, and CEW. Moreover, we propose a novel combination term weighting scheme, namely, CmTLB. We utilize the mTFIDF that takes into account the missing terms and the number of the documents in which the term appears when calculating the term weight. For further robust term weight, we combine mTFIDF with two weighting methods. We evaluate CmTLB against the studied weighting schemes by the quality of the learned topics (topic visualization and topic coherence), classification, and clustering tasks. We applied weighting schemes to Latent Dirichlet allocation (LDA) and Dirichlet multinomial mixture (DMM) on eight Arabic long and short document datasets, respectively. The experiment results outline that appropriate weighting schemes can effectively improve topic modeling performance on Arabic texts. More importantly, our proposed CmTLB significantly outperforms the other weighting schemes. Secondly, we investigate whether the Arabic stemming process can improve topic modeling performance. We study the three approaches of Arabic stemming including root-based, stem-based, and statistical approaches. We also train topic models with weighting schemes on documents after applying four stemmers related to different stemming approaches. The results outline that applying the stemming process not only reduces the dimensionality of term-document matrix leading to fast estimation process, but also show enhancement of topic modeling performance both on short and long Arabic documents. Moreover, Farasa stemmer achieves the highest performance in most cases, since it prevents the ambiguity that may happen because of the blind removal of the affixes such as in root-based or stem-based stemmers.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {81},
numpages = {23},
keywords = {Dirichlet multinomial mixture (DMM), term weighting schemes, natural language processing (NLP), Latent Dirichlet allocation (LDA), stemming process, arabic text}
}

@inproceedings{10.1145/3308558.3313449,
author = {Mukherjee, Subhabrata and Guennemann, Stephan},
title = {GhostLink: Latent Network Inference for Influence-Aware Recommendation},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313449},
doi = {10.1145/3308558.3313449},
abstract = {Social influence plays a vital role in shaping a user's behavior in online communities dealing with items of fine taste like movies, food, and beer. For online recommendation, this implies that users' preferences and ratings are influenced due to other individuals. Given only time-stamped reviews of users, can we find out who-influences-whom, and characteristics of the underlying influence network? Can we use this network to improve recommendation? While prior works in social-aware recommendation have leveraged social interaction by considering the observed social network of users, many communities like Amazon, Beeradvocate, and Ratebeer do not have explicit user-user links. Therefore, we propose GhostLink, an unsupervised probabilistic graphical model, to automatically learn the latent influence network underlying a review community - given only the temporal traces (timestamps) of users' posts and their content. Based on extensive experiments with four real-world datasets with 13 million reviews, we show that GhostLink improves item recommendation by around 23% over state-of-the-art methods that do not consider this influence. As additional use-cases, we show that GhostLink can be used to differentiate between users' latent preferences and influenced ones, as well as to detect influential users based on the learned influence graph.},
booktitle = {The World Wide Web Conference},
pages = {1310–1320},
numpages = {11},
keywords = {Review Community, Generative Model, Content Analysis, Social Recommendation, Social Influence},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3041021.3055365,
author = {Jin, Hongshan},
title = {Detection and Characterization of Influential Cross-Lingual Information Diffusion on Social Networks},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055365},
doi = {10.1145/3041021.3055365},
abstract = {Social network services (SNSs) have become new global and multilingual information platforms due to their popularity. In SNSs with content-sharing functionality, such as "retweet" in Twitter and "share" in Facebook, posts are easily and quickly shared among users, and some of which can spread over different regions and languages. In this work, we first define the concept of cross-lingual information cascade on the basis of the main language of users and then try to characterize and detect those information cascades which can widely spread over different regions and languages on social networks. Understanding the cross-lingual characteristics of information cascades is not only valuable for sociological research, but also beneficial in the practical sense for those who want to know globally-influential events (e.g. ALS Ice Bucket Challenge and Terrorism in Europe) and estimate the impact of global advertisements on products (e.g. Samsung galaxy phone and a movie, Your Name). On the first attempt, we conducted statistical analysis of cascade growth and language distribution of information cascades with a large Twitter dataset. Based on the results, we propose a feature-based model, by which we successfully detected influential cross-lingual information cascades.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {741–745},
numpages = {5},
keywords = {multilingualism, information cascades, cascade growth, information diffusion, cross-lingual cascades},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.5555/3546258.3546539,
author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
title = {Contrastive Estimation Reveals Topic Posterior Information to Linear Models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers trained on these representations perform well in document classification tasks with very few training examples.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {281},
numpages = {31},
keywords = {contrastive estimation, latent Dirichlet allocation, representation learning}
}

@inproceedings{10.1145/3459637.3482144,
author = {Wang, Shuai and Mao, Wenji},
title = {Modeling Inter-Claim Interactions for Verifying Multiple Claims},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482144},
doi = {10.1145/3459637.3482144},
abstract = {To inhibit the spread of rumorous information, fact checking aims at retrieving evidence to verify the truthfulness of a given statement. Fact checking methods typically use knowledge graphs (KGs) as external repositories and develop reasoning methods to retrieve evidence from KGs. As real-world statement is often complex and contains multiple claims, multi-claim fact verification is not only necessary but more important for practical applications. However, existing methods only focus on verifying a single claim (i.e. a single-claim statement). Multiple claims imply rich context information and modeling the interrelations between claims can facilitate better verification of a multi-claim statement as a whole. In this paper, we propose a computational method to model inter-claim interactions for multi-claim fact checking. To focus on relevant claims within a statement, our method first extracts topics from the statement and connects the triple claims in the statement to form a claim graph. It then learns a policy-based agent to sequentially select topic-related triples from the claim graph. To fully exploit information from the statement, our method further employs multiple agents and develops a hierarchical attention mechanism to verify multiple claims as a whole. Experimental results on two real-world datasets show the effectiveness of our method for multi-claim fact verification.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3503–3507},
numpages = {5},
keywords = {inter-claim interaction, multi-claim fact checking, knowledge graph, verification},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3478905.3478910,
author = {Ni, Zhenni and Yao, Zhizhen and Liu, Yunmei and Qian, Yuxing},
title = {Dynamic User Needs Modeling Based on Social Support in Online Health Communities},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478910},
doi = {10.1145/3478905.3478910},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {22–27},
numpages = {6},
keywords = {Information systems, Information retrieval, Recommender systems, Retrieval tasks and goals},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3201064.3201102,
author = {Shirokanova, Anna and Silyutina, Olga},
title = {Internet Regulation Media Coverage in Russia: Topics and Countries},
year = {2018},
isbn = {9781450355636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3201064.3201102},
doi = {10.1145/3201064.3201102},
abstract = {Russia first introduced Internet regulation in 2012 with site blockings and then progressed to personal data retention and ban on VPNs. This makes an interesting case because online media had spread and established a parallel political agenda in Russia in the 2000s, before the onset of regulations. The focus of this study is the contents and dynamics of media coverage of Internet regulation in Russia over years, particularly the topics covered and the countries involved. It uses topic modeling and social network analysis to analyze 6,140 texts from Russia's largest mass media collection. The automatic modeling approach helps obtain reproducible evidence on the structure and actors of the otherwise highly politicized discourse. The study demonstrated, first, the growing interest of Russian media to Internet regulation, with comparable shares of state-controlled and private media in this discourse. Second, it revealed the structure of 50 topics arranging into nine clusters, from gambling to international relations, with one dominant network segment spanning over five clusters. Third, it identified groups of countries by their appearance in the texts and co-appearance in one text as 'communities' of countries that can 'put on the map' the discourse on certain topics of Internet regulation in Russia.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {359–363},
numpages = {5},
keywords = {topic modeling, internet regulation, social network analysis, russia},
location = {Amsterdam, Netherlands},
series = {WebSci '18}
}

@inproceedings{10.1145/3239283.3239316,
author = {Chen, Yuyun and Dong, Hang and Wang, Wei},
title = {Topic-Graph Based Recommendation on Social Tagging Systems: A Study on Research Gate},
year = {2018},
isbn = {9781450365215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239283.3239316},
doi = {10.1145/3239283.3239316},
abstract = {Social Tagging Systems (STSs), allowing users to annotate online resources with freely chosen key words, are an essential type of application in Web 2.0. Recommendation in STSs can prevent information overload and support users to locate relevant items for interaction. This article applies a Topic-Graph Based Recommendation approach. First, we discover semantics behind tags through topic inferencing with Latent Dirichlet Allocation (LDA). Second, we conduct Graph-Based Recommendation for tags and users. The approach is applied on a real-word representative data sample collected from the Academic Social Networking Site ResearchGate. The widely used Co-occurrence Based Graph Recommendation is implemented as a baseline approach. Our preliminary human evaluation shows that the Topic-Graph Based Recommendation can complement to the Cooccurrence baseline to provide more reliable results. Future studies are provided on leveraging future features and information for recommendation from researcher-generated social media data on a large scale.},
booktitle = {Proceedings of the 2018 International Conference on Data Science and Information Technology},
pages = {138–143},
numpages = {6},
keywords = {probabilistic topic models, academic social networking sites, data mining, graph-based recommendation, social tagging systems},
location = {Singapore, Singapore},
series = {DSIT '18}
}

@inproceedings{10.1145/3562007.3562037,
author = {Chen, Feng and Qin, Weiguang},
title = {Using SinaWeibo Microblogs to Identify Complaints of Food Customers},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562007.3562037},
doi = {10.1145/3562007.3562037},
abstract = {Monitoring and responding to complaints about food by consumers are critical for enterprises and governments. Here, we design a method to identify and analyze microblogs posted on SinaWeibo. We proposed a machine learning model to classify the microblogs. Our model used multimodal features and improved a term classification weighting method. We used LDA to identify the most common topics in complaints and summarized bloggers' complaint behavior. Our model obtained an optimal weighted F value of 0.942 using only 4 features. We found that linguistic and multimedia features were more important than social features. When expressing complaints, bloggers were more inclined to mention (@) other bloggers, and most bloggers (80.08%) used images or videos. However, the communication effect of complaint microblogs was even weaker than that of noncomplaint microblogs. Our proposed methods were effective social media monitoring and analysis tools. Data mining of SinaWeibo may reduce information asymmetry and enhance the ability of governments to rapidly respond to community concerns and awareness.},
booktitle = {2022 3rd International Conference on Control, Robotics and Intelligent System},
pages = {165–170},
numpages = {6},
keywords = {text mining, short text classification, text vectorization, microblogs, complaint},
location = {Virtual Event, China},
series = {CCRIS'22}
}

@inproceedings{10.1145/3400806.3400816,
author = {Stine, Zachary Kimo and Agarwal, Nitin},
title = {Comparative Discourse Analysis Using Topic Models: Contrasting Perspectives on China from Reddit},
year = {2020},
isbn = {9781450376884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400806.3400816},
doi = {10.1145/3400806.3400816},
abstract = {In this study, we conduct a comparative analysis of the linguistic features that differentiate two China-focused discussion communities with contrasting perspectives from Reddit. We utilize probabilistic topic modeling to represent submissions from both communities as distributions of latent patterns of word-usage. Using information theoretic measures, we conduct a series of quantitative comparisons between the language patterns of each community and identify salient features that distinguish the two communities relative to each other. We describe the rhetorical techniques and discursive frames implied by these features and how they are utilized by each community in discussions surrounding the Hong Kong protests during 2019. Additionally, we contribute a novel method for representing collections of documents that preserves interdependencies between topics at the document level.},
booktitle = {International Conference on Social Media and Society},
pages = {73–84},
numpages = {12},
keywords = {information theory, computational social science, Comparative discourse analysis, topic models},
location = {Toronto, ON, Canada},
series = {SMSociety'20}
}

@inproceedings{10.1109/ASONAM49781.2020.9381421,
author = {Vassilakis, Costas and Maniataki, Dimitra and Lepouras, George and Antoniou, Angeliki and Spiliotopoulos, Dimitris and Poulopoulos, Vassilis and Wallace, Manolis and Margaris, Dionisis},
title = {Database Knowledge Enrichment Utilizing Trending Topics from Twitter},
year = {2020},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381421},
doi = {10.1109/ASONAM49781.2020.9381421},
abstract = {Every day, many people use at least one social network (or social media) account. This development has been boosted by the rapid growth of technology, making both smartphones and mobile data much more accessible and inexpensive. Therefore, the number of social networks users is growing rapidly, accounting more than 1 billion active users worldwide. The ease of use, as well as the ability to communicate without spatial and temporal restrictions underpinned the rapid increase of the popularity of social networks, as well as their wide acceptance by the general public. This popularity influences people's opinion on many issues, shapes consumer habits and behaviour, mood, etc. The work of many scientists across multiple disciplines has focused on studying social media from various perspectives, including marketing, journalism and sociology. This paper investigates how trending information from social media can be used to match topics of interest from cultural database indices. Matches identified in this process are then presented to cultural venue curators, who can then review matches, mark them as useful or reject them, and exploit them for various tasks, and most notably for the promotion of the venue and its content. More specifically, we have developed an application, which collects the 10 most popular twitter trends and then matches their content with the contents of a given cultural database. Using the results of this match, items from the database that may be related to current issues may be recommended to the user. As a result, these matches, after being inspected and approved by the administrator, can be used to attract the interest of the target audience, highlighting the correlation of current issues with the database's items.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {870–876},
numpages = {7},
keywords = {processing, trending topics, knowledge enrichment, social networks, application, extraction, business intelligence},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3357384.3358020,
author = {Sharma, Ashish and Rudra, Koustav and Ganguly, Niloy},
title = {Going Beyond Content Richness: Verified Information Aware Summarization of Crisis-Related Microblogs},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358020},
doi = {10.1145/3357384.3358020},
abstract = {High-impact catastrophic events (bomb attacks, shootings) trigger posting of large volume of information on social media platforms such as Twitter. Recent works have proposed content-aware systems for summarizing this information, thereby facilitating post-disaster services. However, a significant proportion of the posted content is unverified, which restricts the practical usage of the existing summarization systems. In this paper, we work on the novel task of generating verified summaries of information posted on Twitter during disasters. We first jointly learn representations of content-classes and expression-classes of tweets posted during disasters using a novel LDA-based generative model. These representations of content &amp; expression classes are used in conjunction with pre-disaster user behavior and temporal signals (replies) for training a Tree-LSTM based tweet-verification model. The model infers tweet verification probabilities which are used, besides information content of tweets, in an Integer Linear Programming (ILP) framework for generating the desired verified summaries. The summaries are fine-tuned using the class information of the tweets as obtained from the LDA-based generative model. Extensive experiments are performed on a publicly-available labeled dataset of man-made disasters which demonstrate the effectiveness of our tweet-verification (3-13% gain over baselines) and summarization (12-48% gain in verified content proportion, 8-13% gain in ROUGE-score over state-of-the-art) systems. We make implementations of our various modules available online.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {921–930},
numpages = {10},
keywords = {summarization, disaster, microblogs, unverified information},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1145/3182165,
author = {Yang, Jing and Eickhoff, Carsten},
title = {Unsupervised Learning of Parsimonious General-Purpose Embeddings for User and Location Modeling},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3182165},
doi = {10.1145/3182165},
abstract = {Many social network applications depend on robust representations of spatio-temporal data. In this work, we present an embedding model based on feed-forward neural networks which transforms social media check-ins into dense feature vectors encoding geographic, temporal, and functional aspects for modeling places, neighborhoods, and users. We employ the embedding model in a variety of applications including location recommendation, urban functional zone study, and crime prediction. For location recommendation, we propose a Spatio-Temporal Embedding Similarity algorithm (STES) based on the embedding model.In a range of experiments on real life data collected from Foursquare, we demonstrate our model’s effectiveness at characterizing places and people and its applicability in aforementioned problem domains. Finally, we select eight major cities around the globe and verify the robustness and generality of our model by porting pre-trained models from one city to another, thereby alleviating the need for costly local training.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {32},
numpages = {33},
keywords = {check-in embedding, Social networks, crime prediction, urban functional zone study, personalized location recommendation}
}

@inproceedings{10.1145/3336191.3371817,
author = {Fu, Jinlan and Li, Yi and Zhang, Qi and Wu, Qinzhuo and Ma, Renfeng and Huang, Xuanjing and Jiang, Yu-Gang},
title = {Recurrent Memory Reasoning Network for Expert Finding in Community Question Answering},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371817},
doi = {10.1145/3336191.3371817},
abstract = {Expert finding is a task designed to enable recommendation of the right person who can provide high-quality answers to a requester's question. Most previous works try to involve a content-based recommendation, which only superficially comprehends the relevance between a requester's question and the expertise of candidate experts by exploring the content or topic similarity between the requester's question and the candidate experts' historical answers. However, if a candidate expert has never answered a question similar to the requester's question, then existing methods have difficulty making a correct recommendation. Therefore, exploring the implicit relevance between a requester's question and a candidate expert's historical records by perception and reasoning should be taken into consideration. In this study, we propose a novel textslrecurrent memory reasoning network (RMRN) to perform this task. This method focuses on different parts of a question, and accordingly retrieves information from the histories of the candidate expert.Since only a small percentage of historical records are relevant to any requester's question, we introduce a Gumbel-Softmax-based mechanism to select relevant historical records from candidate experts' answering histories. To evaluate the proposed method, we constructed two large-scale datasets drawn from Stack Overflow and Yahoo! Answer. Experimental results on the constructed datasets demonstrate that the proposed method could achieve better performance than existing state-of-the-art methods.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {187–195},
numpages = {9},
keywords = {community question answering, expert finding, memory network},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3340531.3412684,
author = {Sybrandt, Justin and Tyagin, Ilya and Shtutman, Michael and Safro, Ilya},
title = {AGATHA: Automatic Graph Mining And Transformer Based Hypothesis Generation Approach},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412684},
doi = {10.1145/3340531.3412684},
abstract = {Medical research is risky and expensive. Drug discovery requires researchers to efficiently winnow thousands of potential targets to a small candidate set. However, scientists spend significant time and money long before seeing the intermediate results that ultimately determine this smaller set. Hypothesis generation systems address this challenge by mining the wealth of publicly available scientific information to predict plausible research directions. We present AGATHA, a deep-learning hypothesis generation system that learns a data-driven ranking criteria to recommend new biomedical connections. We massively validate our system with a temporal holdout wherein we predict connections first introduced after 2015 using data published beforehand. We additionally explore biomedical sub-domains, and demonstrate AGATHA's predictive capacity across the twenty most popular relationship types. Furthermore, we perform an ablation study to examine the aspects of our semantic network that most contribute to recommendation quality. Overall, AGATHA achieves best-in-class recommendation quality when compared to other hypothesis generation systems built to predict across all available biomedical literature. Reproducibility: All code, experimental data, and pre-trained models are available online: sybrandt.com/2020/agatha.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2757–2764},
numpages = {8},
keywords = {literature-based discovery, semantic networks, biomedical recommendation, transformer models, hypothesis generation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.5555/3122009.3122042,
author = {Shi, Tianlin and Zhu, Jun},
title = {Online Bayesian Passive-Aggressive Learning},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive-Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1084–1122},
numpages = {39}
}

@article{10.1162/COLI_r_00310,
author = {Duh, Kevin},
title = {Book Review:},
year = {2018},
issue_date = {March 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {44},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_r_00310},
doi = {10.1162/COLI_r_00310},
journal = {Comput. Linguist.},
month = {mar},
pages = {187–189},
numpages = {3}
}

@inproceedings{10.1145/3336499.3338004,
author = {Chen, Chi-Hung and Raschid, Louiqa and Xue, Jinming},
title = {Understanding Trading Interactions and Behavior in Over-the-Counter Markets},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338004},
doi = {10.1145/3336499.3338004},
abstract = {This research applies machine learning methods, in particular probabilistic topic modeling, to understand patterns of interactions for Over-the-Counter (OTC) trading in corporate bonds. The interactions are between broker-dealers (dealers) and clients, or between dealers. From reports of dealer transactions, we create documents representing the daily activity of each dealer. This includes four types of dealer activities: Buy from / Sell to a client, and Buy from / Sell to another dealer. We use Latent Dirichlet Allocation (LDA) based topic models to identify communities of bonds that are bought or sold (co-traded) on the same day. Some communities reflect an industry sector, while others have a concentration of specific bonds. Several topics temporally align to notable financial events. We group dealers around topics to understand their interactions with clients and other dealers. We observe a range of interaction patterns that merit further study, including the centrality of some dealer(s) to some topics. This research illustrates that topic modeling / community detection can indeed provide insight into dealer behavior for OTC trades.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {2},
numpages = {6},
keywords = {financial communities, FINRA TRACE, OTC markets, Latent Dirichlet allocation},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.1145/3325730.3325751,
author = {Wang, Haoyu and Wang, Bei and Li, Can and Xu, Ling and He, JianJun and Yang, Mengning},
title = {SOTagRec: A Combined Tag Recommendation Approach for Stack Overflow},
year = {2019},
isbn = {9781450362580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325730.3325751},
doi = {10.1145/3325730.3325751},
abstract = {Stack Overflow is one of the most popular online programming question and answer websites for developers around the world. Generally, developers need to provide tags for their posting. High-quality tags are expected to facilitate correct classification and efficient search. Unfortunately, tagging process is distributed and uncoordinated due to developers' understanding of their postings, English skills and preferences. Automatic tag recommendation becomes increasingly important for these information sites. In this paper, we propose SOTagRec, a novel tag recommendation approach combing convolutional neural network model and collaborative filtering method. By learning historical postings and their tags from existing information, SOTagRec can accurately infer tags for new postings. We have evaluated SOTagRec on Stackoverflow and compare with the state-of-the-art methods. Experiments Results show that SOTagRec achieves 81.7% and 88.7% respectively for Recall@5 and Recall@10, which outperforms the previous relevant methods.},
booktitle = {Proceedings of the 2019 4th International Conference on Mathematics and Artificial Intelligence},
pages = {146–152},
numpages = {7},
keywords = {collaborative filtering, Stack Overflow, tag recommendation, convolutional neural network},
location = {Chegndu, China},
series = {ICMAI 2019}
}

@inproceedings{10.1145/3546607.3546621,
author = {Chen, Ruyue and Wan, Fucheng and Yu, Hongzhi},
title = {A Survey of Knowledge Representation Learning Based on Structure and Semantics},
year = {2022},
isbn = {9781450387330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546607.3546621},
doi = {10.1145/3546607.3546621},
abstract = {Knowledge representation methods have played an important role in the field of artificial intelligence especially in machine learning and deep learning. It converts useful information such as images, texts, and languages into low-dimensional and dense entity vectors, and provides NLP with better updated ideas and improves computational efficiency. In order to understand the current knowledge representation learning methods and status, this paper analyzes and categorizes the knowledge representation model based on structure and semantics, and finds that the knowledge represented by graph is easy to understand, but there are high complexity and long-tailed distribution, and semantic information of the relationship is difficult to obtain. Therefore, the semantic composition method of relation is adopted to solve this problem.},
booktitle = {Proceedings of the 6th International Conference on Virtual and Augmented Reality Simulations},
pages = {90–95},
numpages = {6},
keywords = {entity alignment, knowledge representation learning, attention knowledge graph, triplet classification},
location = {Brisbane, QLD, Australia},
series = {ICVARS '22}
}

@inproceedings{10.1145/3298689.3346955,
author = {Zhou, Yayu},
title = {Recommender Systems for Contextually-Aware, Versioned Items},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346955},
doi = {10.1145/3298689.3346955},
abstract = {While existing Recommender systems assume items are fixed entities, this research considers situations where there can be different versions of an item. We propose a process that is a type of contextually-aware post filtering for recommending items, and illustrate the system with real data from a newspaper. The novel framework decides whether or not to recommend particular news articles based on news trend and incorporates user states as additional contextual information and recommends versioned items based on user preferences.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {606–610},
numpages = {5},
keywords = {news recommendation, context-aware recommendation, versioned items},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/3503161.3547898,
author = {Lyu, Hanjia and Luo, Jiebo},
title = {Understanding Political Polarization via Jointly Modeling Users, Connections and Multimodal Contents on Heterogeneous Graphs},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547898},
doi = {10.1145/3503161.3547898},
abstract = {Understanding political polarization on social platforms is important as public opinions may become increasingly extreme when they are circulated in homogeneous communities, thus potentially causing damage in the real world. Automatically detecting the political ideology of social media users can help better understand political polarization. However, it is challenging due to the scarcity of ideology labels, complexity of multimodal contents, and cost of time-consuming data collection process. Most previous frameworks either focus on unimodal content or do not scale up well. In this study, we adopt a heterogeneous graph neural network to jointly model user characteristics, multimodal post contents as well as user-item relations in a bipartite graph to learn a comprehensive and effective user embedding without requiring ideology labels. We apply our framework to online discussions about economy and public health topics. The learned embeddings are then used to detect political ideology and understand political polarization. Our framework outperforms the unimodal, early/late fusion baselines, and homogeneous GNN frameworks by a margin of at least 9% absolute gain in the area under the receiver operating characteristic on two social media datasets. More importantly, our work does not require a time-consuming data collection process, which allows faster detection and in turn allows the policy makers to conduct analysis and design policies in time to respond to crises. We also show that our framework learns meaningful user embeddings and can help better understand political polarization. Notable differences in user descriptions, topics, images, and levels of retweet/quote activities are observed. Our framework for decoding user-content interaction shows wide applicability in understanding political polarization. Furthermore, it can be extended to user-item bipartite information networks for other applications such as content and product recommendation.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4072–4082},
numpages = {11},
keywords = {political polarization, user-content interaction, multimedia, heterogeneous graph},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3488560.3498496,
author = {Arora, Akhil and Gerlach, Martin and Piccardi, Tiziano and Garc\'{\i}a-Dur\'{a}n, Alberto and West, Robert},
title = {Wikipedia Reader Navigation: When Synthetic Data Is Enough},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498496},
doi = {10.1145/3488560.3498496},
abstract = {Every day millions of people read Wikipedia. When navigating the vast space of available topics using hyperlinks, readers describe trajectories on the article network. Understanding these navigation patterns is crucial to better serve readers' needs and address structural biases and knowledge gaps. However, systematic studies of navigation on Wikipedia are hindered by a lack of publicly available data due to the commitment to protect readers' privacy by not storing or sharing potentially sensitive data. In this paper, we ask: How well can Wikipedia readers' navigation be approximated by using publicly available resources, most notably the Wikipedia clickstream data? We systematically quantify the differences between real navigation sequences and synthetic sequences generated from the clickstream data, in 6 analyses across 8 Wikipedia language versions. Overall, we find that the differences between real and synthetic sequences are statistically significant, but with small effect sizes, often well below 10%. This constitutes quantitative evidence for the utility of the Wikipedia clickstream data as a public resource: clickstream data can closely capture reader navigation on Wikipedia and provides a sufficient approximation for most practical downstream applications relying on reader data. More broadly, this study provides an example for how clickstream-like data can generally enable research on user navigation on online platforms while protecting users' privacy.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {16–26},
numpages = {11},
keywords = {user navigation, wikipedia server logs, wikipedia clickstream},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3357384.3358048,
author = {Ahmadvand, Ali and Sahijwani, Harshita and Choi, Jason Ingyu and Agichtein, Eugene},
title = {ConCET: Entity-Aware Topic Classification for Open-Domain Conversational Agents},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358048},
doi = {10.1145/3357384.3358048},
abstract = {Identifying the topic (domain) of each user's utterance in open-domain conversational systems is a crucial step for all subsequent language understanding and response tasks. In particular, for complex domains, an utterance is often routed to a single component responsible for that domain. Thus, correctly mapping a user utterance to the right domain is critical. This is a challenging task: users could mention entities like actors, singers or locations to implicitly indicate the domain, which requires extensive domain knowledge to interpret. To address this problem, we introduce ConCET: a Concurrent Entity-aware conversational Topic classifier, which incorporates entity type information together with the utterance content features. Specifically, ConCET utilizes entity information to enrich the utterance representation, combining character, word, and entity type embeddings into a single representation. However, for rich domains with millions of available entities, unrealistic amounts of labeled training data would be required. To complement our model, we propose a simple and effective method for generating synthetic training data, to augment the typically limited amounts of labeled training data, using commonly available knowledge bases as to generate additional labeled utterances. We extensively evaluate ConCET and our proposed training method first on an openly available human-human conversational dataset called Self-Dialogue, to calibrate our approach against previous state-of-the-art methods; second, we evaluate ConCET on a large dataset of human-machine conversations with real users, collected as part of the Amazon Alexa Prize. Our results show that ConCET significantly improves topic classification performance on both datasets, reaching 8-10% improvements compared to state-of-the-art deep learning methods. We complement our quantitative results with detailed analysis of system performance, which could be used for further improvements of conversational agents.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1371–1380},
numpages = {10},
keywords = {conversational topic classification, open-domain conversational agents, entity-aware conversation domain classification},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3549737.3549759,
author = {Tsapatsoulis, Nicolas},
title = {Classification of Instagram Photos: Topic Modelling vs Transfer Learning},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549759},
doi = {10.1145/3549737.3549759},
abstract = {The existence of pre-trained deep learning models for image classification, such as those trained on the well-known Resnet-50 architecture, allows for easy application of transfer learning to several domains including image retrieval. Recently, we proposed topic modelling for the retrieval of Instagram photos based on the associated hashtags. In this paper we compare content-based image classification, based on transfer learning, with the classification based on topic modelling of Instagram hashtags for a set of 24 different concepts. The comparison was performed on a set of 1944 Instagram photos, 81 per concept. Despite the excellent performance of the pre-trained deep learning models, it appears that text-based retrieval, as performed by the topic models of Instagram hashtags, stills perform better.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {18},
numpages = {7},
keywords = {topic modelling, image classification, deep learning, transfer learning},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3477495.3532008,
author = {Cui, Limeng and Lee, Dongwon},
title = {KETCH: Knowledge Graph Enhanced Thread Recommendation in Healthcare Forums},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532008},
doi = {10.1145/3477495.3532008},
abstract = {Health thread recommendation methods aim to suggest the most relevant existing threads for a user. Most of the existing methods tend to rely on modeling the post contents to retrieve relevant answers. However, some posts written by users with different clinical conditions can be lexically similar, as unrelated diseases (e.g., Angina and Osteoporosis) may have the same symptoms (e.g., back pain), yet irrelevant threads to a user. Therefore, it is critical to not only consider the connections between users and threads, but also the descriptions of users' symptoms and clinical conditions. In this paper, towards this problem of thread recommendation in online healthcare forums, we propose a knowledge graph enhanced Threads Recommendation (KETCH) model, which leverages graph neural networks to model the interactions among users and threads, and learn their representations. In our model, the users, threads and posts are three types of nodes in a graph, linked through their associations. KETCH uses the message passing strategy by aggregating information along with the network. In addition, we introduce a knowledge-enhanced attention mechanism to capture the latent conditions and symptoms. We also apply the method to the task of predicting the side effects of drugs, to show that KETCH has the potential to complement the medical knowledge graph. Comparing with the best results of seven competing methods, in terms of MRR, KETCH outperforms all methods by at least 0.125 on the MedHelp dataset, 0.048 on the Patient dataset and 0.092 on HealthBoards dataset, respectively. We release the source code of KETCH at: https://github.com/cuilimeng/KETCH.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {492–501},
numpages = {10},
keywords = {graph neural networks, recommendation system, medical knowledge graph, online health forum},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3109859.3109890,
author = {Seo, Sungyong and Huang, Jing and Yang, Hao and Liu, Yan},
title = {Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109890},
doi = {10.1145/3109859.3109890},
abstract = {Recently, many e-commerce websites have encouraged their users to rate shopping items and write review texts. This review information has been very useful for understanding user preferences and item properties, as well as enhancing the capability to make personalized recommendations of these websites. In this paper, we propose to model user preferences and item properties using convolutional neural networks (CNNs) with dual local and global attention, motivated by the superiority of CNNs to extract complex features. By using aggregated review texts from a user and aggregated review text for an item, our model can learn the unique features (embedding) of each user and each item. These features are then used to predict ratings. We train these user and item networks jointly which enable the interaction between users and items in a similar way as matrix factorization. The local attention provides us insight on a user's preferences or an item's properties. The global attention helps CNNs focus on the semantic meaning of the whole review text. Thus, the combined local and global attentions enable an interpretable and better-learned representation of users and items. We validate the proposed models by testing on popular review datasets in Yelp and Amazon and compare the results with matrix factorization (MF), the hidden factor and topical (HFT) model, and the recently proposed convolutional matrix factorization (ConvMF+). Our proposed CNNs with dual attention model outperforms HFT and ConvMF+ in terms of mean square errors (MSE). In addition, we compare the user/item embeddings learned from these models for classification and recommendation. These results also confirm the superior quality of user/item embeddings learned from our model.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {297–305},
numpages = {9},
keywords = {attention model, deep learning for recommender systems, convolutional neural network},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3084226.3084241,
author = {Lu, Mengmeng and Liang, Peng},
title = {Automatic Classification of Non-Functional Requirements from Augmented App User Reviews},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084241},
doi = {10.1145/3084226.3084241},
abstract = {Context: The leading App distribution platforms, Apple App Store, Google Play, and Windows Phone Store, have over 4 million Apps. Research shows that user reviews contain abundant useful information which may help developers to improve their Apps. Extracting and considering Non-Functional Requirements (NFRs), which describe a set of quality attributes wanted for an App and are hidden in user reviews, can help developers to deliver a product which meets users' expectations. Objective: Developers need to be aware of the NFRs from massive user reviews during software maintenance and evolution. Automatic user reviews classification based on an NFR standard provides a feasible way to achieve this goal. Method: In this paper, user reviews were automatically classified into four types of NFRs (reliability, usability, portability, and performance), Functional Requirements (FRs), and Others. We combined four classification techniques BoW, TF-IDF, CHI2, and AUR-BoW (proposed in this work) with three machine learning algorithms Naive Bayes, J48, and Bagging to classify user reviews. We conducted experiments to compare the F-measures of the classification results through all the combinations of the techniques and algorithms. Results: We found that the combination of AUR-BoW with Bagging achieves the best result (a precision of 71.4%, a recall of 72.3%, and an F-measure of 71.8%) among all the combinations. Conclusion: Our finding shows that augmented user reviews can lead to better classification results, and the machine learning algorithm Bagging is more suitable for NFRs classification from user reviews than Na\"{\i}ve Bayes and J48.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {344–353},
numpages = {10},
keywords = {Textual Semantics, Non-Functional Requirements, User Reviews, Automatic Classification},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@inproceedings{10.1145/3018661.3018687,
author = {Wang, Pengwei and Zhang, Yong and Ji, Lei and Yan, Jun and Jin, Lianwen},
title = {Concept Embedded Convolutional Semantic Model for Question Retrieval},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018687},
doi = {10.1145/3018661.3018687},
abstract = {The question retrieval, which aims to find similar questions of a given question, is playing pivotal role in various question answering (QA) systems. This task is quite challenging mainly on three aspects: lexical gap, polysemy and word order. In this paper, we propose a unified framework to simultaneously handle these three problems. We use word combined with corresponding concept information to handle the polysemous problem. The concept embedding and word embedding are learned at the same time from both context-dependent and context-independent view. The lexical gap problem is handled since the semantic information has been encoded into the embedding. Then, we propose to use a high-level feature embedded convolutional semantic model to learn the question embedding by inputting the concept embedding and word embedding without manually labeling training data. The proposed framework nicely represent the hierarchical structures of word information and concept information in sentences with their layer-by-layer composition and pooling. Finally, the framework is trained in a weakly-supervised manner on question answer pairs, which can be directly obtained without manually labeling. Experiments on two real question answering datasets show that the proposed framework can significantly outperform the state-of-the-art solutions.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {395–403},
numpages = {9},
keywords = {question retrieval, question embedding, concept embedding},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3091478.3091502,
author = {Hong, Lingzi and Fu, Cheng and Torrens, Paul and Frias-Martinez, Vanessa},
title = {Understanding Citizens' and Local Governments' Digital Communications During Natural Disasters: The Case of Snowstorms},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091502},
doi = {10.1145/3091478.3091502},
abstract = {A growing number of citizens and local governments have embraced the use of Twitter to communicate during natural disasters. Studies have shown that online communications during disasters can be explained using crisis communication taxonomies. However, such taxonomies are broad and general, and offer little insight into the detailed content of the communications. In this paper, we propose a semi-automatic framework to extract and compare, in retrospect, the digital communication footprints of citizens and governments during disasters. These footprints, which characterize the topics discussed during a disaster at different spatio-temporal scales, are computed in an unsupervised manner using topic models, and manually labelled to identify specific issues affecting the population. The end objective is to offer detailed information about issues affecting citizens during natural disasters and to compare these against local governments' communications. We evaluate the framework using Twitter communications from 18 snowstorms (including two blizzards) on the US east coast.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {141–150},
numpages = {10},
keywords = {spatio-temporal modeling, topic models, crisis communication, disaster analytics},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@inproceedings{10.1145/3486622.3493952,
author = {Almgerbi, Mohamad and De Mauro, Andrea and Kahlawi, Adham and Poggioni, Valentina},
title = {Improving Topic Modeling Performance through N-Gram Removal},
year = {2021},
isbn = {9781450391153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486622.3493952},
doi = {10.1145/3486622.3493952},
abstract = {In recent years, topic modeling has been increasingly adopted for finding conceptual patterns in large corpora of digital documents to organize them accordingly. In order to enhance the performance of topic modeling algorithms, such as Latent Dirichlet Allocation (LDA), multiple preprocessing steps have been proposed. In this paper, we introduce N-gram Removal, a novel preprocessing procedure based on the systematic elimination of a dynamic number of repeated words in text documents. We have evaluated the effects of the utilization of N-gram Removal through four different performance metrics: we concluded that its application is effective at improving the performance of LDA and enhances the human interpretation of topics models.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {162–169},
numpages = {8},
keywords = {Big data, Perplexity, Coherence, LDA, Topic Modeling, Data Preprocessing},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1145/3426882,
author = {Wang, Hao and Wang, Bin and Duan, Jianyong and Zhang, Jiajun},
title = {Chinese Spelling Error Detection Using a Fusion Lattice LSTM},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3426882},
doi = {10.1145/3426882},
abstract = {Spelling error detection serves as a crucial preprocessing in many natural language processing applications. Unlike English, where every single word is directly typed by keyboard, we have to use an input method to input Chinese characters. The pinyin input method is the most widely used. By intuition, pinyin should be helpful in detecting spelling errors. However, when detect spelling errors, most of the current methods ignore the pinyin information and adopt a pipeline framework that leads to error propagation. In this article, we propose a fusion lattice-LSTM model under the end-to-end framework to integrate character, word, and pinyin features for error detection. Experiments on the SIGHAN Bake-off-2015 dataset show that pinyin is a discriminating feature, and our end-to-end model outperforms the baseline models obviously.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {28},
numpages = {11},
keywords = {Chinese spelling error, spelling check, neural networks}
}

@inproceedings{10.1145/3129676.3129709,
author = {Hong, Beomseok and Kim, Yanggon and Lee, Sang Ho},
title = {An Efficient Tag Recommendation Method Using Topic Modeling Approaches},
year = {2017},
isbn = {9781450350273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129676.3129709},
doi = {10.1145/3129676.3129709},
abstract = {Software information sites such as Stack Overflow, Super User, and Ask Ubuntu allow users to post software-related questions, answer the questions asked by other users, and add tags to their questions. Tagging is a popular system across web communities because allowing users to classify their contents is less costly than employing an expert to categorize them. However, tagging systems suffer from the problem of the tag explosion and the tag synonym. To solve these problems, we propose a tag recommendation method using topic modeling approaches. Topic models have advantages of dimensionality reduction and document similarity. We also emphasize highest topics in calculating document similarity to retrieve more relevant documents. Our tag recommendation method considers the document similarity and the historical tag occurrence to calculate tag scores. Experiment results show that emphasizing highest topic distributions increases overall performance of tag recommendation.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {56–61},
numpages = {6},
keywords = {Tag recommendation, Stack Overflow, topic modeling, latent Dirichlet allocation, document similarity},
location = {Krakow, Poland},
series = {RACS '17}
}

@inproceedings{10.1145/3097983.3098110,
author = {Wu, Tianyi and Sugawara, Shinya and Yamanishi, Kenji},
title = {Decomposed Normalized Maximum Likelihood Codelength Criterion for Selecting Hierarchical Latent Variable Models},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098110},
doi = {10.1145/3097983.3098110},
abstract = {We propose a new model selection criterion based on the minimum description length principle in a name of the decomposed normalized maximum likelihood criterion. Our criterion can be applied to a large class of hierarchical latent variable models, such as the Naive Bayes models, stochastic block models and latent Dirichlet allocations, for which many conventional information criteria cannot be straightforwardly applied due to irregularity of latent variable models. Our method also has an advantage that it can be exactly evaluated without asymptotic approximation with small time complexity. Our experiments using synthetic and real data demonstrated validity of our method in terms of computational efficiency and model selection accuracy, while our criterion especially dominated the other criteria when sample size is small and when data are noisy.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1165–1174},
numpages = {10},
keywords = {topic and latent variable models, clustering, model selection},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{10.1162/coli_a_00335,
author = {Li, Jing and Song, Yan and Wei, Zhongyu and Wong, Kam-Fai},
title = {A Joint Model of Conversational Discourse and Latent Topics on Microblogs},
year = {2018},
issue_date = {December 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {44},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00335},
doi = {10.1162/coli_a_00335},
abstract = {Conventional topic models are ineffective for topic extraction from microblog messages, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns. To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an unsupervised model that jointly learns word distributions to represent: 1 different roles of conversational discourse, and 2 various latent topics in reflecting content information. By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content. In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better coherence scores than competitive topic models from previous studies. Qualitative analysis on model outputs indicates that our model induces meaningful representations for both discourse and topics. We further present an empirical study on microblog summarization based on the outputs of our joint model. The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.},
journal = {Comput. Linguist.},
month = {dec},
pages = {719–754},
numpages = {36}
}

@inproceedings{10.1145/3197026.3197052,
author = {Salatino, Angelo A. and Osborne, Francesco and Motta, Enrico},
title = {AUGUR: Forecasting the Emergence of New Research Topics},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197052},
doi = {10.1145/3197026.3197052},
abstract = {Being able to rapidly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies. The literature presents several approaches to identifying the emergence of new research topics, which rely on the assumption that the topic is already exhibiting a certain degree of popularity and consistently referred to by a community of researchers. However, detecting the emergence of a new research area at an embryonic stage, i.e., before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge. We address this issue by introducing Augur, a novel approach to the early detection of research topics. Augur analyses the diachronic relationships between research areas and is able to detect clusters of topics that exhibit dynamics correlated with the emergence of new research topics. Here we also present the Advanced Clique Percolation Method (ACPM), a new community detection algorithm developed specifically for supporting this task. Augur was evaluated on a gold standard of 1,408 debutant topics in the 2000-2011 interval and outperformed four alternative approaches in terms of both precision and recall.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {303–312},
numpages = {10},
keywords = {clustering algorithms, scholarly data, topic trends, embryonic topic, ontologies, topic detection, semantic technologies},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3313831.3376380,
author = {Yen, Yu-Chun Grace and Kim, Joy O. and Bailey, Brian P.},
title = {Decipher: An Interactive Visualization Tool for Interpreting Unstructured Design Feedback from Multiple Providers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376380},
doi = {10.1145/3313831.3376380},
abstract = {Feedback from diverse audiences can vary in focus, differ in structure, and contradict each other, making it hard to interpret and act on. While prior work has explored generating quality feedback, our work helps a designer interpret that feedback. Through a formative study with professional designers (N=10), we discovered that the interpretation process includes categorizing feedback, identifying valuable feedback, and prioritizing which feedback to incorporate in a revision. We also found that designers leverage feedback topic and sentiment, and the status of the provider to aid interpretation. Based on the findings, we created a new tool (Decipher) that enables designers to visualize and navigate a collection of feedback using its topic and sentiment structure. In a preliminary evaluation (N=20), we found that Decipher helped users feel less overwhelmed during feedback interpretation tasks and better attend to critical issues and conflicting opinions compared to using a typical document-editing tool.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {feedback, creativity, creativity support tools, sense-making},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3320435.3320477,
author = {Nambhi, Aadhavan M. and Reddy, Bhanu Prakash and Agarwal, Aarsh Prakash and Verma, Gaurav and Singh, Harvineet and Burhanuddin, Iftikhar Ahamath},
title = {Stuck? No Worries! Task-Aware Command Recommendation and Proactive Help for Analysts},
year = {2019},
isbn = {9781450360210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320435.3320477},
doi = {10.1145/3320435.3320477},
abstract = {Data analytics software applications have become an integral part of the decision-making process of analysts. Users of such a software face challenges due to insufficient product and domain knowledge, and find themselves in need of help. To alleviate this, we propose a task-aware command recommendation system, to guide the user on what commands could be executed next. We rely on topic modeling techniques to incorporate information about user's task into our models. We also present a help prediction model to detect if a user is in need of help, in which case the system proactively provides the aforementioned command recommendations. We leverage the log data of a web-based analytics software to quantify the superior performance of our neural models, in comparison to competitive baselines.},
booktitle = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {271–275},
numpages = {5},
keywords = {user tasks, command recommendation, help prediction, topic modeling, application logs},
location = {Larnaca, Cyprus},
series = {UMAP '19}
}

@inproceedings{10.1145/3289600.3291009,
author = {Kim, Jooyeon and Kim, Dongkwan and Oh, Alice},
title = {Homogeneity-Based Transmissive Process to Model True and False News in Social Networks},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291009},
doi = {10.1145/3289600.3291009},
abstract = {An overwhelming number of true and false news stories are posted and shared in social networks, and users diffuse the stories based on multiple factors. Diffusion of news stories from one user to another depends not only on the stories' content and the genuineness but also on the alignment of the topical interests between the users. In this paper, we propose a novel Bayesian nonparametric model that incorporates homogeneity of news stories as the key component that regulates the topical similarity between the posting and sharing users' topical interests. Our model extends hierarchical Dirichlet process to model the topics of the news stories and incorporates Bayesian Gaussian process latent variable model to discover the homogeneity values. We train our model on a real-world social network dataset and find homogeneity values of news stories that strongly relate to their labels of genuineness and their contents. Finally, we show that the supervised version of our model predicts the labels of news stories better than the state-of-the-art neural network and Bayesian models.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {348–356},
numpages = {9},
keywords = {social network, lda, hdp, fake news, false news, homophily, topic modeling, hierarchical dirichlet processes, latent dirichlet allocation, homogeneity, nonparametric bayesian models, gaussian processes},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3284869.3284902,
author = {Amati, Giambattista and Angelini, Simone and Gambosi, Giorgio and Pasquin, Daniele and Rossi, Gianluca and Vocca, Paola},
title = {Twitter: Temporal Events Analysis: Extended Abstract},
year = {2018},
isbn = {9781450365819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284869.3284902},
doi = {10.1145/3284869.3284902},
abstract = {We perform a temporal analysis of the Twitter stream to investigate the evolution of unique events based on the burst of popularity of associated hashtags. We derive a classification of events according to the different patterns corresponding to the peak of the volume of exchanged message and to how these events propagate on any social network with the same characteristics as Twitter. We first provide a precise definition of unique events and correlate them to hashtags. With reference to a specific interval of time, the most popular - with respect to number of tweets- hashtags are then detected using the Seasonal Hybrid ESD (S-H-ESD) technique introduced by Twitter. After identifying the unique hashtags among the 1000 most popular, we have identified, through an unsupervised Machine Learning algorithm applied to the historical temporal series of hashtags limited around the maximum peak, the temporal patterns (clusters) of the events. Finally, using the Twitter features, for each cluster, we have studied both the process at the origin of the event and how they evolve over the network.},
booktitle = {Proceedings of the 4th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {298–303},
numpages = {6},
keywords = {Anomalies Detection, social network temporal evolution, Twitter, LDA, Event analysis},
location = {Bologna, Italy},
series = {Goodtechs '18}
}

@inproceedings{10.1145/3184558.3191592,
author = {Ying, Qiu Fang and Chiu, Dah Ming and Venkatramanan, Srinivasan and Zhang, Xiaopeng},
title = {Profiling OSN Users Based on Temporal Posting Patterns},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191592},
doi = {10.1145/3184558.3191592},
abstract = {In this paper, we study the posting behavior of OSN users, in particular the posting frequency and temporal patterns, and consider possible interpretations of how users use the platform. At the aggregate (macro) level, we find two distinct peaks, one during morning working hours, and one in the evening. The morning peak is more pronounced for frequent posters, while the evening peak is pronounced in the remaining users. We postulate that this difference results from qualitatively different usage of the OSN platform (e.g. for work, with customers, etc.) than purely social interactions (e.g., friends, family, etc.). We also study user posting behavior at an individual (micro) level and apply LDA to cluster user temporal patterns, interpret our results. Our study provides possibly new insights into user activity in today's OSNs, and suggests a framework for profiling users based on their posting activities. In the process, we provide a novel application of LDA, to temporal user posting behavior by equating the time epochs of posts to words in documents. We believe our approach will complement other methods of user profiling based on static demographic information and friendship network information.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1451–1456},
numpages = {6},
keywords = {temporal pattern, user posting behavior, lda, time series analysis},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3394486.3403233,
author = {Lin, Lu and Wang, Hongning},
title = {Graph Attention Networks over Edge Content-Based Channels},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403233},
doi = {10.1145/3394486.3403233},
abstract = {Edges play a crucial role in passing information on a graph, especially when they carry textual content reflecting semantics behind how nodes are linked and interacting with each other. In this paper, we propose a channel-aware attention mechanism enabled by edge text content when aggregating information from neighboring nodes; and we realize this mechanism in a graph autoencoder framework. Edge text content is encoded as low-dimensional mixtures of latent topics, which serve as semantic channels for topic-level information passing on edges. We embed nodes and topics in the same latent space to capture their mutual dependency when decoding the structural and textual information on graph. We evaluated the proposed model on Yelp user-item bipartite graph and StackOverflow user-user interaction graph. The proposed model outperformed a set of baselines on link prediction and content prediction tasks. Qualitative evaluations also demonstrated the descriptive power of the learnt node embeddings, showing its potential as an interpretable representation of graphs.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1819–1827},
numpages = {9},
keywords = {representation learning, graph neural networks, variational auto-encoder, topic modeling},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3364510.3364531,
author = {Ajanovski, Vangel V.},
title = {Body of Knowledge Explorer: Long-Term Student Guidance Across the Computer-Science Domain},
year = {2019},
isbn = {9781450377157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364510.3364531},
doi = {10.1145/3364510.3364531},
abstract = {A visual domain exploration tool is introduced that offers several viewpoints as parts of a personalized student advice and guidance solution: explanation of the personal progress over a reference body of knowledge areas, within the field of study; mapping of the actual courses in relation to the field of study where the taught topics belong; guidance in choosing courses aligned towards personal interests and abilities in respective areas. The student can visualize areas that are successfully covered by past courses, areas that are a probable point of risk, mandating greater focus and determination, and out-of-topic areas where the student is predicted to perform with greater success. The student can view the past trends of personal progress, and experiment with future tracks.},
booktitle = {Proceedings of the 19th Koli Calling International Conference on Computing Education Research},
articleno = {5},
numpages = {5},
keywords = {learning analytics dashboards, student progress tracking, educational recommender systems, curriculum mapping},
location = {Koli, Finland},
series = {Koli Calling '19}
}

@article{10.1109/TASLP.2018.2819941,
author = {Yu, Kai and Zhao, Zijian and Wu, Xueyang and Lin, Hongtao and Liu, Xuan},
title = {Rich Short Text Conversation Using Semantic-Key-Controlled Sequence Generation},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2819941},
doi = {10.1109/TASLP.2018.2819941},
abstract = {With the recent advances of the sequence-to-sequence framework, generation approaches for the short text conversation STC become attractive. The traditional sequence-to-sequence approaches for the STC often suffer from poor diversity and general reply without substantiality. It is also hard to control the topic or semantics of the selected reply from multiple generated candidates. In this paper, a novel external-memory-driven sequence-to-sequence learning approach is proposed to address these problems. A tensor of the external memory is constructed to represent interpretable topics or semantics. During generation, a controllable memory trigger is extracted given the input sequence, and a reply is then generated using the memory trigger as well as the sequence-to-sequence model. Experiments show that the proposed approach can generate much richer diversity than the traditional sequence-to-sequence training with attention. Meanwhile, it achieves better quality score in human evaluation. It is also observed that by manually manipulating the memory trigger, it is possible to interpretably guide the topics or semantics of the reply.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {1359–1368},
numpages = {10}
}

@inproceedings{10.1145/3394486.3403242,
author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Zhang, Yu and Zhang, Chao and Han, Jiawei},
title = {Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403242},
doi = {10.1145/3394486.3403242},
abstract = {Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1908–1917},
numpages = {10},
keywords = {tree embedding, topic mining, text embedding, topic hierarchy},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1145/3474087,
author = {Manogaran, Gunasekaran and Qudrat-Ullah, Hassan and Xin, Qin},
title = {Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3474087},
doi = {10.1145/3474087},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
articleno = {37e},
numpages = {3}
}

@article{10.1145/3487291,
author = {Guo, Aibo and Li, Xinyi and Pang, Ning and Zhao, Xiang},
title = {Adversarial Cross-Domain Community Question Retrieval},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3487291},
doi = {10.1145/3487291},
abstract = {Community Q&amp;A forum is a special type of social media that provides a platform to raise questions and to answer them (both by forum participants), to facilitate online information sharing. Currently, community Q&amp;A forums in professional domains have attracted a large number of users by offering professional knowledge. To support information access and save users’ efforts of raising new questions, they usually come with a question retrieval function, which retrieves similar existing questions (and their answers) to a user’s query. However, it can be difficult for community Q&amp;A forums to cover all domains, especially those emerging lately with little labeled data but great discrepancy from existing domains. We refer to this scenario as cross-domain question retrieval. To handle the unique challenges of cross-domain question retrieval, we design a model based on adversarial training, namely, X-QR, which consists of two modules—a domain discriminator and a sentence matcher. The domain discriminator aims at aligning the source and target data distributions and unifying the feature space by domain-adversarial training. With the assistance of the domain discriminator, the sentence matcher is able to learn domain-consistent knowledge for the final matching prediction. To the best of our knowledge, this work is among the first to investigate the domain adaption problem of sentence matching for community Q&amp;A forums question retrieval. The experiment results suggest that the proposed X-QR model offers better performance than conventional sentence matching methods in accomplishing cross-domain community Q&amp;A tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {61},
numpages = {22},
keywords = {domain adaption, community question retrieval, adversarial training, Community Q&amp;A}
}

@article{10.1145/3086665,
author = {Wang, Hongning and Li, Rui and Shokouhi, Milad and Li, Hang and Chang, Yi},
title = {Search, Mining, and Their Applications on Mobile Devices: Introduction to the Special Issue},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086665},
doi = {10.1145/3086665},
abstract = {In recent years, mobile devices have become the most popular interface for users to retrieve and access information: recent reports show that users spend significantly more time and issue more search queries on mobile devices than on desktops in the United States.1 The accelerated growth of mobile usage brings unique opportunities to the information retrieval and data mining research communities.Mobile devices capture rich contextual and personal signals that can be leveraged to accurately predict users’ intent for serving more relevant content and can even proactively provide novel zero-query recommendations. Apple Siri, Google Now, and Microsoft Cortana are recent examples of such emerging systems. Furthermore, mobile devices constantly generate a huge amount of sensor footprints (e.g., GPS, motion sensors) and user activity data (e.g., used apps) that are often missing from their desktop counterparts. These new sources of implicit and explicit user feedback are valuable for discovering actionable knowledge, and designing better systems that serve each individual the right content at the right time and location. In addition, by aggregating mobile interactions across individuals, one can infer interesting conclusions beyond search and recommendation. Generating real-time traffic estimates is one example of such applications.This special issue focuses on research problems of search, mining, and their applications in mobile devices. Topics of interest in this special issue include but are not limited to mobile data mining and management, mobile search, personalization and recommendation, mobile user interfaces and human-computer interaction, and new applications in the mobile environment. The aim of this special issue is to bring together top experts across multiple disciplines, including information retrieval, data mining, mobile computing, and cyberphysical systems, such that academic and industrial researchers can exchange ideas and share the latest developments on the state of the art and practice of mobile search and mobile data mining.},
journal = {ACM Trans. Inf. Syst.},
month = {aug},
articleno = {29},
numpages = {17},
keywords = {recommendation, Personalization, mobile user interfaces, new applications in mobile environment}
}

@inproceedings{10.1145/3099023.3099031,
author = {W\"{o}rndl, Wolfgang},
title = {A Web-Based Application for Recommending Travel Regions},
year = {2017},
isbn = {9781450350679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3099023.3099031},
doi = {10.1145/3099023.3099031},
abstract = {This demo paper presents a Web application for recommending travel regions for independent travelers. Users can specify preferences such as budget and preferred activities and receive suggested trips consisting of multiple regions. We explain the main ideas behind the data model and algorithm of our solution, and give an overview on the implementation},
booktitle = {Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {105–106},
numpages = {2},
keywords = {web application, recommender system, tourist trip design problem, travel},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@inproceedings{10.1145/3412841.3442016,
author = {Brito, Miguel and Cunha, J\'{a}come and Saraiva, Jo\~{a}o},
title = {Identification of Microservices from Monolithic Applications through Topic Modelling},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442016},
doi = {10.1145/3412841.3442016},
abstract = {Microservices emerged as one of the most popular architectural patterns in the recent years given the increased need to scale, grow and flexibilize software projects accompanied by the growth in cloud computing and DevOps. Many software applications are being submitted to a process of migration from its monolithic architecture to a more modular, scalable and flexible architecture of microservices. This process is slow and, depending on the project's complexity, it may take months or even years to complete.This paper proposes a new approach on microservice identification by resorting to topic modelling in order to identify services according to domain terms. This approach in combination with clustering techniques produces a set of services based on the original software. The proposed methodology is implemented as an open-source tool for exploration of monolithic architectures and identification of microservices. A quantitative analysis using the state of the art metrics on independence of functionality and modularity of services was conducted on 200 open-source projects collected from GitHub. Cohesion at message and domain level metrics' showed medians of roughly 0.6. Interfaces per service exhibited a median of 1.5 with a compact interquartile range. Structural and conceptual modularity revealed medians of 0.2 and 0.4 respectively.Our first results are positive demonstrating beneficial identification of services due to overall metrics' results.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1409–1418},
numpages = {10},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3020165.3020182,
author = {Singh, Jaspreet and Zerr, Sergej and Siersdorfer, Stefan},
title = {Structure-Aware Visualization of Text Corpora},
year = {2017},
isbn = {9781450346771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020165.3020182},
doi = {10.1145/3020165.3020182},
abstract = {Trying to comprehend the structure and content of large text corpora can be a daunting and often time consuming task. In this paper, we introduce a novel tool that exploits the structural properties for extracting and visualizing the underlying topics in a given dataset. To this end, we make use of a combination of latent topic analysis, discriminative feature selection applied on top of the category structure of corpora, and various ranking methods in order to extract the most representative topics for a given corpus. The visual moniker to depict the outcome of these methods can be chosen based on the context. Such visual representations can be useful for depicting trends, identifying ``hot'' topics, and discovering interesting patterns in the underlying data. As applications, we create example representations for a variety of corpora obtained from conference proceedings, movie summaries, and newsgroup postings. Our user experiments demonstrate the viability of our approach, with a flower-like visualization inspired by the ``wheel of emotion'', for generating high quality representative topics and for unearthing hidden structures and connections in large document corpora.},
booktitle = {Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval},
pages = {107–116},
numpages = {10},
keywords = {mutual information, visualizing structured text, text corpora, evenness index, latent topics, lda, visualizing topics, flower},
location = {Oslo, Norway},
series = {CHIIR '17}
}

@inproceedings{10.1145/3308558.3313397,
author = {Li, Changchun and Ouyang, Jihong and Li, Ximing},
title = {Classifying Extremely Short Texts by Exploiting Semantic Centroids in Word Mover's Distance Space},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313397},
doi = {10.1145/3308558.3313397},
abstract = {Automatically classifying extremely short texts, such as social media posts and web page titles, plays an important role in a wide range of content analysis applications. However, traditional classifiers based on bag-of-words (BoW) representations often fail in this task. The underlying reason is that the document similarity can not be accurately measured under BoW representations due to the extreme sparseness of short texts. This results in significant difficulty to capture the generality of short texts. To address this problem, we use a better regularized word mover's distance (RWMD), which can measure distances among short texts at the semantic level. We then propose a RWMD-based centroid classifier for short texts, named RWMD-CC. Basically, RWMD-CC computes a representative semantic centroid for each category under the RWMD measure, and predicts test documents by finding the closest semantic centroid. The testing is much more efficient than the prior art of K nearest neighbor classifier based on WMD. Experimental results indicate that our RWMD-CC can achieve very competitive classification performance on extremely short texts.},
booktitle = {The World Wide Web Conference},
pages = {939–949},
numpages = {11},
keywords = {Semantic Centroid, Hypothesis Margin, Regularized Word Mover's Distance, Extremely Short Texts},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3397271.3401099,
author = {Montazeralghaem, Ali and Zamani, Hamed and Allan, James},
title = {A Reinforcement Learning Framework for Relevance Feedback},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401099},
doi = {10.1145/3397271.3401099},
abstract = {We present RML, the first known general reinforcement learning framework for relevance feedback that directly optimizes any desired retrieval metric, including precision-oriented, recall-oriented, and even diversity metrics: RML can be easily extended to directly optimize any arbitrary user satisfaction signal. Using the RML framework, we can select effective feedback terms and weight them appropriately, improving on past methods that fit parameters to feedback algorithms using heuristic approaches or methods that do not directly optimize for retrieval performance. Learning an effective relevance feedback model is not trivial since the true feedback distribution is unknown. Experiments on standard TREC collections compare RML to existing feedback algorithms, demonstrate the effectiveness of RML at optimizing for MAP and α-n DCG, and show the impact on related measures.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {59–68},
numpages = {10},
keywords = {neural network, reinforcement learning, query language model, relevance feedback model},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3130935,
author = {Li, Yuanchun and Jia, Baoxiong and Guo, Yao and Chen, Xiangqun},
title = {Mining User Reviews for Mobile App Comparisons},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130935},
doi = {10.1145/3130935},
abstract = {As the number of mobile apps keeps increasing, users often need to compare many apps, in order to choose one that best fits their needs. Fortunately, as there are so many users sharing an app market, it is likely that some other users with the same preferences have already made the comparisons and shared their opinions. For example, a user may state that an app is better in power consumption than another app in a review, then the review would help other users who care about battery life while choosing apps. This paper presents a method to identify comparative reviews for mobile apps from an app market, which can be used to provide fine-grained app comparisons based on different topics. According to experiments on 5 million reviews from Google Play and manual assessments on 900 reviews, our method is able to identify opinions accurately and provide meaningful comparisons between apps, which could in turn help users find desired apps based on their preferences.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {75},
numpages = {15},
keywords = {user review, text processing, comparative opinion, Mobile application}
}

@inproceedings{10.1145/3269206.3272012,
author = {Geyik, Sahin Cem and Dialani, Vijay and Meng, Meng and Smith, Ryan},
title = {In-Session Personalization for Talent Search},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3272012},
doi = {10.1145/3269206.3272012},
abstract = {Previous efforts in recommendation of candidates for talent search followed the general pattern of receiving an initial search criteria and generating a set of candidates utilizing a pre-trained model. Traditionally, the generated recommendations are final, that is, the list of potential candidates is not modified unless the user explicitly changes his/her search criteria. In this paper, we are proposing a candidate recommendation model which takes into account the immediate feedback of the user, and updates the candidate recommendations at each step. This setting also allows for very uninformative initial search queries, since we pinpoint the user's intent due to the feedback during the search session. To achieve our goal, we employ an intent clustering method based on topic modeling which separates the candidate space into meaningful, possibly overlapping, subsets (which we call intent clusters) for each position. On top of the candidate segments, we apply a multi-armed bandit approach to choose which intent cluster is more appropriate for the current session. We also present an online learning scheme which updates the intent clusters within the session, due to user feedback, to achieve further personalization. Our offline experiments as well as the results from the online deployment of our solution demonstrate the benefits of our proposed methodology.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {2107–2115},
numpages = {9},
keywords = {in session recommendations, model selection, talent search, online learning},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3214277,
author = {Peng, Liangying and Chen, Ling and Ye, Zhenan and Zhang, Yi},
title = {AROMA: A Deep Multi-Task Learning Based Simple and Complex Human Activity Recognition Method Using Wearable Sensors},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3214277},
doi = {10.1145/3214277},
abstract = {Human activity recognition (HAR) is a promising research issue in ubiquitous and wearable computing. However, there are some problems existing in traditional methods: 1) They treat HAR as a single label classification task, and ignore the information from other related tasks, which is helpful for the original task. 2) They need to predesign features artificially, which are heuristic and not tightly related to HAR task. To address these problems, we propose AROMA (human activity recognition using deep multi-task learning). Human activities can be divided into simple and complex activities. They are closely linked. Simple and complex activity recognitions are two related tasks in AROMA. For simple activity recognition task, AROMA utilizes a convolutional neural network (CNN) to extract deep features, which are task dependent and non-handcrafted. For complex activity recognition task, AROMA applies a long short-term memory (LSTM) network to learn the temporal context of activity data. In addition, there is a shared structure between the two tasks, and the object functions of these two tasks are optimized jointly. We evaluate AROMA on two public datasets, and the experimental results show that AROMA is able to yield a competitive performance in both simple and complex activity recognitions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jul},
articleno = {74},
numpages = {16},
keywords = {Human activity recognition, deep learning, multi-task learning, LSTM}
}

@inproceedings{10.1145/3386392.3399297,
author = {Misztal-Radecka, Joanna and Indurkhya, Bipin},
title = {Persona Prototypes for Improving the Qualitative Evaluation of Recommendation Systems},
year = {2020},
isbn = {9781450379502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386392.3399297},
doi = {10.1145/3386392.3399297},
abstract = {The majority of existing research in the field of recommendation systems is aimed at optimizing accuracy metrics for given datasets, which leads to an algorithm-driven design of resulting solutions. Given a lack of understanding of the dataset characteristics and insufficient diversity of represented individuals, such approaches lead to amplifying the hidden data biases and existing disparities. In this research, we address this problem by proposing a Persona Prototyping approach that selects a set of the most representative user individuals to help in understanding the complex distribution of user interests and performing a proper qualitative evaluation of recommendation algorithms. A hierarchical density-based clustering technique is applied to distinguish diverse user groups and select their prototypes. Each of the selected representatives is presented in an easily understandable form of a textual user story describing the prototype behaviors, inspired by the concept of persona from the interaction design. We evaluated the diversity and representativeness of selected individuals and the results show that the proposed method is capable of identifying diverse interest archetypes and can be used to improve the qualitative analysis of recommendations and to test how well they respond to the diversity of user needs.},
booktitle = {Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {206–212},
numpages = {7},
keywords = {model interpretability, unsupervised learning, interaction design, recommendation explanations, recommender systems, hierarchical clustering, prototype selection, user modeling},
location = {Genoa, Italy},
series = {UMAP '20 Adjunct}
}

@inproceedings{10.1145/3447548.3469471,
author = {You, Shan and Xu, Chang and Wang, Fei and Zhang, Changshui},
title = {Workshop on Model Mining},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469471},
doi = {10.1145/3447548.3469471},
abstract = {How to mine the knowledge in the pretrained models is of significance in achieving more promising performance, since practitioners have access to many pretrained models easily. This Workshop on Model Mining aims to investigate more diverse and advanced manners in mining knowledge within models, which tends to leverage the pretrained models more wisely, elegantly and systematically. There are many topics related to this workshop, such as distilling a lightweight model from a well-trained heavy model via teacher-student paradigm, and boosting the performance of the model by carefully designing the predecessor tasks, e.g., pre-training, self-supervised and contrastive learning. Model mining as a special way of data mining is relevant to SIGKDD, and its audience including researchers and engineers will benefit a lot for designing more advanced algorithms for their tasks.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4177–4178},
numpages = {2},
keywords = {model mining, knowledge distillation, transfer learning, large-scale pretraining},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3293883.3301496,
author = {Xie, Xiaolong and Liang, Yun and Li, Xiuhong and Tan, Wei},
title = {CuLDA_CGS: Solving Large-Scale LDA Problems on GPUs},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3301496},
doi = {10.1145/3293883.3301496},
abstract = {GPUs have benefited many ML algorithms. However, we observe that the performance of existing Latent Dirichlet Allocation(LDA) solutions on GPUs are not satisfying. We present CuLDA_CGS, an efficient approach to accelerate large-scale LDA problems. We delicately design workload partition and synchronization mechanism to exploit multiple GPUs. We also optimize the algorithm from the sampling algorithm, parallelization, and data compression perspectives. Experiment evaluations show that compared with the state-of-the-art LDA solutions, CuLDA_CGS outperforms them by a large margin (up to 7.3X) on a single GPU.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {435–436},
numpages = {2},
keywords = {topic modeling, LDA, GPU},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3297280.3297384,
author = {Ramponi, Giorgia and Brambilla, Marco and Ceri, Stefano and Daniel, Florian and Di Giovanni, Marco},
title = {Vocabulary-Based Community Detection and Characterization},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297384},
doi = {10.1145/3297280.3297384},
abstract = {With the increase of digital interaction, social networks are becoming an essential ingredient of our life, by progressively becoming the dominant media, e.g. in influencing political choices. Interaction within social networks tends to take place within communities, sets of social accounts which share friendships, ideas, interests and passions; detecting digital communities is of increasing relevance, from a social and economical point of view.In this paper, we argue that the vocabulary of terms used in social interaction is a very distinctive feature of a community, hence it can be effectively used for community detection. We show that, by inspecting the vocabulary used by tweets, we can achieve very efficient classifiers and predictors of account membership within a given community. We describe the syntactic and semantic features that best constitute a vocabulary, then we provide their comparative evaluation and select the best features for the task, and finally we illustrate several applications of our approach to concrete community detection scenarios.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1043–1050},
numpages = {8},
keywords = {content-based data analytics, social analytics, community detection},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3151759.3151780,
author = {Hirchoua, Badr and Ouhbi, Brahim and Frikh, Bouchra},
title = {A New Knowledge Capitalization Framework in Big Data Context},
year = {2017},
isbn = {9781450352994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151759.3151780},
doi = {10.1145/3151759.3151780},
abstract = {In many companies data is used as a source for creating knowledge in their sphere of business. Therefore this paper presents a knowledge capitalization framework in big data context. Based on the technology derived from distributed systems, our research concerns the design and development of a knowledge engineering framework in big data context. It can be integrated in any knowledge management system. The proposed framework is based on four layers: we start by extracting hidden topics, using the LDA approach in batch processing to handle the complexity of multi knowledge domains and to keep the semantic relations between knowledge entities. Then we use clustering mechanisms to pick the best combination between topics from different sources. As a result, we get, in every distributed site, the related topics (knowledge), in order to facilitate the research and access, to get the useful knowledge in real time processing.},
booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {40–48},
numpages = {9},
keywords = {data intelligence, knowledge capitalization, machine learning, topic modeling, big data computing},
location = {Salzburg, Austria},
series = {iiWAS '17}
}

@inproceedings{10.1145/3411408.3411463,
author = {Gialitsis, Nikolaos and Giannakopoulos, George and Athanasouli, Marina},
title = {Evaluation of Distributed DNA Representations on the Classification of Conserved Non-Coding Elements},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411463},
doi = {10.1145/3411408.3411463},
abstract = {The representation of DNA sequences has been an interesting topic of discussion for many years. Presently, given the usefulness of representations built upon embeddings for Natural Language Processing (NLP), there have been efforts to transfer such paradigms to the DNA world and related problems. In this paper, we study different DNA representations on the well-studied problem of Conserved Non-coding Elements (CNEs), trying to understand how well existing representations utilize the value of context, both in terms of local, near context, but also of long-distance interactions in genomic sequences. To this end, we apply a number of methods, including probabilistic models (LDA) and hybrid probabilistic-neural models (lda2vec) on appropriate datasets, compare the results to pre-existing methods and discuss the findings to better understand the value and challenges of different representations in the given domain.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {41–47},
numpages = {7},
location = {Athens, Greece},
series = {SETN 2020}
}

@inproceedings{10.1145/3213586.3225245,
author = {Al-Ghossein, Marie and Abdessalem, Talel and Barr\'{e}, Anthony},
title = {Exploiting Contextual and External Data for Hotel Recommendation},
year = {2018},
isbn = {9781450357845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213586.3225245},
doi = {10.1145/3213586.3225245},
abstract = {The recommendation problem in the hotel industry introduces several interesting and unique challenges leading to the insufficiency of classical approaches. Traveling is not a frequent activity and users tend to have multifaceted behaviors affected by their specific context. While context-aware recommender systems are a promising way to address this problem, the context's dimensions do not contribute equally to the decision-making process and users are not equally sensible to all of the dimensions. In this paper, we propose novel context-aware methods for addressing the hotel recommendation problem, taking into account geography, temporality, textual reviews extracted from social media, and the trip's intent. We present the architecture of the system developed in industry, combining the proposed approaches and addressing each user segment differently. Our experiments show the impact of considering contextual data, external data, and user segmentation on improving the quality of recommendation.},
booktitle = {Adjunct Publication of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {323–328},
numpages = {6},
keywords = {hotel recommendation, context-aware recommendation, industrial application},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inproceedings{10.1145/3084226.3084287,
author = {Sharma, Abhishek and Thung, Ferdian and Kochhar, Pavneet Singh and Sulistya, Agus and Lo, David},
title = {Cataloging GitHub Repositories},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084287},
doi = {10.1145/3084226.3084287},
abstract = {GitHub is one of the largest and most popular repository hosting service today, having about 14 million users and more than 54 million repositories as of March 2017. This makes it an excellent platform to find projects that developers are interested in exploring. GitHub showcases its most popular projects by cataloging them manually into categories such as DevOps tools, web application frameworks, and game engines. We propose that such cataloging should not be limited only to popular projects. We explore the possibility of developing such cataloging system by automatically extracting functionality descriptive text segments from readme files of GitHub repositories. These descriptions are then input to LDA-GA, a state-of-the-art topic modeling algorithm, to identify categories. Our preliminary experiments demonstrate that additional meaningful categories which complement existing GitHub categories can be inferred. Moreover, for inferred categories that match GitHub categories, our approach can identify additional projects belonging to them. Our experimental results establish a promising direction in realizing automatic cataloging system for GitHub.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {314–319},
numpages = {6},
keywords = {Latent Dirichlet Allocation, GitHub, Genetic Algorithm},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@article{10.1145/3320277,
author = {Zhang, Mingyue and Wei, Xuan and Guo, Xunhua and Chen, Guoqing and Wei, Qiang},
title = {Identifying Complements and Substitutes of Products: A Neural Network Framework Based on Product Embedding},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320277},
doi = {10.1145/3320277},
abstract = {Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jun},
articleno = {34},
numpages = {29},
keywords = {Complements, online reviews, product embedding, product recommendation, product relationship, substitutes}
}

@article{10.14778/3137628.3137635,
author = {Aslay, Cigdem and Bonchi, Francesco and Lakshmanan, Laks V. S. and Lu, Wei},
title = {Revenue Maximization in Incentivized Social Advertising},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137635},
doi = {10.14778/3137628.3137635},
abstract = {Incentivized social advertising, an emerging marketing model, provides monetization opportunities not only to the owners of the social networking platforms but also to their influential users by offering a "cut" on the advertising revenue. We consider a social network (the host) that sells ad-engagements to advertisers by inserting their ads, in the form of promoted posts, into the feeds of carefully selected "initial endorsers" or seed users: these users receive monetary incentives in exchange for their endorsements. The endorsements help propagate the ads to the feeds of their followers. Whenever any user engages with an ad, the host is paid some fixed amount by the advertiser, and the ad further propagates to the feed of her followers, potentially recursively. In this context, the problem for the host is is to allocate ads to influential users, taking into account the propensity of ads for viral propagation, and carefully apportioning the monetary budget of each of the advertisers between incentives to influential users and ad-engagement costs, with the rational goal of maximizing its own revenue.We show that, taking all important factors into account, the problem of revenue maximization in incentivized social advertising corresponds to the problem of monotone submodular function maximization, subject to a partition matroid constraint on the ads-to-seeds allocation, and submodular knapsack constraints on the advertisers' budgets. We show that this problem is NP-hard and devise two greedy algorithms with provable approximation guarantees, which differ in their sensitivity to seed user incentive costs.Our approximation algorithms require repeatedly estimating the expected marginal gain in revenue as well as in advertiser payment. By exploiting a connection to the recent advances made in scalable estimation of expected influence spread, we devise efficient and scalable versions of our two greedy algorithms. An extensive experimental assessment confirms the high quality of our proposal.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1238–1249},
numpages = {12}
}

@inproceedings{10.1145/3379174.3392315,
author = {Qiu, Yuchen and Qiao, Yuanyuan and Zhang, Aimin and Yang, Jie},
title = {Residence and Workplace Recovery: User Privacy Risk in Mobility Data},
year = {2020},
isbn = {9781450375092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379174.3392315},
doi = {10.1145/3379174.3392315},
abstract = {Mobility data has been collected through mobile devices and cellular networks used in academic research and commercial purposes for the last decade. Since releasing individual's mobility records or trajectories gives rise to privacy issues, datasets owners tend to only publish encrypted mobility data, which doesn't contains users' identification symbol like telephone number. However, we argue and prove that even publishing encrypted mobility data could lead to privacy problem, of which the critical problem is users' residence and workplace identification. We develop an attack system that is able to identify users' important locations by a semi-supervised learning model. In addition to traditional time features, our system takes the users' mobility and living patterns into consideration, which are important and affect each other. Our model demands for less ground truth labels and produces considerable improvement in learning accuracy. With large-scale factual mobile data and long-time tracking ground truth data captured from a big city, we reveal that our attack system is able to identify users' residence and workplace with accuracy about 98%, which indicates severe privacy leakage in such datasets. And we provide advice for this kind of privacy-preserving problem.},
booktitle = {Proceedings of the 2020 on Intelligent Cross-Data Analysis and Retrieval Workshop},
pages = {15–20},
numpages = {6},
keywords = {location extract, residence identification, semi-supervised learning, privacy preserving, mobility mining},
location = {Dublin, Ireland},
series = {ICDAR '20}
}

@inproceedings{10.1145/3201064.3201086,
author = {Nerghes, Adina and Kerkhof, Peter and Hellsten, Iina},
title = {Early Public Responses to the Zika-Virus on YouTube: Prevalence of and Differences Between Conspiracy Theory and Informational Videos},
year = {2018},
isbn = {9781450355636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3201064.3201086},
doi = {10.1145/3201064.3201086},
abstract = {In this paper, we analyze the content of the most popular videos posted on YouTube in the first phase of the Zika-virus outbreak in 2016, and the user responses to those videos. More specifically, we examine the extent to which informational and conspiracy theory videos differ in terms of user activity (number of comments, shares, likes and dislikes), and the sentiment and content of the user responses. Our results show that 12 out of the 35 videos in our data set focused on conspiracy theories, but no statistical differences were found in the number of user activity and sentiment between the two types of videos. The content of the user responses shows that users respond differently to sub-topics related to Zika-virus. The implications of the results for future online health promotion campaigns are discussed.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {127–134},
numpages = {8},
keywords = {topic modeling, semantic networks, zika-virus, youtube, informational and conspiracy theory videos},
location = {Amsterdam, Netherlands},
series = {WebSci '18}
}

@inproceedings{10.1145/3041021.3055132,
author = {Zhang, Yating and Jatowt, Adam and Tanaka, Katsumi},
title = {Is Tofu the Cheese of Asia? Searching for Corresponding Objects across Geographical Areas},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055132},
doi = {10.1145/3041021.3055132},
abstract = {Keyword-based search engines are widely used nowadays for content retrieval. Creating queries is relatively easy when users wish to retrieve content in familiar domains (e.g., information about things within their own country). However, they often struggle when searching in unfamiliar domains (e.g., searching for information related to a foreign country). In this paper, we approach the vocabulary gap problem by allowing users to search by analogical examples, that is, by letting them utilize information in familiar domains to perform search in domains unfamiliar to them. In particular, we focus on geographical domains. We propose to build connections between two different spaces (e.g., USA and Japan) by mapping the distributed word representations in one space with the ones in the other space. We first introduce an effective technique for automatically constructing seed pairs of terms to be used for finding the optimal mapping function. Then we propose general and topic-based transformations of terms from one space to another. We test the performance of the proposed approaches on datasets derived from Wikipedia which are related to two quite diverse countries: Japan and USA.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1033–1042},
numpages = {10},
keywords = {object search, spatial transformation, spatial counterpart},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3308558.3313432,
author = {Gordon, Mitchell and Althoff, Tim and Leskovec, Jure},
title = {Goal-Setting And Achievement In Activity Tracking Apps: A Case Study Of MyFitnessPal},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313432},
doi = {10.1145/3308558.3313432},
abstract = {Activity tracking apps often make use of goals as one of their core motivational tools. There are two critical components to this tool: setting a goal, and subsequently achieving that goal. Despite its crucial role in how a number of prominent self-tracking apps function, there has been relatively little investigation of the goal-setting and achievement aspects of self-tracking apps. Here we explore this issue, investigating a particular goal setting and achievement process that is extensive, recorded, and crucial for both the app and its users' success: weight loss goals in MyFitnessPal. We present a large-scale study of 1.4 million users and weight loss goals, allowing for an unprecedented detailed view of how people set and achieve their goals. We find that, even for difficult long-term goals, behavior within the first 7 days predicts those who ultimately achieve their goals, that is, those who lose at least as much weight as they set out to, and those who do not. For instance, high amounts of early weight loss, which some researchers have classified as unsustainable, leads to higher goal achievement rates. We also show that early food intake, self-monitoring motivation, and attitude towards the goal are important factors. We then show that we can use our findings to predict goal achievement with an accuracy of 79% ROC AUC just 7 days after a goal is set. Finally, we discuss how our findings could inform steps to improve goal achievement in self-tracking apps.},
booktitle = {The World Wide Web Conference},
pages = {571–582},
numpages = {12},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3475716.3475789,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Wang, Dandan and Wang, Qing},
title = {Characterizing and Predicting Good First Issues},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475789},
doi = {10.1145/3475716.3475789},
abstract = {Background. Where to start contributing to a project is a critical challenge for newcomers of open source projects. To support newcomers, GitHub utilizes the Good First Issue (GFI) label, with which project members can manually tag issues in an open source project that are suitable for the newcomers. However, manually labeling GFIs is time- and effort-consuming given the large number of candidate issues. In addition, project members need to have a close understanding of the project to label GFIs accurately.Aims. This paper aims at providing a thorough understanding of the characteristics of GFIs and an automatic approach in GFIs prediction, to reduce the burden of project members and help newcomers easily onboard.Method. We first define 79 features to characterize the GFIs and further analyze the correlation between each feature and GFIs. We then build machine learning models to predict GFIs with the proposed features.Results. Experiments are conducted with 74,780 issues from 10 open source projects from GitHub. Results show that features related to the semantics, readability, and text richness of issues can be used to effectively characterize GFIs. Our prediction model achieves a median AUC of 0.88. Results from our user study further prove its potential practical value.Conclusions. This paper provides new insights and practical guidelines to facilitate the understanding of GFIs and the automation of GFIs labeling.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {13},
numpages = {12},
keywords = {Newcomers, Machine Learning, Open Source Software, Issue Report},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/3471158.3472232,
author = {Bi, Keping and Ai, Qingyao and Croft, W. Bruce},
title = {Asking Clarifying Questions Based on Negative Feedback in Conversational Search},
year = {2021},
isbn = {9781450386111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471158.3472232},
doi = {10.1145/3471158.3472232},
abstract = {Users often need to look through multiple search result pages or reformulate queries when they have complex information-seeking needs. Conversational search systems make it possible to improve user satisfaction by asking questions to clarify users' search intents. This, however, can take significant effort to answer a series of questions starting with "what/why/how". To quickly identify user intent and reduce effort during interactions, we propose an intent clarification task based on yes/no questions where the system needs to ask the correct question about intents within the fewest conversation turns. In this task, it is essential to use negative feedback about the previous questions in the conversation history. To this end, we propose a Maximum-Marginal-Relevance (MMR) based BERT model (MMR-BERT) to leverage negative feedback based on the MMR principle for the next clarifying question selection. Experiments on the Qulac dataset show that MMR-BERT outperforms state-of-the-art baselines significantly on the intent identification task and the selected questions also achieve significantly better performance in the associated document retrieval tasks.},
booktitle = {Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {157–166},
numpages = {10},
keywords = {negative feedback, intent clarification, conversational search},
location = {Virtual Event, Canada},
series = {ICTIR '21}
}

@inproceedings{10.1145/3442381.3449936,
author = {Yao, Jing and Dou, Zhicheng and Wen, Ji-Rong},
title = {FedPS: A Privacy Protection Enhanced Personalized Search Framework},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449936},
doi = {10.1145/3442381.3449936},
abstract = {Personalized search returns each user more accurate results by collecting the user’s historical search behaviors to infer her interests and query intents. However, it brings the risk of user privacy leakage, and this may greatly limit the practical application of personalized search. In this paper, we focus on the problem of privacy protection in personalized search, and propose a privacy protection enhanced personalized search framework, denoted with FedPS. Under this framework, we keep each user’s private data on her individual client, and train a shared personalized ranking model with all users’ decentralized data by means of federated learning. We implement two models within the framework: the first one applies a personalization model with a personal module that fits the user’s data distribution to alleviate the challenge of data heterogeneity in federated learning; the second model introduces trustworthy proxies and group servers to solve the problems of limited communication, performance bottleneck and privacy attack for FedPS. Experimental results verify that our proposed framework can enhance privacy protection without losing too much accuracy.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3757–3766},
numpages = {10},
keywords = {personalized search, privacy protection, federated learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3173574.3173978,
author = {Le Bras, Pierre and Robb, David A. and Methven, Thomas S. and Padilla, Stefano and Chantler, Mike J.},
title = {Improving User Confidence in Concept Maps: Exploring Data Driven Explanations},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173978},
doi = {10.1145/3173574.3173978},
abstract = {Automated tools are increasingly being used to generate highly engaging concept maps as an aid to strategic planning and other decision-making tasks. Unless stakeholders can understand the principles of the underlying layout process, however, we have found that they lack confidence and are therefore reluctant to use these maps. In this paper, we present a qualitative study exploring the effect on users' confidence of using data-driven explanation mechanisms, by conducting in-depth scenario-based interviews with ten participants. To provide diversity in stimulus and approach we use two explanation mechanisms based on projection and agglomerative layout methods. The themes exposed in our results indicate that the data-driven explanations improved user confidence in several ways, and that process clarity and layout density also affected users' views of the credibility of the concept maps. We discuss how these factors can increase uptake of automated tools and affect user confidence.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {qualitative study, concept map, user confidence, data driven explanation},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3269206.3271810,
author = {Chin, Jin Yao and Zhao, Kaiqi and Joty, Shafiq and Cong, Gao},
title = {ANR: Aspect-Based Neural Recommender},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271810},
doi = {10.1145/3269206.3271810},
abstract = {Textual reviews, which are readily available on many e-commerce and review websites such as Amazon and Yelp, serve as an invaluable source of information for recommender systems. However, not all parts of the reviews are equally important, and the same choice of words may reflect a different meaning based on its context. In this paper, we propose a novel end-to-end Aspect-based Neural Recommender (ANR) to perform aspect-based representation learning for both users and items via an attention-based component. Furthermore, we model the multi-faceted process behind how users rate items by estimating the aspect-level user and item importance by adapting the neural co-attention mechanism. Our proposed model concurrently address several shortcomings of existing recommender systems, and a thorough experimental study on 25 benchmark datasets from Amazon and Yelp shows that ANR significantly outperforms recently proposed state-of-the-art baselines such as DeepCoNN, D-Attn and ALFM.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {147–156},
numpages = {10},
keywords = {recommender systems, co-attention, neural attention, aspect-based recommendation},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3097983.3098057,
author = {Sybrandt, Justin and Shtutman, Michael and Safro, Ilya},
title = {MOLIERE: Automatic Biomedical Hypothesis Generation System},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098057},
doi = {10.1145/3097983.3098057},
abstract = {Hypothesis generation is becoming a crucial time-saving technique which allows biomedical researchers to quickly discover implicit connections between important concepts. Typically, these systems operate on domain-specific fractions of public medical data. MOLIERE, in contrast, utilizes information from over 24.5 million documents and does not limit the document vocabulary. At the heart of our approach lies a multi-modal and multi-relational network of biomedical objects extracted from several heterogeneous datasets from the National Center for Biotechnology Information (NCBI). These objects include but are not limited to scientific papers, keywords, genes, proteins, diseases, and diagnoses. We model hypotheses using Latent Dirichlet Allocation applied on abstracts found near shortest paths discovered within this network. We demonstrate the effectiveness of MOLIERE by performing hypothesis generation on historical data. Our network, implementation, and resulting data are all publicly available for the broad scientific community.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1633–1642},
numpages = {10},
keywords = {topic modeling, mining scientific publications, undiscovered public knowledge, hypothesis generation},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3442188.3445911,
author = {Mulder, Mats and Inel, Oana and Oosterman, Jasper and Tintarev, Nava},
title = {Operationalizing Framing to Support Multiperspective Recommendations of Opinion Pieces},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445911},
doi = {10.1145/3442188.3445911},
abstract = {Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {478–488},
numpages = {11},
keywords = {framing aspects, viewpoint diversity, recommender systems},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/3336191.3371770,
author = {Gong, Lin and Lin, Lu and Song, Weihao and Wang, Hongning},
title = {JNET: Learning User Representations via Joint Network Embedding and Topic Embedding},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371770},
doi = {10.1145/3336191.3371770},
abstract = {User representation learning is vital to capture diverse user preferences, while it is also challenging as user intents are latent and scattered among complex and different modalities of user-generated data, thus, not directly measurable. Inspired by the concept of user schema in social psychology, we take a new perspective to perform user representation learning by constructing a shared latent space to capture the dependency among different modalities of user-generated data. Both users and topics are embedded to the same space to encode users' social connections and text content, to facilitate joint modeling of different modalities, via a probabilistic generative framework. We evaluated the proposed solution on large collections of Yelp reviews and StackOverflow discussion posts, with their associated network structures. The proposed model outperformed several state-of-the-art topic modeling based user models with better predictive power in unseen documents, and state-of-the-art network embedding based user models with improved link prediction quality in unseen nodes. The learnt user representations are also proved to be useful in content recommendation, e.g., expert finding in StackOverflow.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {205–213},
numpages = {9},
keywords = {social networks, topic modeling, representation learning, network embedding},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3501247.3539506,
author = {Anning, Stephen and Goldberg, Zachary},
title = {Assessing The Ethical Implications of Artificial Intelligence In Policing},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539506},
doi = {10.1145/3501247.3539506},
abstract = {This workshop addresses successful approaches and challenges to assessing the ethical implications of artificial intelligence in policing. It is divided into three main streams: A) How to conduct an ethics assessment of AI in policing? B) Applying Explainable AI in a Policing Context; C) The Practicalities of Co-Design Between Police and Developers},
booktitle = {14th ACM Web Science Conference 2022},
pages = {464–465},
numpages = {2},
keywords = {Co-Design, Digital Policing, Ethical Impact Assessment, Ethical AI, Explainability},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@article{10.1145/3468268,
author = {Wu, Qiong and Hare, Adam and Wang, Sirui and Tu, Yuwei and Liu, Zhenming and Brinton, Christopher G. and Li, Yanhua},
title = {BATS: A Spectral Biclustering Approach to Single Document Topic Modeling and Segmentation},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3468268},
doi = {10.1145/3468268},
abstract = {Existing topic modeling and text segmentation methodologies generally require large datasets for training, limiting their capabilities when only small collections of text are available. In this work, we reexamine the inter-related problems of “topic identification” and “text segmentation” for sparse document learning, when there is a single new text of interest. In developing a methodology to handle single documents, we face two major challenges. First is sparse information: with access to only one document, we cannot train traditional topic models or deep learning algorithms. Second is significant noise: a considerable portion of words in any single document will produce only noise and not help discern topics or segments. To tackle these issues, we design an unsupervised, computationally efficient methodology called Biclustering Approach to Topic modeling and Segmentation (BATS). BATS leverages three key ideas to simultaneously identify topics and segment text: (i) a new mechanism that uses word order information to reduce sample complexity, (ii) a statistically sound graph-based biclustering technique that identifies latent structures of words and sentences, and (iii) a collection of effective heuristics that remove noise words and award important words to further improve performance. Experiments on six datasets show that our approach outperforms several state-of-the-art baselines when considering topic coherence, topic diversity, segmentation, and runtime comparison metrics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {oct},
articleno = {54},
numpages = {29},
keywords = {Biclustering, topic modeling, text segmentation}
}

@inproceedings{10.1145/3494193.3494202,
author = {Lachana, Zoi and Avgerinos Loutsaris, Michalis and Alexopoulos, Charalampos and Charalabidis, Yannis},
title = {Clustering Legal Artifacts Using Text Mining},
year = {2021},
isbn = {9781450390118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494193.3494202},
doi = {10.1145/3494193.3494202},
abstract = {The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.},
booktitle = {14th International Conference on Theory and Practice of Electronic Governance},
pages = {65–70},
numpages = {6},
location = {Athens, Greece},
series = {ICEGOV 2021}
}

@inproceedings{10.1145/3438872.3439101,
author = {Yang, JinXiong and Bai, Liang and Guo, Yanming},
title = {A Survey of Text Classification Models},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439101},
doi = {10.1145/3438872.3439101},
abstract = {With the rapid development of artificial intelligence, text classification method based on deep learning model has surpassed traditional machine learning method in various aspects. This paper introduces dozens of deep learning models for text classification according to the different network structures of the models. In addition, this paper briefly introduces the evaluation indicators and application scenarios of text classification, summarizes and forecasts the current challenges and future development trend of text classification.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {327–334},
numpages = {8},
keywords = {Text classification, Deep learning, Model},
location = {Shanghai, China},
series = {RICAI 2020}
}

@inproceedings{10.1145/3380625.3380665,
author = {Wang, Ying and Zheng, Liwei and Li, Ning},
title = {ROM: A Requirement Opinions Mining Method Preliminary Try Based on Software Review Data},
year = {2020},
isbn = {9781450376419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380625.3380665},
doi = {10.1145/3380625.3380665},
abstract = {Requirement opinion mining aims to mine user opinions that can be used to help the mining of software requirements from various data sources. However, in the development of social network systems, software application platforms or stores and other data sources, the massive, noisy, non-standard data, makes the mining of effective requirement opinions more difficult. Therefore, there is less work in software requirements mining based on the data of software review in development social media or application market. This paper attempts to provide some knowledge support for requirement user story establishing in RE based on the opinion mining and clustering of massively software review data. First of all, this paper combines the requirements of the requirements engineering field to define the requirement opinions, functional requirement opinions and non-functional requirements opinions. Secondly, using the deep learning model to classify the functional requirement reviews and non-functional requirements reviews included in the reviews; Based on the differences between functional data and non-functional data, this paper defines three categories in the description of software functional data, and chooses to use sequence labeling methods to identify functional requirements. Then use the K-means clustering method based on word vector to cluster the review data, and combine TF-IDF and syntactic analysis to extract the aspect and aspect requirements or specific requirements of the requirement opinion respectively, so as to realize the requirement opinion mining of software review data. Finally, this article will give a case study based on the user review data of the mobile phone application service platform 360 mobile assistants.},
booktitle = {Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {26–33},
numpages = {8},
keywords = {Review data, requirement opinion mining, clustering},
location = {Wuhan, China},
series = {ICMSS 2020}
}

@inproceedings{10.1145/3297001.3297018,
author = {Kumar, Nagendra and G, Srikanth and Mudda, Karthik Yadav and Trishal, Gayam and Konjengbam, Anand and Singh, Manish},
title = {Where to Post: Routing Questions to Right Community in Community Question Answering Systems},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297018},
doi = {10.1145/3297001.3297018},
abstract = {At present, question-answer (QA) sites have become one of the most important sources of information sharing. In order to ease search and categorization, QA sites create communities to discuss a specific topic or interest. As a consequence, a large number of communities have been created in the last few years. A lot of research has been conducted on community QA sites to address various problems including expert identification and tag recommendation. However, an important problem that has been neglected so far is to automatically route a question to the right community. In this paper, we propose a novel word-embedding based method to route a question to the right community. We use syntactic as well as semantic features to characterize a question and community. Although this approach of characterization performs well, it is highly computationally expensive. To deal with this problem, we use topic modeling, which effectively summarizes a community and reduces the computation time. Our experimental results reveal that usage of both syntactic and semantic features helps in question routing and leads to a better community prediction. We evaluate our methods on a well-known question answering system Stack Exchange and show the effectiveness of the proposed method.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {136–142},
numpages = {7},
keywords = {text mining, community recommendation, data characterization, Community question answering},
location = {Kolkata, India},
series = {CoDS-COMAD '19}
}

@inproceedings{10.1145/3230905.3230920,
author = {Zarra, Taoufiq and Chiheb, Raddouane and Faizi, Rdouan and El Afia, Abdellatif},
title = {Student Interactions in Online Discussion Forums: Visual Analysis with LDA Topic Models},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230920},
doi = {10.1145/3230905.3230920},
abstract = {Virtual collaboration is intuitive and highly developed for most students attending today's schools. Our contribution aims to analysis, using the machine learning technique, some pedagogical benefits of discussions forums for the teaching staff, to help identify keys successes or weakness in courses assimilation. For this purpose, we use a variant of the probabilistic model of latent Dirichlet allocation (LDA) to ensure a better visual supervision of the topics discussed by the students. We will present our platform and discuss the obtained results.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {30},
numpages = {5},
keywords = {Online discussion, LDA, Visualization, topic modeling},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@article{10.1145/3415215,
author = {Mendu, Sanjana and Baglione, Anna and Baee, Sonia and Wu, Congyu and Ng, Brandon and Shaked, Adi and Clore, Gerald and Boukhechba, Mehdi and Barnes, Laura},
title = {A Framework for Understanding the Relationship between Social Media Discourse and Mental Health},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415215},
doi = {10.1145/3415215},
abstract = {Over 35% of the world's population uses social media. Platforms like Facebook, Twitter, and Instagram have radically influenced the way individuals interact and communicate. These platforms facilitate both public and private communication with strangers and friends alike, providing rich insight into an individual's personality, health, and wellbeing. To date, many researchers have employed a variety of methods for extracting mental health-centric features from digital text communication (DTC) data, including natural language processing, social network analysis, and extraction of temporal discourse patterns. However, none have explored a hierarchical framework for extracting features from private messages with the goal of unifying approaches across methodological domains. Furthermore, while analyses of large, public corpora abound in existing literature, limited work has been done to explore the relationship between of private textual communications, personality traits, and symptoms of mental illness. We present a framework for constructing rich feature spaces from digital text communications. We then demonstrate the efficacy of our framework by applying it to a dataset of private Facebook messages in a college student population (N=103). Our results reveal key individual differences in temporal and relational behaviors, as well as language usage in relation to validated measures of trait-level anxiety, loneliness, and personality. This work represents a critical step forward in linking features of private social media messages to validated measures of mental health, wellbeing, and personality.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {144},
numpages = {23},
keywords = {mental health, text mining, language, social media, machine learning}
}

@inproceedings{10.1145/3379597.3387472,
author = {Abdellatif, Ahmad and Costa, Diego and Badran, Khaled and Abdalkareem, Rabe and Shihab, Emad},
title = {Challenges in Chatbot Development: A Study of Stack Overflow Posts},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387472},
doi = {10.1145/3379597.3387472},
abstract = {Chatbots are becoming increasingly popular due to their benefits in saving costs, time, and effort. This is due to the fact that they allow users to communicate and control different services easily through natural language. Chatbot development requires special expertise (e.g., machine learning and conversation design) that differ from the development of traditional software systems. At the same time, the challenges that chatbot developers face remain mostly unknown since most of the existing studies focus on proposing chatbots to perform particular tasks rather than their development.Therefore, in this paper, we examine the Q&amp;A website, Stack Overflow, to provide insights on the topics that chatbot developers are interested and the challenges they face. In particular, we leverage topic modeling to understand the topics that are being discussed by chatbot developers on Stack Overflow. Then, we examine the popularity and difficulty of those topics. Our results show that most of the chatbot developers are using Stack Overflow to ask about implementation guidelines. We determine 12 topics that developers discuss (e.g., Model Training) that fall into five main categories. Most of the posts belong to chatbot development, integration, and the natural language understanding (NLU) model categories. On the other hand, we find that developers consider the posts of building and integrating chatbots topics more helpful compared to other topics. Specifically, developers face challenges in the training of the chatbot's model. We believe that our study guides future research to propose techniques and tools to help the community at its early stages to overcome the most popular and difficult topics that practitioners face when developing chatbots.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {174–185},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1145/3374214,
author = {Pu, Calton and Suprem, Abhijit and Lima, Rodrigo Alves and Musaev, Aibek and Wang, De and Irani, Danesh and Webb, Steve and Ferreira, Joao Eduardo},
title = {Beyond Artificial Reality: Finding and Monitoring Live Events from Social Sensors},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3374214},
doi = {10.1145/3374214},
abstract = {With billions of active social media accounts and millions of live video cameras, live new big data offer many opportunities for smart applications. However, the main consumers of the new big data have been humans. We envision the research on live knowledge, to automatically acquire real-time, validated, and actionable information. Live knowledge presents two significant and diverging technical challenges: big noise and concept drift. We describe the EBKA (evidence-based knowledge acquisition) approach, illustrated by the LITMUS landslide information system. LITMUS achieves both high accuracy and wide coverage, demonstrating the feasibility and promise of EBKA approach to achieve live knowledge.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {2},
numpages = {21},
keywords = {live knowledge, true novelty, concept drift, Artificial reality, evidence-based knowledge acquisition, real-time event detection}
}

@inproceedings{10.1145/3180155.3180218,
author = {Gao, Cuiyun and Zeng, Jichuan and Lyu, Michael R. and King, Irwin},
title = {Online App Review Analysis for Identifying Emerging Issues},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180218},
doi = {10.1145/3180155.3180218},
abstract = {Detecting emerging issues (e.g., new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (e.g., misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the identified issues can facilitate app development in practice. Moreover, we have successfully applied IDEA to several products of Tencent, which serve hundreds of millions of users.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {48–58},
numpages = {11},
keywords = {emerging issues, app reviews, online analysis},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3180155.3180218,
author = {Gao, Cuiyun and Zeng, Jichuan and Lyu, Michael R. and King, Irwin},
title = {Online App Review Analysis for Identifying Emerging Issues},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180218},
doi = {10.1145/3180155.3180218},
abstract = {Detecting emerging issues (e.g., new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (e.g., misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the identified issues can facilitate app development in practice. Moreover, we have successfully applied IDEA to several products of Tencent, which serve hundreds of millions of users.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {48–58},
numpages = {11},
keywords = {emerging issues, app reviews, online analysis},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3170521.3170546,
author = {Das, Subha Jyoti and Chakraborty, Basabi},
title = {A Proposal for Efficient Automatic Summarization of Online Product Reviews},
year = {2018},
isbn = {9781450363976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170521.3170546},
doi = {10.1145/3170521.3170546},
abstract = {A popular application of social data mining is sentiment detection or opinion analysis of consumer products or social events. In this work, a proposal for automatic summarization of a set of online review data on a particular consumer product is outlined so that the overall quality of the product can be assessed without going through all the reviews manually one by one. In this approach the most discussed aspects of the products will be highlighted in a constructed manner.},
booktitle = {Proceedings of the Workshop Program of the 19th International Conference on Distributed Computing and Networking},
articleno = {25},
numpages = {2},
keywords = {ACM proceedings, text tagging, LATEX},
location = {Varanasi, India},
series = {Workshops ICDCN '18}
}

@inproceedings{10.5555/3382225.3382354,
author = {Kamarudin, Nur Shazwani and Rakesh, Vineeth and Beigi, Ghazaleh and Manikonda, Lydia and Liu, Huan},
title = {A Study of Reddit-User's Response to Rape},
year = {2018},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {The growth of social media has created an open web where people freely share their opinion and even discuss sensitive subjects in online forums. Forums such as Reddit help support seekers by serving as a portal for open discussions for various stigmatized subjects such as rape. This paper investigates the potential roles of online forums and if such forums provide intended resources to the people who seek support. Specifically, the open nature of forums allows us to study how online users respond to seeker's queries or needs; through their response, we attempt to assess the range of topics covered by responders in regards to the issues, concerns and, obstacles faced by the victims of rape and sexual abuse, using rape-related posts from Reddit. We employ natural language processing techniques to extract topics of responses, examine how diverse these topics are to answer research questions such as whether responses are limited to emotional support; if not, what other topics are; what the diversity of topics manifests; and how online response differs from traditional response found in a physical world.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {591–592},
numpages = {2},
keywords = {linguistic behavior, online forum, rape, reddit},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1109/ICSE-C.2017.65,
author = {Guzman, Emitza and Ibrahim, Mohamed and Glinz, Martin},
title = {Mining Twitter Messages for Software Evolution},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.65},
doi = {10.1109/ICSE-C.2017.65},
abstract = {Twitter is a widely used social network. Previous research showed that users engage in Twitter to communicate about software applications via short messages, referred to as tweets, and that some of these tweets are relevant for software evolution. However, a manual analysis is impractical due to the large number of tweets - in the range of thousands per day for popular apps.In this work we present ALERTme, an approach to automatically classify, group and rank tweets about software applications. We apply machine learning techniques for automatically classifying tweets requesting improvements, topic modeling for grouping semantically related tweets and a weighted function for ranking tweets according to their relevance for software evolution.We ran our approach on 68,108 tweets from three different software applications and compared the results against practitioners' assessments. Our results are promising and could help incorporate short, informal user feedback with social components into the software evolution process.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {283–284},
numpages = {2},
keywords = {software evolution, user feedback, text mining},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1109/ASE51524.2021.9678554,
author = {Galappaththi, Akalanka and Anvik, John and Islam, Rafat Bin},
title = {Automatically Annotating Sentences for Task-Specific Bug Report Summarization},
year = {2021},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678554},
doi = {10.1109/ASE51524.2021.9678554},
abstract = {There is a need to summarize bug reports as they can become long due to many comments from conversations between developers and various DevOps tools. Although automated approaches to bug report summarization have been developed, we believe they aim at the wrong target - getting as close as possible to a gold-standard summary. Instead, researchers should create automated bug report annotation approaches that allow project members to create summaries based on their task-specific information needs. We present such an approach.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1177–1179},
numpages = {3},
keywords = {text tagging, natural language processing, text annotation, bug report summarization},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3493612.3520470,
author = {Silva, Jorge Sassaki Resende and Freire, Andr\'{e} Pimenta and Cardoso, Paula Christina Figueira},
title = {When Headers Are Not There: Design and User Evaluation of an Automatic Topicalisation and Labelling Tool to Aid the Exploration of Web Documents by Blind Users},
year = {2022},
isbn = {9781450391702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493612.3520470},
doi = {10.1145/3493612.3520470},
abstract = {Using headers to grasp a document's structure has been one of the main strategies employed by blind users on web documents when using screen readers. However, when headers are not available or not appropriately marked up, they can cause serious difficulties. This paper presents the design and evaluation of a tool for automatically generating headers for screen readers with topicalisation and labelling algorithms. The proposed tool uses Natural Language Processing techniques to divide a web document into topic segments and label each segment based on its content. We conducted an initial user study with eight blind and partially-sighted screen reader users. The evaluation involved tasks with questions answered by participants with information from texts with and without automatically generated headers. Results provided preliminary indicators of improvement in performance and reduction of cognitive load. The findings contribute to knowledge to improve tools to aid in text exploration. It also provides initial empirical evidence to be further explored to analyze the impact of automatically-generated headings in improving performance and reducing cognitive load for blind users.},
booktitle = {Proceedings of the 19th International Web for All Conference},
articleno = {18},
numpages = {11},
keywords = {natural language processing, automatic topicalisation and labelling, accessibility, screen readers},
location = {Lyon, France},
series = {W4A '22}
}

@inproceedings{10.1145/3406865.3418319,
author = {Sengupta, Subhasree},
title = { 'Learning to Code in a Virtual World': A Preliminary Comparative Analysis of Discourse and Learning in Two Online Programming Communities},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418319},
doi = {10.1145/3406865.3418319},
abstract = {Software programming is increasingly becoming a community-driven effort, with online discussion channels becoming vital resources for learning and knowledge sharing. This study explores differences in the discourse patterns of two popular online programming communities (Stack Overflow and r/Askprogramming) to provide preliminary insights into the type of learning practices these collectives support and scaffold. A three-step content analysis framework that investigates a sample of 8639 and 6126 contributions from Stack Overflow and r/Askprogramming respectively is presented. Preliminary results indicate that differences emerge in the scope of topics and the nature of responses the communities provide. While Stack Overflow is more task-specific, r/Askprogramming supports a greater sense of bonding and camaraderie among community members in addition to task-specific discussions. These results provide insights into the type of practices these communities support, which can be essential in considering how online communities that support learning activities should be designed.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {389–394},
numpages = {6},
keywords = {community culture, stack overflow, informal learning, reddit, discourse norms, content analysis},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3372923.3404833,
author = {Trujillo, Milo and Gruppi, Maur\'{\i}cio and Buntain, Cody and Horne, Benjamin D.},
title = {What is BitChute? Characterizing The},
year = {2020},
isbn = {9781450370981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372923.3404833},
doi = {10.1145/3372923.3404833},
abstract = {In this paper, we characterize the content and discourse on BitChute, a social video-hosting platform. Launched in 2017 as an alternative to YouTube, BitChute joins an ecosystem of alternative, low content moderation platforms, including Gab, Voat, Minds, and 4chan. Uniquely, BitChute is the first of these alternative platforms to focus on video content and is growing in popularity. Our analysis reveals several key characteristics of the platform. We find that only a handful of channels receive any engagement, and almost all of those channels contain conspiracies or hate speech. This high rate of hate speech on the platform as a whole, much of which is anti-Semitic, is particularly concerning. Our results suggest that BitChute has a higher rate of hate speech than Gab but less than 4chan. Lastly, we find that while some BitChute content producers have been banned from other platforms, many maintain profiles on mainstream social media platforms, particularly YouTube. This paper contributes a first look at the content and discourse on BitChute and provides a building block for future research on low content moderation platforms.},
booktitle = {Proceedings of the 31st ACM Conference on Hypertext and Social Media},
pages = {139–140},
numpages = {2},
keywords = {hate speech, online communities, social networks, social media},
location = {Virtual Event, USA},
series = {HT '20}
}

@inproceedings{10.1145/3308557.3313112,
author = {Lim, Brian and Sarkar, Advait and Smith-Renner, Alison and Stumpf, Simone},
title = {ExSS: Explainable Smart Systems 2019},
year = {2019},
isbn = {9781450366731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308557.3313112},
doi = {10.1145/3308557.3313112},
abstract = {Smart systems that apply complex reasoning to make decisions and plan behavior are often difficult for users to understand. While research to make systems more explainable and therefore more intelligible and transparent is gaining pace, there are numerous issues and problems regarding these systems that demand further attention. The ExSS 2019 workshop is a follow-on from the very successful ExSS 2018 workshop previously held at IUI, to bring academia and industry together to address these issues. This workshop includes a keynote, paper panels, poster session, and group activities, with the goal of developing concrete approaches to handling challenges related to the design and development of explainable smart systems.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
pages = {125–126},
numpages = {2},
keywords = {intelligibility, machine learning, explanations, visualizations, transparency, intelligent systems},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/3210713.3210747,
author = {Wan, Han and Yu, Qiaoye and Liu, Kangxu and Gao, Xiaopeng and Ding, Jun},
title = {Recommend Related Discussion Forum Posts to Students in the Small Private Online Course},
year = {2018},
isbn = {9781450364157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210713.3210747},
doi = {10.1145/3210713.3210747},
abstract = {Computer Organization course from Beihang University adopted hybrid teaching mode based on Open edX platform - project related materials were published online through videos and e-texts, an external grader was integrated online to feedback the evaluation of students' submissions, a virtual online lab system was built to support the remote hardware experiments, and a discussion forum was used to enhancing the communication between participants. The benefits of active online discussion have been noted in literature on evidence-based education in general.In this poster, the authors report last three iterations' work about how to support continual engagement in the online discussion forum. The survey launched at the end of Fall 2016 semester had shown that, most students said the discussion forum could help in their learning. But due to its asynchronous nature, students could not receive the reply in time, which affected their enthusiasm greatly. Furthermore, the highly effective habit is searching first. If they've searched the forum and could not find a question or topic that is the same or similar to theirs, it's time to post their own.Here we set up a recommender system that could feed back to student with similar posts when they want to post in the forum. The research includes how to build a corpus related to our course, how to represent the post in a vector and then to compute the distance between the posts. First, we use the Chinese wiki, the "Principles of Computer Organization" text book, the online course materials and forum posts in 2015 semester to build the course related corpus. And then, the posts are represented as vectors using the word2vec technology. Finally, the Word Mover Distance (WMD) and keywords Jaccard Distance between the posts in the forum are calculated. The poster will display some preliminary results: we calculated the course forum posts' WMD and professional contexts keywords Jaccard Distance between Fall 2015 and 2016; when input one forum post in the Fall 2016, the posts in Fall 2015 are set as similar ones if their keywords Jaccard Distance &lt; 0.6, then recommend the top5 most similar posts that are sorted by WMD; and the precision of recommended posts could reach around 0.25 which was evaluated by the course designer.},
booktitle = {Proceedings of ACM Turing Celebration Conference - China},
pages = {134–135},
numpages = {2},
keywords = {recommend system, the small private online course, discussion forum},
location = {Shanghai, China},
series = {TURC '18}
}

@inproceedings{10.1145/3320269.3405435,
author = {Huang, Shin-Ying and Wu, Yiju},
title = {POSTER: Dynamic Software Vulnerabilities Threat Prediction through Social Media Contextual Analysis},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3405435},
doi = {10.1145/3320269.3405435},
abstract = {Publicly available software vulnerabilities and exploit codes are often utilized by malicious actors to launch cyberattack to vulnerable targets. Therefore, organizations not only need to update their software to the latest version, they need to do effective patch management and also prioritize the patching schedule. In order to prevent future cyber threat based on publicly available resources on the Internet, this study propose a dynamic vulnerability threat assessment model to predict the exploited tendency for each vulnerability (i.e., CVE). The model considers many aspects of vulnerability which are gathered from multiple sources. Features range from profile information to contextual information of Twitter discussion about these vulnerabilities. When applied to predict the vulnerabilities exploitation in real world data, it showed better prediction accuracy using our approach and has been deployed into our threat intelligence platform as one of the analytic functions.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {892–894},
numpages = {3},
keywords = {vulnerability exploit prediction, topic modeling, social media vulnerability mentions, machine learning},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@inproceedings{10.1145/3132847.3132864,
author = {Yin, Peifeng and Liu, Zhe and Xu, Anbang and Nakamura, Taiga},
title = {Tone Analyzer for Online Customer Service: An Unsupervised Model with Interfered Training},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132864},
doi = {10.1145/3132847.3132864},
abstract = {Emotion analysis of online customer service conservation is important for good user experience and customer satisfaction. However, conventional metrics do not fit this application scenario. In this work, by collecting and labeling online conversations of customer service on Twitter, we identify 8 new metrics, named as tones, to describe emotional information. To better interpret each tone, we extend the Latent Dirichlet Allocation (LDA) model to Tone LDA (T-LDA). In T-LDA, each latent topic is explicitly associated with one of three semantic categories, i.e., tone-related, domain-specific and auxiliary. By integrating tone label into learning, T-LDA can interfere the original unsupervised training process and thus is able to identify representative tone-related words. In evaluation, T-LDA shows better performance than baselines in predicting tone intensity. Also, a case study is conducted to analyze each tone via T-LDA output.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1887–1895},
numpages = {9},
keywords = {online customer service, emotion, topic modeling, tone},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3128128.3128155,
author = {Zarra, Taoufiq and Chiheb, Raddouane and Moumen, Rajae and Faizi, Rdouan and Afia, Abdellatif El},
title = {Topic and Sentiment Model Applied to the Colloquial Arabic: A Case Study of Maghrebi Arabic},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128155},
doi = {10.1145/3128128.3128155},
abstract = {Recently, the multiplication of communication and sharing platforms such as social networks, personal blogs, forums, etc., has facilitated the expression of views and opinions about products, personalities, and public policy. However, gathering these points of view is a complex task that requires resolution of many problems in different disciplines, especially issues related to our language. Among the research areas, topic modeling and sentiment analysis stimulates interest and curiosity of the scientific community. Lately, the current economic, geo-political and geostrategic trends have made researchers specifically more interested in Arabic language, except that the majority of these studies focus on the classical Arabic; nevertheless it is a language of the elites which is different from what is mainly used on the Web. Our paper focuses on Maghrebi colloquial Arabic since the little research that exists in this area is limited to East colloquial Arabic. On a corpus extracted from different Facebook pages we implemented a supervised approach to extract the sentiments, and an unsupervised approach to extract topic, then we proposed a new, semi-supervised, approach in the Arabic language that combines the topic and the sentiment in a single model, in order to join each topic to a specific sentiment.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {174–181},
numpages = {8},
keywords = {colloquial Arabic, topic modeling, naive Bayes, sentiment analysis, Maghrebi Arabic, latent dirichlet allocution},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@article{10.1145/3392734,
author = {Lin, Hao and Zhu, Hengshu and Wu, Junjie and Zuo, Yuan and Zhu, Chen and Xiong, Hui},
title = {Enhancing Employer Brand Evaluation with Collaborative Topic Regression Models},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3392734},
doi = {10.1145/3392734},
abstract = {Employer Brand Evaluation (EBE) is to understand an employer’s unique characteristics to identify competitive edges. Traditional approaches rely heavily on employers’ financial information, including financial reports and filings submitted to the Securities and Exchange Commission (SEC), which may not be readily available for private companies. Fortunately, online recruitment services provide a variety of employers’ information from their employees’ online ratings and comments, which enables EBE from an employee’s perspective. To this end, in this article, we propose a method named Company Profiling–based Collaborative Topic Regression (CPCTR) to collaboratively model both textual (i.e., reviews) and numerical information (i.e., salaries and ratings) for learning latent structural patterns of employer brands. With identified patterns, we can effectively conduct both qualitative opinion analysis and quantitative salary benchmarking. Moreover, a Gaussian processes--based extension, GPCTR, is proposed to capture the complex correlation among heterogeneous information. Extensive experiments are conducted on three real-world datasets to validate the effectiveness and generalizability of our methods in real-life applications. The results clearly show that our methods outperform state-of-the-art baselines and enable a comprehensive understanding of EBE.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {32},
numpages = {33},
keywords = {collaborative topic regression, Gaussian processes, Employer brand evaluation, salary benchmarking}
}

@inproceedings{10.1145/3233347.3233370,
author = {Phillips, Ross C. and Gorse, Denise},
title = {Mutual-Excitation of Cryptocurrency Market Returns and Social Media Topics},
year = {2018},
isbn = {9781450364720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233347.3233370},
doi = {10.1145/3233347.3233370},
abstract = {Cryptocurrencies have recently experienced a new wave of price volatility and interest; activity within social media communities relating to cryptocurrencies has increased significantly. There is currently limited documented knowledge of factors which could indicate future price movements. This paper aims to decipher relationships between cryptocurrency price changes and topic discussion on social media to provide, among other things, an understanding of which topics are indicative of future price movements. To achieve this a well-known dynamic topic modelling approach is applied to social media communication to retrieve information about the temporal occurrence of various topics. A Hawkes model is then applied to find interactions between topics and cryptocurrency prices. The results show particular topics tend to precede certain types of price movements, for example the discussion of 'risk and investment vs trading' being indicative of price falls, the discussion of 'substantial price movements' being indicative of volatility, and the discussion of 'fundamental cryptocurrency value' by technical communities being indicative of price rises. The knowledge of topic relationships gained here could be built into a real-time system, providing trading or alerting signals.},
booktitle = {Proceedings of the 4th International Conference on Frontiers of Educational Technologies},
pages = {80–86},
numpages = {7},
keywords = {Hawkes models, topic modelling, LDA, cryptocurrency trading, social media data mining},
location = {Moscow, Russian Federation},
series = {ICFET '18}
}

@article{10.1145/3446209,
author = {Xu, Qianli and Molino, Ana Garcia Del and Lin, Jie and Fang, Fen and Subbaraju, Vigneshwaran and Li, Liyuan and Lim, Joo-Hwee},
title = {Lifelog Image Retrieval Based on Semantic Relevance Mapping},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3446209},
doi = {10.1145/3446209},
abstract = {Lifelog analytics is an emerging research area with technologies embracing the latest advances in machine learning, wearable computing, and data analytics. However, state-of-the-art technologies are still inadequate to distill voluminous multimodal lifelog data into high quality insights. In this article, we propose a novel semantic relevance mapping (SRM) method to tackle the problem of lifelog information access. We formulate lifelog image retrieval as a series of mapping processes where a semantic gap exists for relating basic semantic attributes with high-level query topics. The SRM serves both as a formalism to construct a trainable model to bridge the semantic gap and an algorithm to implement the training process on real-world lifelog data. Based on the SRM, we propose a computational framework of lifelog analytics to support various applications of lifelog information access, such as image retrieval, summarization, and insight visualization. Systematic evaluations are performed on three challenging benchmarking tasks to show the effectiveness of our method.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jul},
articleno = {92},
numpages = {18},
keywords = {semantic mapping, image retrieval, summarization, Lifelog}
}

@inproceedings{10.1145/3079452.3079483,
author = {Ghenai, Amira},
title = {Health Misinformation in Search and Social Media},
year = {2017},
isbn = {9781450352499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079452.3079483},
doi = {10.1145/3079452.3079483},
abstract = {People regularly use web search and social media to investigate health related issues. This type of Internet data might contain misinformation i.e incorrect information which contradicts current established medical understanding. If people are influenced by the presented misinformation in these sources, they can make harmful decisions about their health. Our research goal is to investigate the affect of Internet data on people's health. Our current findings suggest that people can be potentially harmed by search engine results. Furthermore, we successfully built a high precision approach to track misinformation in social media. In this paper, we briefly discuss our ongoing work results. Thereafter, we propose a research plan to understand possible mechanisms of misinformation's effect on people and possible impacts of these misinformation on public health.},
booktitle = {Proceedings of the 2017 International Conference on Digital Health},
pages = {235–236},
numpages = {2},
keywords = {misinformation, health search, social computing for health, rumor, user study},
location = {London, United Kingdom},
series = {DH '17}
}

@article{10.1145/3481045,
author = {Baron, Jason R. and Sayed, Mahmoud F. and Oard, Douglas W.},
title = {Providing More Efficient Access to Government Records: A Use Case Involving Application of Machine Learning to Improve FOIA Review for the Deliberative Process Privilege},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3481045},
doi = {10.1145/3481045},
abstract = {At present, the review process for material that is exempt from disclosure under the Freedom of Information Act (FOIA) in the United States of America, and under many similar government transparency regimes worldwide, is entirely manual. Public access to the records of their government is thus inhibited by the long backlogs of material awaiting such reviews. This article studies one aspect of that problem by first creating a new public test collection with annotations for one class of exempt material subject to the deliberative process privilege, and then by using that test collection to study the ability of current text classification techniques to identify those materials that are exempt from release under that privilege. Results show that when the system is trained and evaluated using annotations from the same reviewer, even difficult cases can often be reliably detected. However, results also show that differences in reviewer interpretations, differences in record custodians, and differences in topics of the records used for training and testing can pose challenges.},
journal = {J. Comput. Cult. Herit.},
month = {jan},
articleno = {5},
numpages = {19},
keywords = {deliberative process privilege, Sensitivity review, evaluation, freedom of information act}
}

@inproceedings{10.1145/3183440.3194965,
author = {Wu, Di and Jing, Xiao-Yuan and Chen, Haowen and Zhu, Xiaoke and Zhang, Hongyu and Zuo, Mei and Zi, Lu and Zhu, Chen},
title = {Automatically Answering API-Related Questions},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3194965},
doi = {10.1145/3183440.3194965},
abstract = {Automatically recommending API-related tutorial fragments or Q&amp;A pairs from Stack Overflow (SO) is very helpful for developers, especially when they need to use unfamiliar APIs to complete programming tasks. However, in practice developers are more likely to express the API-related questions using natural language when they do not know the exact name of an unfamiliar API. In this paper, we propose an approach, called SOTU, to automatically find answers for API-related natural language questions (NLQs) from tutorials and SO. We first identify relevant API-related tutorial fragments and extract API-related Q&amp;A pairs from SO. We then construct an API-Answer corpus by combining these two sources of information. For an API-related NLQ given by the developer, we parse it into several potential APIs and then retrieve potential answers from the API-Answer corpus. Finally, we return a list of potential results ranked by their relevancy. Experiments on API-Answer corpus demonstrate the effectiveness of SOTU.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {270–271},
numpages = {2},
keywords = {tutorials, stack overflow, natural language question, application programming interface},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3292522.3326057,
author = {Graells-Garrido, Eduardo and Baeza-Yates, Ricardo and Lalmas, Mounia},
title = {How Representative is an Abortion Debate on Twitter?},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326057},
doi = {10.1145/3292522.3326057},
abstract = {Today, more than ever, social networks and micro-blogging platforms are used as tools for political exchange. However, these platforms are biased in several aspects, from their algorithms to the population participating in them. With respect to the latter, we analyze the discussion on Twitter about an abortion bill in Chile, proposed in January 2015, and approved as law in September 2017. We find that Twitter has strong biases in population representation. Still, when carefully paired with demographic attributes, Twitter-based insights on the characteristics of political discussion match those from national-level surveys.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {133–134},
numpages = {2},
keywords = {bias on the web, abortion, political discussion, representativeness},
location = {Boston, Massachusetts, USA},
series = {WebSci '19}
}

@inproceedings{10.1145/3328526.3329582,
author = {Yan, Hao and Das, Sanmay and Lavoie, Allen and Li, Sirui and Sinclair, Betsy},
title = {The Congressional Classification Challenge: Domain Specificity and Partisan Intensity},
year = {2019},
isbn = {9781450367929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328526.3329582},
doi = {10.1145/3328526.3329582},
abstract = {In this paper, we study the effectiveness and generalizability of techniques for classifying partisanship and ideology from text in the context of US politics. In particular, we are interested in how well measures of partisanship transfer across domains as well as the potential to rely upon measures of partisan intensity as a proxy for political ideology. We construct novel datasets of English texts from (1) the Congressional Record, (2) prominent conservative and liberal media websites, and (3) conservative and liberal wikis, and apply text classification algorithms to evaluate domain specificity via a domain adaptation technique. Surprisingly, we find that the cross-domain learning performance, benchmarking the ability to generalize from one of these datasets to another, is in general poor, even though the algorithms perform very well in within-dataset cross-validation tests. While party affiliation of legislators is not predictable based on models learned from other sources, we do find some ability to predict the leanings of the media and crowdsourced websites based on models learned from the Congressional Record. This predictivity is different across topics, and itself a priori predictable based on within-topic cross-validation results. Temporally, phrases tend to move from politicians to the media, helping to explain this predictivity. Finally, when we compare legislators themselves across different media (the Congressional Record and press releases), we find that while party affiliation is highly predictable, within-party ideology is completely unpredictable. Legislators are communicating different messages through different channels while clearly signaling party identity systematically across all channels. Choice of language is a clearly strategic act, among both legislators and the media, and we must therefore proceed with extreme caution in extrapolating from language to partisanship or ideology across domains.},
booktitle = {Proceedings of the 2019 ACM Conference on Economics and Computation},
pages = {71–89},
numpages = {19},
keywords = {political science, political ideology, text classification, partisanship, domain adaptation},
location = {Phoenix, AZ, USA},
series = {EC '19}
}

@inproceedings{10.1145/3472456.3472499,
author = {Yu, Bowen and Cao, Huanqi and Shan, Tianyi and Wang, Haojie and Tang, Xiongchao and Chen, Wenguang},
title = {Sparker: Efficient Reduction for More Scalable Machine Learning with Spark},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472499},
doi = {10.1145/3472456.3472499},
abstract = {Machine learning applications on Spark suffers from poor scalability. In this paper, we reveal that the key reasons is the non-scalable reduction, which is restricted by the non-splittable object programming interface in Spark. This insight guides us to propose Sparker, Spark with Efficient Reduction. By providing a split aggregation interface, Sparker is able to perform split aggregation with scalable reduction while being backward compatible with existing applications. We implemented Sparker in 2,534 lines of code. Sparker can improve the aggregation performance by up to 6.47 \texttimes{} and can improve the end-to-end performance of MLlib model training by up to 3.69 \texttimes{} with a geometric mean of 1.81 \texttimes{} .},
booktitle = {50th International Conference on Parallel Processing},
articleno = {58},
numpages = {11},
keywords = {Aggregation, Spark, Machine Learning, Reduction},
location = {Lemont, IL, USA},
series = {ICPP 2021}
}

@inproceedings{10.1145/3077136.3080814,
author = {Zhang, Chao and Zhang, Keyang and Yuan, Quan and Tao, Fangbo and Zhang, Luming and Hanratty, Tim and Han, Jiawei},
title = {ReAct: Online Multimodal Embedding for Recency-Aware Spatiotemporal Activity Modeling},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080814},
doi = {10.1145/3077136.3080814},
abstract = {Spatiotemporalactivity modeling is an important task for applications like tour recommendation and place search. The recently developed geographical topic models have demonstrated compelling results in using geo-tagged social media (GTSM) for spatiotemporal activity modeling. Nevertheless, they all operate in batch and cannot dynamically accommodate the latest information in the GTSM stream to reveal up-to-date spatiotemporal activities. We propose ReAct, a method that processes continuous GTSM streams and obtains recency-aware spatiotemporal activity models on the fly. Distinguished from existing topic-based methods, ReAct embeds all the regions, hours, and keywords into the same latent space to capture their correlations. To generate high-quality embeddings, it adopts a novel semi-supervised multimodal embedding paradigm that leverages the activity category information to guide the embedding process. Furthermore, as new records arrive continuously, it employs strategies to effectively incorporate the new information while preserving the knowledge encoded in previous embeddings. Our experiments on the geo-tagged tweet streams in two major cities have shown that ReAct significantly outperforms existing methods for location and activity retrieval tasks.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {245–254},
numpages = {10},
keywords = {representation learning, location-based service, spatiotemporal data mining, multimodal embedding, information retrieval, social media, online learning},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3491102.3501943,
author = {Windl, Maximiliane and Feger, Sebastian S. and Zijlstra, Lara and Schmidt, Albrecht and Wozniak, Pawel W.},
title = {‘It Is Not Always Discovery Time’: Four Pragmatic Approaches in Designing AI Systems},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501943},
doi = {10.1145/3491102.3501943},
abstract = {While systems that use Artificial Intelligence (AI) are increasingly becoming part of everyday technology use, we do not fully understand how AI changes design processes. A structured understanding of how designers work with AI is needed to improve the design process and educate future designers. To that end, we conducted interviews with designers who participated in projects which used AI. While past work focused on AI systems created by experienced designers, we focus on the perspectives of a diverse sample of interaction designers. Our results show that the design process of an interactive system is affected when AI is integrated and that design teams adapt their processes to accommodate AI. Based on our data, we contribute four approaches adopted by interaction designers working with AI: a priori, post-hoc, model-centric, and competence-centric. Our work contributes a pragmatic account of how design processes for AI systems are enacted.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {50},
numpages = {12},
keywords = {artificial intelligence, design, data work, process},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3428361.3428396,
author = {Pozo, Ada and Phan, Thanh-Trung and Gatica-Perez, Daniel},
title = {Learning Urban Nightlife Routines from Mobile Data},
year = {2020},
isbn = {9781450388702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428361.3428396},
doi = {10.1145/3428361.3428396},
abstract = {The use of smartphone sensing for public health studies is appealing to understand routines. We present an approach to learn nightlife routines in a smartphone sensing dataset volunteered by 184 young people (1586 weekend nights with location data captured between 8PM and 4AM.) Human activity is represented at two levels, namely as the types of places visited and as the areas of the city where those places are. Routines extracted with two topic models (Latent Dirichlet Allocation and Hierarchical Dirichlet Process) are semantically meaningful and represent different moments of the weekend night, depicting activities such as pub crawling. The inference capacity of the routine representation is demonstrated with two classification tasks of value for alcohol research (alcohol consumption throughout the night, and heavy alcohol consumption.) The results suggest that nightlife routine mining could be used as a complementary tool to traditional survey-based methods in public health studies, and also inform other institutional actors interested in understanding and supporting youth well-being.},
booktitle = {19th International Conference on Mobile and Ubiquitous Multimedia},
pages = {107–118},
numpages = {12},
location = {Essen, Germany},
series = {MUM '20}
}

@article{10.1109/TCBB.2019.2901676,
author = {Tercan, Bahar and Acar, Aybar C.},
title = {The Use of Informed Priors in Biclustering of Gene Expression with the Hierarchical Dirichlet Process},
year = {2020},
issue_date = {Sept.-Oct. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2901676},
doi = {10.1109/TCBB.2019.2901676},
abstract = {We motivate and describe the application of Hierarchical Dirichlet Process (HDP) models to the “soft” biclustering of gene expression data, in which we obtain modules (biclusters) where the affiliation of genes and samples with the modules are weighted, instead of being hard memberships. As a distinct contribution, we propose a method which HDP is informed with prior beliefs, significantly increasing the quality of the biclustering in terms of both the correctness of the number of modules inferred, and the precision of these modules, especially when evidence is sparse. We outline two such informed priors; one based on co-expression relationships inherent in the data, the other based on an externally provided regulatory network. We validate these results and compare the performance of our approach to Weighted Gene Correlation Network Analysis (WGCNA), another model that features weighted modules. We have, to this end, performed experiments on semi-synthetic data. The results show that HDP, with the addition of a well-informed prior, is able to capture the correct number of modules with increased accuracy. Furthermore, the model becomes robust to changes in the strength of the prior. We conclude by discussing these results and the benefits provided by our approach for gene expression analysis and network validation.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {sep},
pages = {1810–1821},
numpages = {12}
}

@inproceedings{10.1145/3343413.3377980,
author = {Lin, Jimmy and Milligan, Ian and Oard, Douglas W. and Ruest, Nick and Shilton, Katie},
title = {We Could, but Should We? Ethical Considerations for Providing Access to GeoCities and Other Historical Digital Collections},
year = {2020},
isbn = {9781450368926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343413.3377980},
doi = {10.1145/3343413.3377980},
abstract = {We live in an era in which the ways that we can make sense of our past are evolving as more artifacts from that past become digital. At the same time, the responsibilities of traditional gatekeepers who have negotiated the ethics of historical data collection and use, such as librarians and archivists, are increasingly being sidelined by the system builders who decide whether and how to provide access to historical digital collections, often without sufficient reflection on the ethical issues at hand. It is our aim to better prepare system builders to grapple with these issues. This paper focuses discussions around one such digital collection from the dawn of the web, asking what sorts of analyses can and should be conducted on archival copies of the GeoCities web hosting platform that dates to 1994.},
booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
pages = {135–144},
numpages = {10},
keywords = {search, ethical frameworks, re-identification, contextual integrity, distant reading},
location = {Vancouver BC, Canada},
series = {CHIIR '20}
}

@article{10.5555/3122009.3176857,
author = {Lauly, Stanislas and Zheng, Yin and Allauzen, Alexandre and Larochelle, Hugo},
title = {Document Neural Autoregressive Distribution Estimation},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We present an approach based on feed-forward neural networks for learning the distribution over textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator (NADE) model which has been shown to be a good estimator of the distribution over discrete-valued high-dimensional vectors. In this paper, we present how NADE can successfully be adapted to textual data, retaining the property that sampling or computing the probability of an observation can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-level context.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {4046–4069},
numpages = {24},
keywords = {neural networks, autoregressive models, topic models, deep learning, language models}
}

@inproceedings{10.1145/3308558.3313625,
author = {Liang, Shangsong},
title = {Unsupervised Semantic Generative Adversarial Networks for Expert Retrieval},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313625},
doi = {10.1145/3308558.3313625},
abstract = {Sources in computer-based collaborative systems such as webpages can help employees to connect and cooperate with each other. It is natural to enable the systems to look not only for documents but also for experts. In this paper, we study the problem of expert retrieval in enterprise corpora: given a topic, also known as query containing a set of words, identify a rank list of candidate experts who have expertise on the topic. To tackle the problem, we propose an unsupervised semantic two-player minimax game, i.e., our unsupervised semantic generative adversarial networks (USGAN). Unlike almost all the previous generative adversarial networks-based algorithms that require ground truth training data, our USGAN is an unsupervised semantic expert retrieval algorithm that consists of a discriminative network and a generative network aiming at capturing the representations of words and experts in an unsupervised way. Candidates that have similar semantic representations to that of the topic are retrieved as relevant to the topic. Our USGAN would provide inspiration on how to extend the standard GAN and its variants by unsupervised ways to address other retrieval tasks where labelled data are missing. Experimental results on public datasets validate the effectiveness of the proposed expert retrieval algorithm.},
booktitle = {The World Wide Web Conference},
pages = {1039–1050},
numpages = {12},
keywords = {Language Models, Generative Adversarial Networks, Expert Retrieval},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/2998181.2998220,
author = {De Choudhury, Munmun and Sharma, Sanket S. and Logar, Tomaz and Eekhout, Wouter and Nielsen, Ren\'{e} Clausen},
title = {Gender and Cross-Cultural Differences in Social Media Disclosures of Mental Illness},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998220},
doi = {10.1145/2998181.2998220},
abstract = {Cultural and gender norms shape how mental illness and therapy are perceived. However, there is a paucity of adequate empirical evidence around gender and cultural dimensions of mental illness. In this paper we situate social media as a "lens" to examine these dimensions. We focus on a large dataset of individuals who self-disclose to have an underlying mental health concern on Twitter. Having identified genuine disclosures in this data via semi-supervised learning, we examine differences in their posts, as measured via linguistic attributes and topic models. Our findings reveal significant differences between the content shared by female and male users, and by users from two western and two majority world countries. Males express higher negativity and lower desire for social support, whereas majority world users demonstrate more inhibition in their expression. We discuss the implications of our work in providing insights into the relationship of gender and culture with mental health, and in the design of gender and culture-aware health interventions.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {353–369},
numpages = {17},
keywords = {suicide, gender, depression, culture, social media, mental health, reddit, health, twitter},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3184558.3186920,
author = {Roy, Anurag and Ghosh, Kripabandhu and Basu, Moumita and Gupta, Parth and Ghosh, Saptarshi},
title = {Retrieving Information from Multiple Sources},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186920},
doi = {10.1145/3184558.3186920},
abstract = {The Web has several information sources on which an ongoing event is discussed. To get a complete picture of the event, it is important to retrieve information from multiple sources. We propose a novel neural network based model which integrates the embeddings from multiple sources, and thus retrieves information from them jointly, %all the sources together, as opposed to combining multiple retrieval results. The importance of the proposed model is that no document-aligned comparable data is needed. Experiments on posts related to a particular event from three different sources - Facebook, Twitter and WhatsApp - exhibit the efficacy of the proposed model.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {43–44},
numpages = {2},
keywords = {deep learning, word embedding, multi-view retrieval},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3292522.3326058,
author = {Hoang, Tuan-Anh and Nguyen, Thi-Huyen and Nejdl, Wolfgang},
title = {Efficient Tracking of Breaking News in Twitter},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326058},
doi = {10.1145/3292522.3326058},
abstract = {We present an efficient graph-based method for filtering tweets relevant to a given breaking news from large tweet streams. Unlike existing models that either require manual effort, strong supervision, and/or not scalable, our method can automatically and effectively filter incoming relevant tweets starting from just a small number of past relevant tweets. Extensive experiments on both synthetic and real datasets show that our proposed method significantly outperforms other methods in filtering the relevant tweets while being as fast as the most efficient state-of-the-art method.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {135–136},
numpages = {2},
keywords = {twitter, realtime filtering, social media},
location = {Boston, Massachusetts, USA},
series = {WebSci '19}
}

@inproceedings{10.1145/3477314.3506997,
author = {Lenzi, Andrea and Velardi, Paola},
title = {Collaborative is Better than Adversarial: Generative Cooperative Networks for Topic Clustering},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3506997},
doi = {10.1145/3477314.3506997},
abstract = {Topic modeling is a popular technique for learning the thematic structure of large corpora composed of unlabeled documents, without human supervision. In recent years, various neural network-based algorithms have been proposed to solve this task. In particular, there is an extensive literature showing how Variational AutoEncoders (VAEs) and Generative Adversarial Networks (GANs) approaches have been successful in identifying recurrent discussion topics. In this paper we propose a new neural topic detection model called Generative Cooperative Topic Modeling (GCTM), in which a Generator and a denoising AutoEncoder, rather than learning through a competitive process, act cooperatively. We show that this cooperative model has a faster convergence and surpasses the adversarial approach, as well as other popular topic detection algorithms based on VAEs, when tested on three common public datasets and with a variety of performance indicators.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {688–695},
numpages = {8},
keywords = {clustering, generative adversarial networks, topic modeling, autoencoders, natural language processing, neural networks},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3446132.3446173,
author = {El Akrouchi, Manal and Benbrahim, Houda and Kassou, Ismail},
title = {Monitoring Early Warning Signs Evolution Through Time},
year = {2020},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446173},
doi = {10.1145/3446132.3446173},
abstract = {In excessive business competition, detecting weak signals is very important to anticipate future changes and events. The process of detecting weak signals is very challenging, and many techniques were proposed to automatize this challenge but still needs the intervention of experts’ opinion. Understanding those detected signals and their evolution in time is crucial to reveal the alertness of possible future events and warnings. For this reason, this paper proposes a new algorithm to strengthen weak signals into early warning signs. The proposed algorithm aims to monitor and track weak signals’ evolution within time. The output will be a list of early warning signs and visualization to illustrate their evolution in time. Finally, to adequately understand the early warning signs obtained and enhance their semantic alertness, we used Word2Vec modeling to provide semantically similar words to these warning signs and improve their contextual alertness. We tested this algorithm on a web news dataset of 2006-2007 to detect early warning signs related to the 2008 financial crisis ahead of time. We obtained prominent results in strengthening and monitoring the evolution of early warning signs related to this crisis.},
booktitle = {2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {41},
numpages = {9},
keywords = {Futures knowledge, Weak Signals Evolution, Early Warning Signs},
location = {Sanya, China},
series = {ACAI 2020}
}

@inproceedings{10.1145/3106426.3106442,
author = {Ghosh, Krishnendu and Bhowmick, Plaban Kumar and Goyal, Pawan},
title = {Using Re-Ranking to Boost Deep Learning Based Community Question Retrieval},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106442},
doi = {10.1145/3106426.3106442},
abstract = {The current study presents a two-stage question retrieval approach which, in the first phase, retrieves similar questions for a given query using a deep learning based approach and in the second phase, re-ranks initially retrieved questions on the basis of inter-question similarities. The suggested deep learning based approach is trained using several surface features of texts and the associated weights are pre-trained using a deep generative model for better initialization. The proposed retrieval model outperforms standard baseline question retrieval approaches. The proposed re-ranking approach performs inference over a similarity graph constructed with the initially retrieved questions and re-ranks the questions based on their similarity with other relevant questions. Suggested re-ranking approach significantly improves the precision for the retrieval task.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {807–814},
numpages = {8},
keywords = {question retrieval, community question answering, re-ranking},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3077136.3080823,
author = {Mehrotra, Rishabh and Yilmaz, Emine},
title = {Extracting Hierarchies of Search Tasks &amp; Subtasks via a Bayesian Nonparametric Approach},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080823},
doi = {10.1145/3077136.3080823},
abstract = {A significant amount of search queries originate from some real world information need or tasks [13]. In order to improve the search experience of the end users, it is important to have accurate representations of tasks. As a result, significant amount of research has been devoted to extracting proper representations of tasks in order to enable search systems to help users complete their tasks, as well as providing the end user with better query suggestions [9], for better recommendations [41], for satisfaction prediction [36] and for improved personalization in terms of tasks [24, 38]. Most existing task extraction methodologies focus on representing tasks as flat structures. However, tasks often tend to have multiple subtasks associated with them and a more naturalistic representation of tasks would be in terms of a hierarchy, where each task can be composed of multiple (sub)tasks. To this end, we propose an efficient Bayesian nonparametric model for extracting hierarchies of such tasks &amp; subtasks. We evaluate our method based on real world query log data both through quantitative and crowdsourced experiments and highlight the importance of considering task/subtask hierarchies.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {285–294},
numpages = {10},
keywords = {hierarchical model, bayesian non-parametrics, search tasks},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1145/3052772,
author = {Zhang, Dongxiang and Li, Yuchen and Fan, Ju and Gao, Lianli and Shen, Fumin and Shen, Heng Tao},
title = {Processing Long Queries Against Short Text: Top-k Advertisement Matching in News Stream Applications},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052772},
doi = {10.1145/3052772},
abstract = {Many real applications in real-time news stream advertising call for efficient processing of long queries against short text. In such applications, dynamic news feeds are regarded as queries to match against an advertisement (ad) database for retrieving the k most relevant ads. The existing approaches to keyword retrieval cannot work well in this search scenario when queries are triggered at a very high frequency. To address the problem, we introduce new techniques to significantly improve search performance. First, we devise a two-level partitioning for tight upper bound estimation and a lazy evaluation scheme to delay full evaluation of unpromising candidates, which can bring three to four times performance boosting in a database with 7 million ads. Second, we propose a novel rank-aware block-oriented inverted index to further improve performance. In this index scheme, each entry in an inverted list is assigned a rank according to its importance in the ad. Then, we introduce a block-at-a-time search strategy based on the index scheme to support a much tighter upper bound estimation and a very early termination. We have conducted experiments with real datasets, and the results show that the rank-aware method can further improve performance by an order of magnitude.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {28},
numpages = {27},
keywords = {inverted index, top-k retrieval, Long queries, short text, rank-aware partitioning}
}

@article{10.1145/3130348.3130376,
author = {Lavrenko, Victor and Croft, W. Bruce},
title = {Relevance-Based Language Models},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3130348.3130376},
doi = {10.1145/3130348.3130376},
abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate a relevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
journal = {SIGIR Forum},
month = {aug},
pages = {260–267},
numpages = {8}
}

@inproceedings{10.1145/3538969.3543795,
author = {Ishii, Masahiro and Mori, Kento and Kuwana, Ryoichi and Matsuura, Satoshi},
title = {Multi-Label Classification of Cybersecurity Text with Distant Supervision},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3543795},
doi = {10.1145/3538969.3543795},
abstract = {Detailed analysis of cybersecurity intelligence in various data is essential to counter the recent advanced and complex evolution of cyber security attacks and threats. In particular, highly sophisticated learning models are required to classify cyberattacks and threats or extract security intelligence from unstructured data described in natural language. This study addresses text classification as the first step toward such sophisticated models. More specifically, we performed a multi-label classification of cybersecurity documents to reduce the cost of threat analysis and incident response. Detailed analysis of security incidents requires an integrated model that performs security intelligence extraction and event extraction tasks that leverage their relationships. We performed document-level multi-label classification with the standard categories of MITRE for cybersecurity attack and threat models. Furthermore, to reduce the cost of creating a large set of annotated data to improve the accuracy of the model, we automated generating of training data by using distant supervision&nbsp;[18]. We compared some methods for extracting keywords obtained from texts related to a defined classification category and multiple label assignment rules. We used cybersecurity documents from social news sites, threat reports, blog articles posted by security vendors as training and test data. We train a multi-label classification model on these texts using their document-level embedding vector obtained from a pre-trained language model. We also reported the experimental classification result for each category and compare several models and labeling with distant supervision. In addition, we performed human annotation for the sampled documents in the test data and evaluated the accuracy of classification on the annotated data. We showed that the machine learning models are slightly more accurate than the rule-based classifying with distant supervision on the test data. In some cases, the classification accuracy of distant supervision labeling is higher than the machine learning model on the human-annotated data. Furthermore, we analyzed and discussed the statistics of labels assigned by distant supervision, their co-occurrence with the predicted categories by the trained model, and how to utilize the classification model in cybersecurity incident response.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {93},
numpages = {9},
keywords = {Text Classification, Security Intelligence, Multi-label Classification, Distant Supervision, Threat Detection},
location = {Vienna, Austria},
series = {ARES '22}
}

@article{10.1109/TASLP.2021.3084105,
author = {Chen, Haibin and Ma, Qianli and Yu, Liuhong and Lin, Zhenxi and Yan, Jiangyue},
title = {Corpus-Aware Graph Aggregation Network for Sequence Labeling},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3084105},
doi = {10.1109/TASLP.2021.3084105},
abstract = {Current state-of-the-art sequence labeling models are typically based on sequential architecture such as Bi-directional LSTM (BiLSTM). However, the structure of processing a word at a time based on the sequential order restricts the full utilization of non-sequential features, including syntactic relationships, word co-occurrence relations, and document topics. They can be regarded as the corpus-level features and critical for sequence labeling. In this paper, we propose a <bold>Corpus-Aware Graph Aggregation Network</bold>. Specifically, we build three types of graphs, i.e., a word-topic graph, a word co-occurrence graph, and a word syntactic dependency graph, to express different kinds of corpus-level non-sequential features. After that, a graph convolutional network (GCN) is adapted to model the relations between words and non-sequential features. Finally, we employ a label-aware attention mechanism to aggregate corpus-aware non-sequential features and sequential ones for sequence labeling. The experimental results on four sequence labeling tasks (named entity recognition, chunking, multilingual sequence labeling, and target-based sentiment analysis) show that our model achieves state-of-the-art performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {2048–2057},
numpages = {10}
}

@inproceedings{10.1145/3341161.3342898,
author = {Paudel, Pujan and Nguyen, Trung T. and Hatua, Amartya and Sung, Andrew H.},
title = {How the Tables Have Turned: Studying the New Wave of Social Bots on Twitter Using Complex Network Analysis Techniques},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3342898},
doi = {10.1145/3341161.3342898},
abstract = {Twitter bots have evolved from easily-detectable, simple content spammers with bogus identities to sophisticated players embedded in deep levels of social networks, silently promoting affiliate campaigns, marketing various products and services, and orchestrating or coordinating political activities. Much research has been reported on building accurate machine learning classifiers to identifying bots in social networks; recent works on social bots have started the new line of research on the existence, placement, and functions of the bots in a collective manner. In this paper, we study two families of Twitter bots which have been studied previously with respect to spamming activities through advertisement and political campaigns, and perform an evolutionary comparison with the new waves of bots currently found in Twitter. We uncover various evolved tendencies of the new social bots under social, communication, and behavioral patterns. Our findings show that these bots demonstrate evolved core-periphery structure; are deeply embedded in their networks of communication; exhibit complex information diffusion and heterogeneous content authoring patterns; perform mobilization of leaders across communication roles; and reside in niche topic communities. These characteristics make them highly deceptive as well as more effective in achieving operational goals than their traditional counterparts. We conclude by discussing some possible applications of the discovered behavioral and social traits of the evolved bots, and ways to build effective bot detection systems.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {501–508},
numpages = {8},
keywords = {network analysis, Twitter, social bots},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3368926.3369668,
author = {Ho, Thi Kim Thoa and Bui, Quang Vu and Bui, Marc},
title = {Co-Author Relationship Prediction in Bibliographic Network: A New Approach Using Geographic Factor and Latent Topic Information},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369668},
doi = {10.1145/3368926.3369668},
abstract = {In this research, we propose a novel approach for co-author relationship prediction in a bibliographic network utilizing geographic factor and latent topic information. We utilize a supervised method to predict the co-author relationship formation where combining dissimilar features with the dissimilar measuring coefficient. Firstly, besides existing relations have been studied in previous researches, we exploit new relation related to the geographic factor which contributes as a topological feature. Moreover, we discover content feature based on textual information from author's papers using topic modeling. Finally, we amalgamate topological features and content feature in co-author relationship prediction. We conducted experiments on dissimilar datasets of the bibliographic network and have attained satisfactory results.},
booktitle = {Proceedings of the Tenth International Symposium on Information and Communication Technology},
pages = {69–77},
numpages = {9},
keywords = {topic modeling, multi-relation network, bibliographic network, Link prediction},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT 2019}
}

@inproceedings{10.1145/3365109.3368787,
author = {Kato, Sota and Nakanishi, Takafumi and Shimauchi, Hirokazu and Ahsan, Budrul},
title = {Topic Variation Detection Method for Detecting Political Business Cycles},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368787},
doi = {10.1145/3365109.3368787},
abstract = {In this paper, we present a new topic variation detection method that combines a topic extraction method with a change point detection method. It extracts topics from time-series text data as the feature of each time and detects change points from the changing patterns of the extracted topics. We applied the method to analyze effective albeit underutilized text data that contained the Japanese Prime Minister's (PM's) detailed daily activities of over 32 years. Our method and data provide novel insights into the empirical analysis of the political business cycle, a classical issue in economics and political science. For example, because our approach enabled us to directly observe and analyze the PM's actions, it overcame empirical challenges that previous researchers encountered owing to the unobservability of the PM's behavior. Our empirical observations are mostly consistent with recent theoretical developments regarding this topic. Despite limitations, by employing a completely new method and data, our approach enhances our understanding and provides new insights into this classic issue.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {85–93},
numpages = {9},
keywords = {singular spectrum transformation, topic variation detection, political business cycles, latent dirichlet allocation},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@inproceedings{10.1145/3308560.3316502,
author = {Tavabi, Nazgol and Bartley, Nathan and Abeliuk, Andres and Soni, Sandeep and Ferrara, Emilio and Lerman, Kristina},
title = {Characterizing Activity on the Deep and Dark Web},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316502},
doi = {10.1145/3308560.3316502},
abstract = {The deep and darkweb (d2web) refers to limited access web sites that require registration, authentication, or more complex encryption protocols to access them. These web sites serve as hubs for a variety of illicit activities: to trade drugs, stolen user credentials, hacking tools, and to coordinate attacks and manipulation campaigns. Despite its importance to cyber crime, the d2web has not been systematically investigated. In this paper, we study a large corpus of messages posted to 80 d2web forums over a period of more than a year. We identify topics of discussion using LDA and use a non-parametric HMM to model the evolution of topics across forums. Then, we examine the dynamic patterns of discussion and identify forums with similar patterns. We show that our approach surfaces hidden similarities across different forums and can help identify anomalous events in this rich, heterogeneous data.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {206–213},
numpages = {8},
keywords = {LDA, D2web, Beta Process, Cyber Crime, Non-Parametric HMM, Cluster Time Series, Darkweb, Cyber Security, Deepweb, multivariate Time Series},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{10.1145/3447875,
author = {Vuong, Tung and Andolina, Salvatore and Jacucci, Giulio and Ruotsalo, Tuukka},
title = {Spoken Conversational Context Improves Query Auto-Completion in Web Search},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3447875},
doi = {10.1145/3447875},
abstract = {Web searches often originate from conversations in which people engage before they perform a search. Therefore, conversations can be a valuable source of context with which to support the search process. We investigate whether spoken input from conversations can be used as a context to improve query auto-completion. We model the temporal dynamics of the spoken conversational context preceding queries and use these models to re-rank the query auto-completion suggestions. Data were collected from a controlled experiment and comprised conversations among 12 participant pairs conversing about movies or traveling. Search query logs during the conversations were recorded and temporally associated with the conversations. We compared the effects of spoken conversational input in four conditions: a control condition without contextualization; an experimental condition with the model using search query logs; an experimental condition with the model using spoken conversational input; and an experimental condition with the model using both search query logs and spoken conversational input. We show the advantage of combining the spoken conversational context with the Web-search context for improved retrieval performance. Our results suggest that spoken conversations provide a rich context for supporting information searches beyond current user-modeling approaches.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {31},
numpages = {32},
keywords = {voice, QAC, background speech, speech input, query auto-completion}
}

@article{10.1145/3512980,
author = {Zhou, Jiawei and Saha, Koustuv and Lopez Carron, Irene Michelle and Yoo, Dong Whi and Deeter, Catherine R. and De Choudhury, Munmun and Arriaga, Rosa I.},
title = {Veteran Critical Theory as a Lens to Understand Veterans' Needs and Support on Social Media},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512980},
doi = {10.1145/3512980},
abstract = {Veterans are a unique marginalized group facing multiple vulnerabilities. Current assessments of veteran needs and support largely come from first-person accounts guided by researchers' prompts. Social media platforms not only enable veterans to connect with each other, but also to self-disclose experiences and seek support. This paper addresses the gap in our understanding of veteran needs and their own support dynamics by examining self-initiated and ecologically-valid self-expressions. In particular, we adopt the Veteran Critical Theory (VCT) to conduct a computational study on the Reddit community of veterans. Using topic modeling, we find veteran-friendly gestures with good intentions might not be appreciated in the subreddit. By employing transfer learning methodologies, we find this community has more informational and emotional support behaviors than general online communities and a higher prevalence of informational support than emotional support. Lastly, an examination of support dynamics reveals some contrasts to previous scholarship in military culture and social media. We discover that positive language and author platform tenure have negative relations with posts receiving replies and replies getting votes, and that replies reflecting personal disclosures tend to get more votes. Through the lens of VCT, we discuss how online communities can help uncover veterans' needs and provide more effective social support.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {133},
numpages = {28},
keywords = {need discovery, topic modeling, reddit, language analysis, social support, online community, veteran, critical theory, community dynamic}
}

@inproceedings{10.1145/3400806.3400808,
author = {Sang, Yisi and Stanton, Jeffrey},
title = {Analyzing Hate Speech with Incel-Hunters’ Critiques},
year = {2020},
isbn = {9781450376884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400806.3400808},
doi = {10.1145/3400806.3400808},
abstract = {The ubiquity of online media services helps to promote free speech but also provides opportunities for the spread of problematic content such as hate speech. A group of individuals known as “incels” (involuntary celibates) sometimes use online media services to publish hateful content. To expose and condemn incel hate speech, other individuals, sometimes called “incel hunters” create online communities where they critique screenshots of content posted by incels. In this paper, using 18,187 posts collected from a subreddit named r/IncelTears, we explore the potential for transforming screenshots of incel hate speech and oppositional statements into training data that could be used as the basis for automated or semi-automated content moderation tools.},
booktitle = {International Conference on Social Media and Society},
pages = {5–13},
numpages = {9},
keywords = {Reddit, online communities, content moderation, Incel, semi-automated moderation, automated moderation, hate speech},
location = {Toronto, ON, Canada},
series = {SMSociety'20}
}

@inproceedings{10.1145/3148044.3148048,
author = {Nguyen, Phuc and Huang, Yan and Trampier, Joshua R.},
title = {Leveraging Spatial Community Information in Location Recognition in Tweets},
year = {2017},
isbn = {9781450355001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148044.3148048},
doi = {10.1145/3148044.3148048},
abstract = {Location names are very helpful in event extraction. Informal social texts pose significant challenges for recognizing location names. However, social texts have an advantage that can be leveraged: spatial and social network contexts. We address the location recognizing task as a part of named entity recognition, and introduce a new approach which leverages community contexts and captures language variations among groups of users. Specifically, we incorporate a community component into a topic modeling method and harness unlabeled tweets. Experiments on a large Twitter dataset show that our proposed method can improve the location classification F1 score by 5%.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Analytics for Local Events and News},
articleno = {4},
numpages = {9},
keywords = {named entity recognition, topic modeling, location recognition, twitter},
location = {Redondo Beach, CA, USA},
series = {LENS'17}
}

@inproceedings{10.1145/3133202.3133205,
author = {Bahrainian, Seyed Ali and Crestani, Fabio},
title = {Are Conversation Logs Useful Sources for Generating Memory Cues for Recalling Past Memories?},
year = {2017},
isbn = {9781450355032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133202.3133205},
doi = {10.1145/3133202.3133205},
abstract = {In recent years with the emergence of wearable devices for lifelogging, every day huge archives of data consisting of images, audio, etc are generated from a person's life. As a result researchers in this field have focused on building applications that utilize lifelog archives for various human-aid applications.Several studies have shown the effectiveness of replaying recordings of one's life in aiding human memory to recall past events. Specifically, lifelog images captured automatically by wearable cameras have been very popular in these studies.In this paper, we examine the effectiveness of lifelog conversations in a workplace environment as useful sources of information for extracting memory cues to recall past events. We present a method for generating memory cues for augmenting one's memory. Furthermore, we conduct a user study with five groups of people, where each group consists of two individuals, in order to examine the effectiveness of our approach.Our results on real-world data demonstrate the effectiveness of our method in aiding people recalling the contents of past conversations. Such results were achieved by showing summary memory cues of past conversations to each participant for a maximum duration of only five minutes to aid recall of past events.},
booktitle = {Proceedings of the 2nd Workshop on Lifelogging Tools and Applications},
pages = {13–20},
numpages = {8},
keywords = {lifelogging, conversations, user study, episodic memory},
location = {Mountain View, California, USA},
series = {LTA '17}
}

@inproceedings{10.1145/3368089.3409672,
author = {Zhao, Nengwen and Chen, Junjie and Wang, Zhou and Peng, Xiao and Wang, Gang and Wu, Yong and Zhou, Fang and Feng, Zhen and Nie, Xiaohui and Zhang, Wenchi and Sui, Kaixin and Pei, Dan},
title = {Real-Time Incident Prediction for Online Service Systems},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409672},
doi = {10.1145/3368089.3409672},
abstract = {Incidents in online service systems could dramatically degrade system availability and destroy user experience. To guarantee service quality and reduce economic loss, it is essential to predict the occurrence of incidents in advance so that engineers can take some proactive actions to prevent them. In this work, we propose an effective and interpretable incident prediction approach, called eWarn, which utilizes historical data to forecast whether an incident will happen in the near future based on alert data in real time. More specifically, eWarn first extracts a set of effective features (including textual features and statistical features) to represent omen alert patterns via careful feature engineering. To reduce the influence of noisy alerts (that are not relevant to the occurrence of incidents), eWarn then incorporates the multi-instance learning formulation. Finally, eWarn builds a classification model via machine learning and generates an interpretable report about the prediction result via a state-of-the-art explanation technique (i.e., LIME). In this way, an early warning signal along with its interpretable report can be sent to engineers to facilitate their understanding and handling for the incoming incident. An extensive study on 11 real-world online service systems from a large commercial bank demonstrates the effectiveness of eWarn, outperforming state-of-the-art alert-based incident prediction approaches and the practice of incident prediction with alerts. In particular, we have applied eWarn to two large commercial banks in practice and shared some success stories and lessons learned from real deployment.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {315–326},
numpages = {12},
keywords = {Real-time Prediction, Incident Prediction, Online Service Systems},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1109/TASLP.2021.3133206,
author = {Ma, Bing and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
title = {Extractive Dialogue Summarization Without Annotation Based on Distantly Supervised Machine Reading Comprehension in Customer Service},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3133206},
doi = {10.1109/TASLP.2021.3133206},
abstract = {Given a long dialogue, the dialogue summarization system aims to obtain a shorter highlight which retains the important information in the original text. For the customer service scenarios, the summaries of most dialogues between an agent and a user focus on several fixed key points, such as users’ question, users’ purpose, the agent’s solution, and so on. Traditional extractive methods are difficult to extract all predefined key points exactly. Furthermore, there is a lack of large-scale and high-quality extractive summarization datasets containing the annotation for key points. Moreover, the speaker’s role information is ignored or not fully utilized in previous work. In order to solve the above challenges, we propose a Distant Supervision based Machine Reading Comprehension model for extractive Summarization (DSMRC-S). DSMRC-S transforms the summarization task into the machine reading comprehension problem, to fetch key points from the original text exactly according to the predefined questions. In addition, a distant supervision method is proposed to alleviate the lack of eligible extractive summarization datasets. What’s more, a speaker’s role token and the solver classification task are proposed to make full use of speaker’s role information. We conduct experiments on a real-world summarization dataset collected in customer service scenarios, and the results show that the proposed method outperforms the strong baseline methods by 6 percentage points on ROUGE<inline-formula><tex-math notation="LaTeX">$_L$</tex-math></inline-formula>.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {87–97},
numpages = {11}
}

@inproceedings{10.1145/3341162.3349297,
author = {Gatica-Perez, Daniel and Biel, Joan-Isaac and Labbe, David and Martin, Nathalie},
title = {Discovering Eating Routines in Context with a Smartphone App},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3349297},
doi = {10.1145/3341162.3349297},
abstract = {In everyday life, eating follows patterns and occurs in context. We present an approach to discover daily eating routines of a population following a multidimensional representation of eating episodes, using data collected with the Bites'n'Bits smartphone app. Our approach integrates multiple contextual cues provided in-situ (food type, time, location, social context, concurrent activities, and motivations) with probabilistic topic models, which discover representative patterns across these contextual dimensions. We show that this approach, when applied on eating episode data for over 120 people and 1200 days, allows describing the main eating routines of the population in meaningful ways. This approach, resulting from a collaboration between ubiquitous computing and nutrition science, can support interdisciplinary work on contextual analytics for promotion of healthy eating.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {422–429},
numpages = {8},
keywords = {eating behavior, routines, mobile crowdsensing, smartphones},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3240508.3240607,
author = {Zhan, Yibing and Yu, Jun and Yu, Zhou and Zhang, Rong and Tao, Dacheng and Tian, Qi},
title = {Comprehensive Distance-Preserving Autoencoders for Cross-Modal Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240607},
doi = {10.1145/3240508.3240607},
abstract = {In this paper, we propose a novel method with comprehensive distance-preserving autoencoders (CDPAE) to address the problem of unsupervised cross-modal retrieval. Previous unsupervised methods rely primarily on pairwise distances of representations extracted from cross media spaces that co-occur and belong to the same objects. However, besides pairwise distances, the CDPAE also considers heterogeneous distances of representations extracted from cross media spaces as well as homogeneous distances of representations extracted from single media spaces that belong to different objects. The CDPAE consists of four components. First, denoising autoencoders are used to retain the information from the representations and to reduce the negative influence of redundant noises. Second, a comprehensive distance-preserving common space is proposed to explore the correlations among different representations. This aims to preserve the respective distances between the representations within the common space so that they are consistent with the distances in their original media spaces. Third, a novel joint loss function is defined to simultaneously calculate the reconstruction loss of the denoising autoencoders and the correlation loss of the comprehensive distance-preserving common space. Finally, an unsupervised cross-modal similarity measurement is proposed to further improve the retrieval performance. This is carried out by calculating the marginal probability of two media objects based on a kNN classifier. The CDPAE is tested on four public datasets with two cross-modal retrieval tasks: "query images by texts" and "query texts by images". Compared with eight state-of-the-art cross-modal retrieval methods, the experimental results demonstrate that the CDPAE outperforms all the unsupervised methods and performs competitively with the supervised methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1137–1145},
numpages = {9},
keywords = {cross-modal retrieval, comprehensive distancepreserving, autoencoder, unsupervised, similarity measurement},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3209219.3209228,
author = {Wibowo, Agung Toto and Siddharthan, Advaith and Masthoff, Judith and Lin, Chenghua},
title = {Incorporating Constraints into Matrix Factorization for Clothes Package Recommendation},
year = {2018},
isbn = {9781450355896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209219.3209228},
doi = {10.1145/3209219.3209228},
abstract = {Recommender systems have been widely applied in the literature to suggest individual items to users. In this paper, we consider the harder problem of package recommendation, where items are recommended together as a package. We focus on the clothing domain, where a package recommendation involves a combination of a 'top' (e.g. a shirt) and a 'bottom' (e.g. a pair of trousers). The novelty in this work is that we combined matrix factorisation methods for collaborative filtering with hand-crafted and learnt fashion constraints on combining item features such as colour, formality and patterns. Finally, to better understand where the algorithms are underperforming, we conducted focus groups, which lead to deeper insights into how to use constraints to improve package recommendation in this domain.},
booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {111–119},
numpages = {9},
keywords = {matrix factorization, package recommendation, constraints, clothes domain},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inproceedings{10.1145/3356395.3365540,
author = {Daughton, Ashlynn R. and Fairchild, Geoffrey and Watson Ross, Chrysm and Del Valle, Sara Y.},
title = {Topic Modeling To Contextualize Event-Based Datasets: The Colombian Peace Process},
year = {2019},
isbn = {9781450369541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356395.3365540},
doi = {10.1145/3356395.3365540},
abstract = {Colombia suffered civil conflict for over five decades resulting in thousands of deaths and kidnappings and millions of displaced citizens. A peace process between the government and the Revolutionary Armed Forces of Colombia (FARC) was negotiated in 2016. Quantifying public sentiment during the process may help us understand the role of social media in shaping opinions and influencing decision makers. Obtaining these viewpoints using traditional survey approaches is costly and logistically challenging. Instead, we used Twitter and news data between 2010-2018 to analyze trends before, during, and after the settlement. We used unsupervised learning methods to identify topics and measure their sentiment over time; we then compare those results to events in the Integrated Crisis Early Warning System (ICEWS) dataset.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances on Resilient and Intelligent Cities},
pages = {9–12},
numpages = {4},
keywords = {topic modeling, Integrated Crisis Early Warning System (ICEWS), social media, unsupervised learning, Latent Dirichlet Allocation (LDA), Colombia peace process, Twitter},
location = {Chicago, IL, USA},
series = {ARIC'19}
}

@inproceedings{10.1145/3184558.3186338,
author = {Mu, Lin and Jin, Peiquan and Zheng, Lizhou and Chen, En-Hong and Yue, Lihua},
title = {Lifecycle-Based Event Detection from Microblogs},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186338},
doi = {10.1145/3184558.3186338},
abstract = {Microblog like Twitter and Sina Weibo has been an important information source for event detection and monitoring. In many decision-making scenarios, it is not enough to only provide a structural tuple for an event, e.g., a 5W1H record like <who, where,="" when,="" what,="" whom,="" how="">. However, in addition to event structural tuples, people need to know the evolution lifecycle of an event. The lifecycle description of an event is more helpful for decision making because people can focus on the progress and trend of events. In this paper, we propose a novel method for efficiently detecting and tracking event evolution on microblogging platforms. The major features of our study are: (1) It provides a novel event-type-driven method to extract event tuples, which forms the foundation for event evolution analysis. (2) It describes the lifecycle of an event by a staged model, and provides effective algorithms for detecting the stages of an event. (3) It offers emotional analysis over the stages of an event, through which people are able to know the public emotional tendency over a specific event at different time periods. We build a prototype system and present its architecture and implemental details in the paper. In addition, we conduct experiments on real microblog datasets and the results in terms of precision, recall, and F-measure suggest the effectiveness and efficiency of our proposal.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {283–290},
numpages = {8},
keywords = {microblog, lifecycle, event evolution, event detection},
location = {Lyon, France},
series = {WWW '18}
}</who,>

@inproceedings{10.1145/3145574.3145577,
author = {Sharma, Eva and Saha, Koustuv and Ernala, Sindhu Kiranmai and Ghoshal, Sucheta and De Choudhury, Munmun},
title = {Analyzing Ideological Discourse on Social Media: A Case Study of the Abortion Debate},
year = {2017},
isbn = {9781450352697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3145574.3145577},
doi = {10.1145/3145574.3145577},
abstract = {Social media provides a unique platform enabling public discourse around cross-cutting ideologies. In this paper, we provide a methodological lens for studying the discourses around the controversial topic of abortion on social media. Drawing from the theoretical framework of "Critical Discourse Analysis", we study discourse around abortion on Twitter through analysis of language and the manifested socio-cultural practices. First, employing a large dataset of over 700 thousand posts, we find that abortion discourse can be classified into three ideologies: For, Against, and Neutral to Abortion. We observe these ideological categories to be characterized by distinctive textual and psycholinguistic cues. Finally, we analyze the nature of discourse across ideologies against the backdrop of socio-cultural practices associated with abortion. Our findings reveal how the hegemonic nature of the rhetoric that has historically shaped the abortion debate in society is reconceptualized on Twitter. We discuss the role of social media as a public sphere that shapes critical discourse around controversial topics.},
booktitle = {Proceedings of the 2017 International Conference of The Computational Social Science Society of the Americas},
articleno = {3},
numpages = {8},
keywords = {public sphere, twitter, social media, abortion debate, critical discourse analysis},
location = {Santa Fe, NM, USA},
series = {CSS 2017}
}

@inproceedings{10.1145/3110025.3110054,
author = {Rony, Md Main Uddin and Hassan, Naeemul and Yousuf, Mohammad},
title = {Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110054},
doi = {10.1145/3110025.3110054},
abstract = {The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the wide-spread practice of clickbait makes the reader's reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3%. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {232–239},
numpages = {8},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3318265.3319613,
author = {Li, Hongbo and Zhu, Hong and Cui, Zongmin},
title = {A Spatial Text Query Scheme Based on Semantic-Aware},
year = {2019},
isbn = {9781450366380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318265.3319613},
doi = {10.1145/3318265.3319613},
abstract = {In some application scenarios, the strategy based on text similarity cannot accurately find the spatial text data that users need. Therefore, we propose a spatial text query scheme based on semantic-aware. We name this scheme as SSA. We study spatial text queries based on semantic-aware. We improve LDA algorithm to filter out some worthless candidate data. The experimental results show that our scheme has good basic attributes and query accuracy.},
booktitle = {Proceedings of the 3rd International Conference on High Performance Compilation, Computing and Communications},
pages = {55–58},
numpages = {4},
keywords = {data query, semantic-aware, spatial text query},
location = {Xi'an, China},
series = {HP3C '19}
}

@article{10.1145/3332185,
author = {Comito, Carmela and Forestiero, Agostino and Pizzuti, Clara},
title = {Bursty Event Detection in Twitter Streams},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332185},
doi = {10.1145/3332185},
abstract = {Social media, in recent years, have become an invaluable source of information for both public and private organizations to enhance the comprehension of people interests and the onset of new events. Twitter, especially, allows a fast spread of news and events happening real time that can contribute to situation awareness during emergency situations, but also to understand trending topics of a period. The article proposes an online algorithm that incrementally groups tweet streams into clusters. The approach summarizes the examined tweets into the cluster centroid by maintaining a number of textual and temporal features that allow the method to effectively discover groups of interest on particular themes. Experiments on messages posted by users addressing different issues, and a comparison with state-of-the-art approaches show that the method is capable to detect discussions regarding topics of interest, but also to distinguish bursty events revealed by a sudden spreading of attention on messages published by users.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {aug},
articleno = {41},
numpages = {28},
keywords = {event detection, Twitter, online clustering, bursty event}
}

@article{10.1109/TCBB.2016.2595575,
author = {Lovato, Pietro and Cristani, Marco and Bicego, Manuele},
title = {Soft Ngram Representation and Modeling for Protein Remote Homology Detection},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2595575},
doi = {10.1109/TCBB.2016.2595575},
abstract = {Remote homology detection represents a central problem in bioinformatics, where the challenge is to detect functionally related proteins when their sequence similarity is low. Recent solutions employ representations derived from the sequence profile, obtained by replacing each amino acid of the sequence by the corresponding most probable amino acid in the profile. However, the information contained in the profile could be exploited more deeply, provided that there is a representation able to capture and properly model such crucial evolutionary information. In this paper, we propose a novel profile-based representation for sequences, called soft Ngram. This representation, which extends the traditional Ngram scheme obtained by grouping N consecutive amino acids, permits considering all of the evolutionary information in the profile: this is achieved by extracting Ngrams from the whole profile, equipping them with a weight directly computed from the corresponding evolutionary frequencies. We illustrate two different approaches to model the proposed representation and to derive a feature vector, which can be effectively used for classification using a support vector machine SVM. A thorough evaluation on three benchmarks demonstrates that the new approach outperforms other Ngram-based methods, and shows very promising results also in comparison with a broader spectrum of techniques.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {nov},
pages = {1482–1488},
numpages = {7}
}

@inproceedings{10.1145/3123266.3123272,
author = {Min, Weiqing and Jiang, Shuqiang and Wang, Shuhui and Sang, Jitao and Mei, Shuhuan},
title = {A Delicious Recipe Analysis Framework for Exploring Multi-Modal Recipes with Various Attributes},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123272},
doi = {10.1145/3123266.3123272},
abstract = {Human beings have developed a diverse food culture. Many factors like ingredients, visual appearance, courses (e.g., breakfast and lunch), flavor and geographical regions affect our food perception and choice. In this work, we focus on multi-dimensional food analysis based on these food factors to benefit various applications like summary and recommendation. For that solution, we propose a delicious recipe analysis framework to incorporate various types of continuous and discrete attribute features and multi-modal information from recipes. First, we develop a Multi-Attribute Theme Modeling (MATM) method, which can incorporate arbitrary types of attribute features to jointly model them and the textual content. We then utilize a multi-modal embedding method to build the correlation between the learned textual theme features from MATM and visual features from the deep learning network. By learning attribute-theme relations and multi-modal correlation, we are able to fulfill different applications, including (1) flavor analysis and comparison for better understanding the flavor patterns from different dimensions, such as the region and course, (2) region-oriented multi-dimensional food summary with both multi-modal and multi-attribute information and (3) multi-attribute oriented recipe recommendation. Furthermore, our proposed framework is flexible and enables easy incorporation of arbitrary types of attributes and modalities. Qualitative and quantitative evaluation results have validated the effectiveness of the proposed method and framework on the collected Yummly dataset.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {402–410},
numpages = {9},
keywords = {flavor analysis, food summary, multi-attribute theme modeling, multi-dimensional food analysis, recipe recommendation},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3267809.3267810,
author = {Xie, Pengtao and Kim, Jin Kyu and Ho, Qirong and Yu, Yaoliang and Xing, Eric},
title = {Orpheus: Efficient Distributed Machine Learning via System and Algorithm Co-Design},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3267810},
doi = {10.1145/3267809.3267810},
abstract = {Numerous existing works have shown that, key to the efficiency of distributed machine learning (ML) is proper system and algorithm co-design: system design should be tailored to the unique mathematical properties of ML algorithms, and algorithms can be re-designed to better exploit the system architecture. While existing research has made attempts along this direction, many algorithmic and system properties that are characteristic of ML problems remain to be explored. Through an exploration of system-algorithm co-design, we build a new decentralized system Orpheus to support distributed training of a general class of ML models whose parameters are represented with large matrices. Training such models at scale is challenging: transmitting and checkpointing large matrices incur substantial network traffic and disk IO, which aggravates the inconsistency among parameter replicas. To cope with these challenges, Orpheus jointly exploits system and algorithm designs which (1) reduce the size and number of network messages for efficient communication, 2) incrementally checkpoint vectors for light-weight and fine-grained fault tolerance without blocking computation, 3) improve the consistency among parameter copies via periodic centralized synchronization and parameter-replicas rotation. As a result of these co-designs, communication and fault tolerance costs are linear to both matrix dimension and number of machines in the network, as opposed to being quadratic in existing systems. And the improved parameter consistency accelerates algorithmic convergence. Empirically, we show our system outperforms several existing baseline systems on training several representative large-scale ML models.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {1–13},
numpages = {13},
keywords = {matrix-parameterized models, sufficient factor broadcasting, system algorithm co-design, Distributed machine learning},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@inproceedings{10.1145/3394171.3413540,
author = {Semedo, David and Magalh\~{a}es, Jo\~{a}o},
title = {Adaptive Temporal Triplet-Loss for Cross-Modal Embedding Learning},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413540},
doi = {10.1145/3394171.3413540},
abstract = {There are many domains where the temporal dimension is critical to unveil how different modalities, such as images and texts, are correlated. Notably, in the social media domain, information is constantly evolving over time according to the events that take place in the real world. In this work, we seek for highly expressive loss functions that allow the encoding of data temporal traits into cross-modal embedding spaces. To achieve this goal, we propose to steer the learning procedure of such embedding through a set of adaptively enforced temporal constraints. In particular, we propose a new formulation of the triplet loss function, where the traditional static margin is superseded by a novel temporally adaptive maximum margin function. This novel redesign of the static margin formulation, allows the embedding to effectively capture not only the semantic correlations across data modalities, but also data's fine-grained temporal correlations. Our experiments confirm the effectiveness of our model in structuring different modalities, while organizing data according to temporal correlations. Moreover, we experimentally highlight how can these embeddings be used for multimedia understanding.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1152–1161},
numpages = {10},
keywords = {multimedia-understanding, temporal cross-modal embeddings, cross-modal embeddings, adaptive temporal triplet-loss},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3297001.3297008,
author = {Sinha, Manjira and Guha, Satarupa and Varma, Preethy and Mukherjee, Tridib and Mannarswamy, Sandya},
title = {My City, My Voice: Listening to the Citizen Views from Web Sources},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297008},
doi = {10.1145/3297001.3297008},
abstract = {To facilitate an environment of inclusive urban management, civic agencies need to listen to the voices of citizens on web sources such as social media, online blogs, public forums and so on. Owing to the vastness and noisy nature of online data, it is challenging, yet important to mine actionable issues related to a city as faced by the citizens firsthand, so that timely measures can be taken by the administration to remedy them. In this work, we filter, analyze, and model web data on urban civic issues of a city, with respect to three modalities - semantics, spatial and temporal. We have come up with a novel approach that captures the contexts through dense distributed word embedding as well as identifies the latent issues through a generative model. Due to the scarcity of geo-tagged posts and delayed reporting, we rely primarily on the textual content of the data for location mining and temporal resolution. We present a first-of-a-kind unified system named CUrb that introduces a novel pipeline to construct long term topology of issues across three dimensions, aggregated over a variety of documents. Through extensive experimentation, we demonstrate the efficacy of our system both qualitatively and quantitatively. It achieves improvement upto 24% compared to the state-of-the-art technique.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {52–60},
numpages = {9},
keywords = {Urban Informatics, Natural Language Processing, Issue Identification, Web Data Mining},
location = {Kolkata, India},
series = {CoDS-COMAD '19}
}

@article{10.1145/3372118,
author = {Chen, Lei and Wu, Zhiang and Cao, Jie and Zhu, Guixiang and Ge, Yong},
title = {Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372118},
doi = {10.1145/3372118},
abstract = {As an e-commerce feature, the personalized recommendation is invariably highly-valued by both consumers and merchants. The e-tourism has become one of the hottest industries with the adoption of recommendation systems. Several lines of evidence have confirmed the travel-product recommendation is quite different from traditional recommendations. Travel products are usually browsed and purchased relatively infrequently compared with other traditional products (e.g., books and food), which gives rise to the extreme sparsity of travel data. Meanwhile, the choice of a suitable travel product is affected by an army of factors such as departure, destination, and financial and time budgets. To address these challenging problems, in this article, we propose a Probabilistic Matrix Factorization with Multi-Auxiliary Information (PMF-MAI) model in the context of the travel-product recommendation. In particular, PMF-MAI is able to fuse the probabilistic matrix factorization on the user-item interaction matrix with the linear regression on a suite of features constructed by the multiple auxiliary information. In order to fit the sparse data, PMF-MAI is built by a whole-data based learning approach that utilizes unobserved data to increase the coupling between probabilistic matrix factorization and linear regression. Extensive experiments are conducted on a real-world dataset provided by a large tourism e-commerce company. PMF-MAI shows an overwhelming superiority over all competitive baselines on the recommendation performance. Also, the importance of features is examined to reveal the crucial auxiliary information having a great impact on the adoption of travel products.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {22},
numpages = {24},
keywords = {recommender systems, linear regression, multiple auxiliary information, probabilistic matrix factorization, Travel product recommendation}
}

@inproceedings{10.1145/3539637.3556995,
author = {Gon\c{c}alves, Rodrigo and Dorneles, Carina Friedrich},
title = {Context Injection in Expert Finding},
year = {2022},
isbn = {9781450394093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539637.3556995},
doi = {10.1145/3539637.3556995},
abstract = {Expert finding is a subject of research in information retrieval and, often, is taken to mean expertise retrieval within a specific organization. The task involves finding an expert on a given topic of interest. Even though there are several proposals in the literature, they do not consider the context in which the given expertise is bound. This paper introduces an approach to inject context into existing expertise evidence based on data extracted from the evidence. Our motivation is to provide context when describing the expertise associated with a candidate expert, allowing a user to understand the results better and choose the best candidate for the task.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {168–177},
numpages = {10},
keywords = {context, expertise retrieval, Expert finding, data retrieval.},
location = {Curitiba, Brazil},
series = {WebMedia '22}
}

@article{10.1109/TASLP.2020.3001390,
author = {Fei, Hao and Ji, Donghong and Zhang, Yue and Ren, Yafeng},
title = {Topic-Enhanced Capsule Network for Multi-Label Emotion Classification},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3001390},
doi = {10.1109/TASLP.2020.3001390},
abstract = {Identifying multiple emotions in a piece of text is an important research topic in the NLP community.Existing methods usually model the task as a multi-label classification problem, while these work has two issues. First, these methods fail to leverage the topic information of the text, which has been shown to be effective for sentiment analysis task. Second, different parts of the text can contribute differently to predicting different emotion labels, so the proposed model needs to capture effective features for each corresponding emotion, which is not considered by existing models. To tackle these problems, we propose a topic-enhanced capsule network, which contains two main parts: a variational autoencoder and a capsule module, for multi-label emotion detection task. Specifically, the variational autoencoder can learn the latent topic information of the text, and the capsule module can capture rich features for corresponding emotion. Experimental results on two benchmark datasets show that the proposed model achieves the current best performance, outperforming previous methods and strong baselines by a large margin.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {1839–1848},
numpages = {10}
}

@inproceedings{10.1145/2998181.2998269,
author = {Ma, Xiao and Hancock, Jeffrey T. and Lim Mingjie, Kenneth and Naaman, Mor},
title = {Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998269},
doi = {10.1145/2998181.2998269},
abstract = {Online peer-to-peer platforms like Airbnb allow hosts to list a property (e.g. a house, or a room) for short-term rentals. In this work, we examine how hosts describe themselves on their Airbnb profile pages. We use a mixed-methods study to develop a categorization of the topics that hosts self-disclose in their profile descriptions, and show that these topics differ depending on the type of guest engagement expected. We also examine the perceived trustworthiness of profiles using topic-coded profiles from 1,200 hosts, showing that longer self-descriptions are perceived to be more trustworthy. Further, we show that there are common strategies (a mix of topics) hosts use in self-disclosure, and that these strategies cause differences in perceived trustworthiness scores. Finally, we show that the perceived trustworthiness score is a significant predictor of host choice--especially for shorter profiles that show more variation. The results are consistent with uncertainty reduction theory, reflect on the assertions of signaling theory, and have important design implications for sharing economy platforms, especially those facilitating online-to-offline social exchange.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {2397–2409},
numpages = {13},
keywords = {airbnb, self-disclosure, trustworthiness, social exchange, sharing economy},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3368089.3409760,
author = {Lou, Yiling and Chen, Zhenpeng and Cao, Yanbin and Hao, Dan and Zhang, Lu},
title = {Understanding Build Issue Resolution in Practice: Symptoms and Fix Patterns},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409760},
doi = {10.1145/3368089.3409760},
abstract = {Build systems are essential for modern software maintenance and development, while build failures occur frequently across software systems, inducing non-negligible costs in development activities. Build failure resolution is a challenging problem and multiple studies have demonstrated that developers spend non-trivial time in resolving encountered build failures; to relieve manual efforts, automated resolution techniques are emerging recently, which are promising but still limitedly effective. Understanding how build failures are resolved in practice can provide guidelines for both developers and researchers on build issue resolution. Therefore, this work presents a comprehensive study of fix patterns in practical build failures. Specifically, we study 1,080 build issues of three popular build systems Maven, Ant, and Gradle from Stack Overflow, construct a fine-granularity taxonomy of 50 categories regarding to the failure symptoms, and summarize the fix patterns for different failure types. Our key findings reveal that build issues stretch over a wide spectrum of symptoms; 67.96% of the build issues are fixed by modifying the build script code related to plugins and dependencies; and there are 20 symptom categories, more than half of whose build issues can be fixed by specific patterns. Furthermore, we also address the challenges in applying non-intuitive or simplistic fix patterns for developers.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {617–628},
numpages = {12},
keywords = {Empirical study, Build failure resolution, Build systems},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3474124.3474194,
author = {Singh, Neetu and Kumar Singh, Sandeep},
title = {MABTriage: Multi Armed Bandit Triaging Model Approach},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474194},
doi = {10.1145/3474124.3474194},
abstract = {Recommendation of bugs to appropriate developers about whom we have very less or no information is a challenging problem faced in many open source developers community. In most of the reported works, this bug-triaging problem is handled through popular machine learning algorithms. However, in the absence of sufficient information of either a developer or a bug, it is difficult to build, train and test a conventional machine-learning model. One of the possible solutions in such a scenario is a reinforcement-learning model. In this paper, we propose an approach called MABTriage, to help a triager assign bugs to developers under uncertainty. To the best of our knowledge, it is the first work that has formulated bug-triaging process as a MAB problem. Experiments conducted on five publicly available open source datasets have shown that MABTriage approach performed better than a random selection. We have also evaluated the performance of six MAB algorithms -Greedy, -Decay, Softmax, Thompson Sampling, Optimistic Agent and UCB based on cumulative rewards. Results have shown that all five performed well in comparison to random selection.},
booktitle = {2021 Thirteenth International Conference on Contemporary Computing (IC3-2021)},
pages = {457–460},
numpages = {4},
keywords = {Rewards, Multi Armed Bandit, Bug triaging, Random Agent},
location = {Noida, India},
series = {IC3 '21}
}

@article{10.1145/3511712,
author = {Choudhary, Nurendra and Aggarwal, Charu C. and Subbian, Karthik and Reddy, Chandan K.},
title = {Self-Supervised Short-Text Modeling through Auxiliary Context Generation},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3511712},
doi = {10.1145/3511712},
abstract = {Short text is ambiguous and often relies predominantly on the domain and context at hand in order to attain semantic relevance. Existing classification models perform poorly on short text due to data sparsity and inadequate context. Auxiliary context, which can often provide sufficient background regarding the domain, is typically available in several application scenarios. While some of the existing works aim to leverage real-world knowledge to enhance short-text representations, they fail to place appropriate emphasis on the auxiliary context. Such models do not harness the full potential of the available context in auxiliary sources. To address this challenge, we reformulate short-text classification as a dual channel self-supervised learning problem (that leverages auxiliary context) with a generation network and a corresponding prediction model. We propose a self-supervised framework, Pseudo-Auxiliary Context generation network for Short-text Modeling (PACS), to comprehensively leverage auxiliary context and it is jointly learned with a prediction network in an end-to-end manner. Our PACS model consists of two sub-networks: a Context Generation Network (CGN) that models the auxiliary context’s distribution and a Prediction Network (PN) to map the short-text features and auxiliary context distribution to the final class label. Our experimental results on diverse datasets demonstrate that PACS outperforms formidable state-of-the-art baselines. We also demonstrate the performance of our model on cold-start scenarios (where contextual information is non-existent) during prediction. Furthermore, we perform interpretability and ablation studies to analyze various representational features captured by our model and the individual contribution of its modules to the overall performance of PACS, respectively.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {51},
numpages = {21},
keywords = {Self-attention, context learning, self-supervision, short-text classification}
}

@inproceedings{10.1145/3209889.3209894,
author = {Alkowaileet, Wail and Alsubaiee, Sattam and Carey, Michael J. and Li, Chen and Ramampiaro, Heri and Sinthong, Phanwadee and Wang, Xikui},
title = {End-to-End Machine Learning with Apache AsterixDB},
year = {2018},
isbn = {9781450358286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209889.3209894},
doi = {10.1145/3209889.3209894},
abstract = {Recent developments in machine learning and data science provide a foundation for extracting underlying information from Big Data. Unfortunately, current platforms and tools often require data scientists to glue together and maintain custom-built platforms consisting of multiple Big Data component technologies. In this paper, we explain how Apache AsterixDB, an open source Big Data Management System, can help to reduce the burden involved in using machine learning algorithms in Big Data analytics. In particular, we describe how AsterixDB's built-in support for user-defined functions (UDFs), the availability of UDFs in data ingestion pipelines and queries, and the provision of machine learning platform and notebook inter-operation capabilities can together enable data analysts to more easily create and manage end-to-end analytical dataflows.},
booktitle = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},
articleno = {6},
numpages = {10},
location = {Houston, TX, USA},
series = {DEEM'18}
}

@inproceedings{10.1145/3209542.3209578,
author = {Lu, Yihan and Hsiao, I-Han},
title = {Modeling Semantics between Programming Codes and Annotations},
year = {2018},
isbn = {9781450354271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209542.3209578},
doi = {10.1145/3209542.3209578},
abstract = {It is a common practice for programmers to leave annotations during program development. Most of the annotated documentations are predominantly being used as the archive of the coding events for limited developers. We hypothesize that these annotations captured mass amount of valuable information which can be utilized to identify similar codes or to examine code quality. However, due to the annotating behaviors vary and the language composition can be complex, this work sets out to investigate a systematic method to examine the annotation semantics and their relations with codes. We designed a semantic parser to extract concepts from codes and the corresponding annotations. Additionally, text mining techniques are applied to summarize linguistic features from the annotations. We then build models to predict concepts in programming code annotations. Results show that the proposed semantic modeling method achieved a higher performance compared to a random guessed baseline.},
booktitle = {Proceedings of the 29th on Hypertext and Social Media},
pages = {101–105},
numpages = {5},
keywords = {coding concept detection, programming semantics, semantic modeling, text based classification},
location = {Baltimore, MD, USA},
series = {HT '18}
}

@inproceedings{10.1145/3292522.3326026,
author = {Paul, Indraneil and Khattar, Abhinav and Chopra, Shaan and Kumaraguru, Ponnurangam and Gupta, Manish},
title = {What Sets Verified Users Apart? Insights, Analysis and Prediction of Verified Users on Twitter},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326026},
doi = {10.1145/3292522.3326026},
abstract = {Social network and publishing platforms, such as Twitter, support the concept of a secret proprietary verification process, for handles they deem worthy of platform-wide public interest. In line with significant prior work which suggests that possessing such a status symbolizes enhanced credibility in the eyes of the platform audience, a verified badge is clearly coveted among public figures and brands. What are less obvious are the inner workings of the verification process and what being verified represents. This lack of clarity, coupled with the flak that Twitter received by extending aforementioned status to political extremists in 2017, backed Twitter into publicly admitting that the process and what the status represented needed to be rethought. With this in mind, we seek to unravel the aspects of a user's profile which likely engender or preclude verification. The aim of the paper is two-fold: First, we test if discerning the verification status of a handle from profile metadata and content features is feasible. Second, we unravel the features which have the greatest bearing on a handle's verification status. We collected a dataset consisting of profile metadata of all 231,235 verified English-speaking users (as of July 2018), a control sample of 175,930 non-verified English-speaking users and all their 494 million tweets over a one year collection period. Our proposed models are able to reliably identify verification status (Area under curve AUC &gt; 99%). We show that number of public list memberships, presence of neutral sentiment in tweets and an authoritative language style are the most pertinent predictors of verification status. To the best of our knowledge, this work represents the first attempt at discerning and classifying verification worthy users on Twitter.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {215–224},
numpages = {10},
keywords = {social influence, twitter, verified users},
location = {Boston, Massachusetts, USA},
series = {WebSci '19}
}

@inproceedings{10.1145/3335595.3335648,
author = {G\'{o}mez-L\'{o}pez, Pedro and Simarro, Francisco Montero and Bonal, Mar\'{\i}a Teresa L\'{o}pez},
title = {Analysing the UX Scope through Its Definitions},
year = {2019},
isbn = {9781450371766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335595.3335648},
doi = {10.1145/3335595.3335648},
abstract = {The term user experience (UX), even today, is a concept surrounded by ambiguity in its definition, which makes it difficult to be fully understood. There is a wide variety of interpretations around the UX concept, and although there have been attempts to develop a unified view of UX, there is still no common understanding of the nature and scope for the term "user experience". Benefits of a shared vision of the concept fall mainly on the relationship between research and industry. The effectiveness and efficiency of the UX study and learning have a direct impact on the relationship of consumers with the products and services available in the market. The challenge addressed in this article is to analyse the different definitions and interpretations of UX with the aim of deriving common knowledge about the meaning and scope of the term, through machine learning (ML) and natural language processing (NLP).},
booktitle = {Proceedings of the XX International Conference on Human Computer Interaction},
articleno = {20},
numpages = {4},
keywords = {statistical and predictive models, User experience, characterisation, unsupervised machine learning algorithms, standards},
location = {Donostia, Gipuzkoa, Spain},
series = {Interacci\'{o}n '19}
}

@inproceedings{10.1145/3544795.3544844,
author = {Cataldi, Mario and Di Caro, Luigi and Schifanella, Claudio},
title = {SIDEWAYS-2022 @ HT-2022: 7th International Workshop on Social Media World Sensors},
year = {2022},
isbn = {9781450394253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544795.3544844},
doi = {10.1145/3544795.3544844},
abstract = {This seventh edition of the workshop aims at bringing together academics and practitioners from different areas to promote the vision of social media as social sensors. Nowadays, Social media platforms represent freely-accessible information networks allowing registered (and unregistered) users to read, share and broadcast messages referring to a potentially-unlimited range of arguments, by also exploiting the immediateness of handy smart devices. This long-running workshop aims at focusing the attention on a particular perspective of these powerful communication channels, which is that of social sensors, where each user reacts in real time to the underlying reality by providing some own interpretation. Technologies and AI artifacts may support automatic or semi-automatic applications for information detection and integration, offering sideways to the existing authoritative information media and the information reported by the surrounding community.},
booktitle = {Proceedings of the 7th International Workshop on Social Media World Sensors},
articleno = {1},
numpages = {4},
keywords = {data mining, topic detection, sensors, social media},
location = {Barcelona, Spain},
series = {Sideways '22}
}

@inproceedings{10.1145/3194206.3194240,
author = {Huang, Yanming and Jiang, Y. and Hasan, Touhidul and Jiang, Q. and Li, Chao},
title = {A Topic BiLSTM Model for Sentiment Classification},
year = {2018},
isbn = {9781450363457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194206.3194240},
doi = {10.1145/3194206.3194240},
abstract = {The Long Short Term Memory (LSTM) network is very effective for capturing sequence information which can help to analyze sentiments. However, it fails to capture the meaning of polysemous word under different contexts. In this paper, we propose topic information-based bidirectional LSTM (BiLSTM) model for sentiment classification. BiLSTM model learns topic information to obtain the sensitive representation of the polysemous word under given circumstance. The topic information is generated through a topic modeling via Latent Dirichlet Allocation (LDA). The topic information-based BiLSTM network allows the model to capture the meaning of the polysemous word and long sequence information automatically. The experimental results on real-world datasets demonstrate that the proposed method outperforms the task of benchmark sentiment classification on SemEval 2013 and IMDB.},
booktitle = {Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence},
pages = {143–147},
numpages = {5},
keywords = {LDA, bidirectional LSTM, sentiment classification},
location = {Shanghai, China},
series = {ICIAI '18}
}

@article{10.1145/3352683.3352688,
author = {Du, Xu and Kowalski, Matthew and Varde, Aparna S. and de Melo, Gerard and Taylor, Robert W.},
title = {Public Opinion Matters: Mining Social Media Text for Environmental Management},
year = {2020},
issue_date = {Autumn 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {Autumn},
issn = {1931-1745},
url = {https://doi.org/10.1145/3352683.3352688},
doi = {10.1145/3352683.3352688},
abstract = {Social media mining has proven useful in multiple research fields as a tool for public opinion extraction and analysis. Such mining can discover knowledge from unstructured data in booming social media sources that provide instant public responses and also capture long-term data. Environmental scientists have realized its potential and conducted various studies where public opinion matters. We focus our discussion in this article on mining social media text on environmental issues, with particular emphasis on sentiment analysis, fitting the theme of Data Science and Sustainability. The data science community today is interested in topics that overlap with environmental issues and their broader impacts on sustainability. Such work appeals to scientists focusing on areas such as smart cities, climate change and geo-informatics. Future issues emerging from this research include domain-specific multilingual mining, and advanced geo-location tagging with demographically focused sentiment analysis.},
journal = {SIGWEB Newsl.},
month = {feb},
articleno = {5},
numpages = {15}
}

@inproceedings{10.1145/3234944.3234957,
author = {Gollub, Tim and Genc, Erdan and Lipka, Nedim and Stein, Benno},
title = {Pseudo Descriptions for Meta-Data Retrieval},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234957},
doi = {10.1145/3234944.3234957},
abstract = {Search in meta-data is challenging due to the sparsity of the available textual information. To alleviate the sparsity problem, the paper in hand evolves from the existing document expansion paradigm and proposes pseudo-descriptions as a new paradigm. Instead of encoding paradigmatic term relations implicitly in an expansion vector, we generate an explicit cohesive text field for meta-data records that describes the entity associated with the record. In contrast to document expansions, pseudo-descriptions allow to reveal why a certain document is considered relevant although the original meta-data does not contain the query terms. Moreover, they are easier to operationalize and facilitate the use of sophisticated retrieval features such as phrase search and query term proximity. To generate pseudo-descriptions, we propose a relevance dependent strategy that depends on the search engine result pages obtained from issuing the meta-data as a search query to a designated reference search engine. To demonstrate the validity of the pseudo-description paradigm, we experiment with different TREC collections where we withhold the content information to simulate a meta-data retrieval scenario. Though retrieval with full content information remains superior, our approach achieves retrieval performance improvements en par with document expansion.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {139–146},
numpages = {8},
keywords = {meta-data retrieval, headroom problem, document expansion, vocabulary mismatch, trec experiments, pseudo descriptions},
location = {Tianjin, China},
series = {ICTIR '18}
}

@inproceedings{10.1145/3341161.3343687,
author = {Gasparini, Mattia and Ramponi, Giorgia and Brambilla, Marco and Ceri, Stefano},
title = {Assigning Users to Domains of Interest Based on Content and Network Similarity with Champion Instances},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343687},
doi = {10.1145/3341161.3343687},
abstract = {In this paper, we propose two approaches to the problem of finding similar users to a set of champions representing domains of interest on social media. The first approach is based on the content shared by the users, while the second one relies on the social network connections (following, followers, and mentions). Given a small set of champion accounts, we construct a centroid and we rank candidates by computing their distance from the centroid. Experiments show that social network features provide better performance, but they are computationally much more intensive. This approach can be used for providing highly reliable recommendations of the top-k instances which are most similar to a given target, specified through examples rather than through specific properties.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {589–592},
numpages = {4},
keywords = {top-k recommendation, social network, representation learning on graphs, NLP},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3397271.3401143,
author = {Wang, Zizhen and Fan, Yixing and Guo, Jiafeng and Yang, Liu and Zhang, Ruqing and Lan, Yanyan and Cheng, Xueqi and Jiang, Hui and Wang, Xiaozhao},
title = {Match²: A Matching over Matching Model for Similar Question Identification},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401143},
doi = {10.1145/3397271.3401143},
abstract = {Community Question Answering (CQA) has become a primary means for people to acquire knowledge, where people are free to ask questions or submit answers. To enhance the efficiency of the service, similar question identification becomes a core task in CQA which aims to find a similar question from the archived repository whenever a new question is asked. However, it has long been a challenge to properly measure the similarity between two questions due to the inherent variation of natural language, i.e., there could be different ways to ask a same question or different questions sharing similar expressions. To alleviate this problem, it is natural to involve the existing answers for the enrichment of the archived questions. Traditional methods typically take aone-side usage, which leverages the answer as some expanded representation of the corresponding question. Unfortunately, this may introduce unexpected noises into the similarity computation since answers are often long and diverse, leading to inferior performance. In this work, we propose atwo-side usage, which leverages the answer as a bridge of the two questions. The key idea is based on our observation that similar questions could be addressed by similar parts of the answer while different questions may not. In other words, we can compare the matching patterns of the two questions over the same answer to measure their similarity. In this way, we propose a novel matching over matching model, namely Match2, which compares the matching patterns between two question-answer pairs for similar question identification. Empirical experiments on two benchmark datasets demonstrate that our model can significantly outperform previous state-of-the-art methods on the similar question identification task.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {559–568},
numpages = {10},
keywords = {community question answering, matching over matching, similar question identification},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3041021.3055130,
author = {Lian, Jianxun and Zhang, Fuzheng and Xie, Xing and Sun, Guangzhong},
title = {Restaurant Survival Analysis with Heterogeneous Information},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055130},
doi = {10.1145/3041021.3055130},
abstract = {For shopkeepers, one of their biggest common concerns is whether their business will thrive or fail in the future. With the development of new ways to collect business data, it is possible to leverage multiple domains' knowledge to build an intelligent model for business assessment. In this paper, we discuss what the potential indicators are for the long-term survival of a physical store. To this end, we study factors from four pillars: geography, user mobility, user rating, and review text. We start by exploring the impact of geographic features, which describe the location environment of the retailer store. The location and nearby places play an important role in the popularity of the shop, and usually less competitiveness and more heterogeneity is better. Then we study user mobility. It can be viewed as supplementary to the geographical placement, showing how the location can attract users from anywhere. Another important factor is how the shop can serve and satisfy users. We find that restaurant survival prediction is a hard task that can not be solved simply using consumers' ratings or sentiment metrics. Compared with conclusive and well-formatted ratings, the various review words provide more insight of the shop and deserve in-depth mining. We adopt several language models to fully explore the textual message. Comprehensive experiments demonstrate that review text indeed have the strongest predictive power. We further compare different cities' models and find the conclusions are highly consistent. Although we focus on the class of restaurant in this paper, the method can be easily extended to other shop categories.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {993–1002},
numpages = {10},
keywords = {location-based services, restaurant survival analysis, data mining},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3512576.3512617,
author = {Lawas, Leodivino and Dalino Gorro, Ken and Ranolo, Elmo and Ilano, Anthony},
title = {Exploring and Analyzing Facebook as Crowdsourcing Platform for Traffic Updates Using Selenium Support Vector Machine and Non-Parametric LDA},
year = {2021},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512617},
doi = {10.1145/3512576.3512617},
abstract = {Traffic is a major problem in the Philippines. Facebook is one of the social media platforms that is commonly used by Filipinos. Machine learning is a field of computer science that allows computers to perform tasks like human beings. In this study, the proponents explored Facebook as a source of traffic updates and as a source of traffic information. In this paper, as a partial result, a machine learning model was created to classify Facebook posts as related to traffic. To gather Facebook posts, a total of 1000 respondents were asked for consent to scrape their public post using the username link and selenium. The Support vector machine model was trained with 3000 Facebook posts. The SVM model was only trained to 3 classes {Road accident, Road activities and Other}. The SVM model was evaluated using 10-cross fold validation. The result shows that the accuracy is 76% and the recall is 69%. To analyze the narrative of the corpus, the Hierarchical Dirichlet Process model was created with the log-likelihood of -4.06 with 10 topic models. The following are the narratives of the corpus: {Traffic Management, Immediate Emergency Response, Seeking help, Busses causes majority of accidents.}},
booktitle = {2021 The 9th International Conference on Information Technology: IoT and Smart City},
pages = {226–230},
numpages = {5},
keywords = {Social Media, Facebook, LDA, HDP},
location = {Guangzhou, China},
series = {ICIT 2021}
}

@inproceedings{10.1145/3341161.3343532,
author = {Rashed, Mohammed and Piorkowski, John and McCulloh, Ian},
title = {Evaluation of Extremist Cohesion in a Darknet Forum Using ERGM and LDA},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343532},
doi = {10.1145/3341161.3343532},
abstract = {ISIS and similar extremist communities are increasingly using forums in the darknet to connect with each other and spread news and propaganda. In this paper, we attempt to understand their network in an online forum by using descriptive statistics, an exponential random graph model (ERGM) and Topic Modeling. Our analysis shows how the cohesion between active members forms and grows over time and under certain thread topics. We find that the top attendants of the forum have high centrality measures and other attributes of influencers.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {899–902},
numpages = {4},
keywords = {darknet forums, exponential random graph modeling, network analysis, topic modeling},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3121050.3121063,
author = {Nguyen, Gia-Hung and Soulier, Laure and Tamine, Lynda and Bricon-Souf, Nathalie},
title = {DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge Resource Driven Representation of Documents},
year = {2017},
isbn = {9781450344906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121050.3121063},
doi = {10.1145/3121050.3121063},
abstract = {The state-of-the-art solutions to the vocabulary mismatch in information retrieval (IR) mainly aim at leveraging either the relational semantics provided by external resources or the distributional semantics, recently investigated by deep neural approaches. Guided by the intuition that the relational semantics might improve the effectiveness of deep neural approaches, we propose the Deep Semantic Resource Inference Model (DSRIM) that relies on: 1) a representation of raw-data that models the relational semantics of text by jointly considering objects and relations expressed in a knowledge resource, and 2) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries. The experimental evaluation carried out on two TREC datasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet and MeSH resources, indicates that our model outperforms state-of-the-art semantic and deep neural IR models.},
booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {19–26},
numpages = {8},
keywords = {knowledge resource, deep neural architecture, semantic document representation, ad-hoc ir},
location = {Amsterdam, The Netherlands},
series = {ICTIR '17}
}

@article{10.14778/3415478.3415559,
author = {Suri, Sahaana and Chanda, Raghuveer and Bulut, Neslihan and Narayana, Pradyumna and Zeng, Yemao and Bailis, Peter and Basu, Sugato and Narlikar, Girija and R\'{e}, Christopher and Sethi, Abishek},
title = {Leveraging Organizational Resources to Adapt Models to New Data Modalities},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415559},
doi = {10.14778/3415478.3415559},
abstract = {As applications in large organizations evolve, the machine learning (ML) models that power them must adapt the same predictive tasks to newly arising data modalities (e.g., a new video content launch in a social media application requires existing text or image models to extend to video). To solve this problem, organizations typically create ML pipelines from scratch. However, this fails to utilize the domain expertise and data they have cultivated from developing tasks for existing modalities. We demonstrate how organizational resources, in the form of aggregate statistics, knowledge bases, and existing services that operate over related tasks, enable teams to construct a common feature space that connects new and existing data modalities. This allows teams to apply methods for data curation (e.g., weak supervision and label propagation) and model training (e.g., forms of multi-modal learning) across these different data modalities. We study how this use of organizational resources composes at production scale in over 5 classification tasks at Google, and demonstrate how it reduces the time needed to develop models for new modalities from months to weeks or days.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3396–3410},
numpages = {15}
}

@inproceedings{10.1145/3402942.3409599,
author = {Freiknecht, Jonas and Effelsberg, Wolfgang},
title = {Procedural Generation of Interactive Stories Using Language Models},
year = {2020},
isbn = {9781450388078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402942.3409599},
doi = {10.1145/3402942.3409599},
abstract = {In this paper we introduce an architecture, an implementation and an evaluation of a system for the automatic creation of interactive stories for games. Our goal is to algorithmically create a branched story for the entire game; in each game run a different variant is generated. The architecture uses natural language processing (NLP) to generate meaningful stories. For NLP we use a statistical language model based on a neural network (Generative Pretrained Transformer, GPT-2). The basic architecture generates stories with too many characters which tend to get incoherent for longer texts, so we add a component restricting the number of persons and improving the consistency. The system is initialized with a hand-written game introduction that defines the main characters and the inventory; it also sets the goals for the game. From that text the remainder of the game story is generated algorithmically. We have fully implemented our system, and we report initial, encouraging experimental results.},
booktitle = {International Conference on the Foundations of Digital Games},
articleno = {97},
numpages = {8},
keywords = {interactive stories, language models, natural language processing, procedural content generation},
location = {Bugibba, Malta},
series = {FDG '20}
}

@inproceedings{10.1109/ICSSP.2019.00018,
author = {Link, Daniel and Behnamghader, Pooyan and Moazeni, Ramin and Boehm, Barry},
title = {Recover and RELAX: Concern-Oriented Software Architecture Recovery for Systems Development and Maintenance},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00018},
doi = {10.1109/ICSSP.2019.00018},
abstract = {The stakeholders of a system are legitimately interested in whether and how its architecture reflects their respective concerns at each point of its development and maintenance processes. Having such knowledge available at all times would enable them to continually adjust their systems structure at each juncture and reduce the buildup of technical debt that can be hard to reduce once it has persisted over many iterations. Unfortunately, software systems often lack reliable and current documentation about their architecture. In order to remedy this situation, researchers have conceived a number of architectural recovery methods, some of them concern-oriented. However, the design choices forming the bases of most existing recovery methods make it so none of them have a complete set of desirable qualities for the purpose stated above. Tailoring a recovery to a system is either not possible or only through iterative experiments with numeric parameters. Furthermore, limitations in the scalability of the employed recovery algorithms make it prohibitive to apply the existing techniques to large systems. Finally, since several current recovery methods employ non-deterministic sampling, their inconsistent results do not lend themselves well to tracking a systems course over several versions, as needed by its stakeholders.RELAX (RELiable Architecture EXtraction), a new concern-based recovery method that uses text classification, addresses these issues efficiently (1) by assembling the overall recovery result from smaller, independent parts, (2) basing it on an algorithm with linear time complexity and (3) being tailorable to the recovery of a single system or a sequence thereof through the selection of meaningfully named, semantic topics. An intuitive and informative architectural visualization rounds out RELAX's contributions. RELAX is illustrated on a number of existing open-source systems and compared to other recovery methods.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {64–73},
numpages = {10},
keywords = {software architecture, open source software, software maintenance, architectural change, software development management, software evolution, architecture recovery},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@article{10.1145/3450946,
author = {Gupta, Amulya and Zhang, Zhu},
title = {Vector-Quantization-Based Topic Modeling},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3450946},
doi = {10.1145/3450946},
abstract = {With the purpose of learning and utilizing explicit and dense topic embeddings, we propose three variations of novel vector-quantization-based topic models (VQ-TMs): (1) Hard VQ-TM, (2) Soft VQ-TM, and (3) Multi-View Soft VQ-TM. The model family capitalize on vector quantization techniques, embedded input documents, and viewing words as mixtures of topics. Guided by a comprehensive set of evaluation metrics, we conduct systematic quantitative and qualitative empirical studies, and demonstrate the superior performance of VQ-TMs compared to important baseline models. Through a unique case study on code generation from natural language descriptions, we further illustrate the power of VQ-TMs in downstream tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {may},
articleno = {34},
numpages = {30},
keywords = {Knowledge discovery, self-supervised learning, deep learning}
}

@article{10.1145/3154526,
author = {Ding, Zhijun and Li, Xiaolun and Jiang, Changjun and Zhou, Mengchu},
title = {Objectives and State-of-the-Art of Location-Based Social Network Recommender Systems},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154526},
doi = {10.1145/3154526},
abstract = {Because of the widespread adoption of GPS-enabled devices, such as smartphones and GPS navigation devices, more and more location information is being collected and available. Compared with traditional ones (e.g., Amazon, Taobao, and Dangdang), recommender systems built on location-based social networks (LBSNs) have received much attention. The former mine users’ preferences through the relationship between users and items, e.g., online commodity, movies and music. The latter add location information as a new dimension to the former, hence resulting in a three-dimensional relationship among users, locations, and activities. In this article, we summarize LBSN recommender systems from the perspective of such a relationship. User, activity, and location are called objects, and recommender objectives are formed and achieved by mining and using such 3D relationships. From the perspective of the 3D relationship among these objects, we summarize the state-of-the-art of LBSN recommender systems to fulfill the related objectives. We finally indicate some future research directions in this area.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {18},
numpages = {28},
keywords = {Location-based social networks, recommender objectives}
}

@article{10.1145/3353401.3353406,
author = {Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel},
title = {Recognizing Experts on Social Media: A Heuristics-Based Approach},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/3353401.3353406},
doi = {10.1145/3353401.3353406},
abstract = {Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.},
journal = {SIGMIS Database},
month = {jul},
pages = {66–84},
numpages = {19},
keywords = {expertise location, social media, data analytics.}
}

@article{10.1145/3151957,
author = {Wang, Pengwei and Ji, Lei and Yan, Jun and Dou, Dejing and Silva, Nisansa De and Zhang, Yong and Jin, Lianwen},
title = {Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3151957},
doi = {10.1145/3151957},
abstract = {Question retrieval, which aims to find similar versions of a given question, is playing a pivotal role in various question answering (QA) systems. This task is quite challenging, mainly in regard to five aspects: synonymy, polysemy, word order, question length, and data sparsity. In this article, we propose a unified framework to simultaneously handle these five problems. We use the word combined with corresponding concept information to handle the synonymy problem and the polysemous problem. Concept embedding and word embedding are learned at the same time from both the context-dependent and context-independent views. To handle the word-order problem, we propose a high-level feature-embedded convolutional semantic model to learn question embedding by inputting concept embedding and word embedding. Due to the fact that the lengths of some questions are long, we propose a value-based convolutional attentional method to enhance the proposed high-level feature-embedded convolutional semantic model in learning the key parts of the question and the answer. The proposed high-level feature-embedded convolutional semantic model nicely represents the hierarchical structures of word information and concept information in sentences with their layer-by-layer convolution and pooling. Finally, to resolve data sparsity, we propose using the multi-view learning method to train the attention-based convolutional semantic model on question–answer pairs. To the best of our knowledge, we are the first to propose simultaneously handling the above five problems in question retrieval using one framework. Experiments on three real question-answering datasets show that the proposed framework significantly outperforms the state-of-the-art solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {41},
numpages = {24},
keywords = {value-based convolutional attentional method, question embedding, concept embedding, Question retrieval}
}

@inproceedings{10.1145/3331184.3331251,
author = {Vinh Tran, Lucas and Nguyen Pham, Tuan-Anh and Tay, Yi and Liu, Yiding and Cong, Gao and Li, Xiaoli},
title = {Interact and Decide: Medley of Sub-Attention Networks for Effective Group Recommendation},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331251},
doi = {10.1145/3331184.3331251},
abstract = {This paper proposes Medley of Sub-Attention Networks (MoSAN), a new novel neural architecture for the group recommendation task. Group-level recommendation is known to be a challenging task, in which intricate group dynamics have to be considered. As such, this is to be contrasted with the standard recommendation problem where recommendations are personalized with respect to a single user. Our proposed approach hinges upon the key intuition that the decision making process (in groups) is generally dynamic, i.e., a user's decision is highly dependent on the other group members. All in all, our key motivation manifests in a form of an attentive neural model that captures fine-grained interactions between group members. In our MoSAN model, each sub-attention module is representative of a single member, which models a user's preference with respect to all other group members. Subsequently, a Medley of Sub-Attention modules is then used to collectively make the group's final decision. Overall, our proposed model is both expressive and effective. Via a series of extensive experiments, we show that MoSAN not only achieves state-of-the-art performance but also improves standard baselines by a considerable margin.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {255–264},
numpages = {10},
keywords = {recommender systems, group recommendation, neural attention mechanism, collaborative filtering},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3289600.3290993,
author = {Yang, Longqi and Wang, Yu and Dunne, Drew and Sobolev, Michael and Naaman, Mor and Estrin, Deborah},
title = {More Than Just Words: Modeling Non-Textual Characteristics of Podcasts},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290993},
doi = {10.1145/3289600.3290993},
abstract = {Recent years have witnessed the flourishing of podcasts, a unique type of audio medium. Prior work on podcast content modeling focused on analyzing Automatic Speech Recognition outputs, which ignored vocal, musical, and conversational properties (e.g., energy, humor, and creativity) that uniquely characterize this medium. In this paper, we present an Adversarial Learning-based Podcast Representation (ALPR) that captures non-textual aspects of podcasts. Through extensive experiments on a large-scale podcast dataset (88,728 episodes from 18,433 channels), we show that (1) ALPR significantly outperforms the state-of-the-art features developed for music and speech in predicting theseriousness andenergy of podcasts, and (2) incorporating ALPR significantly improves the performance of topic-based podcast-popularity prediction. Our experiments also reveal factors that correlate with podcast popularity.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {276–284},
numpages = {9},
keywords = {content modeling, podcast, popularity prediction, spoken word},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3077584.3077588,
author = {Turner, Jason and Kantardzic, Mehmed},
title = {Geo-Social Analytics Based on Spatio-Temporal Dynamics of Marijuana-Related Tweets},
year = {2017},
isbn = {9781450348331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077584.3077588},
doi = {10.1145/3077584.3077588},
abstract = {In the United States, marijuana is considered illegal in several states, whereas some states have decriminalized certain amounts and others have legalized marijuana for medical and recreational uses. We have created a novel approach that examines if state policy on marijuana impacts the amount and type of conversations regarding marijuana on Twitter, as well as the social networks of the those who contribute to marijuana conversations. The findings in this research may be useful for research in policy reform, law enforcement, and public health.},
booktitle = {Proceedings of the 2017 International Conference on Information System and Data Mining},
pages = {28–38},
numpages = {11},
keywords = {document classification, policy research, Social network analysis, topic modeling, natural language processing},
location = {Charleston, SC, USA},
series = {ICISDM '17}
}

@inproceedings{10.1145/3339311.3339346,
author = {Kumari, Pooja and Sharma, Shilpa},
title = {Fuzzy Based Medicine Recommendation System: An Example of Thyroid Medicine},
year = {2019},
isbn = {9781450366526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339311.3339346},
doi = {10.1145/3339311.3339346},
abstract = {Many expert system or decision support system or recommendation system is developed to assist medical experts for better result and immediate result. Everyday there is new advancement comes into play with the help of Machine learning algorithms (neural network, Bayesian network) and data mining (clustering, classification) algorithms for effective prediction of disease like cancer, heart disease, diabetes and many more. Now, researchers using machine learning with datamining to automate the medicine recommendation. So, the risk of prescribing incorrect medicine can be decreased as many deaths comes in news are due to wrong medication or patients can have some allergies which medical experts don't know The motive of this work is to draft an Expert system for diagnosis of Thyroid disease as well as to prescribe medicine for the same. The system has 7 inputs and and on basis of these, two outputs are drawn which is disease and medicine The output field refers to the presence of thyroid disease in the patient as well as medicine according to level of disease.},
booktitle = {Proceedings of the Third International Conference on Advanced Informatics for Computing Research},
articleno = {35},
numpages = {7},
keywords = {thyroid disease, computerization, recommendation system, expert system, data mining, machine learning},
location = {Shimla, India},
series = {ICAICR '19}
}

@inproceedings{10.1145/3394486.3403244,
author = {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Zhang, Yunyi and Han, Jiawei},
title = {CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403244},
doi = {10.1145/3394486.3403244},
abstract = {Taxonomy is not only a fundamental form of knowledge representation, but also crucial to vast knowledge-rich applications, such as question answering and web search. Most existing taxonomy construction methods extract hypernym-hyponym entity pairs to organize a "universal" taxonomy. However, these generic taxonomies cannot satisfy user's specific interest in certain areas and relations. Moreover, the nature of instance taxonomy treats each node as a single word, which has low semantic coverage for people to fully understand. In this paper, we propose a method for seed-guided topical taxonomy construction, which takes a corpus and a seed taxonomy described by concept names as input, and constructs a more complete taxonomy based on user's interest, wherein each node is represented by a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill this goal. A relation transferring module learns and transfers the user's interested relation along multiple paths to expand the seed taxonomy structure in width and depth. A concept learning module enriches the semantics of each concept node by jointly embedding the taxonomy and text. Comprehensive experiments conducted on real-world datasets show that CoRel generates high-quality topical taxonomies and outperforms all the baselines significantly.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1928–1936},
numpages = {9},
keywords = {relation extraction, taxonomy construction, semantic computing, topic discovery},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3219819.3220064,
author = {Zhang, Chao and Tao, Fangbo and Chen, Xiusi and Shen, Jiaming and Jiang, Meng and Sadler, Brian and Vanni, Michelle and Han, Jiawei},
title = {TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220064},
doi = {10.1145/3219819.3220064},
abstract = {Taxonomy construction is not only a fundamental task for semantic analysis of text corpora, but also an important step for applications such as information filtering, recommendation, and Web search. Existing pattern-based methods extract hypernym-hyponym term pairs and then organize these pairs into a taxonomy. However, by considering each term as an independent concept node, they overlook the topical proximity and the semantic correlations among terms. In this paper, we propose a method for constructing topic taxonomies, wherein every node represents a conceptual topic and is defined as a cluster of semantically coherent concept terms. Our method, TaxoGen, uses term embeddings and hierarchical clustering to construct a topic taxonomy in a recursive fashion. To ensure the quality of the recursive process, it consists of: (1) an adaptive spherical clustering module for allocating terms to proper levels when splitting a coarse topic into fine-grained ones; (2) a local embedding module for learning term embeddings that maintain strong discriminative power at different levels of the taxonomy. Our experiments on two real datasets demonstrate the effectiveness of TaxoGen compared with baseline methods.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2701–2709},
numpages = {9},
keywords = {taxonomy construction, text mining, word embedding},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3164541.3164636,
author = {Ji, Seonmi and Kim, Yongsung and Jung, Seungwon and Hwang, Eenjun},
title = {An Educational Video Scoring Scheme for Level and Topic-Based Recommendation},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164636},
doi = {10.1145/3164541.3164636},
abstract = {Recently, a huge amount of educational video contents have become available due to diverse IT technologies and rapidly growing demand for online learning. Popular resources for various high quality educational contents include OCW (OpenCourseWare), MOOC (Massive Open Online Course) and TED (Technology, Entertainment, Design). Moreover, learners can easily access such contents via the Internet by using common video sharing platforms such as YouTube and Vimeo. However, one critical problem that many users experience in this environment is that it is difficult and time consuming to find out educational contents that is appropriate to them in terms of topic and difficulty level. To solve this problem, in this paper, we propose an educational contents scoring scheme for topic and difficulty level based contents recommendation. The scoring is based on the linguistic difficulty of the video and the topic similarity. To measure the linguistic difficulty, we apply various readability metrics to the video script. To measure the topic similarity between videos, we extract keywords from each video script and then calculate their cosine similarity. We show the effectiveness of our proposed scheme by various experiments.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {12},
numpages = {5},
keywords = {Level and Topic-based Recommendation, Educational Video, Ranking Score},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@inproceedings{10.1145/3394171.3416294,
author = {Wang, Kai and Wang, Penghui and Chen, Xin and Huang, Qiushi and Mao, Zhendong and Zhang, Yongdong},
title = {A Feature Generalization Framework for Social Media Popularity Prediction},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3416294},
doi = {10.1145/3394171.3416294},
abstract = {Social media is an indispensable part in modern life and social media popularity prediction can be applied to many aspects of sociality. In this paper, we propose a novel combined framework for social media popularity prediction, which accomplishes feature generalization and temporal modeling based on multi-modal feature extraction. On the one hand, in order to address the generalization problem caused by massive missing data, we train two CatBoost models with different datasets and integrate their outputs with a linear combination. On the other hand, sliding window average is employed to mine potential short-term dependency for each user's post sequence. Extensive experiments show that our proposed framework has superiorities in both feature generalization and temporal modeling. Besides, our approach achieves the 1st place on the leader board of the SMP Challenge in 2020, which proves the effectiveness of our proposed framework.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4570–4574},
numpages = {5},
keywords = {social media prediction, multi-modal, regression, CatBoost},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3341215.3356304,
author = {Marchenko, Ekaterina and Musabirov, Ilya},
title = {Media Metrics in Esports: The Case of Dota 2},
year = {2019},
isbn = {9781450368711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341215.3356304},
doi = {10.1145/3341215.3356304},
abstract = {Esports organisations and players rely on revenue model based on spectators attention, which makes player and team brands crucial for their success. In this work, we demonstrate our approach to analyse brand value formation based on computational analysis of spectators' discussions of esports players and teams. We show the early results revealing (1) topic patterns of discussions, (2) the influence of transfers from one team to another on the player's brand perception, and (3) the interconnection between personal and team brands on the particular cases from the ongoing study.},
booktitle = {Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts},
pages = {519–524},
numpages = {6},
keywords = {mediametrics, brand value, esports},
location = {Barcelona, Spain},
series = {CHI PLAY '19 Extended Abstracts}
}

@inproceedings{10.1145/3328833.3328837,
author = {Taj, Soonh and Arain, Qasim and Memon, Imran and Zubedi, Asma},
title = {To Apply Data Mining for Classification of Crowd Sourced Software Requirements},
year = {2019},
isbn = {9781450361057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328833.3328837},
doi = {10.1145/3328833.3328837},
abstract = {Now a day's main focus of developers is to build quality software that works according to customer needs and for this reason it is necessary to gather right requirements as requirement elicitation is the critical step that impacts on the success of software project as misinterpreted requirements leads to the failure of software project. By keeping this in mind a research is carried out on improving requirements elicitation process and automating the process of classifying requirements. In this research, a model is proposed which will help in this scenario for requirements elicitation and requirement classification. This paper presents a model in which crowd sourcing approach is used so that customers, end users, stakeholders, developers and software engineers can make active participation for requirement elicitation process and requirements gathered using crowdsourcing approach are used by model for classification process i.e. classification of requirements into functional and non-functional requirements. For the proof of proposed model a case study is conducted. Results of case study provided the usefulness and efficiency of proposed model for classification of crowd sourced software requirements.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Information Engineering},
pages = {42–46},
numpages = {5},
keywords = {Data mining, Crowdsourcing, Requirement elicitation, Requirement classification, Functional Requirements and Non-Functional Requirements},
location = {Cairo, Egypt},
series = {ICSIE '19}
}

@inproceedings{10.1145/3269206.3272011,
author = {Sharchilev, Boris and Roizner, Michael and Rumyantsev, Andrey and Ozornin, Denis and Serdyukov, Pavel and de Rijke, Maarten},
title = {Web-Based Startup Success Prediction},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3272011},
doi = {10.1145/3269206.3272011},
abstract = {We consider the problem of predicting the success of startup companies at their early development stages. We formulate the task as predicting whether a company that has already secured initial (seed or angel) funding will attract a further round of investment in a given period of time. Previous work on this task has mostly been restricted to mining structured data sources, such as databases of the startup ecosystem consisting of investors, incubators and startups. Instead, we investigate the potential of using web-based open sources for the startup success prediction task and model the task using a very rich set of signals from such sources. In particular, we enrich structured data about the startup ecosystem with information from a business- and employment-oriented social networking service and from the web in general. Using these signals, we train a robust machine learning pipeline encompassing multiple base models using gradient boosting. We show that utilizing companies' mentions on the Web yields a substantial performance boost in comparison to only using structured data about the startup ecosystem. We also provide a thorough analysis of the obtained model that allows one to obtain insights into both the types of useful signals discoverable on the Web and market mechanisms underlying the funding process.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {2283–2291},
numpages = {9},
keywords = {gradient boosting, predictive modeling, mining open sources, heterogeneous web data},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3132847.3132967,
author = {Wang, Chun and Pan, Shirui and Long, Guodong and Zhu, Xingquan and Jiang, Jing},
title = {MGAE: Marginalized Graph Autoencoder for Graph Clustering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132967},
doi = {10.1145/3132847.3132967},
abstract = {Graph clustering aims to discovercommunity structures in networks, the task being fundamentally challenging mainly because the topology structure and the content of the graphs are difficult to represent for clustering analysis. Recently, graph clustering has moved from traditional shallow methods to deep learning approaches, thanks to the unique feature representation learning capability of deep learning. However, existing deep approaches for graph clustering can only exploit the structure information, while ignoring the content information associated with the nodes in a graph. In this paper, we propose a novel marginalized graph autoencoder (MGAE) algorithm for graph clustering. The key innovation of MGAE is that it advances the autoencoder to the graph domain, so graph representation learning can be carried out not only in a purely unsupervised setting by leveraging structure and content information, it can also be stacked in a deep fashion to learn effective representation. From a technical viewpoint, we propose a marginalized graph convolutional network to corrupt network node content, allowing node content to interact with network features, and marginalizes the corrupted features in a graph autoencoder context to learn graph feature representations. The learned features are fed into the spectral clustering algorithm for graph clustering. Experimental results on benchmark datasets demonstrate the superior performance of MGAE, compared to numerous baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {889–898},
numpages = {10},
keywords = {graph clustering, graph convolutional network, autoencoder, network representation, graph autoencoder},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3121050.3121071,
author = {Bahrainian, Seyed Ali and Crestani, Fabio},
title = {Towards the Next Generation of Personal Assistants: Systems That Know When You Forget},
year = {2017},
isbn = {9781450344906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121050.3121071},
doi = {10.1145/3121050.3121071},
abstract = {Recently a new class of personal assistants that are capable of addressing users' information needs proactively is emerging.Users' information needs may include timely notifications about a certain context such as location, social interactions with other people, weather, other events, etc. Personal assistants can assist people by recommending the right information at just the right time and help them in accomplishing tasks. Because of the ubiquitous nature of mobile personal assistants, they have a broad range of potential capabilities. One of these potential capabilities is to carry out sophisticated tasks for supporting failing memories. Such support of human memory has been thus far limited, merely to setting reminders and calendar events.In this paper, we present our work on developing a cutting-edge personal assistant for supporting failing memories in every day social interactions. Specifically, we envision a personal assistant that can anticipate the parts of a past conversation that you are likely to forget, hence remind you about them. Our experimental results on a real-world dataset of meetings reveals evidence that developing such systems is viable and can produce promising results.},
booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {169–176},
numpages = {8},
keywords = {meetings, human memory augmentation, personal assistants},
location = {Amsterdam, The Netherlands},
series = {ICTIR '17}
}

@inproceedings{10.1145/3164541.3164636,
author = {Ji, Seonmi and Kim, Yongsung and Jung, Seungwon and Hwang, Eenjun},
title = {An Educational Video Scoring Scheme for Level and Topic-Based Recommendation},
year = {2018},
isbn = {9781450363853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3164541.3164636},
doi = {10.1145/3164541.3164636},
abstract = {Recently, a huge amount of educational video contents have become available due to diverse IT technologies and rapidly growing demand for online learning. Popular resources for various high quality educational contents include OCW (OpenCourseWare), MOOC (Massive Open Online Course) and TED (Technology, Entertainment, Design). Moreover, learners can easily access such contents via the Internet by using common video sharing platforms such as YouTube and Vimeo. However, one critical problem that many users experience in this environment is that it is difficult and time consuming to find out educational contents that is appropriate to them in terms of topic and difficulty level. To solve this problem, in this paper, we propose an educational contents scoring scheme for topic and difficulty level based contents recommendation. The scoring is based on the linguistic difficulty of the video and the topic similarity. To measure the linguistic difficulty, we apply various readability metrics to the video script. To measure the topic similarity between videos, we extract keywords from each video script and then calculate their cosine similarity. We show the effectiveness of our proposed scheme by various experiments.},
booktitle = {Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication},
articleno = {12},
numpages = {5},
keywords = {Level and Topic-based Recommendation, Educational Video, Ranking Score},
location = {Langkawi, Malaysia},
series = {IMCOM '18}
}

@article{10.1145/3156667,
author = {Chong, Wen-Haw and Lim, Ee-Peng},
title = {Exploiting User and Venue Characteristics for Fine-Grained Tweet Geolocation},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3156667},
doi = {10.1145/3156667},
abstract = {Which venue is a tweet posted from? We call this a fine-grained geolocation problem. Given an observed tweet, the task is to infer its discrete posting venue, e.g., a specific restaurant. This recovers the venue context and differs from prior work, which geolocats tweets to location coordinates or cities/neighborhoods.First, we conduct empirical analysis to uncover venue and user characteristics for improving geolocation. For venues, we observe spatial homophily, in which venues near each other have more similar tweet content (i.e., text representations) compared to venues further apart. For users, we observe that they are spatially focused and more likely to visit venues near their previous visits. We also find that a substantial proportion of users post one or more geocoded tweet(s), thus providing their location history data. We then propose geolocation models that exploit spatial homophily and spatial focus characteristics plus posting time information. Our models rank candidate venues of test tweets such that the actual posting venue is ranked high. To better tune model parameters, we introduce a learning-to-rank framework. Our best model significantly outperforms state-of-the-art baselines. Furthermore, we show that tweets without any location-indicative words can be geolocated meaningfully as well.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {26},
numpages = {34},
keywords = {spatial homophily, spatial focus, learning to rank, Tweet geolocation}
}

@inproceedings{10.1145/3368089.3418540,
author = {Costa Silva, Camila Mariane},
title = {Reusing Software Engineering Knowledge from Developer Communication},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418540},
doi = {10.1145/3368089.3418540},
abstract = {Software development requires many different types of knowledge, such as knowledge about software development processes, practices and techniques, and about the domain of an application. Software, developers often share knowledge in informal communication channels (e.g., instant messaging tools, e-mails, or online forums). Considering that this informal communication contains knowledge that may be potentially relevant for other developers and given that this knowledge is not necessarily captured and formally documented for reuse, in this work we propose (a) exploring whether developer communication (via instant messaging) is a suitable source of reusable software engineering knowledge; (b) investigating how to identify that knowledge using data mining; (c) and analysing through action research how to present it to developers in a useful way for reuse. The envisioned theories and solutions approaches will analyze existing software development data captured in communication, rather than data that were captured and stored specifically to be reused.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1682–1685},
numpages = {4},
keywords = {information needs, natural language processing, developer communication},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3397271.3401400,
author = {Kuzi, Saar and Zhai, ChengXiang and Tian, Yin and Tang, Haichuan},
title = {FigExplorer: A System for Retrieval and Exploration of Figures from Collections of Research Articles},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401400},
doi = {10.1145/3397271.3401400},
abstract = {In this paper, we present FigExplorer, a novel general system that supports the retrieval and exploration of research article figures. Specifically, FigExplorer can support 1) figure retrieval using keyword queries, 2) exploration of related figures of a given figure, 3) exploration of a figure topic using the citation network, and 4) search result re-ranking using an example figure. The different functions were implemented using either classical IR models or neural network-based figure embeddings. Finally, the system was designed to facilitate the collection of user data for training and test purposes and it is flexible enough such that it can be extended to include new functions and algorithms. As an open-source system, FigExplorer can help advance the research, evaluation, and development of applications in this area.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2133–2136},
numpages = {4},
keywords = {research literature systems, figure retrieval, exploratory search, retrieval applications},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3180496.3180620,
author = {Li, Shan-Qing and Du, Sheng-Mei and Xing, Xiao-Zhao},
title = {A Keyword Extraction Method for Chinese Scientific Abstracts},
year = {2017},
isbn = {9781450353441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180496.3180620},
doi = {10.1145/3180496.3180620},
abstract = {Keyword extraction plays an essential role for text mining and further semantic analysis. It is a big challenge to extract keywords from short text, especially from short Chinese text. This paper presents a keyword extraction method for Chinese scientific abstracts. Firstly, an abstract is divided into meaningful units of Chinese words. Secondly, after excluding stop words, the TextRank method is adopted to extract keywords with a graph-based ranking model, and generate multi-word candidates by concatenating keywords adjacent in the abstract text. Thirdly, a keyword dictionary and a probability calculation algorithm based on a Chinese corpus are presented to check whether a word sequence is a correct multi-word keyword. To demonstrate the effectiveness of our method, a comparison experiment is conducted to show that our method outperforms the TextRank algorithm and TFIDF algorithm in Chinese multi-word keyword extraction.},
booktitle = {Proceedings of the 2017 International Conference on Wireless Communications, Networking and Applications},
pages = {133–137},
numpages = {5},
keywords = {multi-word keywords, TextRank algorithm, Keyword extraction, scientific abstracts},
location = {Shenzhen, China},
series = {WCNA 2017}
}

@inproceedings{10.1145/3170427.3188447,
author = {Musabirov, Ilya and Bulygin, Denis and Okopny, Paul and Konstantinova, Ksenia},
title = {Event-Driven Spectators' Communication in Massive ESports Online Chats},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188447},
doi = {10.1145/3170427.3188447},
abstract = {ESports tournaments, such as Dota 2»s The International (TI), attract millions of spectators to watch broadcasts on online streaming platforms, communicate, and share their experience and emotions. Unlike traditional streams, tournament broadcasts lack a streamer figure to which spectators can appeal directly. Using topic modelling and cross-correlation analysis of more than 3 million messages from 86 games of TI7, we uncover main topical and temporal patterns of communication. First, we disentangle contextual meanings of emotes and memes, which play a salient role in communication, and show a meta-topics semantic map of streaming slang. Second, our analysis shows a prevalence of the event-driven game communication during tournament broadcasts and particular topics associated with the event peaks. Third, we show that »copypasta» cascades and other related practices, while occupying a significant share of messages, are strongly associated with periods of lower in-game activity.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {dota2, esports, online communication},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3027385.3027416,
author = {Yeomans, Michael and Reich, Justin},
title = {Planning Prompts Increase and Forecast Course Completion in Massive Open Online Courses},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027416},
doi = {10.1145/3027385.3027416},
abstract = {Among all of the learners in Massive Open Online Courses (MOOCs) who intend to complete a course, the majority fail to do so. This intention-action gap is found in many domains of human experience, and research in similar goal pursuit domains suggests that plan-making is a cheap and effective nudge to encourage follow-through. In a natural field experiment in three HarvardX courses, some students received open-ended planning prompts at the beginning of a course. These prompts increased course completion by 29%, and payment for certificates by 40%. This effect was largest for students enrolled in traditional schools. Furthermore, the contents of students' plans could predict which students were least likely to succeed - in particular, students whose plans focused on specific times were unlikely to complete the course. Our results suggest that planning prompts can help learners adopted productive frames of mind at the outset of a learning goal that encourage and forecast student success.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {464–473},
numpages = {10},
keywords = {learning analytics, motivation, natural language processing, decision-making, goal pursuit, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3485447.3511994,
author = {Liang, Bin and Chen, Zixiao and Gui, Lin and He, Yulan and Yang, Min and Xu, Ruifeng},
title = {Zero-Shot Stance Detection via Contrastive Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511994},
doi = {10.1145/3485447.3511994},
abstract = {Zero-shot stance detection (ZSSD) is challenging as it requires detecting the stance of previously unseen targets during the inference stage. Being able to detect the target-related transferable stance features from the training data is arguably an important step in ZSSD. Generally speaking, stance features can be grouped into target-invariant and target-specific categories. Target-invariant stance features carry the same stance regardless of the targets they are associated with. On the contrary, target-specific stance features only co-occur with certain targets. As such, it is important to distinguish these two types of stance features when learning stance features of unseen targets. To this end, in this paper, we revisit ZSSD from a novel perspective by developing an effective approach to distinguish the types (target-invariant/-specific) of stance features, so as to better learn transferable stance features. To be specific, inspired by self-supervised learning, we frame the stance-feature-type identification as a pretext task in ZSSD. Furthermore, we devise a novel hierarchical contrastive learning strategy to capture the correlation and difference between target-invariant and -specific features and further among different stance labels. This essentially allows the model to exploit transferable stance features more effectively for representing the stance of previously unseen targets. Extensive experiments on three benchmark datasets show that the proposed framework achieves the state-of-the-art performance in ZSSD.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2738–2747},
numpages = {10},
keywords = {pretext task, zero-shot stance detection, contrastive learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3488560.3498536,
author = {Mukherjee, Rajdeep and Vishnu, Uppada and Peruri, Hari Chandana and Bhattacharya, Sourangshu and Rudra, Koustav and Goyal, Pawan and Ganguly, Niloy},
title = {MTLTS: A Multi-Task Framework To Obtain Trustworthy Summaries From Crisis-Related Microblogs},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498536},
doi = {10.1145/3488560.3498536},
abstract = {Occurrences of catastrophes such as natural or man-made disasters trigger the spread of rumours over social media at a rapid pace. Presenting a trustworthy and summarized account of the unfolding event in near real-time to the consumers of such potentially unreliable information thus becomes an important task. In this work, we propose MTLTS, the first end-to-end solution for the task that jointly determines the credibility and summary-worthiness of tweets. Our credibility verifier is designed to recursively learn the structural properties of a Twitter conversation cascade, along with the stances of replies towards the source tweet. We then take a hierarchical multi-task learning approach, where the verifier is trained at a lower layer, and the summarizer is trained at a deeper layer where it utilizes the verifier predictions to determine the salience of a tweet. Different from existing disaster-specific summarizers, we model tweet summarization as a supervised task. Such an approach can automatically learn summary-worthy features, and can therefore generalize well across domains. When trained on the PHEME dataset [29], not only do we outperform the strongest baselines for the auxiliary task of verification/rumour detection, we also achieve 21 - 35% gains in the verified ratio of summary tweets, and 16 - 20% gains in ROUGE1-F1 scores over the existing state-of-the-art solutions for the primary task of trustworthy summarization.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {755–763},
numpages = {9},
keywords = {trustworthy summarization, rumour detection, disaster, twitter},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3151759.3151798,
author = {Suzuki, Yu and Ohara, Hiromitsu and Nadamoto, Akiyo},
title = {Finding Missing Tweets Using Topic Structure and Browsing Time},
year = {2017},
isbn = {9781450352994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151759.3151798},
doi = {10.1145/3151759.3151798},
abstract = {Microblogging services such as Twitter and Facebook become popular in recent years. In these services, many users post short messages which correspond to many topics such as daily activities, opinions, and new events. Therefore, users need a system to summarize messages if the users receive tons of messages. If the following users tweet about important things which the user does not know, these tweets should be noticed. However, which tweets should be noticed is one important problem. Users should need which topics are on their timeline. However, if the summarization method does not consider topics of tweets, the summarized tweets do not contain rarely tweeted topics. To solve this problem, we propose a method for automatically extracting missing tweets based on topic granularity and missing time of the users. In this study, we map the missing tweets to the Wikipedia category tree by considering topic structure granularity; then we present the topic structures of missing tweets using our proposed visualization interface. In our experiments, we confirmed the effectiveness of our proposed hierarchal topic structure.},
booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {102–110},
numpages = {9},
keywords = {Twitter, uninformed information, browsing time},
location = {Salzburg, Austria},
series = {iiWAS '17}
}

@inproceedings{10.1145/3404835.3462938,
author = {Zogan, Hamad and Razzak, Imran and Jameel, Shoaib and Xu, Guandong},
title = {DepressionNet: Learning Multi-Modalities with User Post Summarization for Depression Detection on Social Media},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462938},
doi = {10.1145/3404835.3462938},
abstract = {Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {133–142},
numpages = {10},
keywords = {machine learning, deep learning, text summarization, social network, depression detection},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3383313.3412255,
author = {Aggarwal, Samarth and Garg, Rohin and Sancheti, Abhilasha and Guda, Bhanu Prakash Reddy and Burhanuddin, Iftikhar Ahamath},
title = {Goal-Driven Command Recommendations for Analysts},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412255},
doi = {10.1145/3383313.3412255},
abstract = {Recent times have seen data analytics software applications become an integral part of the decision-making process of analysts. The users of these software applications generate a vast amount of unstructured log data. These logs contain clues to the user’s goals, which traditional recommender systems may find difficult to model implicitly from the log data. With this assumption, we would like to assist the analytics process of a user through command recommendations. We categorize the commands into software and data categories based on their purpose to fulfill the task at hand. On the premise that the sequence of commands leading up to a data command is a good predictor of the latter, we design, develop, and validate various sequence modeling techniques. In this paper, we propose a framework to provide goal-driven data command recommendations to the user by leveraging unstructured logs. We use the log data of a web-based analytics software to train our neural network models and quantify their performance, in comparison to relevant and competitive baselines. We propose a custom loss function to tailor the recommended data commands according to the goal information provided exogenously. We also propose an evaluation metric that captures the degree of goal orientation of the recommendations. We demonstrate the promise of our approach by evaluating the models with the proposed metric and showcasing the robustness of our models in the case of adversarial examples, where the user activity is misaligned with selected goal, through offline evaluation.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {160–169},
numpages = {10},
keywords = {topic modeling, context-aware recommendation, application logs, user goals, command recommendation},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3396802.3396805,
author = {Bernius, Jan Philip and Kovaleva, Anna and Krusche, Stephan and Bruegge, Bernd},
title = {Towards the Automation of Grading Textual Student Submissions to Open-Ended Questions},
year = {2020},
isbn = {9781450377522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396802.3396805},
doi = {10.1145/3396802.3396805},
abstract = {Growing student numbers at universities worldwide pose new challenges for instructors. Providing feedback to textual exercises is a challenge in large courses while being important for student's learning success. Exercise submissions and their grading are a primary and individual communication channel between instructors and students. The pure amount of submissions makes it impossible for a single instructor to provide regular feedback to large student bodies. Employing tutors in the process introduces new challenges. Feedback should be consistent and fair for all students. Additionally, interactive teaching models strive for real-time feedback and multiple submissions.We propose a support system for grading textual exercises using an automatic segment-based assessment concept. The system aims at providing suggestions to instructors by reusing previous comments as well as scores. The goal is to reduce the workload for instructors, while at the same time creating timely and consistent feedback to the students. We present the design and a prototypical implementation of an algorithm using topic modeling for segmenting the submissions into smaller blocks. Thereby, the system derives smaller units for assessment and allowing the creation of reusable and structured feedback.We have evaluated the algorithm qualitatively by comparing automatically produced segments with manually produced segments created by humans. The results show that the system can produce topically coherent segments. The segmentation algorithm based on topic modeling is superior to approaches purely based on syntax and punctuation.},
booktitle = {Proceedings of the 4th European Conference on Software Engineering Education},
pages = {61–70},
numpages = {10},
keywords = {Assessment Support Systems, Automatic Assessment, Software Engineering Education, Textual Exercise},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '20}
}

@inproceedings{10.1145/3343031.3351036,
author = {Semedo, David and Magalhaes, Joao},
title = {Diachronic Cross-Modal Embeddings},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351036},
doi = {10.1145/3343031.3351036},
abstract = {Understanding the semantic shifts of multimodal information is only possible with models that capture cross-modal interactions over time. Under this paradigm, a new embedding is needed that structures visual-textual interactions according to the temporal dimension, thus, preserving data's original temporal organisation. This paper introduces a novel diachronic cross-modal embedding (DCM), where cross-modal correlations are represented in embedding space, throughout the temporal dimension, preserving semantic similarity at each instant t. To achieve this, we trained a neural cross-modal architecture, under a novel ranking loss strategy, that for each multimodal instance, enforces neighbour instances' temporal alignment, through subspace structuring constraints based on a temporal alignment window. Experimental results show that our DCM embedding successfully organises instances over time. Quantitative experiments, confirm that DCM is able to preserve semantic cross-modal correlations at each instant t while also providing better alignment capabilities. Qualitative experiments unveil new ways to browse multimodal content and hint that multimodal understanding tasks can benefit from this new embedding.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2061–2069},
numpages = {9},
keywords = {neural cross-modal learning, diachronic embeddings, data mining, neural networks, semantic shifts},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3294032.3294079,
author = {Xu, Sihan and Zhang, Sen and Wang, Weijing and Cao, Xinya and Guo, Chenkai and Xu, Jing},
title = {Method Name Suggestion with Hierarchical Attention Networks},
year = {2019},
isbn = {9781450362269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3294032.3294079},
doi = {10.1145/3294032.3294079},
abstract = {Method Rename has been a widely used refactoring operation that improves program comprehension and maintenance. Descriptive method names that summarize functionalities of source code can facilitate program comprehension. Much research has been done to suggest method names through source code summarization. However, unlike natural language, a code snippet consists of basic blocks organized by complicated structures. In this work, we observe a hierarchical structure --- tokens form basic blocks and basic blocks form a code snippet. Based on this observation, we exploit a hierarchical attention network to learn the representation of methods. Specifically, we apply two-level attention mechanism to learn the importance of each token in a basic block and that of a basic block in a method respectively. We evaluated our approach on 10 open source repositories and compared it against three state-of-the-art approaches. The results on these open-source data show the superiority of our hierarchical attention networks in terms of effectiveness.},
booktitle = {Proceedings of the 2019 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
pages = {10–21},
numpages = {12},
keywords = {Program Comprehension, Rename Method, Refactoring},
location = {Cascais, Portugal},
series = {PEPM 2019}
}

@inproceedings{10.1145/3123266.3123433,
author = {Gao, Junyu and Zhang, Tianzhu and Xu, Changsheng},
title = {A Unified Personalized Video Recommendation via Dynamic Recurrent Neural Networks},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123433},
doi = {10.1145/3123266.3123433},
abstract = {Personalized video recommender systems play an essential role in bridging users and videos. However, most existing video recommendation methods assume that user profiles (interests) are static. In fact, the static assumption is inadequate to reflect users' dynamic interests as time goes by, especially in the online video recommendation scenarios with dramatic changes of video contents and frequent drift of users' interests over different topics. To overcome the above issue, we propose a dynamic recurrent neural network to model users' dynamic interests over time in a unified framework for personalized video recommendation. Furthermore, to build a much more comprehensive recommendation system, the proposed model is designed to exploit video semantic embedding, user interest modeling, and user relevance mining jointly to model users' preferences. By considering these three factors, the RNN model becomes an interest network which can capture users' high level interests effectively. Extensive experimental results on both single-network and cross-network video recommendation scenarios demonstrate the superior performance of the proposed model compared with other state-of-the-art algorithms.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {127–135},
numpages = {9},
keywords = {recurrent neural networks, social data science, personalized video recommendation, user interest modeling},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3148055.3148080,
author = {Rahman, Muhammad Mahbubur and Finin, Tim},
title = {Deep Understanding of a Document's Structure},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148080},
doi = {10.1145/3148055.3148080},
abstract = {Current language understanding approaches focus on small documents, such as newswire articles, blog posts, product reviews and discussion forum discussions. Understanding and extracting information from large documents like legal briefs, proposals, technical manuals and research articles is still a challenging task. We describe a framework that can analyze a large document and help people to locate desired information in it. We aim to automatically identify and classify different sections of documents and understand their purpose within the document. A key contribution of our research is modeling and extracting the logical structure of electronic documents using machine learning techniques, including deep learning. We also make available a dataset of information about a collection of scholarly articles from the arXiv eprints collection that includes a wide range of metadata for each article, including a table of contents, section labels, section summarizations and more. We hope that this dataset will be a useful resource for the machine learning and language understanding communities for information retrieval, content-based question answering and language modeling tasks.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {63–73},
numpages = {11},
keywords = {natural language processing, machine learning, document structure, deep learning},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@article{10.1145/3009976,
author = {Sciascio, Cecilia Di and Sabol, Vedran and Veas, Eduardo},
title = {Supporting Exploratory Search with a Visual User-Driven Approach},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3009976},
doi = {10.1145/3009976},
abstract = {Whenever users engage in gathering and organizing new information, searching and browsing activities emerge at the core of the exploration process. As the process unfolds and new knowledge is acquired, interest drifts occur inevitably and need to be accounted for. Despite the advances in retrieval and recommender algorithms, real-world interfaces have remained largely unchanged: results are delivered in a relevance-ranked list. However, it quickly becomes cumbersome to reorganize resources along new interests, as any new search brings new results. We introduce an interactive user-driven tool that aims at supporting users in understanding, refining, and reorganizing documents on the fly as information needs evolve. Decisions regarding visual and interactive design aspects are tightly grounded on a conceptual model for exploratory search. In other words, the different views in the user interface address stages of awareness, exploration, and explanation unfolding along the discovery process, supported by a set of text-mining methods. A formal evaluation showed that gathering items relevant to a particular topic of interest with our tool incurs in a lower cognitive load compared to a traditional ranked list. A second study reports on usage patterns and usability of the various interaction techniques within a free, unsupervised setting.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {dec},
articleno = {18},
numpages = {35},
keywords = {textual document ranking, Exploratory search, advanced search interface}
}

@inproceedings{10.1145/3440084.3441188,
author = {Zhang, Jiang and Zhu, Qun-Xiong and He, Yan-Lin},
title = {Hierarchical Attention-Based BiLSTM Network for Document Similarity Calculation},
year = {2020},
isbn = {9781450388894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440084.3441188},
doi = {10.1145/3440084.3441188},
abstract = {Neural network model is a momentous method to calculate semantic similarity. Taking into account the complexity of document structure, introducing hierarchical structure and attention mechanism into neural network can calculate document semantic representation more precisely. In order to verify the validity of the model, LP50 dataset was tested. The experimental results reveal that accurate document representation can be obtained by using the attention mechanism at two levels of words and sentences. Since this method has taken both the influence of context information and the contribution of components to the document into consideration. Compared with several conventional methods, there is a significant improvement of performance in our model.},
booktitle = {Proceedings of the 2020 4th International Symposium on Computer Science and Intelligent Control},
articleno = {12},
numpages = {5},
keywords = {Natural Language Processing, Hierarchical Attention Network, Document Similarity Calculation},
location = {Newcastle upon Tyne, United Kingdom},
series = {ISCSIC 2020}
}

@inproceedings{10.1145/3324884.3421837,
author = {Liu, Lei and Bahrami, Mehdi and Chen, Wei-Peng},
title = {Automatic Generation of IFTTT Mashup Infrastructures},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3421837},
doi = {10.1145/3324884.3421837},
abstract = {In recent years, IF-This-Then-That (IFTTT) services are becoming more and more popular. Many platforms such as Zapier, IFTTT.com, and Workato provide such services, which allow users to create workflows with "triggers" and "actions" by using Web Application Programming Interfaces (APIs). However, the number of IFTTT recipes in the above platforms increases much slower than the growth of Web APIs. This is because human efforts are still largely required to build and deploy IFTTT recipes in the above platforms. To address this problem, in this paper, we present an automation tool to automatically generate the IFTTT mashup infrastructure. The proposed tool provides 5 REST APIs, which can automatically generate triggers, rules, and actions in AWS, and create a workflow XML to describe an IFTTT mashup by connecting the triggers, rules, and actions. This workflow XML is automatically sent to Fujitsu RunMyProcess (RMP) to set up and execute IFTTT mashup. The proposed tool, together with its associated method and procedure, enables an end-to-end solution for automatically creating, deploying, and executing IFTTT mashups in a few seconds, which can greatly reduce the development cycle and cost for new IFTTT mashups.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1179–1183},
numpages = {5},
keywords = {RMP, IFTTT, automation, Web APIs, AWS},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3416028.3416042,
author = {Pi, Sainan and An, Xin and Xu, Shuo and Li, Jinghong},
title = {A Comparative Study on Three Multi-Label Classification Tools},
year = {2020},
isbn = {9781450375467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416028.3416042},
doi = {10.1145/3416028.3416042},
abstract = {Many science, technology and innovation (STI) resources are attached with several different labels, such as IPC and CPC for patents, and PACS (Physics and Astronomy Classification Scheme) numbers for scientific publications. This problem is well known as the multi-label classification. Though there are a number of approaches and open-source tools for this task in the literature that work well on benchmark datasets, real-world is more complex in terms of both the number and hierarchy of labels. This work aims to compare comprehensively the performance of three state-of-the-art tools, Dependency LDA, Scikit-Multilearn and Neural Classifier on Scigraph of academic resource data. It is found that Neural Classifier works better on an unbalanced distribution dataset with more complex hierarchical structure and a larger number of label scale in terms of Micro F1, Micro F1 and Hamming Loss than the other two tools. On the basis of our comparisons, several directions are suggested in the near future.},
booktitle = {Proceedings of the 2020 3rd International Conference on Information Management and Management Science},
pages = {8–12},
numpages = {5},
keywords = {Neural Classifier, Dependency LDA, Scikit-Multilearn, Multi-label classification},
location = {London, United Kingdom},
series = {IMMS 2020}
}

@inproceedings{10.1145/3090354.3090384,
author = {Imane, Allaouzi and Mohamed, Ben Ahmed},
title = {Multi-Label Categorization of French Death Certificates Using NLP and Machine Learning},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090384},
doi = {10.1145/3090354.3090384},
abstract = {The medical information represents an invaluable source of knowledge concerning the medical history of the patient, but the manner of their presentation make it badly exploited.The idea of this paper is based on the analysis of the death reports written in natural language, which are rich of information, and can be exploited in the calculation of mortality statistics, giving preventive solutions, as well as, help medical professional in their research.This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to ICD-10 (International Classification of Diseases) codes. This approach is based on Machine learning techniques, which is evaluated over C\'{e}piDC corpus. The experiment showed that our approach gives interesting results, with an average F1-measue of 79.02%.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {29},
numpages = {4},
keywords = {French Death Certificates, Multi-label categorization, NLP, Text Mining, Machine learning},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@article{10.1145/3449181,
author = {Mueller, Aaron and Wood-Doughty, Zach and Amir, Silvio and Dredze, Mark and Nobles, Alicia Lynn},
title = {Demographic Representation and Collective Storytelling in the Me Too Twitter Hashtag Activism Movement},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449181},
doi = {10.1145/3449181},
abstract = {The #MeToo movement on Twitter has drawn attention to the pervasive nature of sexual harassment and violence. While #MeToo has been praised for providing support for self-disclosures of harassment or violence and shifting societal response, it has also been criticized for exemplifying how women of color have been discounted for their historical contributions to and excluded from feminist movements. Through an analysis of over 600,000 tweets from over 256,000 unique users, we examine online #MeToo conversations across gender and racial/ethnic identities and the topics that each demographic emphasized. We found that tweets authored by white women were overrepresented in the movement compared to other demographics, aligning with criticism of unequal representation. We found that intersected identities contributed differing narratives to frame the movement, co-opted the movement to raise visibility in parallel ongoing movements, employed the same hashtags both critically and supportively, and revived and created new hashtags in response to pivotal moments. Notably, tweets authored by black women often expressed emotional support and were critical about differential treatment in the justice system and by police. In comparison, tweets authored by white women and men often highlighted sexual harassment and violence by public figures and weaved in more general political discussions. We discuss the implications of this work for digital activism research and design, including suggestions to raise visibility by those who were under-represented in this hashtag activism movement. Content warning: this article discusses issues of sexual harassment and violence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {107},
numpages = {28},
keywords = {metoo, demographic inference, topic modeling}
}

@inproceedings{10.1145/3269206.3271694,
author = {Jiang, Zhuoren and Gao, Liangcai and Yuan, Ke and Gao, Zheng and Tang, Zhi and Liu, Xiaozhong},
title = {Mathematics Content Understanding for Cyberlearning via Formula Evolution Map},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271694},
doi = {10.1145/3269206.3271694},
abstract = {Although the scientific digital library is growing at a rapid pace, scholars/students often find reading Science, Technology, Engineering, and Mathematics (STEM) literature daunting, especially for the math-content/formula. In this paper, we propose a novel problem, "mathematics content understanding", for cyberlearning and cyberreading. To address this problem, we create a Formula Evolution Map (FEM) offline and implement a novel online learning/reading environment, PDF Reader with Math-Assistant (PRMA), which incorporates innovative math-scaffolding methods. The proposed algorithm/system can auto-characterize student emerging math-information need while reading a paper and enable students to readily explore the formula evolution trajectory in FEM. Based on a math-information need, PRMA utilizes innovative joint embedding, formula evolution mining, and heterogeneous graph mining algorithms to recommend high quality Open Educational Resources (OERs), e.g., video, Wikipedia page, or slides, to help students better understand the math-content in the paper. Evaluation and exit surveys show that the PRMA system and the proposed formula understanding algorithm can effectively assist master and PhD students better understand the complex math-content in the class readings.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {37–46},
numpages = {10},
keywords = {education, formula layout, cyberlearning, formula evolution, formula understanding},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3239235.3239524,
author = {Ahmed, Syed and Bagherzadeh, Mehdi},
title = {What Do Concurrency Developers Ask about? A Large-Scale Study Using Stack Overflow},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239524},
doi = {10.1145/3239235.3239524},
abstract = {Background Software developers are increasingly required to write concurrent code. However, most developers find concurrent programming difficult. To better help developers, it is imperative to understand their interest and difficulties in terms of concurrency topics they encounter often when writing concurrent code.Aims In this work, we conduct a large-scale study on the textual content of the entirety of Stack Overflow to understand the interests and difficulties of concurrency developers.Method First, we develop a set of concurrency tags to extract concurrency questions that developers ask. Second, we use latent Dirichlet allocation (LDA) topic modeling and an open card sort to manually determine the topics of these questions. Third, we construct a topic hierarchy by repeated grouping of similar topics into categories and lower level categories into higher level categories. Fourth, we investigate the coincidence of our concurrency topics with findings of previous work. Fifth, we measure the popularity and difficulty of our concurrency topics and analyze their correlation. Finally, we discuss the implications of our findings.Results A few findings of our study are the following. (1) Developers ask questions about a broad spectrum of concurrency topics ranging from multithreading to parallel computing, mobile concurrency to web concurrency and memory consistency to run-time speedup. (2) These questions can be grouped into a hierarchy with eight major categories: concurrency models, programming paradigms, correctness, debugging, basic concepts, persistence, performance and GUI. (3) Developers ask more about correctness of their concurrent programs than performance. (4) Concurrency questions about thread safety and database management systems are among the most popular and the most difficult, respectively. (5) Difficulty and popularity of concurrency topics are negatively correlated.Conclusions The results of our study can not only help concurrency developers but also concurrency educators and researchers to better decide where to focus their efforts, by trading off one concurrency topic against another.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {30},
numpages = {10},
keywords = {concurrency topic difficulty, concurrency topic popularity, concurrency topic hierarchy, concurrency topics, stack overflow},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1109/TASLP.2016.2626965,
author = {Chen, Hongjie and Xie, Lei and Leung, Cheung-Chi and Lu, Xiaoming and Ma, Bin and Li, Haizhou and Hongjie Chen and Lei Xie and Cheung-Chi Leung and Xiaoming Lu and Bin Ma and Haizhou Li and Leung, Cheung-Chi and Li, Haizhou and Lu, Xiaoming and Ma, Bin and Chen, Hongjie and Xie, Lei},
title = {Modeling Latent Topics and Temporal Distance for Story Segmentation of Broadcast News},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2626965},
doi = {10.1109/TASLP.2016.2626965},
abstract = {This paper studies a strategy to model latent topics and temporal distance of text blocks for story segmentation, that we call graph regularization in topic modeling or GRTM. We propose two novel approaches that consider both temporal distance and lexical similarity of text blocks, collectively referred to as data proximity, in learning latent topic representation, where a graph regularizer is involved to derive the latent topic representation while preserving data proximity. In the first approach, we extend the idea of Laplacian probabilistic latent semantic analysis LapPLSA by introducing a distance penalty function in the affinity matrix of a graph for latent topic estimation. The estimated latent topic distributions are used to replace the traditional term-frequency vectors as the data representation of the text blocks and to measure the cohesive strength between them. In the second approach, we perform Laplacian eigenmaps, which makes use of the graph regularizer for dimensionality reduction, on latent topic distributions estimated by conventional topic modeling. We conduct the experiments on the automatic speech recognition transcripts of the TDT2 English broadcast news corpus. The experiments show the proposed strategy outperforms the conventional techniques. LapPLSA performs the best with the highest F1-measure of 0.816. The effects of the penalty constant in the distance penalty function, the number of latent topics, and the size of training data on the segmentation performances are also studied.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {112–123},
numpages = {12}
}

@article{10.1145/3457949,
author = {Guo, Lei and Yin, Hongzhi and Chen, Tong and Zhang, Xiangliang and Zheng, Kai},
title = {Hierarchical Hyperedge Embedding-Based Representation Learning for Group Recommendation},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3457949},
doi = {10.1145/3457949},
abstract = {Group recommendation aims to recommend items to a group of users. In this work, we study group recommendation in a particular scenario, namely occasional group recommendation, where groups are formed ad hoc and users may just constitute a group for the first time—that is, the historical group-item interaction records are highly limited. Most state-of-the-art works have addressed the challenge by aggregating group members’ personal preferences to learn the group representation. However, the representation learning for a group is most complex beyond the aggregation or fusion of group member representation, as the personal preferences and group preferences may be in different spaces and even orthogonal. In addition, the learned user representation is not accurate due to the sparsity of users’ interaction data. Moreover, the group similarity in terms of common group members has been overlooked, which, however, has the great potential to improve the group representation learning. In this work, we focus on addressing the aforementioned challenges in the group representation learning task, and devise a hierarchical hyperedge embedding-based group recommender, namely HyperGroup. Specifically, we propose to leverage the user-user interactions to alleviate the sparsity issue of user-item interactions, and design a graph neural network-based representation learning network to enhance the learning of individuals’ preferences from their friends’ preferences, which provides a solid foundation for learning groups’ preferences. To exploit the group similarity (i.e., overlapping relationships among groups) to learn a more accurate group representation from highly limited group-item interactions, we connect all groups as a network of overlapping sets (a.k.a. hypergraph), and treat the task of group preference learning as embedding hyperedges (i.e., user sets/groups) in a hypergraph, where an inductive hyperedge embedding method is proposed. To further enhance the group-level preference modeling, we develop a joint training strategy to learn both user-item and group-item interactions in the same process. We conduct extensive experiments on two real-world datasets, and the experimental results demonstrate the superiority of our proposed HyperGroup in comparison to the state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {3},
numpages = {27},
keywords = {representation learning, Group recommendation, hyperedge embedding}
}

@article{10.1145/3022185,
author = {Liu, Guannan and Fu, Yanjie and Chen, Guoqing and Xiong, Hui and Chen, Can},
title = {Modeling Buying Motives for Personalized Product Bundle Recommendation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3022185},
doi = {10.1145/3022185},
abstract = {Product bundling is a marketing strategy that offers several products/items for sale as one bundle. While the bundling strategy has been widely used, less efforts have been made to understand how items should be bundled with respect to consumers’ preferences and buying motives for product bundles. This article investigates the relationships between the items that are bought together within a product bundle. To that end, each purchased product bundle is formulated as a bundle graph with items as nodes and the associations between pairs of items in the bundle as edges. The relationships between items can be analyzed by the formation of edges in bundle graphs, which can be attributed to the associations of feature aspects. Then, a probabilistic model BPM (Bundle Purchases with Motives) is proposed to capture the composition of each bundle graph, with two latent factors node-type and edge-type introduced to describe the feature aspects and relationships respectively. Furthermore, based on the preferences inferred from the model, an approach for recommending items to form product bundles is developed by estimating the probability that a consumer would buy an associative item together with the item already bought in the shopping cart. Finally, experimental results on real-world transaction data collected from well-known shopping sites show the effectiveness advantages of the proposed approach over other baseline methods. Moreover, the experiments also show that the proposed model can explain consumers’ buying motives for product bundles in terms of different node-types and edge-types.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {28},
numpages = {26},
keywords = {recommendation, buying motives, probabilistic graphical model, Product bundle}
}

@article{10.14778/3067421.3067430,
author = {Cai, Hongyun and Zheng, Vincent W. and Zhu, Fanwei and Chang, Kevin Chen-Chuan and Huang, Zi},
title = {From Community Detection to Community Profiling},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3067421.3067430},
doi = {10.14778/3067421.3067430},
abstract = {Most existing community-related studies focus on detection, which aim to find the community membership for each user from user friendship links. However, membership alone, without a complete profile of what a community is and how it interacts with other communities, has limited applications. This motivates us to consider systematically profiling the communities and thereby developing useful community-level applications. In this paper, we for the first time formalize the concept of community profiling. With rich user information on the network, such as user published content and user diffusion links, we characterize a community in terms of both its internal content profile and external diffusion profile. The difficulty of community profiling is often underestimated. We novelly identify three unique challenges and propose a joint Community Profiling and Detection (CPD) model to address them accordingly. We also contribute a scalable inference algorithm, which scales linearly with the data size and it is easily parallelizable. We evaluate CPD on large-scale real-world data sets, and show that it is significantly better than the state-of-the-art baselines in various tasks.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {817–828},
numpages = {12}
}

@inproceedings{10.1145/3148453.3306262,
author = {Sun, Zhouyu and Jin, Hanjun},
title = {Application Research of Accelerated Deep Convolutional Neural Network in Cross-Media Retrieval},
year = {2018},
isbn = {9781450363525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148453.3306262},
doi = {10.1145/3148453.3306262},
abstract = {Deep learning methods are increasingly used in cross-media retrieval research because of their powerful performance. However, due to the large number of parameters and the large amount of calculation of the neural network model, the speed of cross-media retrieval is limited accordingly. In view of the above problems, this paper applies the compressed convolutional neural network VGG-16 to cross-media retrieval, and obtains a better retrieval speed. The specific method is to extract image features using channel-pruned deep convolutional neural network VGG-16 and extract text features with LDA, and then use the multinomial logistic regression to further train and predict the extracted image and text features. At the same time, the high-level semantics of the text is used to regularize the high-level semantics of the image. Finally, the Centred Correlation metric algorithm is used to calculate the similarity between the image and the text, so as to achieve the bidirectional retrieval of the image-text. Experiments show that the proposed method can effectively improve the speed of cross-media retrieval, and the retrieval speed can be increased nearly four times without using GPU acceleration.},
booktitle = {Proceedings of the International Conference on Information Technology and Electrical Engineering 2018},
articleno = {23},
numpages = {5},
keywords = {Accelerated convolutional neural network, Cross media retrieval, Deep learning},
location = {Xiamen, Fujian, China},
series = {ICITEE '18}
}

@inproceedings{10.1145/3292500.3330706,
author = {Qin, Chuan and Zhu, Hengshu and Zhu, Chen and Xu, Tong and Zhuang, Fuzhen and Ma, Chao and Zhang, Jingshuai and Xiong, Hui},
title = {DuerQuiz: A Personalized Question Recommender System for Intelligent Job Interview},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330706},
doi = {10.1145/3292500.3330706},
abstract = {In talent recruitment, the job interview aims at selecting the right candidates for the right jobs through assessing their skills and experiences in relation to the job positions. While tremendous efforts have been made in improving job interviews, a long-standing challenge is how to design appropriate interview questions for comprehensively assessing the competencies that may be deemed relevant and representative for person-job fit. To this end, in this research, we focus on the development of a personalized question recommender system, namely DuerQuiz, for enhancing the job interview assessment. DuerQuiz is a fully deployed system, in which a knowledge graph of job skills, Skill-Graph, has been built for comprehensively modeling the relevant competencies that should be assessed in the job interview. Specifically, we first develop a novel skill entity extraction approach based on a bidirectional Long Short-Term Memory (LSTM) with a Conditional Random Field (CRF) layer (LSTM-CRF) neural network enhanced with adapted gate mechanism. In particular, to improve the reliability of extracted skill entities, we design a label propagation method based on more than 10 billion click-through data from the large-scale Baidu query logs. Furthermore, we discover the hypernym-hyponym relations between skill entities and construct the Skill-Graph by leveraging the classifier trained with extensive contextual features. Finally, we design a personalized question recommendation algorithm based on the Skill-Graph for improving the efficiency and effectiveness of job interview assessment. Extensive experiments on real-world recruitment data clearly validate the effectiveness of DuerQuiz, which had been deployed for generating written exercises in the 2018 Baidu campus recruitment event and received remarkable performances in terms of efficiency and effectiveness for selecting outstanding talents compared with a traditional non-personalized human-only assessment approach.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2165–2173},
numpages = {9},
keywords = {question recommendation, intelligence interview system},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3209978.3210046,
author = {Yang, Xiao and Awadallah, Ahmed Hassan and Khabsa, Madian and Wang, Wei and Wang, Miaosen},
title = {Characterizing and Supporting Question Answering in Human-to-Human Communication},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210046},
doi = {10.1145/3209978.3210046},
abstract = {Email continues to be one of the most important means of online communication. People spend a significant amount of time sending, reading, searching and responding to email in order to manage tasks, exchange information, etc. In this paper, we focus on information exchange over enterprise email in the form of questions and answers. We study a large scale publicly available email dataset to characterize information exchange via questions and answers in enterprise email. We augment our analysis with a survey to gain insights on the types of questions exchanged, when and how do people get back to them and whether this behavior is adequately supported by existing email management and search functionality. We leverage this understanding to define the task of extracting question/answer pairs from threaded email conversations. We propose a neural network based approach that matches the question to the answer considering comparisons at different levels of granularity. We also show that we can improve the performance by leveraging external data of question and answer pairs. We test our approach using a manually labeled email data collected using a crowd-sourcing annotation study. Our findings have implications for designing email clients and intelligent agents that support question answering and information lookup in email.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {345–354},
numpages = {10},
keywords = {email, question answering, information retrieval},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3310273.3322821,
author = {Cheng, Zhou and Qi, Tianmei and Wang, Jixiang and Zhou, Yu and Wang, Zhihong and Guo, Yi and Zhao, Junfeng},
title = {Sentiment Evaluation of Forex News},
year = {2019},
isbn = {9781450366854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310273.3322821},
doi = {10.1145/3310273.3322821},
abstract = {Sentiment analysis is significant for excavating text opinion. There are two issues in the foreign exchange (Forex) field. 1) In sentiment orientation, most researches focus on product reviews, lack fine-grained sentiment analysis for Forex news. 2) In sentiment intensity, most works consider the intensity of sentiment words but ignore the significance of field characteristics. Aiming at the two problems, a fine-grained Sentiment Analysis model (shorted as WD-SA) is established, which integrates with the Weight of sentiment words and Domain features. First, the semantic information of text is embedded into a vector based on word2vec. Then, sentiment orientation is detected by a method, which combines machine learning algorithm and the weight of sentiment words. Finally, features are extracted to investigate the intensity of news. The experimental results show that our algorithm outperforms the state-of-the-art.},
booktitle = {Proceedings of the 16th ACM International Conference on Computing Frontiers},
pages = {197–201},
numpages = {5},
keywords = {machine learning, intensity, fine-grained, sentiment orientation, forex},
location = {Alghero, Italy},
series = {CF '19}
}

@inproceedings{10.1145/3277593.3277634,
author = {Zhang, Yanxia and Hung, Hayley},
title = {Using Topic Models to Mine Everyday Object Usage Routines through Connected IoT Sensors},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277634},
doi = {10.1145/3277593.3277634},
abstract = {With the tremendous progress in sensing and IoT infrastructure, it is foreseeable that IoT systems will soon be available for commercial markets, such as in people's homes. In this paper, we present a deployment study using sensors attached to household objects to capture the resourcefulness of three individuals. The concept of resourcefulness highlights the ability of humans to repurpose objects spontaneously for a different use case than was initially intended. It is a crucial element for human health and wellbeing, which is of great interest for various aspects of HCI and design research. Traditionally, resourcefulness is captured through ethnographic practice. Ethnography can only provide sparse and often short duration observations of human experience, often relying on participants being aware of and remembering behaviours or thoughts they need to report on. Our hypothesis is that resourcefulness can also be captured through continuously monitoring objects being used in everyday life. We developed a system that can record object movement continuously and deployed them in homes of three elderly people for over two weeks. We explored the use of probabilistic topic models to analyze the collected data and identify common patterns.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {27},
numpages = {4},
keywords = {internet of things (IoT), deployment, sensing, machine learning},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/3219788.3219799,
author = {Montenegro, Chuchi and Ligutom, Cerino and Orio, Jay Vincent and Ramacho, Dyannah Alexa Marie},
title = {Using Latent Dirichlet Allocation for Topic Modeling and Document Clustering of Dumaguete City Twitter Dataset},
year = {2018},
isbn = {9781450363938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219788.3219799},
doi = {10.1145/3219788.3219799},
abstract = {Online communication channel, such as social media is predominantly becoming common nowadays as it allows people to fearlessly and instantly share opinions and exchange information at one's convenience. One popular social media site and microblogging service, Twitter, has made it easy for people to express or share their experiences, adventures, and opinions on places they visited. These short messages, called tweets contain useful information that can be analyzed to generate topics of what people are talking about and their sentiments on that particular topic. To process these huge amounts of twitter dataset requires substantial effort of information filtering just to successfully drill down relevant topics and determine sentiments of those topic clusters. This paper discusses the process and the methods of generating topics and topic clusters on Twitter dataset about Dumaguete City and generates a probable sentiment analysis of each topic clusters. Latent Dirichlet Allocation (LDA) model was used to generate topics out of 99,942 tweets and clusters the tweets by calculating the probability on which topic cluster the tweet belongs. A supervised machine learning algorithm, Support Vector Machine (SVM) was used to generalize the sentiment of each cluster into positive, negative, or neutral.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Data Engineering},
pages = {1–5},
numpages = {5},
keywords = {Document clustering, Topic modelling, Sentiment analysis},
location = {Shanghai, China},
series = {ICCDE 2018}
}

@inproceedings{10.1145/3132847.3133067,
author = {Zhang, Yu and Wei, Wei and Huang, Binxuan and Carley, Kathleen M. and Zhang, Yan},
title = {RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133067},
doi = {10.1145/3132847.3133067},
abstract = {Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that "one-hot" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2423–2426},
numpages = {4},
keywords = {location inference, text mining, real-time, microblog},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3378904.3378913,
author = {Xing, SangHao and Fan, Ziling},
title = {A Method for LDA-Based Sina Weibo Recommendation},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378904.3378913},
doi = {10.1145/3378904.3378913},
abstract = {Sina Weibo is one of the most influential social platforms in China. Recommendation system helps user to find celebrities that they may interest in and thus helps to attract more users. User's Weibo contents reflect their personal preferences. In this paper we proposed an LDA topic modeling based recommendation method which can discover topics of user's Weibo contents and recommend celebrities that users are interest in. The comparison result shows that our method outperforms tf-idf-based recommendation method.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
pages = {54–57},
numpages = {4},
keywords = {SinaWeibo, Recommendation, LDA},
location = {Singapore, China},
series = {BDET 2020}
}

@inproceedings{10.1145/3240508.3266434,
author = {Chen, Xusong and Zhao, Rui and Ma, Shengjie and Liu, Dong and Zha, Zheng-Jun},
title = {Content-Based Video Relevance Prediction with Second-Order Relevance and Attention Modeling},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266434},
doi = {10.1145/3240508.3266434},
abstract = {This paper describes our proposed method for the Content-Based Video Relevance Prediction (CBVRP) challenge. Our method is based on deep learning, i.e. we train a deep network to predict the relevance between two video sequences from their features. We explore the usage of second-order relevance, both in preparing training data, and in extending the deep network. Second-order relevance refers to e.g. the relevance between x and z if x is relevant to y and y is relevant to z. In our proposed method, we use second-order relevance to increase positive samples and decrease negative samples, when preparing training data. We further extend the deep network with an attention module, where the attention mechanism is designed for second-order relevant video sequences. We verify the effectiveness of our method on the validation set of the CBVRP challenge.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2018–2022},
numpages = {5},
keywords = {deep learning, attention mechanism, content-based filtering, video relevance prediction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3220547.3220552,
author = {Bjarnadottir, Margret and Hunt, Aaron and Raschid, Louiqa},
title = {Choosing Models to Explore Financial Supply Chain Relationships},
year = {2018},
isbn = {9781450358835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220547.3220552},
doi = {10.1145/3220547.3220552},
booktitle = {Proceedings of the Fourth International Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {7},
numpages = {4},
location = {Houston, TX, USA},
series = {DSMM'18}
}

@inproceedings{10.1145/3110025.3110060,
author = {Reganti, Aishwarya N. and Maheshwari, Tushar and Das, Amitava and Chakraborthy, Tanmoy and Kumaraguru, Ponnurangam},
title = {Understanding Psycho-Sociological Vulnerability of ISIS Patronizers in Twitter},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110060},
doi = {10.1145/3110025.3110060},
abstract = {The Islamic State of Iraq and Syria (ISIS) is a Salafi jihadist militant group that has made extensive use of online social media platforms to promulgate its ideologies and evoke many individuals to support the organization. The psycho-sociological background of an individual plays a crucial role in determining his/her vulnerability of being lured into joining the organisation and indulge in terrorist activities, since his/her behavior largely depends on the society s/he was brought up in. Here, we analyse five sociological aspects -- personality, values &amp; ethics, optimism/pessimism, age and gender to understand the psycho-sociological vulnerability of individuals over Twitter. Experimental results suggest that psycho-sociological aspects indeed act as foundation to discover and differentiate between prominent and unobtrusive users in Twitter.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {621–624},
numpages = {4},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3310986.3311022,
author = {Ren, Hao and Wang, Dong},
title = {TRRS: Temporal Recurrent Recommender System Based on Time-Sync Comments},
year = {2019},
isbn = {9781450366120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310986.3311022},
doi = {10.1145/3310986.3311022},
abstract = {Recent years has witnessed great emerge of online video websites, including the exploded number of videos and users. As a result, there appears a lot of personlized recommender systems. However there remain some challenging problems to tackle such as cold start problem, which scientists have made use of all kinds of sideinformation, e.g. gender, age or comments, to release. Currently a new type of video comments, called TSCs (TSC), plays a more and more important role in video watching activity. In this paper we utilize TSC to recommend videos for users. We developed a deep nueral network model called Temporal Recurrent Recommder System (TRRS) which combine multi-layers neural network to extract feature for users and videos. The first layer convert TSC to embeddings, then RNN layer analyze each comment from user or video, and fianlly the merge layer combine all output from prior layer and produce the feature. We use the feature from the network for users and videos to make personlized recommendation.},
booktitle = {Proceedings of the 3rd International Conference on Machine Learning and Soft Computing},
pages = {123–127},
numpages = {5},
keywords = {Deep Learning, Recommendation, Time-sync Comments},
location = {Da Lat, Viet Nam},
series = {ICMLSC 2019}
}

@inproceedings{10.1145/3231622.3231641,
author = {Kucher, Kostiantyn and Martins, Rafael M. and Kerren, Andreas},
title = {Analysis of VINCI 2009-2017 Proceedings},
year = {2018},
isbn = {9781450365017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231622.3231641},
doi = {10.1145/3231622.3231641},
abstract = {Both the metadata and the textual contents of scientific publications can provide us with insights about the development and the current state of the corresponding scientific community. In this short paper, we take a look at the proceedings of VINCI from the previous years and conduct several types of analyses. We summarize the yearly statistics about different types of publications, identify the overall authorship statistics and the most prominent contributors, and analyze the current community structure with a co-authorship network. We also apply topic modeling to identify the most prominent topics discussed in the publications. We hope that the results of our work will provide insights for the visualization community and will also be used as an overview for researchers previously unfamiliar with VINCI.},
booktitle = {Proceedings of the 11th International Symposium on Visual Information Communication and Interaction},
pages = {97–101},
numpages = {5},
keywords = {meta-analysis, survey, visualization, scientific literature, overview, topic modeling},
location = {V\"{a}xj\"{o}, Sweden},
series = {VINCI '18}
}

@inproceedings{10.1145/3184558.3191583,
author = {Wang, Jia and Feng, Yungang and Naghizade, Elham and Rashidi, Lida and Lim, Kwan Hui and Lee, Kate},
title = {Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191583},
doi = {10.1145/3184558.3191583},
abstract = {Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1401–1405},
numpages = {5},
keywords = {social networks, sentiment analysis, location recommendation},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3102254.3102266,
author = {Celestini, Alessandro and Guarino, Stefano},
title = {Design, Implementation and Test of a Flexible Tor-Oriented Web Mining Toolkit},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102266},
doi = {10.1145/3102254.3102266},
abstract = {Searching and retrieving information from the Web is a primary activity needed to monitor the development and usage of Web resources. Possible benefits include improving user experience (e.g. by optimizing query results) and enforcing data/user security (e.g. by identifying harmful websites). Motivated by the lack of ready-to-use solutions, in this paper we present a flexible and accessible toolkit for structure and content mining, able to crawl, download, extract and index resources from the Web. While being easily configurable to work in the "surface" Web, our suite is specifically tailored to explore the Tor dark Web, i.e. the ensemble of Web servers composing the world's most famous darknet. Notably, the toolkit is not just a Web scraper, but it includes two mining modules, respectively able to prepare content to be fed to an (external) semantic engine, and to reconstruct the graph structure of the explored portion of the Web. Other than discussing in detail the design, features and performance of our toolkit, we report the findings of a preliminary run over Tor, that clarify the potential of our solution.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {19},
numpages = {10},
keywords = {tor web graph, dark web},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3310986.3311022,
author = {Ren, Hao and Wang, Dong},
title = {TRRS: Temporal Recurrent Recommender System Based on Time-Sync Comments},
year = {2019},
isbn = {9781450366120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310986.3311022},
doi = {10.1145/3310986.3311022},
abstract = {Recent years has witnessed great emerge of online video websites, including the exploded number of videos and users. As a result, there appears a lot of personlized recommender systems. However there remain some challenging problems to tackle such as cold start problem, which scientists have made use of all kinds of sideinformation, e.g. gender, age or comments, to release. Currently a new type of video comments, called TSCs (TSC), plays a more and more important role in video watching activity. In this paper we utilize TSC to recommend videos for users. We developed a deep nueral network model called Temporal Recurrent Recommder System (TRRS) which combine multi-layers neural network to extract feature for users and videos. The first layer convert TSC to embeddings, then RNN layer analyze each comment from user or video, and fianlly the merge layer combine all output from prior layer and produce the feature. We use the feature from the network for users and videos to make personlized recommendation.},
booktitle = {Proceedings of the 3rd International Conference on Machine Learning and Soft Computing},
pages = {123–127},
numpages = {5},
keywords = {Deep Learning, Recommendation, Time-sync Comments},
location = {Da Lat, Viet Nam},
series = {ICMLSC 2019}
}

@inproceedings{10.1145/3231622.3231641,
author = {Kucher, Kostiantyn and Martins, Rafael M. and Kerren, Andreas},
title = {Analysis of VINCI 2009-2017 Proceedings},
year = {2018},
isbn = {9781450365017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231622.3231641},
doi = {10.1145/3231622.3231641},
abstract = {Both the metadata and the textual contents of scientific publications can provide us with insights about the development and the current state of the corresponding scientific community. In this short paper, we take a look at the proceedings of VINCI from the previous years and conduct several types of analyses. We summarize the yearly statistics about different types of publications, identify the overall authorship statistics and the most prominent contributors, and analyze the current community structure with a co-authorship network. We also apply topic modeling to identify the most prominent topics discussed in the publications. We hope that the results of our work will provide insights for the visualization community and will also be used as an overview for researchers previously unfamiliar with VINCI.},
booktitle = {Proceedings of the 11th International Symposium on Visual Information Communication and Interaction},
pages = {97–101},
numpages = {5},
keywords = {meta-analysis, survey, visualization, scientific literature, overview, topic modeling},
location = {V\"{a}xj\"{o}, Sweden},
series = {VINCI '18}
}

@inproceedings{10.1145/3184558.3191583,
author = {Wang, Jia and Feng, Yungang and Naghizade, Elham and Rashidi, Lida and Lim, Kwan Hui and Lee, Kate},
title = {Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191583},
doi = {10.1145/3184558.3191583},
abstract = {Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1401–1405},
numpages = {5},
keywords = {social networks, sentiment analysis, location recommendation},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3178876.3186019,
author = {Zhang, Yue and Ramesh, Arti and Golbeck, Jennifer and Sridhar, Dhanya and Getoor, Lise},
title = {A Structured Approach to Understanding Recovery and Relapse in AA},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186019},
doi = {10.1145/3178876.3186019},
abstract = {Alcoholism, also known as Alcohol Use Disorder (AUD), is a serious problem affecting millions of people worldwide. Recovery from AUD is known to be challenging and often leads to relapse at various points after enrolling in a rehabilitation program such as Alcoholics Anonymous (AA). In this work, we take a structured approach to understand recovery and relapse from AUD using social media data. To do so, we combine linguistic and psychological attributes of users with relational features that capture useful structure in the user interaction network. We evaluate our models on AA-attending users extracted from the Twitter social network and predict recovery at two different points---90 days and 1 year after the user joins AA, respectively. Our experiments reveal that our structured approach is helpful in predicting recovery in these users. We perform extensive quantitative analysis of different groups of features and dependencies among them. Our analysis sheds light on the role of each feature group and how they combine to predict recovery and relapse. Finally, we present a qualitative analysis of the different reasons behind users relapsing to AUD. Our models and analysis are helpful in making meaningful predictions in scenarios where only a subset of features are available and can potentially be helpful in identifying and preventing relapse early.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1205–1214},
numpages = {10},
keywords = {probabilistic graphical models, social media analysis, recovery from alcoholism, alcoholics anonymous},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3459637.3482075,
author = {Chen, Chung-Chi and Huang, Hen-Hsen and Huang, Yu-Lieh and Chen, Hsin-Hsi},
title = {Constructing Noise Free Economic Policy Uncertainty Index},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482075},
doi = {10.1145/3459637.3482075},
abstract = {The economic policy uncertainty (EPU) index is one of the important text-based indexes in finance and economics fields. The EPU indexes of more than 26 countries have been constructed to reflect the policy uncertainty on country-level economic environments and serve as an important economic leading indicator. The EPU indexes are calculated based on the number of news articles with some manually-selected keywords related to economic, uncertainty, and policy. We find that the keyword-based EPU indexes contain noise, which will influence their explainability and predictability. In our experimental dataset, over 40% of news articles with the selected keywords are not related to the EPU. Instead of using keywords only, our proposed models take contextual information into account and get good performance on identifying the articles unrelated to EPU. The noise free EPU index performs better than the keyword-based EPU index in both explainability and predictability.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2915–2919},
numpages = {5},
keywords = {economic index, document filtering, economic policy uncertainty, denoise},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3158354.3158355,
author = {Sen, Anirban and Sinha, Manjira and Mannarswamy, Sandya},
title = {Improving Similar Question Retrieval Using a Novel Tripartite Neural Network Based Approach},
year = {2017},
isbn = {9781450363822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3158354.3158355},
doi = {10.1145/3158354.3158355},
abstract = {Collective intelligence of the crowds is distilled together in various Community Question Answering (CQA) Services such as Quora, Yahoo Answers, Stack Overflow forums, wherein users share their knowledge, providing both informational and experiential support to other users. As users often search for similar information, probabilities are high that for a new incoming question, there is a related question-answer pair existing in the CQA dataset. Therefore, an efficient technique for similar question identification is need of the hour. While data is not a bottleneck in this scenario, addressing the vocabulary diversity generated by a variety pool of users certainly is. This paper proposes a novel tripartite neural network based approach towards the similar question retrieval problem. The network takes inputs in the form of question-answer and new question triplet and learns internal representations from similarities among them. Our approach achieves classification performances upto 77% on a real world CQA dataset.We have also compared our method with two other baselines and found that it performs significantly better in handling the problem of vocabulary diversity and 'zero-lexical overlap' among questions.},
booktitle = {Proceedings of the 9th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {1–5},
numpages = {5},
keywords = {tripartite neural network, CQA, similar question retrieval},
location = {Bangalore, India},
series = {FIRE'17}
}

@inproceedings{10.1145/3132847.3133123,
author = {Huang, Longtao and Zhao, Lin and Lv, Shangwen and Lu, Fangzhou and Zhai, Yue and Hu, Songlin},
title = {KIEM: A Knowledge Graph Based Method to Identify Entity Morphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133123},
doi = {10.1145/3132847.3133123},
abstract = {An entity on the web can be referred by numerous morphs that are always ambiguous, implicit and informal, which makes it challenging to accurately identify all the morphs corresponding to a specific entity. In this paper, we introduce a novel method based on knowledge graph, which takes advantage of both knowledge reasoning and statistic learning. First, we present a model to build a knowledge graph for the given entity. The knowledge graph integrates the fragmented knowledge on how humans create morphs. Then, the candidate morphs are generated based on the rules summarized from the knowledge graph. At last, we use a classification method to filter the useless candidates and identify the target morphs. The experiments conducted on real world dataset demonstrate efficiency of our proposed method in terms of precision and recall.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2111–2114},
numpages = {4},
keywords = {entity morphs, web mining, language understanding, knowledge graph},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3269206.3271728,
author = {Ge, Songwei and Dou, Zhicheng and Jiang, Zhengbao and Nie, Jian-Yun and Wen, Ji-Rong},
title = {Personalizing Search Results Using Hierarchical RNN with Query-Aware Attention},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271728},
doi = {10.1145/3269206.3271728},
abstract = {Search results personalization has become an effective way to improve the quality of search engines. Previous studies extracted information such as past clicks, user topical interests, query click entropy and so on to tailor the original ranking. However, few studies have taken into account the sequential information underlying previous queries and sessions. Intuitively, the order of issued queries is important in inferring the real user interests. And more recent sessions should provide more reliable personal signals than older sessions. In addition, the previous search history and user behaviors should influence the personalization of the current query depending on their relatedness. To implement these intuitions, in this paper we employ a hierarchical recurrent neural network to exploit such sequential information and automatically generate user profile from historical data. We propose a query-aware attention model to generate a dynamic user profile based on the input query. Significant improvement is observed in the experiment with data from a commercial search engine when compared with several traditional personalization models. Our analysis reveals that the attention model is able to attribute higher weights to more related past sessions after fine training.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {347–356},
numpages = {10},
keywords = {query-aware attention, hierarchical recurrent neural network, search results personalization},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.5555/3381089.3381097,
author = {Bhattacharyya, Chiranjib and Kannan, Ravindran},
title = {Finding a Latent K---Simplex in O* (k · Nnz(Data)) Time via Subset Smoothing},
year = {2020},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {In this paper we show that the learning problem for a large class of Latent variable models, such as Mixed Membership Stochastic Block Models, Topic Models, and Adversarial Clustering can be posed geometrically as follows: find a latent k--- vertex simplex, K in Rd, given n data points, each obtained by perturbing a latent point in K. This problem does not seem to have been addressed. Our main contribution is an efficient algorithm for the geometric problem under deterministic assumptions which naturally hold for the models considered here.We observe that for a suitable r ≤ n, K is close to a data-determined polytope K' (the subset smoothed polytope) which is the convex hull of the [MATH HERE] points, each obtained by averaging an r subset of data points. Our algorithm is simply stated: it optimizes k carefully chosen linear functions over K' to find the k vertices of the latent simplex. The proof of correctness is more involved, drawing on existing and new tools from Numerical Analysis. Our overall runtime of O*(k nnz) is as good as the best times of existing algorithms (modulo O*(1) factor) for the special cases and is better for sparse data which is the norm in Topic Modelling and Mixed Membership models. Some consequences of our algorithm are:• Mixed Membership Models and Topic Models: We give the first quasi-input-sparsity time algorithm for parameter estimation for k ∈ O*(1)• Adversarial Clustering: In k---means, an adversary is allowed to move many data points from each cluster towards the convex hull of other cluster centers. Our algorithm still estimates cluster centers well.},
booktitle = {Proceedings of the Thirty-First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {122–140},
numpages = {19},
location = {Salt Lake City, Utah},
series = {SODA '20}
}

@inproceedings{10.1145/3404835.3463080,
author = {Mukherjee, Rajdeep and Naik, Atharva and Poddar, Sriyash and Dasgupta, Soham and Ganguly, Niloy},
title = {Understanding the Role of Affect Dimensions in Detecting Emotions from Tweets: A Multi-Task Approach},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463080},
doi = {10.1145/3404835.3463080},
abstract = {We propose VADEC, a multi-task framework that exploits the correlation between the categorical and dimensional models of emotion representation for better subjectivity analysis. Focusing primarily on the effective detection of emotions from tweets, we jointly train multi-label emotion classification and multi-dimensional emotion regression, thereby utilizing the inter-relatedness between the tasks. Co-training especially helps in improving the performance of the classification task as we outperform the strongest baselines with 3.4%, 11%, and 3.9% gains in Jaccard Accuracy, Macro-F1, and Micro-F1 scores respectively on the AIT dataset [17]. We also achieve state-of-the-art results with 11.3% gains averaged over six different metrics on the SenWave dataset [27]. For the regression task, VADEC, when trained with SenWave, achieves 7.6% and 16.5% gains in Pearson Correlation scores over the current state-of-the-art on the EMOBANK dataset [5] for the Valence (V) and Dominance (D) affect dimensions respectively. We conclude our work with a case study on COVID-19 tweets posted by Indians that further helps in establishing the efficacy of our proposed solution.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2303–2307},
numpages = {5},
keywords = {twitter, valence-arousal-dominance, fine-grained emotion analysis, multi-task learning, COVID, coarse-grained emotion analysis},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3372422.3372449,
author = {Kawawa, Yota and Onuma, Ryo and Nakayama, Hiroki and Kaminaga, Hiroaki and Miyadera, Youzou and Nakamura, Shoichi},
title = {A System for Cultivating Exploration Skills by Presenting Clues Based on the Analysis of Page Selection Behaviors},
year = {2019},
isbn = {9781450372596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372422.3372449},
doi = {10.1145/3372422.3372449},
abstract = {Activities performed on the Web have becoming increasingly more complicated. In such activities, the importance of Web exploration through trial and error has increased. To satisfactorily conduct such exploration, it is important for users to comprehend the process of Web exploration and reflect on their search intentions when constructing queries. However, such tasks are difficult for inexperienced people. In this research, we focus on searchers' behaviors of page selection in the Web exploration process. On the basis of behavior analysis, we have developed methods to extract the clues so that unskilled searchers can become aware of the evaluation against pages they acquired and utilize them in further searches. We develop a novel mechanism for cultivating exploration skills by presenting clues in accordance with a searcher's degree of Web exploration experience. In this paper, we describe the design of a support system based on our methods and the characteristics of our prototype with some snapshots.},
booktitle = {Proceedings of the 2019 2nd International Conference on Computational Intelligence and Intelligent Systems},
pages = {134–139},
numpages = {6},
keywords = {Page selection, Evaluation against pages, Cultivating exploration skills, Search intention},
location = {Bangkok, Thailand},
series = {CIIS 2019}
}

@inproceedings{10.1145/3093241.3093246,
author = {Adachi, Yusuke and Onimura, Naoya and Yamashita, Takanori and Hirokawa, Sachio},
title = {Classification of Imbalanced Documents by Feature Selection},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093246},
doi = {10.1145/3093241.3093246},
abstract = {We previously worked on category classification problem of reuter 's newspaper article using SVM and feature selection. In the study, feature selection by SVM-score [Sakai, Hirokawa, 2012] showed high accuracy. It was also expected to be superior to other standard indicators in case data is imbalanced. This study aimed to show the effectiveness of feature selection by SVM-score in machine learning with imbalanced data. For the reuter's data, F-measure was calculated in the classification experiment of all 13 categories. As a result, feature selection by SVM-score shows high f-measure and precision. In addition, we found feature words of negative example improve the classification performance.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {228–232},
numpages = {5},
keywords = {SVM, reuters, feature selection, text mining},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/3196398.3196439,
author = {Moslehi, Parisa and Adams, Bram and Rilling, Juergen},
title = {Feature Location Using Crowd-Based Screencasts},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196439},
doi = {10.1145/3196398.3196439},
abstract = {Crowd-based multi-media documents such as screencasts have emerged as a source for documenting requirements of agile software projects. For example, screencasts can describe buggy scenarios of a software product, or present new features in an upcoming release. Unfortunately, the binary format of videos makes traceability between the video content and other related software artifacts (e.g., source code, bug reports) difficult. In this paper, we propose an LDA-based feature location approach that takes as input a set of screencasts (i.e., the GUI text and/or spoken words) to establish traceability link between the features described in the screencasts and source code fragments implementing them. We report on a case study conducted on 10 WordPress screencasts, to evaluate the applicability of our approach in linking these screencasts to their relevant source code artifacts. We find that the approach is able to successfully pinpoint relevant source code files at the top 10 hits using speech and GUI text. We also found that term frequency rebalancing can reduce noise and yield more precise results.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {192–202},
numpages = {11},
keywords = {feature location, crowd-based documentation, mining video content, information extraction, software traceability},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3495018.3495483,
author = {Zhang, Hanyue},
title = {A Comparative Algorithm of the Similarity in “Blank” Concept for Chinese and Western Poetics in the Context of Internet},
year = {2021},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495483},
doi = {10.1145/3495018.3495483},
abstract = {Traditional cross-language similarity assessment techniques mostly rely on the theories of linguistics and pragmatics, which is also bound up with the natural features of "natural language". This paper mainly studies the similarity comparison algorithm of "blank" concept in Chinese and Western poetics under the background of Internet. In this paper, a sentence-level cross-language similarity assessment framework (SCLSE) is proposed. The framework is based on word embedding as the underlying vector representation, which is used to learn the semantic representation of sentences through the fusion of various neural network structures, and finally outputs the similarity score of sentences. In this paper, we also divide the short text into paragraphs and treat the paragraphs as long sentences as sequence input to realize the iterative calculation of similarity on a larger scale. In this paper, we set up different comparative experiments to verify the effectiveness and application value of SCLSE framework in the cross-language text similarity assessment task under different text unit granularity.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1778–1782},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3488838.3488871,
author = {Yang, Wanchun and Zhang, Shurui and Zhang, Bozheng},
title = {Medical Assistant Diagnosis Method Based on Graph Neural Network and Attention Mechanism},
year = {2021},
isbn = {9781450384094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488838.3488871},
doi = {10.1145/3488838.3488871},
abstract = {The electronic medical record contains the patient description information, which can help doctors quickly make the initial clinical diagnosis when facing similar patients. Aiming at the problem of clinical intelligent aided diagnosis, a method based on graph neural network and attention mechanism is proposed. The graph neural network is used to model the label space, and then label classifier based on attention mechanism is used to classify. In the label space modeling, combined with the hierarchical structure of international classification of diseases (ICD), the structure information of label space is integrated into the model. The experimental results show that, compared with the traditional text classification modeling method, the proposed method has a significant improvement in classification performance.},
booktitle = {2021 The 3rd World Symposium on Software Engineering},
pages = {194–198},
numpages = {5},
keywords = {Attention mechanism, Deep learning, Graph attention network, Disease classification},
location = {Xiamen, China},
series = {WSSE 2021}
}

@inproceedings{10.1145/3446999.3447012,
author = {Gorro, Ken D. and Ali, Moustafa F. and Gorro, Kim D. and Ancheta, Jeffrey Rosario},
title = {Exploring Natural Language Processing Techniques in Social Media Analysis during a Pandemic: Understanding a Corpus of Facebook Posts Using Word2vec and LDA},
year = {2020},
isbn = {9781450388559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446999.3447012},
doi = {10.1145/3446999.3447012},
abstract = {People around the world have used social media extensively to communicate and express opinions especially during this time of the rapid spread of COVID-19. Nowadays, the various narratives of social media users are important that can be used in creating measures to curb the deadly disease. However, the manual collection of data from social media such as Facebook and its analysis can take time. Thus, this study attempted to use natural language processing (NLP) techniques such as topic modeling and word embedding to identify the concepts contained in the posts and comments of Facebook users in the Philippines regarding the pandemic. This study harvested posts and comments in Facebook groups that are primarily Filipino citizens that express opinions and suggestions in COVID-19 responses. Using Latent Dirichlet Allocation (LDA), this study was able to generate 10 topics related to the concepts of (1) self-discipline, (2) prayers for the frontliners, (3) total lockdown, (4) following government guidelines and protocols, and (5) flattening the curve of the disease. Meanwhile, word groups generated by Word2vec developed concepts such as (1) mass testing, (2) hope for faster recovery, and (3) expectation from the government. The average cosine similarity for word groups is 0.92, which implies strong relatedness of each word per group. This study proved that the use of NLP techniques helped in analyzing the themes of Facebook posts and comments related to the pandemic.},
booktitle = {2020 The 8th International Conference on Information Technology: IoT and Smart City},
pages = {69–73},
numpages = {5},
keywords = {Pandemic, Facebook, Social media, COVID-19},
location = {Xi'an, China},
series = {ICIT 2020}
}

@inproceedings{10.1145/3149572.3149591,
author = {Zhen-wu, Wang and Xiao-hui, Han and Hao-ming, Tian},
title = {A Novel Friend Recommendation Algorithm Based on Intimacy and LDA Model},
year = {2017},
isbn = {9781450353373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149572.3149591},
doi = {10.1145/3149572.3149591},
abstract = {With the development of Internet, various social network service (SNS) platforms appeared, such as Facebook, twitter, Flickr, Sina microblog, and so on. Friend recommendation is the key issue for the SNS which can enhance the interactivity among SNS users.A novel recommendation algorithm is proposed in this paper, it applies time line to compute the interactions among target user and his/her recommended friends firstly, which predicts the intimacy trend and fits intimacy with interactive information at different time slots; then a Latent Dirichlet Allocation (LDA) model is used to generate subjects and judge the subject similarities between target user and recommended friends, at last, the two parts have been combined by an information entropy method which adjust the weight information dynastically during the friend recommendation process. Compared with collaborative filtering recommendation algorithm and LDA method, the experimental results proved that the proposed algorithm has got better performance.},
booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
pages = {128–132},
numpages = {5},
keywords = {LDA, Friend Recommendation, Collaborative Filtering, Intimacy, Information Entropy, Social Network Service},
location = {Barcelona, Spain},
series = {ICIME 2017}
}

@inproceedings{10.1145/3110025.3120987,
author = {Saravia, Elvis and Wu, Shao-Chen and Chen, Yi-Shin},
title = {A Dynamic Influence Keyword Model for Identifying Implicit User Interests on Social Networks},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3120987},
doi = {10.1145/3110025.3120987},
abstract = {The rapid growth of social networks have enabled users to instantly share what is happening around them. With the character-limitation and other feature constraints imposed by microblogs, users are obliged to express their intentions in implicit forms. This behavior poses many challenges for contextual approaches that aim to identify user intentions. Furthermore, users have the tendency to display different degree of preferences towards specific interests, simultaneously in time, making it difficult for models to rank the discovered interests. We propose a dynamic interest keyword model, a graph-based ranking mechanism, that identifies the different degrees of interests of a user. Our results show that the proposed system detects human-inferred interests, 94% of the time, showing that the model is feasible and contributes various insights that can be used to improve user intention identification systems.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1160–1163},
numpages = {4},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3397536.3422233,
author = {Dubey, Manisha and Srijith, P.K. and Desarkar, Maunendra Sankar},
title = {HAP-SAP: Semantic Annotation in LBSNs Using Latent Spatio-Temporal Hawkes Process},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422233},
doi = {10.1145/3397536.3422233},
abstract = {The prevalence of location-based social networks (LBSNs) has eased the understanding of human mobility patterns. However, categories which act as semantic characterization of the location, might be missing for some check-ins and can adversely affect modelling the mobility dynamics of users. At the same time, mobility patterns provide cues on the missing semantic categories. In this paper, we simultaneously address the problem of semantic annotation of locations and location adoption dynamics of users. We propose our model HAP-SAP, a latent spatio-temporal multivariate Hawkes process, which considers latent semantic category influences, and temporal and spatial mobility patterns of users. The inferred semantic categories can supplement our model on predicting the next check-in events by users. Our experiments on real datasets demonstrate the effectiveness of the proposed model for the semantic annotation and location adoption modelling tasks.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {377–380},
numpages = {4},
keywords = {Spatio-Temporal Hawkes Process, Semantic Annotation, Location-based Social Networks},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/3321408.3322835,
author = {Zhang, Xiaoyan and Mao, Yue and Meng, Xiangfu},
title = {A Spatial Clustering Approach by Combining the Geo-Social-Comment Relationships of POIs},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3322835},
doi = {10.1145/3321408.3322835},
abstract = {Existing spatial clustering approaches mainly consider the location distance (or/and social relationship) between spatial objects while ignore the user comment relationship between them. This paper proposes a spatial clustering method which combines the location, social, and comment relationships of spatial objects. First, the spatial object's Geographic-Social-Comment (GSC) relation model is constructed. And then, the methods for evaluating the social relationships and comment similarities between spatial objects are proposed, respectively. To improve the efficiency of the spatial clustering algorithm, a novel index structure and the corresponding implementation algorithm are also presented. The experimental results demonstrate that the spatial clustering results obtained by our method are more reasonable, the social connections and comment similarities within the cluster are more closely.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {55},
numpages = {5},
keywords = {geo-social-comment (GSC) relation model, density-based clustering, adjacency list index, spatial clustering},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3279996.3280022,
author = {Hawashin, Bilal and Mansour, Ayman and Kanan, Tarek and Fotouhi, Farshad},
title = {An Efficient Cold Start Solution Based on Group Interests for Recommender Systems},
year = {2018},
isbn = {9781450365369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279996.3280022},
doi = {10.1145/3279996.3280022},
abstract = {This paper proposes an efficient solution for the cold start problem in recommender systems. This problem occurs with new users who do not have sufficient information in their records. This would cause the recommender system to fail in providing recommendations to these users. This problem is one of the common and important problems in recommender systems. Although some solutions have been proposed to solve it in the literature, these solutions would not work properly in some scenarios because they do not concentrate on finding the actual interests of the users and the hidden motives behind their behavior. Our proposed solution uses the hidden interests of the group to which the target user belongs to provide recommendations for that user. The experiments show that our proposed solution is efficient in terms of searching time and space consumption.},
booktitle = {Proceedings of the First International Conference on Data Science, E-Learning and Information Systems},
articleno = {26},
numpages = {5},
keywords = {cold start problem, user interest, group interest, recommender systems, content based filtering, machine learning},
location = {Madrid, Spain},
series = {DATA '18}
}

@inproceedings{10.1145/3077136.3080703,
author = {Takeda, Naoto and Seki, Yohei and Morishita, Mimpei and Inagaki, Yoichi},
title = {Evolution of Information Needs Based on Life Event Experiences with Topic Transition},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080703},
doi = {10.1145/3077136.3080703},
abstract = {We propose a method to clarify the evolution of users' information needs related to a user's interests and actions based upon life events such as "childbirth." First, we extract topic transitions using dynamic topic models from blogs posted by users who have experienced life events. Next, we select the topics by computing the differences in topic probabilities before and after the life event. We evaluated our method based on three life events: "childbirth," "finding employment," and "marriage." Our method selected life event-relevant topics such as "child development," "working life," and "wedding ceremony." We found mothers' information needs such as "how to introduce baby food," employees' information needs such as "preparing an induction programme," and couples' information needs such as "wedding reception planning" in each topic.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1009–1012},
numpages = {4},
keywords = {information needs, life event, dtms (dynamic topic models)},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3095140.3095174,
author = {Hu, Xiaoyan and Xie, Shunbo},
title = {Efficient and Robust Motion Segmentation via Adaptive Similarity Metric},
year = {2017},
isbn = {9781450352284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095140.3095174},
doi = {10.1145/3095140.3095174},
abstract = {This paper introduces an efficient and robust method that segments long motion capture data into distinct behaviors. The method is unsupervised, and is fully automatic. We first apply spectral clustering on motion affinity matrix to get a rough segmentation. We combined two statistical filters to remove the noises and get a good initial guess on the cut points as well as on the number of segments. Then, we analyzed joint usage information within each rough segment and recomputed an adaptive affinity matrix for the motion. Applying spectral clustering again on this adaptive affinity matrix produced a robust and accurate segmentation compared with the ground-truth. The experiments showed that the proposed approach outperformed the available methods on the CMU Mocap database.},
booktitle = {Proceedings of the Computer Graphics International Conference},
articleno = {34},
numpages = {6},
keywords = {adaptive similarity metric, spectral clustering, statistical filtering, variance reweighting, motion segmentation},
location = {Yokohama, Japan},
series = {CGI '17}
}

@inproceedings{10.1145/3019612.3019762,
author = {Azzam, Amr and Tazi, Neamat and Hossny, Ahmad},
title = {Text-Based Question Routing for Question Answering Communities via Deep Learning},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019762},
doi = {10.1145/3019612.3019762},
abstract = {Online Communities for Question Answering (CQA) such as Quora and Stack Overflow face the challenge of providing sufficient answers for the questions asked by users. The exponential growing rate of the unanswered questions compromises the effectiveness of the CQA frameworks as knowledge sharing platforms. The main reason for this issue is the inefficient routing of the questions to the potential answerers, who are the field experts and interested users.This paper proposes the deep-learning-based technique QR-DSSM to increase the accuracy of the question routing process. This technique uses deep semantic similarity model (DSSM) to extract semantic similarity features using deep neural networks and use the features to rank users' profiles. QR-DSSM maps the asked questions and the profiles of the users into a latent semantic space where the ability to answer is measured using the cosine similarity between the questions and the profiles of the users. QR-DSSM experiments outperformed the baseline models such as LDA, SVM, and Rank-SVM techniques and achieved an MRR score of 0.1737.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1674–1678},
numpages = {5},
keywords = {question routing, deep learning, community question answering, semantic modeling},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3209978.3209984,
author = {Piccardi, Tiziano and Catasta, Michele and Zia, Leila and West, Robert},
title = {Structuring Wikipedia Articles with Section Recommendations},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3209984},
doi = {10.1145/3209978.3209984},
abstract = {Sections are the building blocks of Wikipedia articles. They enhance readability and can be used as a structured entry point for creating and expanding articles. Structuring a new or already existing Wikipedia article with sections is a hard task for humans, especially for newcomers or less experienced editors, as it requires significant knowledge about how a well-written article looks for each possible topic. Inspired by this need, the present paper defines the problem of section recommendation for Wikipedia articles and proposes several approaches for tackling it. Our systems can help editors by recommending what sections to add to already existing or newly created Wikipedia articles. Our basic paradigm is to generate recommendations by sourcing sections from articles that are similar to the input article. We explore several ways of defining similarity for this purpose (based on topic modeling, collaborative filtering, and Wikipedia's category system). We use both automatic and human evaluation approaches for assessing the performance of our recommendation system, concluding that the category-based approach works best, achieving precision@10 of about 80% in the human evaluation.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {665–674},
numpages = {10},
keywords = {wikipedia, category network, sections, recommender system},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3321408.3322849,
author = {Kang, Wenying and Zhang, Lei and Li, Bo and Chen, Jing and Sun, Xia and Feng, Jun},
title = {Personalized Exercise Recommendation via Implicit Skills},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3322849},
doi = {10.1145/3321408.3322849},
abstract = {Cognitive diagnosis methods need to assess the students' skills to provide personalized exercise recommendation. To perform this assessment, an initially hand built Q-matrix are presented to students, which would affect the recommendation results in intelligence education. However, very few previous studies have examined this exercise recommendation task on utilizing the implicit skills among exercises, in which the opinions of implicit skill might carry extra specific knowledge. We propose a data-driven frame to reconstruct Q-matrix automatically from implicit skills perspective and explore the utility of Dynamic Key-Value Memory Networks to solve this task. Experimental results demonstrate that our method has a guiding significance in pedagogical theory.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {77},
numpages = {6},
keywords = {dynamic key-value memory networks, Q-matrix, personalized exercise recommendation},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3290420.3290473,
author = {Zeng, Qingtian and Zhao, Hua and Duan, Hua and Li, Chao and Ni, Weijian and Xie, Nengfu and Diao, Xiuli},
title = {State-of-Art: Text Similarity Computing},
year = {2018},
isbn = {9781450365345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290420.3290473},
doi = {10.1145/3290420.3290473},
abstract = {In recent years, there have been extensive studies and rapid progresses in text similarity computing that is one of the host and important techniques in many NLP applications. This paper first introduces the background, the basic computing process, the related resources and the techniques of text similarity computing. By comparing several typical models, three key issues about text similarity computing are addressed in details which include text representation model, the similarity calculation and the quality evaluation. The typical applications of text similarity computing are addressed. Finally, the difficulties to compute the text similarity and many future research directions are discussed..},
booktitle = {Proceedings of the 4th International Conference on Communication and Information Processing},
pages = {33–37},
numpages = {5},
keywords = {natural language processing, text representation, text similarity computing, knowledge acquisition},
location = {Qingdao, China},
series = {ICCIP '18}
}

@article{10.1145/3448256,
author = {Wang, Jingyuan and Lin, Xin and Zuo, Yuan and Wu, Junjie},
title = {DGeye: Probabilistic Risk Perception and Prediction for Urban Dangerous Goods Management},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3448256},
doi = {10.1145/3448256},
abstract = {Recent years have witnessed the emergence of worldwide megalopolises and the accompanying public safety events, making urban safety a top priority in modern urban management. Among various threats, dangerous goods such as gas and hazardous chemicals transported through cities have bred repeated tragedies and become the deadly “bomb” we sleep with every day. While tremendous research efforts have been devoted to dealing with dangerous goods transportation (DGT) issues, further study is still in great need to quantify this problem and explore its intrinsic dynamics from a big data perspective. In this article, we present a novel system called DGeye, to feature a fusion between DGT trajectory data and residential population data for dangers perception and prediction. Specifically, DGeye first develops a probabilistic graphical model-based approach to mine spatio-temporally adjacent risk patterns from population-aware risk trajectories. Then, DGeye builds the novel causality network among risk patterns for risk pain-point identification, risk source attribution, and online risky state prediction. Experiments on both Beijing and Tianjin cities demonstrate the effectiveness of DGeye in real-life DGT risk management. As a case in point, our report powered by DGeye successfully drove the government to lay down gas pipelines for the famous Guijie food street in Beijing.},
journal = {ACM Trans. Inf. Syst.},
month = {may},
articleno = {28},
numpages = {30},
keywords = {dangerous goods transportation, Urban safety, risk management, risk causal network, risk pattern}
}

@inproceedings{10.1145/3487553.3524657,
author = {Berrebbi, Dan and Huynh, Nicolas and Balalau, Oana},
title = {GraphCite: Citation Intent Classification in Scientific Publications via Graph Embeddings},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524657},
doi = {10.1145/3487553.3524657},
abstract = {Citations are crucial in scientific works as they help position a new publication. Each citation carries a particular intent, for example, to highlight the importance of a problem or to compare against results provided by another method. The authors’ intent when making a new citation has been studied to understand the evolution of a field over time or to make recommendations for further citations. In this work, we address the task of citation intent prediction from a new perspective. In addition to textual clues present in the citation phrase, we also consider the citation graph, leveraging high-level information of citation patterns. In this novel setting, we perform a thorough experimental evaluation of graph-based models for intent prediction. We show that our model, GraphCite, improves significantly upon models that take into consideration only the citation phrase. Our code is available online1.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {779–783},
numpages = {5},
keywords = {graph neural network, citation intent classification},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3508546.3508623,
author = {Liu, Lijuan and Min, Zongru and Chen, Yang},
title = {Web Specific Information Detection Based on Features and Deep Neural Network},
year = {2021},
isbn = {9781450385053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508546.3508623},
doi = {10.1145/3508546.3508623},
abstract = {Internet information is spreading rapidly, and the task of detecting web specific information is arduous. To solve this problem, this paper proposes a web specific information detection method that combines features and deep neural network (DNN). It combines text features and uses a DNN model to analyze and calculate topic relevance, so as to improve the quality of web page information detection for specific topics. By a feature-based web specific information extraction algorithm which focuses on the two dimensions of web page characteristics and structure web behavior characteristics, it reduces the omission of web page information, improves the effect of text classification and analyzes the result comprehensively. The DNN model of layered architecture is described to explain the main principles of the process, which reduces the complexity of system operation. Practical application result shows that this method extracts web page information on specific topics more accurately, improves the accuracy and recall rate of web specific information extraction, and improves the efficiency of web specific information extraction quality.},
booktitle = {2021 4th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {77},
numpages = {7},
keywords = {DNN, Information detection, sematic analysis, crawler},
location = {Sanya, China},
series = {ACAI'21}
}

@inproceedings{10.1145/3278252.3278267,
author = {Hajek, Petr and Barushka, Aliaksandr},
title = {Integrating Sentiment Analysis and Topic Detection in Financial News for Stock Movement Prediction},
year = {2018},
isbn = {9781450365451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278252.3278267},
doi = {10.1145/3278252.3278267},
abstract = {Media-expressed information in financial news are critical for stock market prediction. Nevertheless, researchers have primarily focused on the role of sentiment analysis in predicting stock returns and volatility. Here we show that topics discussed in the financial news may carry additional important information. We use a combination of sentiment analysis (using finance-specific dictionary-based approach) and topic detection (using latent dirichlet allocation) to predict one-day-ahead stock movements of major US companies. The proposed system employs a deep neural network to model complex stock market relations. We demonstrate the effectiveness of this approach compared to baselines, such as support vector machines and sentiment- and topic-based models used separately.},
booktitle = {Proceedings of the 2nd International Conference on Business and Information Management},
pages = {158–162},
numpages = {5},
keywords = {financial news, Sentiment analysis, stock movement, topic detection},
location = {Barcelona, Spain},
series = {ICBIM '18}
}

@inproceedings{10.1145/3180374.3181351,
author = {Zhang, MinYi and Zhang, Pingjian},
title = {Academic Circle Mining Based on the Citation Relationship},
year = {2018},
isbn = {9781450354318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180374.3181351},
doi = {10.1145/3180374.3181351},
abstract = {Academic circle are groups of similar scholars in academic networks. The academic circle also called community circle. A reliable and efficient community discovery method can play an important role in the communication of academic field. At present, there is a lack of universal judgment index and authoritative division of academic community in the academic field, and it is difficult to judge the reliability of the algorithm. This paper improves the classic DBSCAN (Density-Based Spatial Clustering of Applications with Noise) Clustering Algorithm by according to the characteristics of the data set itself to determine the input parameters of the algorithm, and propose a new algorithm called improved DBSCAN. The new algorithm is applied to community networks and has been demonstrated superior effectiveness over DBSCAN.},
booktitle = {Proceedings of the 2018 2nd International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {202–206},
numpages = {5},
keywords = {Academic circle, community circle, improved DBSCAN, clustering algorithm, DBSCAN},
location = {Wuhan, China},
series = {ICMSS 2018}
}

@inproceedings{10.1145/3109761.3109772,
author = {Mishra, Nitin and Mishra, Vimal and Chaturvedi, Saumya},
title = {Tools and Techniques for Solving Cold Start Recommendation},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3109772},
doi = {10.1145/3109761.3109772},
abstract = {Recommendation Systems are very important systems that saves users time and resources by saving them from searching the bulk data. The best example is googling which searches and gives list of hundreds of pages. Therefore, a major challenge of Recommendation Systems can be how to make recommendations for a new user, that is called cold-start user problem in this papers we are trying to identify different kinds of cold start problems in Recommendation Systems. We are also trying to explore different types of solutions to these problems in last 10 years. are very important systems that saves users time and resources by saving them from searching the bulk data. The best example is googling which searches and gives list of hundreds of pages. Therefore, a major challenge of Recommender systems can be how to make recommendations for a new user, that is called cold-start user problem in this papers we are trying to identify different kinds of cold start problems recommender systems. We are also trying to explore different types of solutions to these problems in last 10 years.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {11},
numpages = {6},
keywords = {recommendation system, content based methods, cold start problem, collaborative filtering},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@inproceedings{10.1145/3025171.3025223,
author = {Peltonen, Jaakko and Belorustceva, Kseniia and Ruotsalo, Tuukka},
title = {Topic-Relevance Map: Visualization for Improving Search Result Comprehension},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025223},
doi = {10.1145/3025171.3025223},
abstract = {We introduce topic-relevance map, an interactive search result visualization that assists rapid information comprehension across a large ranked set of results. The topic-relevance map visualizes a topical overview of the search result space as keywords with respect to two essential information retrieval measures: relevance and topical similarity. Non-linear dimensionality reduction is used to embed high-dimensional keyword representations of search result data into angles on a radial layout. Relevance of keywords is estimated by a ranking method and visualized as radiuses on the radial layout. As a result, similar keywords are modeled by nearby points, dissimilar keywords are modeled by distant points, more relevant keywords are closer to the center of the radial display, and less relevant keywords are distant from the center of the radial display. We evaluated the effect of the topic-relevance map in a search result comprehension task where 24 participants were summarizing search results and produced a conceptualization of the result space. The results show that topic-relevance map significantly improves participants' comprehension capability compared to a conventional ranked list presentation.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {611–622},
numpages = {12},
keywords = {visualization, dimensionality reduction, sense-making, exploratory search},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3469096.3469866,
author = {Rakib, Md Rashadul Hasan and Zeh, Norbert and Milios, Evangelos},
title = {Efficient Clustering of Short Text Streams Using Online-Offline Clustering},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469866},
doi = {10.1145/3469096.3469866},
abstract = {Short text stream clustering is an important but challenging task since massive amount of text is generated from different sources such as micro-blogging, question-answering, and social news aggregation websites. The two major challenges of clustering such massive amount of text is to cluster them within a reasonable amount of time and to achieve better clustering result. To overcome these two challenges, we propose an efficient short text stream clustering algorithm (called EStream) consisting of two modules: online and offline. The online module of EStream algorithm assigns a text to a cluster one by one as it arrives. To assign a text to a cluster it computes similarity between a text and a selected number of clusters instead of all clusters and thus significantly reduces the running time of the clustering of short text streams. EStream assigns a text to a cluster (new or existing) using the dynamically computed similarity thresholds. Thus EStream efficiently deals with the concept drift problem. The offline module of EStream algorithm enhances the distributions of texts in the clusters obtained by the online module so that the upcoming short texts can be assigned to the appropriate clusters.Experimental results demonstrate that EStream outperforms the state-of-the-art short text stream clustering methods (in terms of clustering result) by a statistically significant margin on several short text datasets. Moreover, the running time of EStream is several orders of magnitude faster than that of the state-of-the-art methods.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {5},
numpages = {10},
keywords = {text stream clustering, dynamic similarity threshold, offline clustering, online clustering},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.1145/3303772.3303823,
author = {Lu, Yihan and Hsiao, I-Han},
title = {Exploring Programming Semantic Analytics with Deep Learning Models},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303823},
doi = {10.1145/3303772.3303823},
abstract = {There are numerous studies have reported the effectiveness of example-based programming learning. However, less is explored recommending code examples with advanced Machine Learning-based models. In this work, we propose a new method to explore the semantic analytics between programming codes and the annotations. We hypothesize that these semantics analytics will capture mass amount of valuable information that can be used as features to build predictive models. We evaluated the proposed semantic analytics extraction method with multiple deep learning algorithms. Results showed that deep learning models outperformed other models and baseline in most cases. Further analysis indicated that in special cases, the proposed method outperformed deep learning models by restricting false-positive classifications.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {155–159},
numpages = {5},
keywords = {Coding concept detection, Text based classification, Semantic modeling, deep learning, Programming semantics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3106426.3115589,
author = {Marivate, Vukosi and Moorosi, Nyalleng},
title = {Employment Relations: A Data Driven Analysis of Job Markets Using Online Job Boards and Online Professional Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3115589},
doi = {10.1145/3106426.3115589},
abstract = {Data from online job boards and online professional networks present an opportunity to understand job markets as well as how professionals transition from one job/career to another. We propose a data driven approach to begin to understand a slice of the South African job market. We do this by analysing data from career websites as well as a South African online professional networks. Our goals are to be able to group jobs given their descriptions, characterise career paths as well as to have some building blocks to be able to extract job position hierarchies given a description.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1110–1113},
numpages = {4},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3269206.3271695,
author = {Wu, Libing and Quan, Cong and Li, Chenliang and Ji, Donghong},
title = {PARL: Let Strangers Speak Out What You Like},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271695},
doi = {10.1145/3269206.3271695},
abstract = {Review-based methods are one of the dominant methods to address the data sparsity problem of recommender system. However, the performance of most existing review-based methods will degrade when the review is also sparse. To this end, we propose a method to exploit user-item p air-dependent features from a uxiliary r eviews written by l ike-minded users (PARL) to address such problem. That is, both the reviews written by the user and the reviews written for the item are incorporated to highlight the useful features covered by the auxiliary reviews. PARL not only alleviates the sparsity problem of reviews but also produce extra informative features to further improve the accuracy of rating prediction. More importantly, it is designed as a plug-and-play model which can be plugged into various deep recommender systems to improve recommendations provided by them. Extensive experiments on five real-world datasets show that PARL achieves better prediction accuracy than other state-of-the-art alternatives. Also, with the exploitation of auxiliary reviews, the performance of PARL is robust on datasets with different characteristics.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {677–686},
numpages = {10},
keywords = {user reviews, deep learning, recommender system, rating prediction},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3173574.3174063,
author = {Ammari, Tawfiq and Schoenebeck, Sarita and Romero, Daniel M.},
title = {Pseudonymous Parents: Comparing Parenting Roles and Identities on the Mommit and Daddit Subreddits},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174063},
doi = {10.1145/3173574.3174063},
abstract = {Gender equality between mothers and fathers is critical for the social and economic wellbeing of children, mothers, and families. Over the past 50 years, gender roles have begun to converge, with mothers doing more work outside of the home and fathers doing more domestic work. However, popular parenting sites in the U.S. continue to be heavily gendered. We explore parenting roles and identities on the platform Reddit.com which is used by both mothers and fathers. We draw on seven years of data from three major parenting subreddits-Parenting, Mommit, and Daddit-to investigate what topics parents discuss on Reddit and how they vary across parenting subreddits. We find some similarities in topics across the three boards, such as sleep training, as well as differences, such as fathers talking about custody cases and Halloween. We discuss the role of pseudonymity for providing parents with a platform to discuss sensitive parenting topics. We conclude by highlighting the benefits of both gender-inclusive and role-specific parenting boards. This work provides a roadmap for using computational techniques to understand parenting practices online at large scale.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {gender, anonymity, social media, reddit, parenting, pseudonymity, language},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3038912.3052601,
author = {Zhang, Chao and Zhang, Keyang and Yuan, Quan and Peng, Haoruo and Zheng, Yu and Hanratty, Tim and Wang, Shaowen and Han, Jiawei},
title = {Regions, Periods, Activities: Uncovering Urban Dynamics via Cross-Modal Representation Learning},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052601},
doi = {10.1145/3038912.3052601},
abstract = {With the ever-increasing urbanization process, systematically modeling people's activities in the urban space is being recognized as a crucial socioeconomic task. This task was nearly impossible years ago due to the lack of reliable data sources, yet the emergence of geo-tagged social media (GTSM) data sheds new light on it. Recently, there have been fruitful studies on discovering geographical topics from GTSM data. However, their high computational costs and strong distributional assumptions about the latent topics hinder them from fully unleashing the power of GTSM.To bridge the gap, we present CrossMap, a novel cross-modal representation learning method that uncovers urban dynamics with massive GTSM data. CrossMap first employs an accelerated mode seeking procedure to detect spatiotemporal hotspots underlying people's activities. Those detected hotspots not only address spatiotemporal variations, but also largely alleviate the sparsity of the GTSM data. With the detected hotspots, CrossMap then jointly embeds all spatial, temporal, and textual units into the same space using two different strategies: one is reconstruction-based and the other is graph-based. Both strategies capture the correlations among the units by encoding their co-occurrence and neighborhood relationships, and learn low-dimensional representations to preserve such correlations. Our experiments demonstrate that CrossMap not only significantly outperforms state-of-the-art methods for activity recovery and classification, but also achieves much better efficiency.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {361–370},
numpages = {10},
keywords = {twitter, social media, urban dynamics, spatiotemporal data, representation learning, activity, geographical topic},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/3451528,
author = {Guo, Yunyan and Li, Jianzhong},
title = {Distributed Latent Dirichlet Allocation on Streams},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3451528},
doi = {10.1145/3451528},
abstract = {Latent Dirichlet Allocation (LDA) has been widely used for topic modeling, with applications spanning various areas such as natural language processing and information retrieval. While LDA on small and static datasets has been extensively studied, several real-world challenges are posed in practical scenarios where datasets are often huge and are gathered in a streaming fashion. As the state-of-the-art LDA algorithm on streams, Streaming Variational Bayes (SVB) introduced Bayesian updating to provide a streaming procedure. However, the utility of SVB is limited in applications since it ignored three challenges of processing real-world streams: topic evolution, data turbulence, and real-time inference. In this article, we propose a novel distributed LDA algorithm—referred to as StreamFed-LDA—to deal with challenges on streams. For topic modeling of streaming data, the ability to capture evolving topics is essential for practical online inference. To achieve this goal, StreamFed-LDA is based on a specialized framework that supports lifelong (continual) learning of evolving topics. On the other hand, data turbulence is commonly present in streams due to real-life events. In that case, the design of StreamFed-LDA allows the model to learn new characteristics from the most recent data while maintaining the historical information. On massive streaming data, it is difficult and crucial to provide real-time inference results. To increase the throughput and reduce the latency, StreamFed-LDA introduces additional techniques that substantially reduce both computation and communication costs in distributed systems. Experiments on four real-world datasets show that the proposed framework achieves significantly better performance of online inference compared with the baselines. At the same time, StreamFed-LDA also reduces the latency by orders of magnitudes in real-world datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {9},
numpages = {20},
keywords = {learning system, variational inference, Distributed streams}
}

@inproceedings{10.1145/3422392.3422424,
author = {Santos, Lucas Rani\'{e}re Juvino and Gadelha, Guilherme and Ramalho, Franklin and Massoni, Tiago},
title = {Improving Traceability Recovery Between Bug Reports and Manual Test Cases},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422424},
doi = {10.1145/3422392.3422424},
abstract = {Many software tasks produce text. These artifacts are co-dependent, but maintaining their consistency is challenging; automation is desirable. Research has investigated traceability between bug reports and manual test cases. Since manual system test scripts are a popular way of documenting requirements in agile projects, this kind of traceability allows, for instance, to analyze how bugs are related to requirements. Previous work has assessed three Information Retrieval (IR) techniques (LSI, LDA, and BM25) and a Deep-Learning (DL) algorithm (Word Vector) to recover those links; results indicate the need for improvements in textual processing and representation. In this paper, we applied five improvement techniques to an existing data set of bug reports and manual test cases from Mozilla Firefox. We employ: (i) text and information cleaning, (ii) spell-checking, (iii) terms weighting, (iv) similarity matrices merging, and (v) traceability matrices merging. Merging was applied to matrices produced by IR and DL techniques. We evaluate the techniques by comparing precision, recall, and f-measures, with those reached by previous work as a baseline. We observe a slight increase in precision and recall rates for all traceability recovery techniques (LSI, LDA, BM25, and Word Vector) by combining text and information cleaning, title duplication, and spell-checking. A hybrid strategy, creating a merged traceability matrix containing all traces recovered by at least one of the four recovery techniques, achieved a recall value of 93%.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {293–302},
numpages = {10},
keywords = {traceability, software artifacts, test cases, bug reports},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3366423.3380175,
author = {Yu, Wenhao and Yu, Mengxia and Zhao, Tong and Jiang, Meng},
title = {Identifying Referential Intention with Heterogeneous Contexts},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380175},
doi = {10.1145/3366423.3380175},
abstract = {Citing, quoting, and forwarding &amp; commenting behaviors are widely seen in academia, news media, and social media. Existing behavior modeling approaches focused on mining content and describing preferences of authors, speakers, and users. However, behavioral intention plays an important role in generating content on the platforms. In this work, we propose to identify the referential intention which motivates the action of using the referred (e.g., cited, quoted, and retweeted) source and content to support their claims. We adopt a theory in sociology to develop a schema of four types of intentions. The challenge lies in the heterogeneity of observed contextual information surrounding the referential behavior, such as referred content (e.g., a cited paper), local context (e.g., the sentence citing the paper), neighboring context (e.g., the former and latter sentences), and network context (e.g., the academic network of authors, affiliations, and keywords). We propose a new neural framework with Interactive Hierarchical Attention (IHA) to identify the intention of referential behavior by properly aggregating the heterogeneous contexts. Experiments demonstrate that the proposed method can effectively identify the type of intention of citing behaviors (on academic data) and retweeting behaviors (on Twitter). And learning the heterogeneous contexts collectively can improve the performance. This work opens a door for understanding content generation from a fundamental perspective of behavior sciences.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {962–972},
numpages = {11},
keywords = {Referential Intention, Interactive Hierarchical Attention, Heterogeneous Contexts},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3077136.3080715,
author = {Giachanou, Anastasia and Mele, Ida and Crestani, Fabio},
title = {A Collection for Detecting Triggers of Sentiment Spikes},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080715},
doi = {10.1145/3077136.3080715},
abstract = {The advent of social media has given the opportunity to users to publicly express and share their opinion about any topic. Public opinion is very important for the interested entities that can leverage such information in the process of making decisions. In addition, identifying sentiment changes and the likely causes that have triggered them allows interested parties to adjust their strategies and attract more positive sentiment. With the aim to facilitate research on this problem, we describe a collection of tweets that can be used for detecting and ranking the likely triggers of sentiment spikes towards different entities. To build the collection, we first group tweets by topic which are then manually annotated according to sentiment polarity and strength. We believe that this collection can be useful for further research on detecting sentiment change triggers, sentiment analysis and sentiment prediction.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1249–1252},
numpages = {4},
keywords = {test collections, social media, sentiment spikes},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3092090.3092095,
author = {Singh, A. K. and Nagwani, N. K. and Pandey, S.},
title = {TAGme: A Topical Folksonomy Based Collaborative Filtering for Tag Recommendation in Community Sites},
year = {2017},
isbn = {9781450348812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092090.3092095},
doi = {10.1145/3092090.3092095},
abstract = {Community Question Answering1 (CQA) sites allow users to share and exchange the knowledge on various fields. In recent years the CQA sites gain the huge popularity on the web. Searching the information is a very difficult task in CQA sites which is solved with the help of tags, but users assign the tags according to their knowledge. The CQA sites maintain metadata related to the posts, user and tags which can be utilized efficiently for recommending the tags. In this paper, a new algorithm, namely, TAGme, is proposed for tag recommendation in CQA sites. The proposed TAGme algorithm uses the concept of topic modeling, folksonomy and collaborative filtering. The proposed algorithm consists of three major stages. In the first stage, topical folksonomies are developed using the CQA metadata user, tag, topics the posts. In the second stage, the generated topical folksonomy is used to construct a topic profile matrix and user-topic profile matrix. Finally, at the third stage, collaborative filtering is used for tag recommendation based on users' previous topical history. Comparison of proposed tag recommendation algorithm TAGme is carried with standard tag recommendation algorithms, namely, TF, TF-IDF, LDA, tag-LDA, RTM and MAT. The experimental results demonstrate that the proposed TAGme algorithm performs better in comparison to the other tag recommendation algorithm.},
booktitle = {Proceedings of the 4th Multidisciplinary International Social Networks Conference},
articleno = {27},
numpages = {7},
keywords = {Community Question Answering, Topic Modeling, Collaborative Filtering, Tag Recommendation, Folksonomy},
location = {Bangkok, Thailand},
series = {MISNC '17}
}

@inproceedings{10.1145/3501409.3501662,
author = {Gao, Jing and Sun, Xuan and Tan, Li and Ma, Zihao},
title = {Rumor Detection Based on Generalized Text Feature},
year = {2021},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501662},
doi = {10.1145/3501409.3501662},
abstract = {This paper proposes a rumor detection method based on topic classification and content understanding in health. It can extract features from different scales of sub-datasets, comprehensively consider the correlation and difference in different topics, and combine the part of speech and word meaning to expand the model's ability to understand the text and enhance the ability to detect the rumor traps spread maliciously. The experimental results show that the method achieves good results in the field of health data sets, improving the results in new indicators such as content understanding, and have more vital generalization ability.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1425–1429},
numpages = {5},
keywords = {Rumor detection, Content understanding, Feature fusion, NLP},
location = {Xiamen, China},
series = {EITCE 2021}
}

@inproceedings{10.1145/3162957.3162984,
author = {Zhang, Chao and Song, Hui and Liu, Zhenyu},
title = {MiSAS: A Multi-Domain Feature-Level Sentiment Analysis System on Micro-Blog},
year = {2017},
isbn = {9781450353656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3162957.3162984},
doi = {10.1145/3162957.3162984},
abstract = {Big data from micro-blog has been an important access to social groups' psychology, market feedback and so on. Unlike the review corpus which is usually related to the specific object (e.g. a product), the micro-blog content covers the opinion of many domains. It is less useful to extract the fine-grained feature-level opinion target without detect the domain. This paper proposed a systematic feature-level sentiment analysis approach on Micro-blog that recognize data related to the interesting topic automatically. Working with the big micro-blog data we figure out valuable text features to train the opinion targets extraction and sentimental polarity detection models that achieve better multi-domain adaption. We implement the MiSAS system, which crawls micro-blog raw data, outputs opinion targets and orientation summarization on the giving domains, offering valuable analytical tool for practical applications.},
booktitle = {Proceedings of the 3rd International Conference on Communication and Information Processing},
pages = {14–18},
numpages = {5},
keywords = {sentiment analysis, sentimental polarity detection, micro-blog, opinion target extraction, MiSAS},
location = {Tokyo, Japan},
series = {ICCIP '17}
}

@inproceedings{10.1145/3503161.3551607,
author = {Tan, YunPeng and Liu, Fangyu and Li, BoWei and Zhang, Zheng and Zhang, Bo},
title = {An Efficient Multi-View Multimodal Data Processing Framework for Social Media Popularity Prediction},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3551607},
doi = {10.1145/3503161.3551607},
abstract = {Popularity of social media is an important symbol of its communication power. Predictions of social media popularity have tremendous business and social value. In this paper, we propose an efficient multimodal data processing framework, which can comprehensively extract the multi-view features from multimodal social media data and achieve accurate popularity prediction. We utilize Transformer and sliding window average to extract time series features of posts, utilize CatBoost to calculate the importance of different features, and integrate important features extracted from multiple views for accurate prediction of social media popularity. We evaluate our proposed approach with the Social Media Prediction Dataset. Experimental results show that our approach achieves excellent performance in the social media popularity prediction task.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7200–7204},
numpages = {5},
keywords = {feature selection, transformer, catboost, social media popularity prediction},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3340631.3394870,
author = {Meng, Yitong and Yan, Xiao and Liu, Weiwen and Wu, Huanhuan and Cheng, James},
title = {Wasserstein Collaborative Filtering for Item Cold-Start Recommendation},
year = {2020},
isbn = {9781450368612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340631.3394870},
doi = {10.1145/3340631.3394870},
abstract = {Item cold-start recommendation, which predicts user preference on new items that have no user interaction records, is an important problem in recommender systems. In this paper, we model the disparity between user preferences on warm items (those having interaction record) and that on cold-start items using the Wasserstein distance. On this basis, we propose Wasserstein Collaborative Filtering (WCF), which predicts user preference on cold-start items by minimizing the Wasserstein distance under user embedding constraint. Our analysis shows that minimizing the Wasserstein distance ensures that users sharing similar tastes on warm items also have similar preferences on cold-start items. Experimental results show that WCF consistently outperform the state-of-the-art methods in recommendation quality, usually by a large margin.},
booktitle = {Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {318–322},
numpages = {5},
keywords = {recommender systems, collaborative filtering, optimal transportation, cold start},
location = {Genoa, Italy},
series = {UMAP '20}
}

@inproceedings{10.1145/3201064.3201101,
author = {Mehrazar, Maryam and Kling, Christoph Carl and Lemke, Steffen and Mazarakis, Athanasios and Peters, Isabella},
title = {Can We Count on Social Media Metrics? First Insights into the Active Scholarly Use of Social Media},
year = {2018},
isbn = {9781450355636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3201064.3201101},
doi = {10.1145/3201064.3201101},
abstract = {Measuring research impact is important for ranking publications in academic search engines and for research evaluation. Social media metrics or altmetrics measure the impact of scientific work based on social media activity. Altmetrics are complementary to traditional, citation-based metrics, e.g. allowing the assessment of new publications for which citations are not yet available. Despite the increasing importance of altmetrics, their characteristics are not well understood: Until now it has not been researched what kind of researchers are actively using which social media services and why - important questions for scientific impact prediction. Based on a survey among 3,430 scientists, we uncover previously unknown and significant differences between social media services: We identify services which attract young and experienced researchers, respectively, and detect differences in usage motivations. Our findings have direct implications for the future design of altmetrics for scientific impact prediction.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {215–219},
numpages = {5},
keywords = {motivations, digital scholarship, altmetrics, social media},
location = {Amsterdam, Netherlands},
series = {WebSci '18}
}

@inproceedings{10.1145/3488560.3502185,
author = {Chen, Xuesong and Ye, Ziyi and Xie, Xiaohui and Liu, Yiqun and Gao, Xiaorong and Su, Weihang and Zhu, Shuqi and Sun, Yike and Zhang, Min and Ma, Shaoping},
title = {Web Search via an Efficient and Effective Brain-Machine Interface},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502185},
doi = {10.1145/3488560.3502185},
abstract = {While search technologies have evolved to be robust and ubiquitous, the fundamental interaction paradigm has remained relatively stable for decades. With the maturity of the Brain-Machine Interface(BMI), we build an efficient and effective communication system between human beings and search engines based on electroencephalogram (EEG) signals, called Brain Machine Search Interface (BMSI)system. The BMSI system provides functions including query reformulation and search result interaction. In our system, users can perform search tasks without having to use the mouse and keyboard. Therefore, it is useful for application scenarios in which hand-based interactions are infeasible, e.g, for users with severe neuromuscular disorders. Besides, based on brain signals decoding, our system can provide abundant and valuable user-side context information (e.g., real-time satisfaction feedback, extensive context information, and a clearer description of information needs) to the search engine, which is hard to capture in the previous paradigm. In our implementation, the system can decode user satisfaction from brain signals in real-time during the interaction process and re-rank the search results list based on user satisfaction feedback.The demo video is available at http://www.thuir.cn/group/YQLiu/videos/BMSISystem.html},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1569–1572},
numpages = {4},
keywords = {user feedback, interaction paradigm, brain machine interface},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3500931.3501016,
author = {Gao, Mengke and Zhang, Yan and Gao, Yue},
title = {Research Progress of User Portrait Technology in Medical Field},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501016},
doi = {10.1145/3500931.3501016},
abstract = {In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {500–504},
numpages = {5},
keywords = {User portrait, Review, Medical treatment},
location = {Beijing, China},
series = {ISAIMS 2021}
}

@inproceedings{10.1145/3478905.3478958,
author = {Niu, Xinyuan and Zheng, Wenguang and Xiao, Yingyuan and Wang, Qian},
title = {Short Text Similarity Computation Method Based on Feature Expansion and Siamese Network},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478958},
doi = {10.1145/3478905.3478958},
abstract = {Text similarity&nbsp;computation issues&nbsp;is a widely&nbsp;studied problem in natural language processing&nbsp;(NLP). Short text similarity computation is a new and more challenging problem, which cannot be effectively solved by using previous regular text similarity computation approach. The main reason is that, a short text generally contains limited number of words and fewer features can be extracted. In this paper, we&nbsp;propose a short text similarity computation method based on feature&nbsp;expansion and Siamese neural network. Firstly, a latent Dirichlet allocation (LDA)&nbsp;based model is constructed to expand the features of a short text. Then, deep features are extracted by using Siamese neural networks model which contains both convolutional neural networks&nbsp;(CNN) and Bi-directional long short-term memory&nbsp;(BiLSTM). Finally, the similarity of two short texts can be achieved by computing the Manhattan distance&nbsp;between generated feature vectors of these two texts. Experimental results show that, based on the&nbsp;data set of&nbsp;Ant Financial NLP Challenge, our&nbsp;method achieves higher accuracy and F1 score.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {268–272},
numpages = {5},
keywords = {attention mechanism, BiLSTM, LDA, CNN, short text similarity},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3319921.3319966,
author = {Lu, Kui and Wu, Jiesheng},
title = {Sentiment Analysis of Film Review Texts Based on Sentiment Dictionary and SVM},
year = {2019},
isbn = {9781450361286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319921.3319966},
doi = {10.1145/3319921.3319966},
abstract = {The sentiment analysis of the film review text is to extract and analyze the hidden sentiment information in the text data, thereby helping the network personnel such as the media platform to analyze the audience's preference for the film. Based on this, this paper proposes a film review text sentiment analysis method based on SVM classification technology in sentiment dictionary and maching learning. Firstly, the basic sentiment dictionary, the domin sentiment dictionary, the negative word dictionary and the degree adverb dictionary are constructed. The four dictionaries are combined to realize the expansion of the dictionary. Secondly, the SVM model training set is construcetd by calculating the combination of sentiment weight and user scoring. Finally, using the test data for sentiment classification experiments, the results show that the method has higher accuracy of sentiment classification than the method based on a basic sentiment dictionary.},
booktitle = {Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence},
pages = {73–77},
numpages = {5},
keywords = {Sentiment Analysis, SVM, Sentiment Dictionary, Film Review Texts},
location = {Suzhou, China},
series = {ICIAI 2019}
}

@inproceedings{10.1145/3269206.3269273,
author = {Yang, Min and Qu, Qiang and Zhu, Jia and Shen, Ying and Zhao, Zhou},
title = {Cross-Domain Aspect/Sentiment-Aware Abstractive Review Summarization},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269273},
doi = {10.1145/3269206.3269273},
abstract = {This study takes the lead to study the aspect/sentiment-aware abstractive review summarization in domain adaptation scenario. The proposed model CASAS (neural attentive model for Cross-domain Aspect/Sentiment-aware Abstractive review Summarization) leverages domain classification task, working on datasets of both source and target domains, to recognize the domain information of texts and transfer knowledge from source domains to target domains. The extensive experiments on Amazon reviews demonstrate that CASAS outperforms the compared methods in both out-of-domain and in-domain setups.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1531–1534},
numpages = {4},
keywords = {topic modeling, domain adaptation, abstractive review summarization},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3267305.3274182,
author = {Zeng, Ming and Yu, Tong and Mengshoel, Ole J. and Qin, Helen and Lee, Chris and Shen, John Paul},
title = {Improving Bag-Of-Words: Capturing Local Information for Motion-Based Activity Recognition},
year = {2018},
isbn = {9781450359665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267305.3274182},
doi = {10.1145/3267305.3274182},
abstract = {Bag-of-Words (BoW) is one of the important techniques for activity recognition. Instead of dividing a continuous sensor streams into sliding windows with fixed time duration, it builds activity recognition models using histograms of primitive motion symbols. However, this BoW method losses the sequential information in the symbol sequences and limits the performance of activity recognition. In this paper, we propose an activity recognition approach to get rid of this limitation and consider longer time dependency by capturing local features from the symbol sequences. We use a set of small sliding windows inside the symbol sequences to capture local features. Our algorithm utilizes the physical knowledge where the sequence of the selected window size of symbols reflects the context and order of an activity. We evaluate the activity recognition approaches on two public datasets. The results show that our approach achieved stable improvement on all the datasets, compared with traditional statistical and BoW approaches.},
booktitle = {Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
pages = {1345–1354},
numpages = {10},
keywords = {multi-instance learning, Activity recognition, bag-of-words, machine learning},
location = {Singapore, Singapore},
series = {UbiComp '18}
}

@inproceedings{10.1145/3503161.3551607,
author = {Tan, YunPeng and Liu, Fangyu and Li, BoWei and Zhang, Zheng and Zhang, Bo},
title = {An Efficient Multi-View Multimodal Data Processing Framework for Social Media Popularity Prediction},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3551607},
doi = {10.1145/3503161.3551607},
abstract = {Popularity of social media is an important symbol of its communication power. Predictions of social media popularity have tremendous business and social value. In this paper, we propose an efficient multimodal data processing framework, which can comprehensively extract the multi-view features from multimodal social media data and achieve accurate popularity prediction. We utilize Transformer and sliding window average to extract time series features of posts, utilize CatBoost to calculate the importance of different features, and integrate important features extracted from multiple views for accurate prediction of social media popularity. We evaluate our proposed approach with the Social Media Prediction Dataset. Experimental results show that our approach achieves excellent performance in the social media popularity prediction task.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7200–7204},
numpages = {5},
keywords = {feature selection, transformer, catboost, social media popularity prediction},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3340631.3394870,
author = {Meng, Yitong and Yan, Xiao and Liu, Weiwen and Wu, Huanhuan and Cheng, James},
title = {Wasserstein Collaborative Filtering for Item Cold-Start Recommendation},
year = {2020},
isbn = {9781450368612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340631.3394870},
doi = {10.1145/3340631.3394870},
abstract = {Item cold-start recommendation, which predicts user preference on new items that have no user interaction records, is an important problem in recommender systems. In this paper, we model the disparity between user preferences on warm items (those having interaction record) and that on cold-start items using the Wasserstein distance. On this basis, we propose Wasserstein Collaborative Filtering (WCF), which predicts user preference on cold-start items by minimizing the Wasserstein distance under user embedding constraint. Our analysis shows that minimizing the Wasserstein distance ensures that users sharing similar tastes on warm items also have similar preferences on cold-start items. Experimental results show that WCF consistently outperform the state-of-the-art methods in recommendation quality, usually by a large margin.},
booktitle = {Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {318–322},
numpages = {5},
keywords = {recommender systems, collaborative filtering, optimal transportation, cold start},
location = {Genoa, Italy},
series = {UMAP '20}
}

@inproceedings{10.1145/3201064.3201101,
author = {Mehrazar, Maryam and Kling, Christoph Carl and Lemke, Steffen and Mazarakis, Athanasios and Peters, Isabella},
title = {Can We Count on Social Media Metrics? First Insights into the Active Scholarly Use of Social Media},
year = {2018},
isbn = {9781450355636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3201064.3201101},
doi = {10.1145/3201064.3201101},
abstract = {Measuring research impact is important for ranking publications in academic search engines and for research evaluation. Social media metrics or altmetrics measure the impact of scientific work based on social media activity. Altmetrics are complementary to traditional, citation-based metrics, e.g. allowing the assessment of new publications for which citations are not yet available. Despite the increasing importance of altmetrics, their characteristics are not well understood: Until now it has not been researched what kind of researchers are actively using which social media services and why - important questions for scientific impact prediction. Based on a survey among 3,430 scientists, we uncover previously unknown and significant differences between social media services: We identify services which attract young and experienced researchers, respectively, and detect differences in usage motivations. Our findings have direct implications for the future design of altmetrics for scientific impact prediction.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {215–219},
numpages = {5},
keywords = {motivations, digital scholarship, altmetrics, social media},
location = {Amsterdam, Netherlands},
series = {WebSci '18}
}

@inproceedings{10.1145/3184558.3191582,
author = {Sottocornola, Gabriele and Symeonidis, Panagiotis and Zanker, Markus},
title = {Session-Based News Recommendations},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191582},
doi = {10.1145/3184558.3191582},
abstract = {In the context of news recommendations, many time-aware approaches were proposed. These approaches have tried to capture the recency of news with respect to their short life span, by using either decaying weights on past articles or even forgetting them. However, most of these approaches have missed to consider sessions, which encapsulate inside them the articles that a user has interacted with in a short time period. In this paper, we provide news recommendations based on user sessions to reveal their short-term intentions. We also combine content-based with collaborative filtering to deal with the severe data sparsity problem that exists in our real-life data set. We have experimentally seen that the users' interests evolve over time and that our strategies can adapt fast to these changes.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1395–1399},
numpages = {5},
keywords = {news recommendation, session-based recommendation, time-aware recommendation},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3107411.3107447,
author = {Wu, Hang and Wang, May D.},
title = {Infer Cause of Death for Population Health Using Convolutional Neural Network},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107447},
doi = {10.1145/3107411.3107447},
abstract = {In biomedical data analysis, inferring the cause of death is a challenging and important task, which is useful for both public health reporting purposes, as well as improving patients' quality of care by identifying severer conditions. Causal inference, however, is notoriously difficult. Traditional causal inference mainly relies on analyzing data collected from experiment of specific design, which is expensive, and limited to a certain disease cohort, making the approach less generalizable. In our paper, we adopt a novel data-driven perspective to analyze and improve the death reporting process, to assist physicians identify the single underlying cause of death. To achieve this, we build state-of-the-art deep learning models, convolution neural network (CNN), and achieve around 75% accuracy in predicting the single underlying cause of death from a list of relevant medical conditions. We also provide interpretations for the black-box neural network models, so that death reporting physicians can apply the model with better understanding of the model.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {526–535},
numpages = {10},
keywords = {causal inference, deep learning, interpretability},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/3397271.3401269,
author = {Mukherjee, Rajdeep and Peruri, Hari Chandana and Vishnu, Uppada and Goyal, Pawan and Bhattacharya, Sourangshu and Ganguly, Niloy},
title = {Read What You Need: Controllable Aspect-Based Opinion Summarization of Tourist Reviews},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401269},
doi = {10.1145/3397271.3401269},
abstract = {Manually extracting relevant aspects and opinions from large volumes of user-generated text is a time-consuming process. Summaries, on the other hand, help readers with limited time budgets to quickly consume the key ideas from the data. State-of-the-art approaches for multi-document summarization, however, do not consider user preferences while generating summaries. In this work, we argue the need and propose a solution for generating personalized aspect-based opinion summaries from large collections of online tourist reviews. We let our readers decide and control several attributes of the summary such as the length and specific aspects of interest among others. Specifically, we take an unsupervised approach to extract coherent aspects from tourist reviews posted onTripAdvisor. We then propose an Integer Linear Programming (ILP) based extractive technique to select an informative subset of opinions around the identified aspects while respecting the user-specified values for various control parameters. Finally, we evaluate and compare our summaries using crowdsourcing and ROUGE-based metrics and obtain competitive results.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1825–1828},
numpages = {4},
keywords = {aspect-based opinion mining, controllable summarization, personalization, tourism, unsupervised extractive opinion summarization},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inbook{10.1145/3434581.3434680,
author = {Niu, Ben and Sui, Ling and Tang, Junri and Dong, Ruisi and Kou, Kaiqi and Wang, Zhiqiong},
title = {Prediction of Microblog Users' Forwarding Behavior Based on Interactive and Active Information},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434680},
abstract = {As a mainstream online social media platform, microblog predicts the individual forwarding behavior from the perspective of a single user, which is helpful to grasp the information dissemination characteristics in microblog network from the micro level and realize further prediction and analysis of information dissemination paths and even dissemination trends. From the perspective of a single user, this paper predicts the forwarding behavior of microblog users. This paper summarizes the previous researches on the influencing factors of microblog users' forwarding willingness, considers the influence of users' interaction and users' activity on forwarding willingness, proposes a prediction method of microblog users' forwarding behavior based on interactive and active information, and verifies its feasibility. Finally, the shortcomings of this method are summarized and the prospect is put forward.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {554–559},
numpages = {6}
}

@inproceedings{10.1145/3341161.3345621,
author = {Bonomo, Mariella and Ciaccio, Gaspare and De Salve, Andrea and Rombo, Simona E.},
title = {Customer Recommendation Based on Profile Matching and Customized Campaigns in On-Line Social Networks},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3345621},
doi = {10.1145/3341161.3345621},
abstract = {We propose a general framework for the recommendation of possible customers (users) to advertisers (e.g., brands) based on the comparison between On-Line Social Network profiles. In particular, we associate suitable categories and subcategories to both user and brand profiles in the considered On-line Social Network. When categories involve posts and comments, the comparison is based on word embedding, and this allows to take into account the similarity between the topics of particular interest for a brand and the user preferences. Furthermore, user personal information, such as age, job or genre, are used for targeting specific advertising campaigns. Results on real Facebook dataset show that the proposed approach is successful in identifying the most suitable set of users to be used as target for a given advertisement campaign.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1155–1159},
numpages = {5},
keywords = {profile matching, recommendation system, semantic similarity, social advertising},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3356994.3365508,
author = {Sikder, Aisha and Z\"{u}fle, Andreas},
title = {Emotion Predictions in Geo-Textual Data Using Spatial Statistics and Recommendation Systems},
year = {2019},
isbn = {9781450369633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356994.3365508},
doi = {10.1145/3356994.3365508},
abstract = {Microblogs are used by millions of users to express their emotions, such as joy, surprise and anger, on a plethora of different topics. For the same topic, different places may exhibit different emotions for identical topics. The goal of this work is to learn, model and predict emotions on various topics and in different cities. For this purpose, we propose a hybrid approach which combines spatial statistics (kriging) and recommendation system (matrix factorization-based). Our experimental evaluations, using millions of tweets across the United States, show that our hybrid approach outperforms individual approaches based on matrix factorization and Kriging alone. This case study shows the potential of combining spatial statistics methods such as Kriging with machine learning solutions to support knowledge discovery on spatial data.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
articleno = {5},
numpages = {4},
keywords = {emotion prediction, spatial statistics, NRC lexicon, recommender systems, singular value decomposition, microblog data, Kriging},
location = {Chicago, Illinois},
series = {LocalRec '19}
}

@inproceedings{10.1145/3121113.3121203,
author = {Ballentine, B.},
title = {Recycling Methods: The Value of Replicated Research in the Classroom},
year = {2017},
isbn = {9781450351607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121113.3121203},
doi = {10.1145/3121113.3121203},
abstract = {Reusing datasets and research methods defined from students in prior Digital Humanities (DH) courses is an effective means to quickly introduce new students to methods-driven research projects. The pedagogical strategy along with two specific assignments are explained in the article. The first assignment uses the open source application ImagePlot to visualize 398 comic book covers for color values such as hue and saturation. The second project uses the open source tool MALLET to render topic models of essays published in the online literary journal Brevity. Coursework prepares technical communication students to contend with complexity and indeterminacy in the workplace. Information design is a valuable skill for technical communicators and the strategies outlined here demonstrate to students the importance of a well-defined, or methods-driven approach to collecting, analyzing, and displaying data.},
booktitle = {Proceedings of the 35th ACM International Conference on the Design of Communication},
articleno = {24},
numpages = {5},
keywords = {research methods, pedagogy, digital humanities, open source tools},
location = {Halifax, Nova Scotia, Canada},
series = {SIGDOC '17}
}

@inproceedings{10.1145/3411764.3445383,
author = {Offenwanger, Anna and Milligan, Alan John and Chang, Minsuk and Bullard, Julia and Yoon, Dongwook},
title = {Diagnosing Bias in the Gender Representation of HCI Research Participants: How It Happens and Where We Are},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445383},
doi = {10.1145/3411764.3445383},
abstract = {In human-computer interaction (HCI) studies, bias in the gender representation of participants can jeopardize the generalizability of findings, perpetuate bias in data driven practices, and make new technologies dangerous for underrepresented groups. Key to progress towards inclusive and equitable gender practices is diagnosing the current status of bias and identifying where it comes from. In this mixed-methods study, we interviewed 13 HCI researchers to identify the potential bias factors, defined a systematic data collection procedure for meta-analysis of participant gender data, and created a participant gender dataset from 1,147 CHI papers. Our analysis provided empirical evidence for the underrepresentation of women, the invisibility of non-binary participants, deteriorating representation of women in MTurk studies, and characteristics of research topics prone to bias. Based on these findings, we make concrete suggestions for promoting inclusive community culture and equitable research practices in HCI.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {399},
numpages = {18},
keywords = {gender bias, dataset, HCI, participants, human subjects, gender, data schema, research, CHI, human-computer interaction, meta-analysis, user studies},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3230706,
author = {Wang, Weiqing and Yin, Hongzhi and Du, Xingzhong and Nguyen, Quoc Viet Hung and Zhou, Xiaofang},
title = {TPM: A Temporal Personalized Model for Spatial Item Recommendation},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3230706},
doi = {10.1145/3230706},
abstract = {With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important way of helping users discover interesting locations to increase their engagement with location-based services. The availability of spatial, temporal, and social information in LBSNs offers an unprecedented opportunity to enhance the spatial item recommendation. Many previous works studied spatial and social influences on spatial item recommendation in LBSNs. Due to the strong correlations between a user’s check-in time and the corresponding check-in location, which include the sequential influence and temporal cyclic effect, it is essential for spatial item recommender system to exploit the temporal effect to improve the recommendation accuracy. Leveraging temporal information in spatial item recommendation is, however, very challenging, considering (1) when integrating sequential influences, users’ check-in data in LBSNs has a low sampling rate in both space and time, which renders existing location prediction techniques on GPS trajectories ineffective, and the prediction space is extremely large, with millions of distinct locations as the next prediction target, which impedes the application of classical Markov chain models; (2) there are various temporal cyclic patterns (i.e., daily, weekly, and monthly) in LBSNs, but existing work is limited to one specific pattern; and (3) there is no existing framework that unifies users’ personal interests, temporal cyclic patterns, and the sequential influence of recently visited locations in a principled manner.In light of the above challenges, we propose a Temporal Personalized Model (TPM), which introduces a novel latent variable topic-region to model and fuse sequential influence, cyclic patterns with personal interests in the latent and exponential space. The advantages of modeling the temporal effect at the topic-region level include a significantly reduced prediction space, an effective alleviation of data sparsity, and a direct expression of the semantic meaning of users’ spatial activities. Moreover, we introduce two methods to model the effect of various cyclic patterns. The first method is a time indexing scheme that encodes the effect of various cyclic patterns into a binary code. However, the indexing scheme faces the data sparsity problem in each time slice. To deal with this data sparsity problem, the second method slices the time according to each cyclic pattern separately and explores these patterns in a joint additive model.Furthermore, we design an asymmetric Locality Sensitive Hashing (ALSH) technique to speed up the online top-k recommendation process by extending the traditional LSH. We evaluate the performance of TPM on two real datasets and one large-scale synthetic dataset. The performance of TPM in recommending cold-start items is also evaluated. The results demonstrate a significant improvement in TPM’s ability to recommend spatial items, in terms of both effectiveness and efficiency, compared with the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {nov},
articleno = {61},
numpages = {25},
keywords = {spatial-temporal modeling, POI, online learning, location-based service}
}

@article{10.1162/coli_a_00339,
author = {Joty, Shafiq and Mohiuddin, Tasnim},
title = {Modeling Speech Acts in Asynchronous Conversations: A Neural-Crf Approach},
year = {2018},
issue_date = {December 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {44},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00339},
doi = {10.1162/coli_a_00339},
abstract = {Participants in an asynchronous conversation e.g., forum, e-mail interact with each other at different times, performing certain communicative acts, called speech acts e.g., question, request. In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network LSTM-RNN first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field CRF model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations e.g., meetings, using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: i LSTM-RNNs provide better task-specific representations, ii conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, iii adversarial training gives better domain-invariant representations, and iv the global CRF model improves over local models.},
journal = {Comput. Linguist.},
month = {dec},
pages = {859–894},
numpages = {36}
}

@article{10.1145/3009924,
author = {Jahanian, Ali and Keshvari, Shaiyan and Vishwanathan, S. V. N. and Allebach, Jan P.},
title = {Colors -- Messengers of Concepts: Visual Design Mining for Learning Color Semantics},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3009924},
doi = {10.1145/3009924},
abstract = {We study the concept of color semantics by modeling a dataset of magazine cover designs, evaluating the model via crowdsourcing, and demonstrating several prototypes that facilitate color-related design tasks. We investigate a probabilistic generative modeling framework that expresses semantic concepts as a combination of color and word distributions -- color-word topics. We adopt an extension to Latent Dirichlet Allocation (LDA) topic modeling, called LDA-dual, to infer a set of color-word topics over a corpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres. Although LDA models text documents as distributions over word topics, we model magazine covers as distributions over color-word topics. The results of our crowdsourcing experiments confirm that the model is able to successfully discover the associations between colors and linguistic concepts. Finally, we demonstrate several prototype applications that use the learned model to enable more meaningful interactions in color palette recommendation, design example retrieval, pattern recoloring, image retrieval, and image color selection.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jan},
articleno = {2},
numpages = {39},
keywords = {pattern recoloring, generative models, image color selection, color palette recommendation, image retrieval, design example retrieval, interaction design, topic modeling, Color semantics, visual design language, aesthetics, visual design mining}
}

@inproceedings{10.1145/3019612.3019809,
author = {de La Robertie, B. and Pitarch, Y. and Takasu, A. and Teste, O.},
title = {Identifying Authoritative Researchers in Digital Libraries Using External a Priori Knowledge},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019809},
doi = {10.1145/3019612.3019809},
abstract = {Numereous digital library projects mine heterogeneous data from different sources to provide expert finding services. However, a variety of models seek experts as simple sources of information and neglect authority signals. In this paper we address the issue of modelling the authority of researchers in academic networks. A model, RAC, is proposed that merges several graph representations and incorporate external knowledge about the authority of some major scientific conferences to improve the identification of authoritative researchers. Based on the provided structural model a biased label propagation algorithm aimed to strenghten the scores calculation of the labelled entities and their neighbors is developped. Both quantitative and qualitative analyses validate the effectiveness of the proposal. Indeed, RAC outperforms state-of-the-art models on a real-world graph containing more than 5 million nodes constructed using Microsoft Academic Search, AMiner and Core.edu databases.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1017–1022},
numpages = {6},
keywords = {expert ranking, digital libraries, graph analysis},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@article{10.1145/3361719,
author = {Zhang, Dong and Zhao, Shu and Duan, Zhen and Chen, Jie and Zhang, Yanping and Tang, Jie},
title = {A Multi-Label Classification Method Using a Hierarchical and Transparent Representation for Paper-Reviewer Recommendation},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361719},
doi = {10.1145/3361719},
abstract = {The paper-reviewer recommendation task is of significant academic importance for conference chairs and journal editors. It aims to recommend appropriate experts in a discipline to comment on the quality of papers of others in that discipline. How to effectively and accurately recommend reviewers for the submitted papers is a meaningful and still tough task. Generally, the relationship between a paper and a reviewer often depends on the semantic expressions of them. Creating a more expressive representation can make the peer-review process more robust and less arbitrary. So the representations of a paper and a reviewer are very important for the paper-reviewer recommendation. Actually, a reviewer or a paper often belongs to multiple research fields, which increases difficulty in paper-reviewer recommendation. In this article, we propose a Multi-Label Classification method using a HIErarchical and transPArent Representation named Hiepar-MLC. First, we introduce HIErarchical and transPArent Representation (Hiepar) to express the semantic information of the reviewer and the paper. Hiepar is learned from a two-level bidirectional gated recurrent unit based network applying the attention mechanism. It is capable of capturing the two-level hierarchical information (word-sentence-document) and highlighting the elements in reviewers or papers to support the labels. This word-sentence-document information mirrors the hierarchical structure of a reviewer or a paper and captures the exact semantics of them. Then we transform the paper-reviewer recommendation problem into a multi-level classification issue, whose multiple research labels exactly guide the learning process. It is flexible in that we can select any multi-label classification method to solve the paper-reviewer recommendation problem. Further, we propose a simple multi-label-based reviewer assignment (MLBRA) strategy to select the appropriate reviewers. It is interesting in that we also explore the paper-reviewer recommendation in the coarse-grain granularity. Extensive experiments on the real-world dataset consisting of the papers in the ACM Digital Library show that Hiepar-MLC achieves better label prediction performance than the existing representation alternatives. In addition, with the MLBRA strategy, we show the effectiveness and the feasibility of our transformation from paper-reviewer recommendation to multi-label classification.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {5},
numpages = {20},
keywords = {multi-label classification, hierarchical, ACM Digital Library, transparent, Paper-reviewer recommendation}
}

@inproceedings{10.1145/3426020.3426062,
author = {Lee, Chang Hyun and Bae, Hyun Jin and Cha, Kyung Jin and Lim, Gyoo Gun},
title = {Unsupervised Method for Measuring Smart Home Service Quality through Gap Analysis and Dependency Parsing},
year = {2020},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426062},
doi = {10.1145/3426020.3426062},
abstract = {Smart home is one of emerging technology issue in recent time. With increasing interest in smart home environment, companies subjectively developed technology related to smart home service, but because the destination of development was incompatible with customer perception and expectation, smart home adoption was not favorable as much as they expected. Hence, provider of smart home service needs to check how consumers accept their smart home service from the consumer's point of view. Therefore, this paper purposes to evaluate service quality comparing between consumer expectation and perception of service using gap analysis by employing dependency parsing, unsupervised sentiment analysis method, with case study of providing smart home service using reviews evaluating one of the front runner companies of smart home industry in South Korea, “A”. In this research, we found advantages in extracting more specific service factors which make customer dissatisfaction or satisfaction, of using dependency parsing instead of using traditional sentiment analysis method. The result of dependency parsing analysis can suggest the priorities to improve service factors and confirm demand of customer recognized from customers’ perspective. Moreover, we check about verification that dependency parsing is a proper method to analysis customers’ review data.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {167–171},
numpages = {5},
keywords = {Service quality, Dependency parsing, Gap analysis, Sentiment analysis},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@inproceedings{10.1145/3414274.3414492,
author = {Wang, Xinyun and Ning, Hongyun},
title = {TF-IDF Keyword Extraction Method Combining Context and Semantic Classification},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414492},
doi = {10.1145/3414274.3414492},
abstract = {Keyword extraction plays the same role as the cornerstone in the field of natural language processing. Text classification, information retrieval, abstract generation and text clustering are all based on keyword extraction. This article takes the research of keyword extraction model as the subject. First, it analyzes the principle and limitations of the traditional keyword extraction model TF-IDF when extracting keywords. Secondly, it focuses on the problem of ignoring context and word polysemy in the keyword extraction model. To improve, introduce the concept of context vector, construct a chain-extensible structure for polysemous words, and propose a new keyword extraction method of TF-IDF that combines context and semantic classification. The specific research contents are as follows:TF-IDF extracts keywords based on multiple texts. The target keywords are words that appear frequently in the current text, but appear in other articles that are significantly lower in frequency than the current article. This method takes into account the characteristic content of the article and makes the article distinct. However, this method ignores the influence of the context of the article and the problem of word polysemy. Obviously, the expression of the thought of the article will not only be affected by the context of the words, but also by the semantics of the words. In order to solve the above problems, this paper proposes a TF-IDF keyword extraction method that combines context and semantic classification. First, the text constructs a linked list expandable structure to solve the problem of polysemy; then, the influence of context is introduced. Before inserting a new word into the extended structure, the semantic similarity is calculated through the context vector. Finally, iterate over the entire expandable structure, sum the word frequency with semantic similarity under each semantic, and then sort the results to obtain TopN words as the text keywords of the corresponding article. In this paper, the Sogou news data set is used as the input data of the model, and the accuracy of the experimental results is significantly better than that of TF-IDF.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {123–128},
numpages = {6},
keywords = {Keyword Extraction, TF-IDF, Vector},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3400903.3400926,
author = {Zheng, Xiuwen and Gupta, Amarnath},
title = {An Algebraic Approach for High-Level Text Analytics},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400926},
doi = {10.1145/3400903.3400926},
abstract = {Text analytical tasks like word embedding, phrase mining and topic modeling, are placing increasing demands as well as challenges to existing database management systems. In this paper, we provide a novel algebraic approach based on associative arrays. Our data model and algebra can bring together relational operators and text operators, which enables interesting optimization opportunities for hybrid data sources that have both relational and textual data. We demonstrate its expressive power in text analytics using several real-world tasks.},
booktitle = {32nd International Conference on Scientific and Statistical Database Management},
articleno = {23},
numpages = {4},
keywords = {associative array, text analytics, natural language processing},
location = {Vienna, Austria},
series = {SSDBM 2020}
}

@inproceedings{10.1145/3366424.3382085,
author = {Jenkins, Porter},
title = {Structured Paragraph Embeddings of Financial Earnings Calls},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382085},
doi = {10.1145/3366424.3382085},
abstract = {Financial earnings calls contain rich information about the quarterly performance and future projections of public companies. Such information is highly relevant to developing trading strategies and understanding economic trends. However, due to the unstructured nature of call transcripts important signals can be difficult to extract. In this preliminary work, we propose a novel paragraph embedding method that leverages the structure inherent in the Q&amp;A format of earnings calls. We show that the proposed method improves classification performance over more general methods and provides a useful measure of similarity between paragraphs.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {264–268},
numpages = {5},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3510033,
author = {Tian, Yuanyishu and Wan, Yao and Lyu, Lingjuan and Yao, Dezhong and Jin, Hai and Sun, Lichao},
title = {FedBERT: When Federated Learning Meets Pre-Training},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3510033},
doi = {10.1145/3510033},
abstract = {The fast growth of pre-trained models (PTMs) has brought natural language processing to a new era, which has become a dominant technique for various natural language processing (NLP) applications. Every user can download the weights of PTMs, then fine-tune the weights for a task on the local side. However, the pre-training of a model relies heavily on accessing a large-scale of training data and requires a vast amount of computing resources. These strict requirements make it impossible for any single client to pre-train such a model. To grant clients with limited computing capability to participate in pre-training a large model, we propose a new learning approach, FedBERT, that takes advantage of the federated learning and split learning approaches, resorting to pre-training BERT in a federated way. FedBERT can prevent sharing the raw data information and obtain excellent performance. Extensive experiments on seven GLUE tasks demonstrate that FedBERT can maintain its effectiveness without communicating to the sensitive local data of clients.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {66},
numpages = {26},
keywords = {pre-training, Federated learning, NLP, BERT}
}

@inproceedings{10.1145/3366423.3380259,
author = {Shang, Jingbo and Zhang, Xinyang and Liu, Liyuan and Li, Sha and Han, Jiawei},
title = {NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380259},
doi = {10.1145/3366423.3380259},
abstract = {The automated construction of topic taxonomies can benefit numerous applications, including web search, recommendation, and knowledge discovery. One of the major advantages of automatic taxonomy construction is the ability to capture corpus-specific information and adapt to different scenarios. To better reflect the characteristics of a corpus, we take the meta-data of documents into consideration and view the corpus as a text-rich network. In this paper, we propose NetTaxo, a novel automatic topic taxonomy construction framework, which goes beyond the existing paradigm and allows text data to collaborate with network structure. Specifically, we learn term embeddings from both text and network as contexts. Network motifs are adopted to capture appropriate network contexts. We conduct an instance-level selection for motifs, which further refines term embedding according to the granularity and semantics of each taxonomy node. Clustering is then applied to obtain sub-topics under a taxonomy node. Extensive experiments on two real-world datasets demonstrate the superiority of our method over the state-of-the-art, and further verify the effectiveness and importance of instance-level motif selection.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1908–1919},
numpages = {12},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3338906.3338939,
author = {Bagherzadeh, Mehdi and Khatchadourian, Raffi},
title = {Going Big: A Large-Scale Study on What Big Data Developers Ask},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338939},
doi = {10.1145/3338906.3338939},
abstract = {Software developers are increasingly required to write big data code. However, they find big data software development challenging. To help these developers it is necessary to understand big data topics that they are interested in and the difficulty of finding answers for questions in these topics. In this work, we conduct a large-scale study on Stackoverflow to understand the interest and difficulties of big data developers. To conduct the study, we develop a set of big data tags to extract big data posts from Stackoverflow; use topic modeling to group these posts into big data topics; group similar topics into categories to construct a topic hierarchy; analyze popularity and difficulty of topics and their correlations; and discuss implications of our findings for practice, research and education of big data software development and investigate their coincidence with the findings of previous work.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {432–442},
numpages = {11},
keywords = {Big data topic difficulty, Big data topic hierarchy, Stackoverflow, Big data topics, Big data topic popularity},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3173574.3173922,
author = {Marathe, Megh and Toyama, Kentaro},
title = {Semi-Automated Coding for Qualitative Research: A User-Centered Inquiry and Initial Prototypes},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173922},
doi = {10.1145/3173574.3173922},
abstract = {Qualitative researchers perform an important and painstaking data annotation process known as coding. However, much of the process can be tedious and repetitive, becoming prohibitive for large datasets. Could coding be partially automated, and should it be? To answer this question, we interviewed researchers and observed them code interview transcripts. We found that across disciplines, researchers follow several coding practices well-suited to automation. Further, researchers desire automation after having developed a codebook and coded a subset of data, particularly in extending their coding to unseen data. Researchers also require any assistive tool to be transparent about its recommendations. Based on our findings, we built prototypes to partially automate coding using simple natural language processing techniques. Our top-performing system generates coding that matches human coders on inter-rater reliability measures. We discuss implications for interface and algorithm design, meta-issues around automating qualitative research, and suggestions for future work.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {qualitative research, qualitative coding, qualitative data analysis, natural language processing, user-centered design},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3132847.3132906,
author = {Chong, Wen-Haw and Lim, Ee-Peng},
title = {Tweet Geolocation: Leveraging Location, User and Peer Signals},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132906},
doi = {10.1145/3132847.3132906},
abstract = {Which venue is a tweet posted from? We referred this as fine-grained geolocation. To solve this problem effectively, we develop novel techniques to exploit each posting user's content history. This is motivated by our finding that most users do not share their visitation history, but have ample content history from tweet posts. We formulate fine-grained geolocation as a ranking problem whereby given a test tweet, we rank candidate venues. We propose several models that leverage on three types of signals from locations, users and peers. Firstly, the location signals are words that are indicative of venues. We propose a location-indicative weighting scheme to capture this. Next we exploit user signals from each user's content history to enrich the very limited content of their tweets which have been targeted for geolocation. The intuition is that the user's other tweets may have been from the test venue or related venues, thus providing informative words. In this regard, we propose query expansion as the enrichment approach. Finally, we exploit the signals from peer users who have similar content history and thus potentially similar visitation behavior as the users of the test tweets. This suggests collaborative filtering where visitation information is propagated via content similarities. We proposed several models incorporating different combinations of the three signals. Our experiments show that the best model incorporates all three signals. It performs 6% to 40% better than the baselines depending on the metric and dataset.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1279–1288},
numpages = {10},
keywords = {query expansion, tweet geolocation, collaborative filtering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1109/ICPC.2017.24,
author = {Lam, An Ngoc and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
title = {Bug Localization with Combination of Deep Learning and Information Retrieval},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.24},
doi = {10.1109/ICPC.2017.24},
abstract = {The automated task of locating the potential buggy files in a software project given a bug report is called bug localization. Bug localization helps developers focus on crucial files. However, the existing automated bug localization approaches face a key challenge, called lexical mismatch. Specifically, the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. To address that, we present a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files. Our empirical evaluation on real-world bug reports in the open-source projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, DnnLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. In 66% of the time, a correct buggy file is in the list of three suggested files. With 5 suggested files, it is correct in almost 70% of the cases.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {218–229},
numpages = {12},
keywords = {code retrieval, bug localization, deep learning},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3095713.3095738,
author = {Mauro, Massimo and Benini, Sergio and Adami, Nicola and Signoroni, Alberto and Leonardi, Riccardo and Canini, Luca},
title = {A Free Web API for Single and Multi-Document Summarization},
year = {2017},
isbn = {9781450353335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3095713.3095738},
doi = {10.1145/3095713.3095738},
abstract = {In this work we present a free Web API for single and multi-text summarization. The summarization algorithm follows an extractive approach, thus selecting the most relevant sentences from a single document or a document set. It integrates in a novel pipeline different text analysis techniques - ranging from keyword and entity extraction, to topic modelling and sentence clustering - and gives SoA competitive results. The application, written in Python, supports as input both plain texts and Web URLs. The API is publicly accessible for free using the specific conference token1 as described in the reference page2. The browser-based demo version, for summarization of single documents only, is publicly accessible at http://yonderlabs.com/demo.},
booktitle = {Proceedings of the 15th International Workshop on Content-Based Multimedia Indexing},
articleno = {24},
numpages = {5},
keywords = {Web API, Text Summarization},
location = {Florence, Italy},
series = {CBMI '17}
}

@inproceedings{10.1145/3494885.3494896,
author = {Zhang, Tao and Feng, Jun and Lu, Jiamin},
title = {Distant Supervision Relation Extraction via Reinforcement Learning with Potential Energy Function},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494896},
doi = {10.1145/3494885.3494896},
abstract = {Distant supervision has become an essential method for relation extraction. Although distant supervision is very effective, there is a large amount of noise in the dataset produced by distant supervision. To filter out the noise in the dataset, most recent models try to handle it by reinforcement learning, which require waiting for all sentences in the bags to be selected when calculating the reward, resulting in large time consumption in model training. To solve the problem, this paper proposes a method called distant supervision relation extraction via Reinforcement Learning with Potential Energy Function (PEF-RL). The model calculates the semantic similarity of sentences and relations through the relational alias table, and we pass the semantic similarity of sentences and relations as an additional reward to the potential energy function to guide the decision making process of the instance selector during reinforcement learning, which improves the learning efficiency of the reinforcement learning relation extraction model. Experiments show that our model has significant improvements in several dimensions compared to existing models.},
booktitle = {2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)},
pages = {61–66},
numpages = {6},
keywords = {Reinforcement learning, Distant supervision, Relation Extraction, Potential Energy Function},
location = {Singapore, Singapore},
series = {CSSE 2021}
}

@inproceedings{10.1145/3357384.3358099,
author = {Fu, Zhenxin and Ji, Feng and Hu, Wenpeng and Zhou, Wei and Zhao, Dongyan and Chen, Haiqing and Yan, Rui},
title = {Query-Bag Matching with Mutual Coverage for Information-Seeking Conversations in E-Commerce},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358099},
doi = {10.1145/3357384.3358099},
abstract = {Information-seeking conversation system aims at satisfying the information needs of users through conversations. Text matching between a user query and a pre-collected question is an important part of the information-seeking conversation in E-commerce. In the practical scenario, a sort of questions always correspond to a same answer. Naturally, these questions can form a bag. Learning the matching between user query and bag directly may improve the conversation performance, denoted as query-bag matching. Inspired by such opinion, we propose a query-bag matching model which mainly utilizes the mutual coverage between query and bag and measures the degree of the content in the query mentioned by the bag, and vice verse. In addition, the learned bag representation in word level helps find the main points of a bag in a fine grade and promotes the query-bag matching performance. Experiments on two datasets show the effectiveness of our model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2337–2340},
numpages = {4},
keywords = {bag, ranking, coverage, matching, e-commerce},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3497623.3497652,
author = {Zhang, Congcong and Zhao, Haifeng and Cao, Mingwei},
title = {Research on General Text Classification Model Integrating Character-Level Attention and Multi-Scale Features},
year = {2021},
isbn = {9781450390439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3497623.3497652},
doi = {10.1145/3497623.3497652},
abstract = {To solve the problem of poor interpretability of the model caused by the random initialization of convolution kernel in convolution neural network,and the problem of local and single feature extraction scale, a general character-level classification model is designed by referring to the method in CV. Firstly,a multi-scale feature extraction module is added to the network embedding layer to extract rich context information, and the problem of matrix sparsity is solved to some extent by setting different void rates. Then, the network depth is controlled by the number of blocks. Different depths have different grasps of global information and can adapt to tasks with different complexity. Next, add a modified attention mechanism module after the block to enhance the attention of the model to key parts. Finally, the full connection layer of the network is replaced by the full convolution layer to simplify the model. The block is a compressed version of deep separable convolution, and the overall model parameters are reduced to about one-tenth of the original, but the accuracy and performance are improved. The results show that the model is very effective.},
booktitle = {2021 10th International Conference on Computing and Pattern Recognition},
pages = {183–187},
numpages = {5},
keywords = {full convolution,compression, attention mechanism, multi-scale features},
location = {Shanghai, China},
series = {ICCPR 2021}
}

@inproceedings{10.1145/3488466.3488480,
author = {Fwa, Hua Leong},
title = {Enhancing Project Based Learning with Unsupervised Learning of Project Reflections},
year = {2021},
isbn = {9781450384995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488466.3488480},
doi = {10.1145/3488466.3488480},
abstract = {Natural Language Processing (NLP) is an area of research and application that uses computers to analyze human text. It has seen wide adoption within several industries but few studies have investigated it for use in evaluating the effectiveness of educational interventions and pedagogies. Pedagogies such as Project based learning (PBL) centers on learners solving an authentic problem or challenge which leads to knowledge creation and higher engagement. PBL also lends itself well in plugging the gap between what is taught in classrooms and applying the knowledge gained to the real working environment. In this study, we seek to investigate how we can use NLP techniques to uncover insights into and enhance our PBL process. Both topic modelling and sentiment analysis techniques are applied to analyze final year students’ reflections written as part of their final year project module. We described the entire process from text cleansing, pre-processing, modelling to visualization and evaluated the use of Latent Dirichlet Allocation and Attention Based Aspect Extraction for topic modelling. The results or visualizations which we derived from the topic and sentiment models showed that we can both effectively infer the key topics as reflected by our learners and extract actionable insights on the PBL process.},
booktitle = {2021 5th International Conference on Digital Technology in Education},
pages = {117–123},
numpages = {7},
keywords = {datasets, project-based learning, neural networks},
location = {Busan, Republic of Korea},
series = {ICDTE 2021}
}

@inproceedings{10.1145/3379597.3387496,
author = {Spinellis, Diomidis and Kotti, Zoe and Mockus, Audris},
title = {A Dataset for GitHub Repository Deduplication},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387496},
doi = {10.1145/3379597.3387496},
abstract = {GitHub projects can be easily replicated through the site's fork process or through a Git clone-push sequence. This is a problem for empirical software engineering, because it can lead to skewed results or mistrained machine learning models. We provide a dataset of 10.6 million GitHub projects that are copies of others, and link each record with the project's ultimate parent. The ultimate parents were derived from a ranking along six metrics. The related projects were calculated as the connected components of an 18.2 million node and 12 million edge denoised graph created by directing edges to ultimate parents. The graph was created by filtering out more than 30 hand-picked and 2.3 million pattern-matched clumping projects. Projects that introduced unwanted clumping were identified by repeatedly visualizing shortest path distances between unrelated important projects. Our dataset identified 30 thousand duplicate projects in an existing popular reference dataset of 1.8 million projects. An evaluation of our dataset against another created independently with different methods found a significant overlap, but also differences attributed to the operational definition of what projects are considered as related.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {523–527},
numpages = {5},
keywords = {project clone, Deduplication, GitHub, dataset, fork},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3331184.3331367,
author = {Wei, Penghui and Mao, Wenji},
title = {Modeling Transferable Topics for Cross-Target Stance Detection},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331367},
doi = {10.1145/3331184.3331367},
abstract = {Targeted stance detection aims to classify the attitude of an opinionated text towards a pre-defined target. Previous methods mainly focus on in-target setting that models are trained and tested using data specific to the same target. In practical cases, the target we concern may have few or no labeled data, which restrains us from training a target-specific model. In this paper we study the problem of cross-target stance detection, utilizing labeled data of a source target to learn models that can be adapted to a destination target. To this end, we propose an effective method, the core intuition of which is to leverage shared latent topics between two targets as transferable knowledge to facilitate model adaptation. Our method acquires topic knowledge with neural variational inference, and further adopts adversarial training that encourages the model to learn target-invariant representations. Experimental results verify that our proposed method is superior to the state-of-the-art methods.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1173–1176},
numpages = {4},
keywords = {cross-target stance detection, transferable topics, variational transfer network},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3319921.3319956,
author = {Jiaxian, Yuan and Gengming, Zhu},
title = {Algorithm Based on Improved Naive Bayesian for Predicting Microblog Behavior},
year = {2019},
isbn = {9781450361286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319921.3319956},
doi = {10.1145/3319921.3319956},
abstract = {This paper is aimed at predicting microblogs' content and three kinds counts of behaviors that including forward counts, comment counts and like counts, which analyse the overall characteristics of microblog, and propose an algorithm based on improved naive Bayesian-NB for predicting microblog behavior. Calculating microblogs' feature words based on TF*IDF, we classify microblog by LDA algorithm, to find the right category set. Select feature words used frequently from microblog, as predictive properties, and construct an improved naive Bayesian algorithm for predicting. The result of experiment shows that the recall rate, precision rate and F1 evaluation value are improved.},
booktitle = {Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence},
pages = {182–187},
numpages = {6},
keywords = {Improved naive Bayesian algorithm, TF*IDF, LDA algorithm, Predicting microblog behavior},
location = {Suzhou, China},
series = {ICIAI 2019}
}

@inproceedings{10.1145/3274895.3274977,
author = {Wei, Hong and Zhou, Hao and Sankaranarayanan, Jagan and Sengupta, Sudipta and Samet, Hanan},
title = {Detecting Latest Local Events from Geotagged Tweet Streams},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274977},
doi = {10.1145/3274895.3274977},
abstract = {Geotagged tweet streams contain invaluable information about the real-world local events like sports games, protests and traffic accidents. Timely detecting and extracting such events has various applications but yet unsolved challenges. In this paper, we present DeLLe, a methodology for automatically Detecting Latest Local Events from geotagged tweet streams. DeLLe first finds unusual locations which have aggregated unexpected number of tweets, and then ranks the unusual locations to select the top ones that are likely to be local event candidates. We evaluate DeLLe on the city of Seattle, WA as well as a larger city of New York. The results show that the proposed method generally outperforms competitive baseline approaches.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {520–523},
numpages = {4},
keywords = {event detection, geotagged tweet stream, Twitter, local events},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/3330430.3333668,
author = {Labhishetty, Sahiti and Bhavya and Pei, Kevin and Boughoula, Assma and Zhai, Chengxiang},
title = {Web of Slides: Automatic Linking of Lecture Slides to Facilitate Navigation},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333668},
doi = {10.1145/3330430.3333668},
abstract = {Lecture slides covering many topics are becoming increasingly available online, but they are scattered, making it a challenge for anyone to instantly access all slides relevant to a learning context. To address this challenge, we propose to create links between those scattered slides to form a Web of Slides (WOS). Using the sequential nature of slides, we present preliminary results of studying how to automatically create a basic link based on similarity of slides as an initial step toward the vision of WOS. We also explore interesting future research directions using different link types and the unique features of slides.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {54},
numpages = {4},
keywords = {MOOC, Link probabilities, Slides},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{10.1145/3022227.3022259,
author = {Minami, Daichi and Ushiama, Taketoshi},
title = {A Collaborative Filtering Method Based on Empathy with Reviewers},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022259},
doi = {10.1145/3022227.3022259},
abstract = {Today, it has become difficult for people to find books that fulfill their preference because the number of books in the world has become very large. In order to select books to read, the reviews on candidate books are helpful. Therefore, the role of online review sites which are web sites storing reviews of books is important. One of the functions in the online review sites is the collaborative filtering (CF) with reviewers. In general, the user-based CF is based on the assumption that the preferences of users who select the same items will be similar. However, in many cases, each user would have different viewpoints for their evaluation of books. In this paper, we focused on the difference of viewpoints when each user evaluate a book. We propose a CF method based on the empathy with other reviewers. Our method utilizes the evaluation for a review as feedback. Experimental results show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {33},
numpages = {6},
keywords = {book recommend, collaborative filtering, LDA, topic extraction, relevance feedback, review analysis},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@inproceedings{10.1145/3390557.3394128,
author = {Wang, Jiawei and Cui, Guorong and Zhu, Xiaoke and Liu, Huijian and Liu, Junsong and Jia, Xuebin},
title = {GSR: A Resource Model and Semantics-Based API Recommendation Algorithm},
year = {2020},
isbn = {9781450376587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3390557.3394128},
doi = {10.1145/3390557.3394128},
abstract = {With the rapid development of Web services, more and more Web services are published on the Internet. A Mashup application that aggregates multiple Web APIs is also becoming more popular. But it also brings a problem that is how to find a suitable API among a wide variety of APIs has become a challenge. To this end, this paper proposes a web service recommendation algorithm that combines graph databases and semantics. In this algorithm, we propose to use graph database to build a two-layer structure resource model. First, we use LDA for topic classification and classify Mashup and API of the same classification into the same category respectively. This helps reduce the number of searches for Mashup and API. When a user enters a requirement document, Word2vec and WMD algorithms are used to find similar Web API description text. Finally, we use similarity and API history invokes to propose a ranking algorithm to generate a recommendation list. Through real-world data, this experiment has a better-recommended performance.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence},
pages = {184–188},
numpages = {5},
keywords = {API recommendation, Word Mover's Distance, Resource Model, LDA},
location = {Xiamen, China},
series = {ICIAI 2020}
}

@inproceedings{10.1145/3388142.3388156,
author = {Akioyamen, Peter and Nicklas, Levi C and Sanchez-Arias, Reinaldo},
title = {A Framework for Intelligent Navigation Using Latent Dirichlet Allocation on Reddit Posts About Opiates},
year = {2020},
isbn = {9781450376440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388142.3388156},
doi = {10.1145/3388142.3388156},
abstract = {Many people look to the internet for support and assistance when faced with issues in life, particularly when these issues are related to behaviors or conditions that are stigmatized within society, generally making open discussion difficult. In this study, we utilize the unique characteristics of the news aggregation and discussion internet forum, reddit, to demonstrate the potential for text mining as an intelligent content filtering and navigation framework; we use online discussion surrounding opiates as a case study. Topic modeling is used as a text mining approach to organize and discover hidden semantic structures within reddit posts, developing a representation of a post through the topics and the words which comprise them. These characterizations may act as an intelligent navigation system of an online community, providing users the ability to actively navigate through similar posts and identify dissimilar ones based on their specific interests.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis},
pages = {190–196},
numpages = {7},
keywords = {online discussion boards, Latent Dirichlet Allocation, text mining},
location = {Silicon Valley, CA, USA},
series = {ICCDA 2020}
}

@inproceedings{10.1145/3366715.3366721,
author = {Jiang, Haiyang and Wang, Yuning and Yang, Yong},
title = {Fast Traffic Accident Identification Method Based on SSD Model},
year = {2019},
isbn = {9781450362429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366715.3366721},
doi = {10.1145/3366715.3366721},
abstract = {Traditional traffic accident identification methods have the problems of complex detection process, poor detection performance and poor real-time performance so far. In this paper, we propose a new type of traffic accident identification method based on target detection algorithm Single Shot MultiBox Detector (SSD). We collect and simulate traffic accident data sets in different scenarios and compare the detection performance of different target detection algorithms, aiming at the problems of traffic accident detection existing in the original SSD, the idea of multi-feature fusion and adaptive default box selection algorithm are proposed to improve it. Finally, we present an evaluation on the collected data, the improved SSD_A method shows considerable performance, which can reach 97% mAP (mean average precision) at the speed of 32 FPS (frames per second).},
booktitle = {Proceedings of the 2019 International Conference on Robotics Systems and Vehicle Technology},
pages = {177–181},
numpages = {5},
keywords = {SSD, Adaptive default box selection algorithm, Traffic accident identification, Multi-feature fusion},
location = {Wuhan, China},
series = {RSVT '19}
}

@inproceedings{10.1145/3323873.3325035,
author = {Patel, Yash and Gomez, Lluis and Rusi\~{n}ol, Mar\c{c}al and Karatzas, Dimosthenis and Jawahar, C.V.},
title = {Self-Supervised Visual Representations for Cross-Modal Retrieval},
year = {2019},
isbn = {9781450367653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323873.3325035},
doi = {10.1145/3323873.3325035},
abstract = {Cross-modal retrieval methods have been significantly improved in last years with the use of deep neural networks and large-scale annotated datasets such as ImageNet and Places. However, collecting and annotating such datasets requires a tremendous amount of human effort and, besides, their annotations are limited to discrete sets of popular visual classes that may not be representative of the richer semantics found on large-scale cross-modal retrieval datasets. In this paper, we present a self-supervised cross-modal retrieval framework that leverages as training data the correlations between images and text on the entire set of Wikipedia articles. Our method consists in training a CNN to predict: (1) the semantic context of the article in which an image is more probable to appear as an illustration, and (2) the semantic context of its caption. Our experiments demonstrate that the proposed method is not only capable of learning discriminative visual representations for solving vision tasks like classification, but that the learned representations are better for cross-modal retrieval when compared to supervised pre-training of the network on the ImageNet dataset.},
booktitle = {Proceedings of the 2019 on International Conference on Multimedia Retrieval},
pages = {182–186},
numpages = {5},
keywords = {visual representations, self-supervised learning, cross-modal retrieval},
location = {Ottawa ON, Canada},
series = {ICMR '19}
}

@inproceedings{10.1145/3184558.3191537,
author = {Otsuka, Atsushi and Nishida, Kyosuke and Bessho, Katsuji and Asano, Hisako and Tomita, Junji},
title = {Query Expansion with Neural Question-to-Answer Translation for FAQ-Based Question Answering},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191537},
doi = {10.1145/3184558.3191537},
abstract = {We propose a novel Frequently Asked Question (FAQ) retrieval technique with a neural query expansion model. With the growth in Question Answering systems and mobile communications, FAQ retrieval systems have become widely used in site searches and call center support. However, FAQ retrieval often has lexical gaps between queries and answer documents. To bridge these gaps, we design a query expansion model on the basis of an Encoder-Decoder model as a type of deep neural network. The model learns the words that appear in answers for questions using Q&amp;A pair documents and generates the expanded queries from inputted queries to retrieve answer documents. We evaluate our proposed technique in a multi-domain FAQ retrieval task. Experimental results show that our technique retrieves FAQs more accurately than the previous methods.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1063–1068},
numpages = {6},
keywords = {encoder-decoder model, query expansion, faq},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3236024.3236036,
author = {Nguyen, Thanh and Tran, Ngoc and Phan, Hung and Nguyen, Trong and Truong, Linh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
title = {Complementing Global and Local Contexts in Representing API Descriptions to Improve API Retrieval Tasks},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236036},
doi = {10.1145/3236024.3236036},
abstract = {When being trained on API documentation and tutorials, Word2vec produces vector representations to estimate the relevance between texts and API elements. However, existing Word2vec-based approaches to measure document similarities aggregate Word2vec vectors of individual words or APIs to build the representation of a document as if the words are independent. Thus, the semantics of API descriptions or code fragments are not well represented. In this work, we introduce D2Vec, a new model that fits with API documentation better than Word2vec. D2Vec is a neural network model that considers two complementary contexts to better capture the semantics of API documentation. We first connect the global context of the current API topic under description to all the text phrases within the description of that API. Second, the local orders of words and API elements in the text phrases are maintained in computing the vector representations for the APIs. We conducted an experiment to verify two intrinsic properties of D2Vec's vectors: 1) similar words and relevant API elements are projected into nearby locations; and 2) some vector operations carry semantics. We demonstrate the usefulness and good performance of D2Vec in three applications: API code search (text-to-code retrieval), API tutorial fragment search (code-to-text retrieval), and mining API mappings between software libraries (code-to-code retrieval). Finally, we provide actionable insights and implications for researchers in using our model in other applications with other types of documents.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {551–562},
numpages = {12},
keywords = {Code Search, API Mappings, API documents, Word2vec, Big Code},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3018661.3018680,
author = {Yuan, Quan and Zhang, Wei and Zhang, Chao and Geng, Xinhe and Cong, Gao and Han, Jiawei},
title = {PRED: Periodic Region Detection for Mobility Modeling of Social Media Users},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018680},
doi = {10.1145/3018661.3018680},
abstract = {The availability of massive geo-annotated social media data sheds light on studying human mobility patterns. Among them, periodic pattern, ie an individual visiting a geographical region with some specific time interval, has been recognized as one of the most important. Mining periodic patterns has a variety of applications, such as location prediction, anomaly detection, and location- and time-aware recommendation. However, it is a challenging task: the regions of a person and the periods of each region are both unknown. The interdependency between them makes the task even harder. Hence, existing methods are far from satisfactory for detecting periodic patterns from the low-sampling and noisy social media data.We propose a Bayesian non-parametric model, named textbf{P}eriodic textbf{RE}gion textbf{D}etection (PRED), to discover periodic mobility patterns by jointly modeling the geographical and temporal information. Our method differs from previous studies in that it is non-parametric and thus does not require priori knowledge about an individual's mobility (eg number of regions, period length, region size). Meanwhile, it models the time gap between two consecutive records rather than the exact visit time, making it less sensitive to data noise. Extensive experimental results on both synthetic and real-world datasets show that PRED outperforms the state-of-the-art methods significantly in four tasks: periodic region discovery, outlier movement finding, period detection, and location prediction.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {263–272},
numpages = {10},
keywords = {periodicity detection, mobility modeling, location prediction},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3494885.3494896,
author = {Zhang, Tao and Feng, Jun and Lu, Jiamin},
title = {Distant Supervision Relation Extraction via Reinforcement Learning with Potential Energy Function},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494896},
doi = {10.1145/3494885.3494896},
abstract = {Distant supervision has become an essential method for relation extraction. Although distant supervision is very effective, there is a large amount of noise in the dataset produced by distant supervision. To filter out the noise in the dataset, most recent models try to handle it by reinforcement learning, which require waiting for all sentences in the bags to be selected when calculating the reward, resulting in large time consumption in model training. To solve the problem, this paper proposes a method called distant supervision relation extraction via Reinforcement Learning with Potential Energy Function (PEF-RL). The model calculates the semantic similarity of sentences and relations through the relational alias table, and we pass the semantic similarity of sentences and relations as an additional reward to the potential energy function to guide the decision making process of the instance selector during reinforcement learning, which improves the learning efficiency of the reinforcement learning relation extraction model. Experiments show that our model has significant improvements in several dimensions compared to existing models.},
booktitle = {2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)},
pages = {61–66},
numpages = {6},
keywords = {Reinforcement learning, Distant supervision, Relation Extraction, Potential Energy Function},
location = {Singapore, Singapore},
series = {CSSE 2021}
}

@inproceedings{10.1145/3357384.3358099,
author = {Fu, Zhenxin and Ji, Feng and Hu, Wenpeng and Zhou, Wei and Zhao, Dongyan and Chen, Haiqing and Yan, Rui},
title = {Query-Bag Matching with Mutual Coverage for Information-Seeking Conversations in E-Commerce},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358099},
doi = {10.1145/3357384.3358099},
abstract = {Information-seeking conversation system aims at satisfying the information needs of users through conversations. Text matching between a user query and a pre-collected question is an important part of the information-seeking conversation in E-commerce. In the practical scenario, a sort of questions always correspond to a same answer. Naturally, these questions can form a bag. Learning the matching between user query and bag directly may improve the conversation performance, denoted as query-bag matching. Inspired by such opinion, we propose a query-bag matching model which mainly utilizes the mutual coverage between query and bag and measures the degree of the content in the query mentioned by the bag, and vice verse. In addition, the learned bag representation in word level helps find the main points of a bag in a fine grade and promotes the query-bag matching performance. Experiments on two datasets show the effectiveness of our model.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2337–2340},
numpages = {4},
keywords = {bag, ranking, coverage, matching, e-commerce},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3236024.3236036,
author = {Nguyen, Thanh and Tran, Ngoc and Phan, Hung and Nguyen, Trong and Truong, Linh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
title = {Complementing Global and Local Contexts in Representing API Descriptions to Improve API Retrieval Tasks},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236036},
doi = {10.1145/3236024.3236036},
abstract = {When being trained on API documentation and tutorials, Word2vec produces vector representations to estimate the relevance between texts and API elements. However, existing Word2vec-based approaches to measure document similarities aggregate Word2vec vectors of individual words or APIs to build the representation of a document as if the words are independent. Thus, the semantics of API descriptions or code fragments are not well represented. In this work, we introduce D2Vec, a new model that fits with API documentation better than Word2vec. D2Vec is a neural network model that considers two complementary contexts to better capture the semantics of API documentation. We first connect the global context of the current API topic under description to all the text phrases within the description of that API. Second, the local orders of words and API elements in the text phrases are maintained in computing the vector representations for the APIs. We conducted an experiment to verify two intrinsic properties of D2Vec's vectors: 1) similar words and relevant API elements are projected into nearby locations; and 2) some vector operations carry semantics. We demonstrate the usefulness and good performance of D2Vec in three applications: API code search (text-to-code retrieval), API tutorial fragment search (code-to-text retrieval), and mining API mappings between software libraries (code-to-code retrieval). Finally, we provide actionable insights and implications for researchers in using our model in other applications with other types of documents.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {551–562},
numpages = {12},
keywords = {Code Search, API Mappings, API documents, Word2vec, Big Code},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3018661.3018680,
author = {Yuan, Quan and Zhang, Wei and Zhang, Chao and Geng, Xinhe and Cong, Gao and Han, Jiawei},
title = {PRED: Periodic Region Detection for Mobility Modeling of Social Media Users},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018680},
doi = {10.1145/3018661.3018680},
abstract = {The availability of massive geo-annotated social media data sheds light on studying human mobility patterns. Among them, periodic pattern, ie an individual visiting a geographical region with some specific time interval, has been recognized as one of the most important. Mining periodic patterns has a variety of applications, such as location prediction, anomaly detection, and location- and time-aware recommendation. However, it is a challenging task: the regions of a person and the periods of each region are both unknown. The interdependency between them makes the task even harder. Hence, existing methods are far from satisfactory for detecting periodic patterns from the low-sampling and noisy social media data.We propose a Bayesian non-parametric model, named textbf{P}eriodic textbf{RE}gion textbf{D}etection (PRED), to discover periodic mobility patterns by jointly modeling the geographical and temporal information. Our method differs from previous studies in that it is non-parametric and thus does not require priori knowledge about an individual's mobility (eg number of regions, period length, region size). Meanwhile, it models the time gap between two consecutive records rather than the exact visit time, making it less sensitive to data noise. Extensive experimental results on both synthetic and real-world datasets show that PRED outperforms the state-of-the-art methods significantly in four tasks: periodic region discovery, outlier movement finding, period detection, and location prediction.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {263–272},
numpages = {10},
keywords = {periodicity detection, mobility modeling, location prediction},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@article{10.1145/3451397,
author = {Gao, Jianliang and Ying, Xiaoting and Xu, Cong and Wang, Jianxin and Zhang, Shichao and Li, Zhao},
title = {Graph-Based Stock Recommendation by Time-Aware Relational Attention Network},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3451397},
doi = {10.1145/3451397},
abstract = {The stock market investors aim at maximizing their investment returns. Stock recommendation task is to recommend stocks with higher return ratios for the investors. Most stock prediction methods study the historical sequence patterns to predict stock trend or price in the near future. In fact, the future price of a stock is correlated not only with its historical price, but also with other stocks. In this article, we take into account the relationships between stocks (corporations) by stock relation graph. Furthermore, we propose a Time-aware Relational Attention Network (TRAN) for graph-based stock recommendation according to return ratio ranking. In TRAN, the time-aware relational attention mechanism is designed to capture time-varying correlation strengths between stocks by the interaction of historical sequences and stock description documents. With the dynamic strengths, the nodes of the stock relation graph aggregate the features of neighbor stock nodes by graph convolution operation. For a given group of stocks, the proposed TRAN model can output the ranking results of stocks according to their return ratios. The experimental results on several real-world datasets demonstrate the effectiveness of our TRAN for stock recommendation.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {4},
numpages = {21},
keywords = {relational attention network, Stock relation graph, stock recommendation, time-aware, knowledge discovery}
}

@inproceedings{10.1145/3411764.3445396,
author = {Zhang, Xiaoyu and Chandrasegaran, Senthil and Ma, Kwan-Liu},
title = {ConceptScope: Organizing and Visualizing Knowledge in Documents Based on Domain Ontology},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445396},
doi = {10.1145/3411764.3445396},
abstract = {Current text visualization techniques typically provide overviews of document content and structure using intrinsic properties such as term frequencies, co-occurrences, and sentence structures. Such visualizations lack conceptual overviews incorporating domain-relevant knowledge, needed when examining documents such as research articles or technical reports. To address this shortcoming, we present ConceptScope, a technique that utilizes a domain ontology to represent the conceptual relationships in a document in the form of a Bubble Treemap visualization. Multiple coordinated views of document structure and concept hierarchy with text overviews further aid document analysis. ConceptScope facilitates exploration and comparison of single and multiple documents respectively. We demonstrate ConceptScope by visualizing research articles and transcripts of technical presentations in computer science. In a comparative study with DocuBurst, a popular document visualization tool, ConceptScope was found to be more informative in exploring and comparing domain-specific documents, but less so when it came to documents that spanned multiple disciplines.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {19},
numpages = {13},
keywords = {Ontology, Visualization, Knowledge Representation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3403949,
author = {Akanfe, Oluwafemi and Valecha, Rohit and Rao, H. Raghav},
title = {Design of an Inclusive Financial Privacy Index (INF-PIE): A Financial Privacy and Digital Financial Inclusion Perspective},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3403949},
doi = {10.1145/3403949},
abstract = {Financial privacy is an important part of an individual's privacy, but efforts to enhance financial privacy have often not been given enough prominence by some countries when advancing financial inclusion. This impedes under-served communities from utilizing financial services. This article adopts a design science approach to create an INclusive Financial Privacy IndEx (INF-PIE) from the two perspectives of financial privacy and digital financial inclusion to help ensure financial services for a wide range of populations. This article first examines the privacy policies of Mobile Wallet and Remittance (MWR) apps (a digital financial solution), uses an analytics approach for extracting semi-structured information components; and based on text categorization and topic modeling, creates privacy policy compliance scores. In particular, it analyses the privacy policies using natural language processing techniques such as Term Frequency-Inverse Document Frequency (tf-idf) and Latent Dirichlet Allocation (LDA). This article then develops a digital financial inclusion score through a multivariate analysis of indexes extracted from the global findex dataset using Principal Component Analysis (PCA). Finally, the INF-PIE framework is established to analyze various countries and assess their financial privacy and digital financial inclusion practices. This framework can show how countries’ relative data privacy compliance and digital financial inclusion practices underscore their inclusive financial privacy.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {dec},
articleno = {7},
numpages = {21},
keywords = {Digital financial inclusion, compliance score, GDPR, inclusive privacy index, digital payments systems, mobile wallet and remittance}
}

@inproceedings{10.1145/3209978.3209983,
author = {Ahmad, Wasi Uddin and Chang, Kai-Wei and Wang, Hongning},
title = {Intent-Aware Query Obfuscation for Privacy Protection in Personalized Web Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3209983},
doi = {10.1145/3209978.3209983},
abstract = {Modern web search engines exploit users' search history to personalize search results, with a goal of improving their service utility on a per-user basis. But it is this very dimension that leads to the risk of privacy infringement and raises serious public concerns. In this work, we propose a client-centered intent-aware query obfuscation solution for protecting user privacy in a personalized web search scenario. In our solution, each user query is submitted with l additional cover queries and corresponding clicks, which act as decoys to mask users' genuine search intent from a search engine. The cover queries are sequentially sampled from a set of hierarchically organized language models to ensure the coherency of fake search intents in a cover search task. Our approach emphasizes the plausibility of generated cover queries, not only to the current genuine query but also to previous queries in the same task, to increase the complexity for a search engine to identify a user's true intent. We also develop two new metrics from an information theoretic perspective to evaluate the effectiveness of provided privacy protection. Comprehensive experiment comparisons with state-of-the-art query obfuscation techniques are performed on the public AOL search log, and the propitious results substantiate the effectiveness of our solution.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {285–294},
numpages = {10},
keywords = {search privacy, query obfuscation, search tasks},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@article{10.14778/3407790.3407793,
author = {Zhang, Dan and Hulsebos, Madelon and Suhara, Yoshihiko and Demiralp, \c{C}a\u{g}atay and Li, Jinfeng and Tan, Wang-Chiew},
title = {Sato: Contextual Semantic Type Detection in Tables},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407793},
doi = {10.14778/3407790.3407793},
abstract = {Detecting the semantic types of data columns in relational tables is important for various data preparation and information retrieval tasks such as data cleaning, schema matching, data discovery, and semantic search. However, existing detection approaches either perform poorly with dirty data, support only a limited number of semantic types, fail to incorporate the table context of columns or rely on large sample sizes for training data. We introduce Sato, a hybrid machine learning model to automatically detect the semantic types of columns in tables, exploiting the signals from the table context as well as the column values. Sato combines a deep learning model trained on a large-scale table corpus with topic modeling and structured prediction to achieve support-weighted and macro average F1 scores of 0.925 and 0.735, respectively, exceeding the state-of-the-art performance by a significant margin. We extensively analyze the overall and per-type performance of Sato, discussing how individual modeling components, as well as feature categories, contribute to its performance.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1835–1848},
numpages = {14}
}

@article{10.1145/3091995,
author = {Zhou, Guang-You and Huang, Jimmy Xiangji},
title = {Modeling and Mining Domain Shared Knowledge for Sentiment Analysis},
year = {2017},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3091995},
doi = {10.1145/3091995},
abstract = {Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). In real applications, these user-generated sentiment data can span so many different domains that it is difficult to label the training data for all of them. Therefore, we study the problem of sentiment classification adaptation task in this article. That is, a system is trained to label reviews from one source domain but is meant to be used on the target domain. One of the biggest challenges for sentiment classification adaptation task is how to deal with the problem when two data distributions between the source domain and target domain are significantly different from one another. However, our observation is that there might exist some domain shared knowledge among certain input dimensions of different domains. In this article, we present a novel method for modeling and mining the domain shared knowledge from different sentiment review domains via a joint non-negative matrix factorization–based framework. In this proposed framework, we attempt to learn the domain shared knowledge and the domain-specific information from different sentiment review domains with several various regularization constraints. The advantage of the proposed method can promote the correspondence under the topic space between the source domain and the target domain, which can significantly reduce the data distribution gap across two domains. We conduct extensive experiments on two real-world balanced data sets from Amazon product reviews for sentence-level and document-level binary sentiment classification. Experimental results show that our proposed approach significantly outperforms several strong baselines and achieves an accuracy that is competitive with the most well-known methods for sentiment classification adaptation.},
journal = {ACM Trans. Inf. Syst.},
month = {aug},
articleno = {18},
numpages = {36},
keywords = {Natural Language Processing, Information Retrieval, Sentiment Analysis}
}

@inproceedings{10.1145/3385032.3385052,
author = {Dhasade, Akash Balasaheb and Venigalla, Akhila Sri Manasa and Chimalakonda, Sridhar},
title = {Towards Prioritizing GitHub Issues},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385052},
doi = {10.1145/3385032.3385052},
abstract = {The vast growth in usage of GitHub by developers to host their projects has led to extensive forking and open source contributions. These contributions occur in the form of issues that report bugs or pull requests to either fix bugs or add new features to the project. On the other hand, massive increase in the number of issues reported by developers and users is a major challenge for integrators, as the number of concurrent issues to be handled is much higher than the number of core contributors. While there exists prior work on prioritizing pull requests, in this paper we make an attempt towards prioritizing issues using machine learning techniques. We present the Issue Prioritizer, a tool to prioritize issues based on three criteria: issue lifetime, issue hotness and category of the issue. We see this work as an initial step towards supporting developers to handle large volumes of issues in projects.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {18},
numpages = {5},
keywords = {GitHub Issues, Priority Ranking, Automatic Issue Prioritisation, Dynamic Tracking, Multiple Concurrent Issues},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@article{10.5555/3122009.3242063,
author = {Ray, Avik and Neeman, Joe and Sanghavi, Sujay and Shakkottai, Sanjay},
title = {The Search Problem in Mixture Models},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the task of learning the parameters of a single component of a mixture model, for the case when we are given side information about that component; we call this the "search problem" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components.Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {7547–7607},
numpages = {61},
keywords = {mixture models, side information, method of moments, semi-supervised, search}
}

@inproceedings{10.1145/3308560.3314195,
author = {Brochier, Robin},
title = {Representation Learning for Recommender Systems with Application to the Scientific Literature},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3314195},
doi = {10.1145/3308560.3314195},
abstract = {The scientific literature is a large information network linking various actors (laboratories, companies, institutions, etc.). The vast amount of data generated by this network constitutes a dynamic heterogeneous attributed network (HAN), in which new information is constantly produced and from which it is increasingly difficult to extract content of interest. In this article, I present my first thesis works in partnership with an industrial company, Digital Scientific Research Technology. This later offers a scientific watch tool, Peerus, addressing various issues, such as the real time recommendation of newly published papers or the search for active experts to start new collaborations. To tackle this diversity of applications, a common approach consists in learning representations of the nodes and attributes of this HAN and use them as features for a variety of recommendation tasks. However, most works on attributed network embedding pay too little attention to textual attributes and do not fully take advantage of recent natural language processing techniques. Moreover, proposed methods that jointly learn node and document representations do not provide a way to effectively infer representations for new documents for which network information is missing, which happens to be crucial in real time recommender systems. Finally, the interplay between textual and graph data in text-attributed heterogeneous networks remains an open research direction.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {12–16},
numpages = {5},
keywords = {recommender systems, scientific literature, network embedding, representation learning},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3404835.3463100,
author = {Huang, Zhiqi and Rahimi, Razieh and Yu, Puxuan and Shang, Jingbo and Allan, James},
title = {AutoName: A Corpus-Based Set Naming Framework},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463100},
doi = {10.1145/3404835.3463100},
abstract = {We propose AutoName, an unsupervised framework that extracts a name for a set of query entities from a large-scale text corpus. Entity-set naming is useful in many tasks related to natural language processing and information retrieval such as session-based and conversational information seeking. Previous studies mainly extract set names from knowledge bases which provide highly reliable entity relations, but suffer from limited coverage of entities and set names that represent broad semantic classes. To address these problems, AutoName generates hypernym-anchored candidate phrases via probing a pre-trained language model and the entities' context in documents. Phrases are then clustered to identify ones that describe common concepts among query entities. Finally, AutoName ranks refined phrases based on the co-occurrences of their words with query entities and the conceptual integrity of their respective clusters. We built a new benchmark dataset for this task, consisting of 130 entity sets with name labels. Experimental results show that AutoName generates coherent and meaningful set names and significantly outperforms all baselines.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2101–2105},
numpages = {5},
keywords = {conceptual clustering, entity set naming, language model probing},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3321408.3326659,
author = {Pan, Suhan and Li, Zhiqiang and Dai, Juan},
title = {An Improved TextRank Keywords Extraction Algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326659},
doi = {10.1145/3321408.3326659},
abstract = {Keywords extraction is widely used in the field of natural language processing. How to quickly and accurately extract keywords has become the key issue in text processing. At present, there are many methods for keywords extraction, but the accuracy and versatility of the method still have much room for improvement. Thus, an improved TextRank keywords extraction algorithm is proposed in this paper. The algorithm uses the TF-IDF algorithm and the average information entropy algorithm to calculate the importance of words, and then calculates the comprehensive weight of words based on the calculation results in the text. The initial weight of the TextRank algorithm node and the node probability transfer matrix are improved by using the comprehensive weight of words, and the weights of all nodes are iteratively calculated until convergence. The weights of the nodes are sorted to obtain the weight information of the words, then the top N words are selected as the keywords.Finally, the keywords extraction function is realized by outputting the keywords. The experimental results show that compared with the traditional TF-IDF method and TextRank method, the improved TextRank keyword extraction method proposed in this paper is more general and its accuracy of extracting keywords is higher.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {131},
numpages = {7},
keywords = {average information entropy, TF-IDF algorithm, keywords extraction, TextRank algorithm, natural language processing},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3478905.3478981,
author = {Wang, Keyang and Zeng, Yiping and Meng, Fanyu and Feiyu and Yang, Lili},
title = {Comparison between Calculation Methods for Semantic Text Similarity Based on Siamese Networks},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478981},
doi = {10.1145/3478905.3478981},
abstract = {In the era of information explosion, people are eager to obtain contents that meet their own needs and interests from massive amounts of information. Therefore, how to understand the needs of Internet users correctly and effectively is one of the urgent problems to be solved. In this case, semantic text similarity task is useful in many application scenarios. To measure semantic text similarity based on text matching model, several Siamese networks are constructed in this paper. Specifically, we firstly use the Stsbenchmark dataset, regarding the GloVe, BERT and DistilBERT as initial models, and add deep neural networks to train and fine-tune, fully utilizing the advantages of the existing models. Next, we test several similarity calculation methods to quantify the semantic similarity of sentence pairs. Moreover, the Pearson and Spearman correlation coefficients are used as evaluation indicators to compare the sentence embedding effects of different models. Finally, experiment result shows the Siamese network based on BERT model has the optimal effect among all, with the highest accuracy rate up to 84.5%. While among several similarity calculation methods, the Cosine Similarity usually obtain the best accuracy rate. In the future, this model can be appropriately used in semantic text similarity tasks, through matching texts between users’ needs and knowledge base. In this way, we can improve machines' language understanding ability as well as meeting the diverse needs of users.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {389–395},
numpages = {7},
keywords = {Siamese network, sentence embedding, Semantic text similarity, similarity calculation methods},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3404835.3462798,
author = {Potthast, Martin and G\"{u}nther, Sebastian and Bevendorff, Janek and Bittner, Jan Philipp and Bondarenko, Alexander and Fr\"{o}be, Maik and Kahmann, Christian and Niekler, Andreas and V\"{o}lske, Michael and Stein, Benno and Hagen, Matthias},
title = {The Information Retrieval Anthology},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462798},
doi = {10.1145/3404835.3462798},
abstract = {We present the IR Anthology, a corpus of information retrieval publications accessible via a metadata browser and a full-text search engine. Following the example of the well-known ACL Anthology, the IR Anthology serves as a hub for researchers interested in information retrieval. Our search engine ChatNoir indexes the publications' full texts, enabling a focused search and linking users to the respective publisher's site for personal access. Listing more than 40,000 publications at the time of writing, the IR Anthology can be freely accessed at https://IR.webis.de.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2550–2555},
numpages = {6},
keywords = {bibliography, scientific literature analysis, scholarly search},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3469877.3495639,
author = {Yu, Shengze and Wang, Xin and Zhu, Wenwu},
title = {CFCR: A Convolution and Fusion Model for Cross-Platform Recommendation},
year = {2021},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3495639},
doi = {10.1145/3469877.3495639},
abstract = {With the emergence of various online platforms, associating different platforms is playing an increasingly important role in many applications. Cross-platform recommendation aims to improve recommendation accuracy through associating information from different platforms. Existing methods do not fully exploit high-order nonlinear connectivity information in cross-domain recommendation scenario and suffer from domain-incompatibility problem. In this paper, we propose an end-to-end convolution and fusion model for cross-platform recommendation (CFCR). The proposed CFCR model utilizes Graph Convolution Networks (GCN) to extract user and item features on graphs from different platforms, and fuses cross-platform information by Multimodal AutoEncoder (MAE) with common latent user features. Therefore, the high-order connectivity information is preserved to the most extent and domain-invariant user representations are automatically obtained. The domain-incompatible information is spontaneously discarded to avoid messing up the cross-platform association. Extensive experiments for the proposed CFCR model on real-world dataset demonstrate its advantages over existing cross-platform recommendation methods in terms of various evaluation metrics.},
booktitle = {ACM Multimedia Asia},
articleno = {65},
numpages = {6},
keywords = {multimodal autoencoder, graph convolution network, cross-platform recommendation},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

@inproceedings{10.1145/3400934.3400955,
author = {Putri, Tsarina Dwi and Zulkarnain},
title = {Proposed Model of Academic Reading Material Recommendation System},
year = {2020},
isbn = {9781450376006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400934.3400955},
doi = {10.1145/3400934.3400955},
abstract = {Cold-start problem and cold-item are something that will happen when an early developed online library of educational institution library tries to recommend scientific articles to users. The reading materials do not even have reviews and/or ratings from previous users, no users have expressed preferences yet, also solely rely on keywords in search engines. The fact that there are abundant holdings in the library, it needs to effectively maintain users' interests to borrow and download academic reading material in accordance with users' interest from holdings in the library repository. This study seeks to provide novelty by finding another way to utilize dataset with only using abstract and title variables as an input parallelly that can provide effective results as a recommendation system. It proposes a word embedding model to be used as topic modeling for the content-based recommendation system to overcome the problems, wherein the attributes are minimum (such as title, author, and abstract) and user data are not available.},
booktitle = {Proceedings of the 3rd Asia Pacific Conference on Research in Industrial and Systems Engineering 2020},
pages = {105–109},
numpages = {5},
keywords = {neural network embedding, word embedding, recommendation system, Academic reading material},
location = {Depok, Indonesia},
series = {APCORISE 2020}
}

@inproceedings{10.1145/3341216.3342209,
author = {Zhu, Lin and Zhao, Juan and Wang, Yiting and Feng, Juanlan and Deng, Chao and Li, Hui},
title = {Smart Prediction of the Complaint Hotspot Problem in Mobile Network},
year = {2019},
isbn = {9781450368728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341216.3342209},
doi = {10.1145/3341216.3342209},
abstract = {In mobile network, a complaint hotspot problem often affects even thousands of users' service and leads to significant economic losses and bulk complaints. In this paper, we propose an approach to predict a customer complaint based on real-time user signalling data. Through analyzing the network and user sevice procedure, 30 key data fields related to user experience have been extracted in XDR data collected from the S1 interface. Furthermore, we augment these basic features with derived features for user experience evaluation, such as one-hot features, statistical features and differential features. Considering the problems of unbalanced data, we use LightGBM as our prediction model. LightGBM has strong generalization ability and was designed to handle unbalanced data. Experiments we conducted prove the effectiveness and efficiency of this proposal. This approach has been deployed for daily routine to locate the hot complaint problem scope as well as to report affected users and area.},
booktitle = {Proceedings of the 2019 Workshop on Network Meets AI &amp; ML},
pages = {22–28},
numpages = {7},
keywords = {Signalling data, LightGBM classifier, Prediction of the complaint hotspot problem, Feature extraction},
location = {Beijing, China},
series = {NetAI'19}
}

@inproceedings{10.1145/3274856.3274862,
author = {Intisar, Chowdhury Md and Watanobe, Yutaka},
title = {Cluster Analysis to Estimate the Difficulty of Programming Problems},
year = {2018},
isbn = {9781450365161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274856.3274862},
doi = {10.1145/3274856.3274862},
abstract = {Programming is one of the vital skills for the next generation. Currently, there are many online platforms where programmers compete and solve programming problems. Those platforms are composed of problems with varying degree of difficulties. For expert programmers, the difficulty level is not a concern, but it is very important for novice programmers to approach programming problems based on their experience and level. Thus it is important to construct an expert system which can categorize the programming problems based on their difficulties. In our research, we have proposed an expert system which is based on fuzzy rules derivation. These fuzzy rules have been derived by performing cluster analysis on submission log data of Aizu Online judge database. Different clustering algorithms were examined based on the features of these programming problems. The performance of the expert system was compared with 3 different learning models (Decision tree, Random forest, K-nearest neighbor). A high accuracy score on the testing set proves the validity of our constructed fuzzy rules for the expert system.},
booktitle = {Proceedings of the 3rd International Conference on Applications in Information Technology},
pages = {23–28},
numpages = {6},
keywords = {Cluster analysis, fuzzy rules, competitive programming, on-line judge system, expert system},
location = {Aizu-Wakamatsu, Japan},
series = {ICAIT'2018}
}

@inproceedings{10.1145/3184558.3191606,
author = {Tietz, Tabea and Pichierri, Francesca and Koutraki, Maria and Hallinan, Dara and Boehm, Franziska and Sack, Harald},
title = {Digital Zombies - the Reanimation of Our Digital Selves},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191606},
doi = {10.1145/3184558.3191606},
abstract = {What happens to our social media profiles when we die The episode "Be Right Back" as part of Netflix's series "Black Mirror" provides a possible scenario. A digital avatar is created to communicate with close relatives which learns from past social media activities of the deceased user. While the users entrust their social media content to one or more companies, even after their death, it may be reasonable to ask: What will the company really do with a deceased user's data: sell it to manipulate users or create advertisements In this paper we tackle the issues of ownership, ethics, and transparency of post mortem user data.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1535–1539},
numpages = {5},
keywords = {black mirror, law, social media, privacy, artificial intelligence, transparency, ethics},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3123024.3124412,
author = {Krumm, John and Kun, Andrew L. and Vars\'{a}nyi, Petra},
title = {TweetCount: Urban Insights by Counting Tweets},
year = {2017},
isbn = {9781450351904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123024.3124412},
doi = {10.1145/3123024.3124412},
abstract = {This paper characterizes an urban region using time series of geotagged tweet counts. Time series are constructed for each cell in a rectangular grid. We show how simple, anonymous tweet counts in the cells can be used to classify the cells into urban land use profiles based on the number of residences and businesses. We discover that Twitter activity for a certain short time of day is especially indicative of a region's profile. We go on to analyze the cells and profiles in a novel way by looking at their ability to predict tweet counts in other parts of the region.},
booktitle = {Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {403–411},
numpages = {9},
keywords = {clustering, Twitter, maps, geography, New York city, prediction, location, urban computing},
location = {Maui, Hawaii},
series = {UbiComp '17}
}

@inproceedings{10.1145/3542954.3542963,
author = {Roy, Tamal Joyti and Mahmood, Md. Ashiq and Mohanta, Aninda},
title = {An Efficient Approach to Validate COVID-19 Related Vaccine Myths Utilizing LDA Algorithm},
year = {2022},
isbn = {9781450397346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542954.3542963},
doi = {10.1145/3542954.3542963},
abstract = {Scientists from the whole world have been working their heart and soul to invent the COVID-19 vaccine. When they are succeed to make the vaccine, various rumors are spread. COVID-19 situation has made our world standstill. When the vaccine came out for the first time, people were enthusiastic to take a shot. But the myth, rumors about vaccination also followed the success. In this paper, we have tried to validate the COVID-19 related vaccine myth and rumors with the help of the LDA algorithm. We have used data mining, text mining and sentiment analysis for the experiment. The outcome of our experiment has shown that most people are positive about vaccination but the negative impact is also there. Our experiment has found that most of the people are talking about “vaccine”, “people”,” moron” and “ever”. We have proposed a technique to validate this kind of vaccine myth. LDA algorithms have been able to predict and validate the myth up to 70% compared to other frameworks out there. Promising efficiency is exhibited by our experimental result.},
booktitle = {Proceedings of the 2nd International Conference on Computing Advancements},
pages = {53–58},
numpages = {6},
keywords = {Text Mining, Data Mining, LDA, Sentiment Analysis},
location = {Dhaka, Bangladesh},
series = {ICCA '22}
}

@inproceedings{10.1145/3395027.3419589,
author = {Rakib, Md Rashadul Hasan and Zeh, Norbert and Milios, Evangelos},
title = {Short Text Stream Clustering via Frequent Word Pairs and Reassignment of Outliers to Clusters},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419589},
doi = {10.1145/3395027.3419589},
abstract = {Short text stream clustering is an important but challenging task since massive amounts of text are generated from different social media. Given streams of texts, the proposed method clusters the streams of texts based on the frequently occurring word pairs (not necessarily consecutive) in texts. It detects outliers in the clusters and reassigns the outliers to appropriate clusters using the semantic similarity between the outliers and the clusters based on the dynamically computed similarity thresholds. Thus the proposed method efficiently deals with the concept drift problem. Experimental results demonstrate that the proposed approach outperforms the state-of-the-art short text stream clustering algorithms by a statistically significant margin on several short text datasets.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {13},
numpages = {4},
keywords = {text stream clustering, word pair, outlier reassignment},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/3456172.3456211,
author = {Hananto, Valentinus R. and Serd\"{u}lt, Uwe and Kryssanov, Victor},
title = {A Tourism Knowledge Model through Topic Modeling from Online Reviews},
year = {2021},
isbn = {9781450388450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456172.3456211},
doi = {10.1145/3456172.3456211},
abstract = {Ontologies and knowledge models have gained more recognition because of their extensive use in recommender systems. The lack of automatic approaches in ontology engineering, however, becomes a challenge to fulfill increasing needs for such knowledge models in the field of tourism. In this study, a system for building tourism knowledge models from online reviews is proposed. The main contribution of the study is the application of topic modeling to build a knowledge model that, in turn, allows for an automated labeling process to train classifiers. Given a collection of unlabeled tourism online reviews, Latent Dirichlet Allocation (LDA) is applied to automatically label each document. Each topic discovered by LDA is labeled with one specific category, representing its semantic meaning based on an existing general ontology as a reference. These automatically labeled documents are used for classification, and the result is compared with manual annotation. Experiments on Indonesian tourism datasets showed that the automatic labeling approach using LDA provides for a precision score of 70%. In classification tasks, this approach can achieve comparable or even better classification performance than the manual labeling. The results obtained suggest that the developed system is capable of building a tourism knowledge model and providing acceptable-quality training data for the development of tourism recommender systems.},
booktitle = {2021 7th International Conference on Computing and Data Engineering},
pages = {87–93},
numpages = {7},
keywords = {recommender systems, topic modeling, tourism knowledge model},
location = {Phuket, Thailand},
series = {ICCDE 2021}
}

@inproceedings{10.1145/3308560.3316605,
author = {Goswami, Anjan and Mohapatra, Prasant and Zhai, Chengxiang},
title = {Quantifying and Visualizing the Demand and Supply Gap from E-Commerce Search Data Using Topic Models},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316605},
doi = {10.1145/3308560.3316605},
abstract = {The demand generation and assortment planning are two critical components of running a retail business. Traditionally, retail companies use the historical sales data for modeling and optimization of assortment selection, and they use a marketing strategy for demand generation. However, today, most retail businesses have e-commerce sites with rapidly growing online sales. An e-commerce site typically has to maintain a large amount of digitized product data, and it also keeps a vast amount of historical customer interaction data that includes search, browse, click, purchase and many other different interactions. In this paper, we show how this digitized product data and the historical search logs can be used in understanding and quantifying the gap between the supply and demand side of a retail market. This gap helps in making an effective strategy for both demand generation and assortment selection. We construct topic models of the historical search queries and the digitized product data from the catalog. We use the former to model the customer demand and the later to model the supply side of the retail business. We then create a tool to visualize the topic models to understand the differences between the supply and demand side. We also quantify the supply and demand gap by defining a metric based on Kullback-Leibler (KL) divergence of topic distributions of queries and the products. The quantification helps us identifying the topics related to excess or less demand and thereby in designing effective strategies for demand generation and assortment selection. Application of this work by e-Commerce retailers can result in the development of product innovations that can be utilized to achieve economic equilibrium. We can identify the excess demand and can provide insight to the teams responsible for improving assortment and catalog quality. Similarly, we can also identify excess supply and can provide that intelligence to the teams responsible for demand generation. Tools of this nature can be developed to systematically drive efficiency in achieving better economic gains for the entire e-commerce engine. We conduct several experiments collecting data from Walmart.com to validate the effectiveness of our approach.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {348–353},
numpages = {6},
keywords = {Topic Models, Marketplace economics, Information retrieval, E-commerce search, Business Analytics},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3514262.3514297,
author = {Li, Xue and Li, Yue},
title = {Talent Skill Demand Analysis in the E-Commerce Industry Based on Online Recruitment Data},
year = {2022},
isbn = {9781450387187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514262.3514297},
doi = {10.1145/3514262.3514297},
abstract = {To alleviate the employment difficulties of college students in the e-commerce industry and the labor shortage of enterprises, this study takes the recruitment information of the e-commerce industry as the research object and explores the talent skill demand of the e-commerce industry as the research objective. Starting from the analysis system of "industry-position-skill," this paper collects the job recruitment information of the e-commerce industry from recruitment websites, carries out text mining of the recruitment information, and analyzes the core skills demand of all positions in the e-commerce industry. The results show differences in the requirements of business skills and technical skills for different types of jobs, and there are high similarities in the provisions of comprehensive skills.},
booktitle = {2022 13th International Conference on E-Education, E-Business, E-Management, and E-Learning (IC4E)},
pages = {377–383},
numpages = {7},
keywords = {network recruitment, talent skill demand, LDA, e-commerce, text mining},
location = {Tokyo, Japan},
series = {IC4E 2022}
}

@inproceedings{10.1145/3341105.3374036,
author = {de Castro-Cabrera, M. del Carmen and Garc\'{\i}a-Dominguez, Antonio and Medina-Bulo, Inmaculada},
title = {Trends in Prioritization of Test Cases: 2017-2019},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374036},
doi = {10.1145/3341105.3374036},
abstract = {A core task in software testing is the design of test suites. Large test suites may take too long to run frequently, and test case prioritization (TCP) techniques have been proposed to speed up the detection of faults. These techniques have become increasingly popular and the number of publications has grown in recent years. Surveys have covered most of the techniques, but the latest included only publications until 2016: interest is growing, and new proposals have been developed in the last three years. This paper aims to complete that survey by providing the latest developments in TCP to respond to this growing interest. Specifically we use the taxonomy proposed by Khatibsyarbin et al. on the most important publications from 2017 to the present day (2019). All in all, we found 320 papers in this period about test case prioritization. The results show that the main techniques used are search-, coverage- and similarity-based.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {2005–2011},
numpages = {7},
keywords = {software testing, systematic literature review, TCP, test case prioritization, regression testing},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3377170.3377278,
author = {Zhai, Chenggong and Fei, Xiande and Yang, Zhiwei},
title = {Design of Integrated Platform for Clothing and Accouterment Support Based on Big Data},
year = {2019},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377278},
doi = {10.1145/3377170.3377278},
abstract = {Based on the in-depth analysis of the necessity and feasibility of the optimization of big data-based quilt support, this paper puts forward the overall framework and implementation concept of the optimization of big data-based quilt support, and describes how to build the basic matching of the optimization of big data-based quilt support and the concept of big data information system system. Design of integrated platform for clothing and accouterment support based on big data that collects, stores, processes, analyzes and mines the supply data of conscription bedding by using big data, cloud computing and other information technologies, which plays an important role in deepening the reform of military bedding and strengthening the scientific management of bedding.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {54–58},
numpages = {5},
keywords = {Integrated Platform, Clothing and Accouterment, Big Data},
location = {Shanghai, China},
series = {ICIT 2019}
}

@inproceedings{10.1145/3041021.3051154,
author = {Kong, Xiangjie and Jiang, Huizhen and Bekele, Teshome Megersa and Wang, Wei and Xu, Zhenzhen},
title = {Random Walk-Based Beneficial Collaborators Recommendation Exploiting Dynamic Research Interests and Academic Influence},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3051154},
doi = {10.1145/3041021.3051154},
abstract = {It is laborious for researchers to find proper collaborators who can provide researching guidance besides collaborating. Beneficial Collaborators (BCs), researchers who have a high academic level and relevant topics, can genuinely help researchers to enrich their research. Though many efforts have made to develop collaborator recommendation, most of existing works have mainly focused on recommending most possible collaborators with no intention to recommend specifically the BCs. In this paper, we propose the Beneficial Collaborator Recommendation (BCR) model that considers the dynamic research interest of researcher's and academic level of collaborators to recommend the BCs. First, we run the LDA model on the abstract of researchers' publications in each year for topic clustering. Second, we fix generated topic distribution matrix by a time function to fit interest dynamic transformation. Third, we compute the similarity between the collaboration candidate's feature matrix and the target researcher. Finally, we combine the similarity and influence in collaborators network to fix rank score and mine the candidates with high academic level and academic social impact. BCR generates the topN BCs recommendation. Extensive experiments on a dataset with citation network demonstrate that BCR performs better in terms of precision, recall, F1 score and the recommendation quality compared to baseline methods.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1371–1377},
numpages = {7},
keywords = {collaborator recommendation, dynamic research interest, academic influence},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1145/3086635,
author = {Cui, Chaoran and Shen, Jialie and Nie, Liqiang and Hong, Richang and Ma, Jun},
title = {Augmented Collaborative Filtering for Sparseness Reduction in Personalized POI Recommendation},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3086635},
doi = {10.1145/3086635},
abstract = {As mobile device penetration increases, it has become pervasive for images to be associated with locations in the form of geotags. Geotags bridge the gap between the physical world and the cyberspace, giving rise to new opportunities to extract further insights into user preferences and behaviors. In this article, we aim to exploit geotagged photos from online photo-sharing sites for the purpose of personalized Point-of-Interest (POI) recommendation. Owing to the fact that most users have only very limited travel experiences, data sparseness poses a formidable challenge to personalized POI recommendation. To alleviate data sparseness, we propose to augment current collaborative filtering algorithms along from multiple perspectives. Specifically, hybrid preference cues comprising user-uploaded and user-favored photos are harvested to study users’ tastes. Moreover, heterogeneous high-order relationship information is jointly captured from user social networks and POI multimodal contents with hypergraph models. We also build upon the matrix factorization algorithm to integrate the disparate sources of preference and relationship information, and apply our approach to directly optimize user preference rankings. Extensive experiments on a large and publicly accessible dataset well verified the potential of our approach for addressing data sparseness and offering quality recommendations to users, especially for those who have only limited travel experiences.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {sep},
articleno = {71},
numpages = {23},
keywords = {personalized POI recommendation, hypergraph-based learning, Geotagged photos}
}

@inproceedings{10.1145/3441417.3441421,
author = {Reza Sadeghi, Hamid Reza and Shiry Shiry Ghidary, Saeed and Bakhtiari Bakhtiari Bastaki, Benhur},
title = {A Method for Improving Unsupervised Intent Detection Using Bi-LSTM CNN Cross Attention Mechanism},
year = {2020},
isbn = {9781450387842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441417.3441421},
doi = {10.1145/3441417.3441421},
abstract = {Spoken Language Understanding (SLU) can be considered the most important sub-system in a goal-oriented dialogue system. SLU consists of User Intent Detection (UID) and Slot Filling (SF) modules. The accuracy of these modules is highly dependent on the collected data. On the other hand, labeling operation is a tedious task due to the large number of labels required. In this paper, intent labeling for two datasets is performed using an unsupervised learning method. In traditional methods of extracting features from text, the feature space that is obtained is very large, therefore we implemented a novel architecture of auto-encoder neural networks that is based on the attention mechanism to extract small and efficient feature space. This architecture which is called Bi-LSTM CNN Cross Attention Mechanism (BCCAM), crosswise applies the attention mechanism from Convolutional Neural Network (CNN) layer to Bi-LSTM layer and vice versa. Then, after finding a bottleneck on this auto-encoder network, the desired features are extracted from it. Once the features are extracted, then we cluster each sentence corresponding to its feature space using different clustering algorithms, including K-means, DEC, Agglomerative, OPTICS and Gaussian mixture model. In order to evaluate the performance of the model, two datasets are used, including ATIS and SNIPS. After executing various algorithms over the extracted feature space, the best obtained accuracy and NMI for ATIS dataset are 86.5 and 91.6, respectively, and for SNIPS dataset are 49.9 and 43.0, respectively.},
booktitle = {2020 The 4th International Conference on Advances in Artificial Intelligence},
pages = {41–46},
numpages = {6},
keywords = {Deep Clustering, Cross Attention Mechanism, Text Clustering, Spoken Language Understanding, Unsupervised Intent Detection, Text Feature Extraction},
location = {London, United Kingdom},
series = {ICAAI 2020}
}

@inproceedings{10.1145/3019612.3019842,
author = {Umar, Hafsah and Arandjelovi\'{c}, Ognjen},
title = {Learning Nuanced Cross-Disciplinary Citation Metric Normalization Using the Hierarchical Dirichlet Process on Big Scholarly Data},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019842},
doi = {10.1145/3019612.3019842},
abstract = {Citation counts have long been used in academia as a way of measuring, inter alia, the importance of journals, quantifying the significance and the impact of a researcher's body of work, and allocating funding for individuals and departments. For example, the h-index proposed by Hirsch is one of the most popular metrics that utilizes citation analysis to determine an individual's research impact. Among many issues, one of the pitfalls of citation metrics is the unfairness which emerges when comparisons are made between researchers in different fields. The algorithm we described in the present paper learns evidence based, nuanced, and probabilistic representations of academic fields, and uses data collected by crawling Google Scholar to perform field of study based normalization of citation based impact metrics such as the h-index.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1842–1847},
numpages = {6},
keywords = {academic, university, publishing, index, quantification, science, publication},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@article{10.1145/3392875,
author = {Shahid, Farhana and Ony, Shahinul Hoque and Albi, Takrim Rahman and Chellappan, Sriram and Vashistha, Aditya and Islam, A. B. M. Alim Al},
title = {Learning from Tweets: Opportunities and Challenges to Inform Policy Making During Dengue Epidemic},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392875},
doi = {10.1145/3392875},
abstract = {Social media platforms are widely used by people to report, access, and share information during outbreaks and epidemics. Although government agencies and healthcare institutions in developed regions are increasingly relying on social media to develop epidemic forecasts and outbreak response, there is a limited understanding of how people in developing regions interact on social media during outbreaks and what useful insights this dataset could offer during public health crises. In this work, we examined 28,688 tweets to identify public health issues during dengue epidemic in Bangladesh and found several insights, such as irregularities in dengue diagnosis and treatment, shortage of blood supply for Rh negative blood groups, and high local transmission of dengue during Eid-ul-Adha, that impact disease preparedness and outbreak response. We discuss the opportunities and challenges in analyzing tweets and outline how government agencies and healthcare institutions can use social media health data to inform policy making during public health crises.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {may},
articleno = {65},
numpages = {27},
keywords = {hci4d, epidemic, dengue, blood donation, bangladesh, twitter, public health crisis, outbreak, health policy}
}

@article{10.1145/3055534,
author = {Al-Ramahi, Mohammad A. and Liu, Jun and El-Gayar, Omar F.},
title = {Discovering Design Principles for Health Behavioral Change Support Systems: A Text Mining Approach},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3055534},
doi = {10.1145/3055534},
abstract = {Behavioral Change Support Systems (BCSSs) aim to change users’ behavior and lifestyle. These systems have been gaining popularity with the proliferation of wearable devices and recent advances in mobile technologies. In this article, we extend the existing literature by discovering design principles for health BCSSs based on a systematic analysis of users’ feedback. Using mobile diabetes applications as an example of Health BCSSs, we use topic modeling to discover design principles from online user reviews. We demonstrate the importance of the design principles through analyzing their existence in users’ complaints. Overall, the results highlight the necessity of going beyond the techno-centric approach used in current practice and incorporating the social and organizational features into persuasive systems design, as well as integrating with medical devices and other systems in their usage context.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jun},
articleno = {5},
numpages = {24},
keywords = {Mobile diabetes apps, topic mining, Latent Dirichlet Allocation (LDA), online user reviews}
}

@article{10.1145/3466641,
author = {Zhang, Chengyuan and Wang, Yang and Zhu, Lei and Song, Jiayu and Yin, Hongzhi},
title = {Multi-Graph Heterogeneous Interaction Fusion for Social Recommendation},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3466641},
doi = {10.1145/3466641},
abstract = {With the rapid development of online social recommendation system, substantial methods have been proposed. Unlike traditional recommendation system, social recommendation performs by integrating social relationship features, where there are two major challenges, i.e., early summarization and data sparsity. Thus far, they have not been solved effectively. In this article, we propose a novel social recommendation approach, namely Multi-Graph Heterogeneous Interaction Fusion (MG-HIF), to solve these two problems. Our basic idea is to fuse heterogeneous interaction features from multi-graphs, i.e., user–item bipartite graph and social relation network, to improve the vertex representation learning. A meta-path cross-fusion model is proposed to fuse multi-hop heterogeneous interaction features via discrete cross-correlations. Based on that, a social relation GAN is developed to explore latent friendships of each user. We further fuse representations from two graphs by a novel multi-graph information fusion strategy with attention mechanism. To the best of our knowledge, this is the first work to combine meta-path with social relation representation. To evaluate the performance of MG-HIF, we compare MG-HIF with seven states of the art over four benchmark datasets. The experimental results show that MG-HIF achieves better performance.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {28},
numpages = {26},
keywords = {meta-path, Social recommendation, graph gan, heterogeneous interaction fusion, multi-graph}
}

@inproceedings{10.1145/3535511.3535520,
author = {Gomes, Anna Luiza and Vianna, Get\'{u}lio and Escovedo, Tatiana and Kalinowski, Marcos},
title = {Predicting IMDb Rating of TV Series with Deep Learning: The Case of Arrow},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535520},
doi = {10.1145/3535511.3535520},
abstract = {Context: The number of TV series offered nowadays is very high. Due to its large amount, many series are canceled due to a lack of originality that generates a low audience.Problem: Having a decision support system that can show why some shows are a huge success or not would facilitate the choices of renewing or starting a show.Solution: We studied the case of the series Arrow broadcasted by CW Network and used descriptive and predictive modeling techniques to predict the IMDb rating. We assumed that the theme of the episode would affect its evaluation by users, so the dataset is composed only by the director of the episode, the number of reviews that episode got, the percentual of each theme extracted by the Latent Dirichlet Allocation (LDA) model of an episode, the number of viewers from Wikipedia and the rating from IMDb. The LDA model is a generative probabilistic model of a collection of documents made up of words.IS Theory: This study was developed under the aegis of Computational Learning Theory, which aims to understand the fundamental principles of learning and contribute to designing better-automated learning methods applied to the entertainment business.Method: In this prescriptive research, the case study method was used, and its results were analyzed using a quantitative approach.Summary of Results: With the features of each episode, the model that performed the best to predict the rating was Catboost due to a similar mean squared error of the KNN model but a better standard deviation during the test phase. It was possible to predict IMDb ratings with an acceptable root mean squared error of 0.55.Contributions and Impact in the IS area: The results show that deep learning techniques can be applied to support decisions in the entertainment field, allowing facilitating the decisions of renewing or starting a show. The rationale for building the model is detailed throughout the paper and can be replicated for other contexts.},
booktitle = {XVIII Brazilian Symposium on Information Systems},
articleno = {9},
numpages = {6},
keywords = {tv-series, LDA, Machine Learning, Deep Learning, IMDb, prediction},
location = {Curitiba, Brazil},
series = {SBSI}
}

@inproceedings{10.1145/3487351.3490972,
author = {Zirbilek, Nadir Emre and Erakin, Mustafa and \"{O}zyer, Tansel and Alhajj, Reda},
title = {Hot Topic Detection and Evaluation of Multi-Relation Effects},
year = {2021},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3490972},
doi = {10.1145/3487351.3490972},
abstract = {With the growth of social media, Twitter has become one of the most popularly used microblogging communication platforms between people. Due to the wide preference of Twitter, popular issues in public, events like local or global news and daily life stories can immediately publish on Twitter. Thus, a substantial number of hot topics are created by Twitter users in real-time. These topics can exhibit every incident of everyday life. Therefore, detection of hot topics can be used in many applications such as observing public judgment, product recommendation, and incidence detection. In this paper, we propose a method for detecting Twitter hot topics and evaluate the effect of multi-relations such as retweets and hashtags on hot topics. The dataset was generated by fetching tweets for a certain time and location by using GetOldTweets3 API. Then using the LDA topic modeling algorithm the hot topics were identified for each multi relation. Finally, the effect of each relation is described by using the coherence scores)},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {416–422},
numpages = {7},
keywords = {social media, hot topic detection, tweeter, multi-relations},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/3308558.3313518,
author = {Xie, Yufei and Liu, Shuchun and Yao, Tangren and Peng, Yao and Lu, Zhao},
title = {Focusing Attention Network for Answer Ranking},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313518},
doi = {10.1145/3308558.3313518},
abstract = {Answer ranking is an important task in Community Question Answering (CQA), by which “Good” answers should be ranked in the front of “Bad” or “Potentially Useful” answers. The state of the art is the attention-based classification framework that learns the mapping between the questions and the answers. However, we observe that existing attention-based methods perform poorly on complicated question-answer pairs. One major reason is that existing methods cannot get accurate alignments between questions and answers for such pairs. We call the phenomenon “attention divergence”. In this paper, we propose a new attention mechanism, called Focusing Attention Network(FAN), which can automatically draw back the divergent attention by adding the semantic, and metadata features. Our Model can focus on the most important part of the sentence and therefore improve the answer ranking performance. Experimental results on the CQA dataset of SemEval-2016 and SemEval-2017 demonstrate that our method respectively attains 79.38 and 88.72 on MAP and outperforms the Top-1 system in the shared task by 0.19 and 0.29.},
booktitle = {The World Wide Web Conference},
pages = {3384–3390},
numpages = {7},
keywords = {Answer Ranking, Semantic Features, Focusing Attention Network, Metadata Features},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3210459.3210475,
author = {Williams, Ashley},
title = {Do Software Engineering Practitioners Cite Research on Software Testing in Their Online Articles? A Preliminary Survey.},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210475},
doi = {10.1145/3210459.3210475},
abstract = {Background: Software engineering (SE) research continues to study the degree to which practitioners perceive that SE research has impact on practice. Such studies typically comprise surveys of practitioner opinions. These surveys could be complemented with other in situ practitioner sources e.g. grey literature.Objective: To investigate whether and how practitioners cite software testing research in their online articles.Method: We conduct 11,200 web searches using a customized Google-based search tool, scrape the pages of 722 unique results, and then analyse the articles for citations to research.Results: We find few citations to research (range 0% - 1% in our datasets) although this is similar to the frequency of citations to practitioner sites (0% - 4%). We find and discuss the only two significant instances of practitioners citing research in our datasets.Conclusion: We conducted a preliminary survey that complements the findings of previous work. But our survey contains a number of threats to validity. Our results should therefore be interpreted as hypotheses to motivate further investigation into the frequency to which practitioners cite research, and into the impact of research on practice.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {151–156},
numpages = {6},
keywords = {research impact, grey literature, research relevance, Evidence},
location = {Christchurch, New Zealand},
series = {EASE'18}
}

@inproceedings{10.1145/3232116.3232131,
author = {Yunsheng, Ge and Jie, Kong},
title = {Chinese Word Segmentation Model Based on BI_GRU_AT_HN_CRF_6},
year = {2018},
isbn = {9781450364966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232116.3232131},
doi = {10.1145/3232116.3232131},
abstract = {Chinese word segmentation is an indispensable step in natural language processing, and it is also the most important step. At present, the use of recurrent neural network to Chinese word segmentation model has become a new trend. The researchers proposed various models based on the LSTM network model of long and short memory and the word segmentation method based on the GRU network model. Both LSTM and GRU are a type of circulatory neural network that inherits the ability to automatically learn and long-short term memory characteristics. However, in the process of Chinese word segmentation, as the length of the sentence becomes longer, the inter-dependent feature distance in the context becomes farther, resulting in the loss of the historical feature information and future feature information that the given sentence depends on, thereby reducing the accuracy of word segmentation. In order to solve this problem, this paper introduces the attention mechanism and proposes the BI_GRU_AT_HW_CRF_6 neural network segmentation model. Experiments show that with the introduction of attentional mechanism, with the change of sentence, there is a better performance in accuracy, training, and forecasting data speed.},
booktitle = {Proceedings of the 3rd International Conference on Intelligent Information Processing},
pages = {88–94},
numpages = {7},
keywords = {Long-Short Term Memory Recurrent Neural Network (LSTM), Gate control cycle unit (GRU), Chinese Word Segmentation, Attention Mechanism},
location = {Guilin, China},
series = {ICIIP '18}
}

@inproceedings{10.1145/3477495.3531784,
author = {Zhang, Qi and Zhou, Jie and Chen, Qin and Bai, Qingchun and He, Liang},
title = {Enhancing Event-Level Sentiment Analysis with Structured Arguments},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531784},
doi = {10.1145/3477495.3531784},
abstract = {Previous studies about event-level sentiment analysis (SA) usually model the event as a topic, a category or target terms, while the structured arguments (e.g., subject, object, time and location) that have potential effects on the sentiment are not well studied. In this paper, we redefine the task as structured event-level SA and propose an End-to-End Event-level Sentiment Analysis (E3SA) approach to solve this issue. Specifically, we explicitly extract and model the event structure information for enhancing event-level SA. Extensive experiments demonstrate the great advantages of our proposed approach over the state-of-the-art methods. Noting the lack of the dataset, we also release a large-scale real-world dataset with event arguments and sentiment labelling for promoting more researches.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1944–1949},
numpages = {6},
keywords = {datasets, structured, event-level sentiment analysis},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3478905.3478916,
author = {Li, Jian-hua and Zhang, Chen-xi and Lei, Chun-li and Zhang, Hong and Chen, Lin-long},
title = {Product Recommendation Algorithm for Score Prediction Based on Joint Feature Vector Extraction},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478916},
doi = {10.1145/3478905.3478916},
abstract = {In order to make full use of multiple types of data and improve the accuracy of the recommendation algorithm, a scoring prediction model combining multi-source heterogeneous data such as product attributes, user reviews and friend relationships is proposed for product recommendation. In this algorithm, the product attributes and user comment information are combined, the combination method of LDA and Word2vec is adopted to extract user-commodity joint features, a community division algorithm is used to divide users into communities, features and product ratings are integrated to establish a rating prediction model and the recommended results are obtained by grading forecasting and ranking. The experiment results show that, compared with collaborative filtering and existing recommendation algorithms, the proposed model combining multi-source heterogeneous data can reduce the root mean square error of prediction scores, improve the accuracy of prediction, and increase the accuracy of product recommendation.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {55–61},
numpages = {7},
keywords = {scoring prediction, multi-source heterogeneous, LDA, Word2vec, community division},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3395260.3395291,
author = {Long, Sun and Yan, Li},
title = {Rank-IDF: A Statistical and Network Based Feature Words Selection in Big Data Text Analysis},
year = {2020},
isbn = {9781450377072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395260.3395291},
doi = {10.1145/3395260.3395291},
abstract = {As big data for text has been one of the core data types in the era of artificial intelligence, feature words selection technique become increasingly important in big data text analysis. The traditional statistical TF-IDF feature words selection algorithm lacks the semantic information extraction ability of text, while the network model Textrank applies the sentence semantic features to feature calculation between words. Network model such as Textrank is very suitable for text feature selection, but it does not take influencing factors of the relationship between documents into consideration, so common words appearing frequently in feature words selected result. Based on the analysis of both feature words selection method, this paper raises a combination of statistical and network model integrated the advantages of Textrank and TF-IDF, and proposes a text feature selection method based on Rank-IDF. The Rank-IDF algorithm has better feature selection and common word filtering effects.},
booktitle = {Proceedings of the 2020 5th International Conference on Mathematics and Artificial Intelligence},
pages = {80–84},
numpages = {5},
keywords = {Textrank, Text mining, TF-IDF, Feature selection},
location = {Chengdu, China},
series = {ICMAI 2020}
}

@inproceedings{10.1145/3131704.3131721,
author = {Yu, Huan and Xia, Xin and Zhao, Xiaoqiong and Qiu, Weiwei},
title = {Combining Collaborative Filtering and Topic Modeling for More Accurate Android Mobile App Library Recommendation},
year = {2017},
isbn = {9781450353137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131704.3131721},
doi = {10.1145/3131704.3131721},
abstract = {The applying of third party libraries is an integral part of many mobile applications. With the rapid development of mobile technologies, there are many free third party libraries for developers to download and use. However, there are a large number of third party libraries which always iterate rapidly, it is hard for developers to find available libraries within them. Several previous studies have proposed approaches to recommend third party libraries, which works in the scenario where a developer knows some required libraries, and needs to find other relevant libraries with limited knowledge. In the paper, to further improve the performance of app library recommendation, we propose an approach which combines collaborative filtering and topic modeling techniques. In the collaborative filtering component, given a new app, our approach recommends libraries by using its similar apps. In the topic modelling component, our approach first extracts the topics from the textual description of mobile apps, and given a new app, our approach recommends libraries based on the libraries used by the apps which has similar topic distributions. We perform experiments on a set of 1,013 apps, and the results show that our approach improves the state-of-the-art by a substantial margin.},
booktitle = {Proceedings of the 9th Asia-Pacific Symposium on Internetware},
articleno = {17},
numpages = {6},
keywords = {Library Recommendation, Topic Modeling, Android App, Collaborative Filtering},
location = {Shanghai, China},
series = {Internetware '17}
}

@inproceedings{10.1145/3068839.3068846,
author = {Tang, Mengzi and Li, Li},
title = {Role-Aware Conformity Influence Analysis in Recommender Systems},
year = {2017},
isbn = {9781450349833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3068839.3068846},
doi = {10.1145/3068839.3068846},
abstract = {Recommender systems play an important role in providing personalized information to users and helping address the information overload problem. Recent research has considered social theories and studied the importance of social influence in social recommendation systems. However, many publications ignored the users' roles information or just considered some single roles. In fact, users often have many different roles. Besides, different types of users (users with different roles) might have different conformity tendency. Thus, this inspires us to study how conformity tendency changes with users' roles in recommender systems. We firstly formalize conformity influence by defining a utility function and then propose a probabilistic graphical model integrating both users' roles and conformity tendency, named as Role Conformity Recommender Systems (RCRS). We evaluate the proposed model on several real-world datasets. The experimental results show that our model significantly outperforms state-of-the-art approaches.},
booktitle = {Proceedings of the 20th International Workshop on the Web and Databases},
pages = {29–34},
numpages = {6},
keywords = {Social conformity, Users roles, Recommender systems, Probabilistic graphical model},
location = {Chicago, IL, USA},
series = {WebDB'17}
}

@inproceedings{10.1145/3018896.3018969,
author = {Onan, Aytu\u{g}},
title = {A Machine Learning Based Approach to Identify Geo-Location of Twitter Users},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018969},
doi = {10.1145/3018896.3018969},
abstract = {Twitter, a popular microblogging platform, has attracted great attention. Twitter enables people from all over the world to interact in an extremely personal way. The immense quantity of user-generated text messages become available on Twitter that could potentially serve as an important source of information for researchers and practitioners. The information available on Twitter may be utilized for many purposes, such as event detection, public health and crisis management. In order to effectively coordinate such activities, the identification of Twitter users' geo-locations is extremely important. Though online social networks can provide some sort of geo-location information based on GPS coordinates, Twitter suffers from geo-location sparseness problem. The identification of Twitter users' geo-location based on the content of send out messages, becomes extremely important. In this regard, this paper presents a machine learning based approach to the problem. In this study, our corpora is represented as a word vector. To obtain a classification scheme with high predictive performance, the performance of five classification algorithms, three ensemble methods and two feature selection methods are evaluated. Among the compared algorithms, the highest results (84.85%) is achieved by AdaBoost ensemble of Random Forest, when the feature set is selected with the use of consistency-based feature selection method in conjunction with best first search.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {69},
numpages = {6},
keywords = {text mining, machine learning, location-based estimation, geo-location identification},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@article{10.1145/3015458,
author = {Yao, Yuan and Zhao, Wayne Xin and Wang, Yaojing and Tong, Hanghang and Xu, Feng and Lu, Jian},
title = {Version-Aware Rating Prediction for Mobile App Recommendation},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3015458},
doi = {10.1145/3015458},
abstract = {With the great popularity of mobile devices, the amount of mobile apps has grown at a more dramatic rate than ever expected. A technical challenge is how to recommend suitable apps to mobile users. In this work, we identify and focus on a unique characteristic that exists in mobile app recommendation—that is, an app usually corresponds to multiple release versions. Based on this characteristic, we propose a fine-grain version-aware app recommendation problem. Instead of directly learning the users’ preferences over the apps, we aim to infer the ratings of users on a specific version of an app. However, the user-version rating matrix will be sparser than the corresponding user-app rating matrix, making existing recommendation methods less effective. In view of this, our approach has made two major extensions. First, we leverage the review text that is associated with each rating record; more importantly, we consider two types of version-based correlations. The first type is to capture the temporal correlations between multiple versions within the same app, and the second type of correlation is to capture the aggregation correlations between similar apps. Experimental results on a large dataset demonstrate the superiority of our approach over several competitive methods.},
journal = {ACM Trans. Inf. Syst.},
month = {jun},
articleno = {38},
numpages = {33},
keywords = {App rating prediction, version correlation, recommender systems}
}

@inproceedings{10.1145/3487553.3524654,
author = {Chakraborty, Prantika and Dutta, Sudakshina and Sanyal, Debarshi Kumar},
title = {Personal Research Knowledge Graphs},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524654},
doi = {10.1145/3487553.3524654},
abstract = {Maintaining research-related information in an organized manner can be challenging for a researcher. In this paper, we envision personal research knowledge graphs (PRKGs) as a means to represent structured information about the research activities of a researcher. PRKGs can be used to power intelligent personal assistants, and personalize various applications. We explore what entities and relations could be potentially included in a PRKG, how to extract them from various sources, and how to share a PRKG within a research group.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {763–768},
numpages = {6},
keywords = {knowledge representation, personal knowledge graphs, Personal research knowledge graphs, scholarly data, entities and relations},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3170427.3174367,
author = {Edge, Darren and Larson, Jonathan and White, Christopher},
title = {Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3174367},
doi = {10.1145/3170427.3174367},
abstract = {The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {ai, hci, visual analytics, business intelligence, data},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3199478.3199500,
author = {Wen, Senhao and Zhao, Zhiyuan and Yan, Hanbing},
title = {Detecting Malicious Websites in Depth through Analyzing Topics and Web-Pages},
year = {2018},
isbn = {9781450363617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199478.3199500},
doi = {10.1145/3199478.3199500},
abstract = {We are increasingly relying on HTML(Hypertext Markup Language) web-pages, including online shopping, obtaining information and handling official business, whether on a mobile phone or on a computer. But the underground industry or deliberate attacker has also targeted us. They forges misleading URLs(UniformResourceLocators) and web-pages with various tricky skills which are difficult to identify even for the professionals, so as to steal money, manipulate our devices, monitor our lives. Currently, most of the malicious websites detecting technology is based on features of URLs or Web page elements, which make us feel upset, because it is difficult to extract all the features, and its hysteresis quality make it hard to meet the requirement of tracking the rapid changes of malicious web sites, especially the phishing websites. This paper design a thorough associated analysis model of web-pages using the technology of topic tracking, topic abnormal discovery, web-page visual similarity assessment, web-page structure analyzing, URL analyzing, Internet Resources analyzing and so on, to solve the problem of missing detecting malicious websites, especially with unknown features. The detecting model is able to discover the forged payment web-pages, fake web page which damage the reputation of the relevant organization, personal attack web-pages, tampered web page, web-page trojan, etc. In the meantime, it can track the topic of underground industry and sense the topic evolution. According to our experiments, it is effective and has high accuracy.},
booktitle = {Proceedings of the 2nd International Conference on Cryptography, Security and Privacy},
pages = {128–133},
numpages = {6},
keywords = {malicious websites, Labeled-LDA, web-page, topic},
location = {Guiyang, China},
series = {ICCSP 2018}
}

@inproceedings{10.1145/3167020.3167045,
author = {Santos, Eduardo F. and Lima, Fernanda},
title = {LBSociam: Lightbase Social Machine on Criminal Data},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167045},
doi = {10.1145/3167020.3167045},
abstract = {Social machine is a rather new approach to deal with relevant problems in society, blending computational and social elements into software. It can be an extension of the Semantic Web, creating processes in which people do the creative work and the machine does the data administration. This article presents a proposal to apply this approach in violence and criminality domain, a relevant matter to Latin America and Caribbean - LAC -- countries. It extends Social Machines by applying two published strategies to obtain semantics over social networks data. The development procedure was documented to provide a systematic procedure and an example application is presented to identify violence and criminality events. The resulting procedure validation was done by testing against developed formal models in the research area. Criminal activity data extraction results were also compared to official data, in order to identify similarities.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {162–167},
numpages = {6},
keywords = {NLP, Crowdsourcing, Social Networks, LDA, Semantic Web, Social Machines, SRL},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@article{10.1145/3530874,
author = {Krausman, Andrea and Neubauer, Catherine and Forster, Daniel and Lakhmani, Shan and Baker, Anthony L. and Fitzhugh, Sean M. and Gremillion, Gregory and Wright, Julia L. and Metcalfe, Jason S. and Schaefer, Kristin E.},
title = {Trust Measurement in Human-Autonomy Teams: Development of a Conceptual Toolkit},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
url = {https://doi.org/10.1145/3530874},
doi = {10.1145/3530874},
abstract = {The rise in artificial intelligence capabilities in autonomy-enabled systems and robotics has pushed research to address the unique nature of human-autonomy team collaboration. The goal of these advanced technologies is to enable rapid decision-making, enhance situation awareness, promote shared understanding, and improve team dynamics. Simultaneously, use of these technologies is expected to reduce risk to those who collaborate with these systems. Yet, for appropriate human-autonomy teaming to take place, especially as we move beyond dyadic partnerships, proper calibration of team trust is needed to effectively coordinate interactions during high-risk operations. But to meet this end, critical measures of team trust for this new dynamic of human-autonomy teams are needed. This article seeks to expand on trust measurement principles and the foundation of human-autonomy teaming to propose a “toolkit” of novel methods that support the development, maintenance, and calibration of trust in human-autonomy teams operating within uncertain, risky, and dynamic environments.},
journal = {J. Hum.-Robot Interact.},
month = {sep},
articleno = {33},
numpages = {58},
keywords = {trust measurement, Human-autonomy teaming, robot, team trust}
}

@inproceedings{10.1145/3506860.3506917,
author = {Tavakoli, Mohammadreza and Faraji, Abdolali and Molavi, Mohammadreza and T. Mol, Stefan and Kismih\'{o}k, G\'{a}bor},
title = {Hybrid Human-AI Curriculum Development for Personalised Informal Learning Environments},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506917},
doi = {10.1145/3506860.3506917},
abstract = {Informal learning procedures have been changing extremely fast over the recent decades not only due to the advent of online learning, but also due to changes in what humans need to learn to meet their various life and career goals. Consequently, online, educational platforms are expected to provide personalized, up-to-date curricula to assist learners. Therefore, in this paper, we propose an Artificial Intelligence (AI) and Crowdsourcing based approach to create and update curricula for individual learners. We show the design of this curriculum development system prototype, in which contributors receive AI-based recommendations to be able to define and update high-level learning goals, skills, and learning topics together with associated learning content. This curriculum development system was also integrated into our personalized online learning platform. To evaluate our prototype we compared experts’ opinion with our system’s recommendations, and resulted in 89%, 79%, and 93% F1-scores when recommending skills, learning topics, and educational materials respectively. Also, we interviewed eight senior level experts from educational institutions and career consulting organizations. Interviewees agreed that our curriculum development method has high potential to support authoring activities in dynamic, personalized learning environments.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {563–569},
numpages = {7},
keywords = {Artificial Intelligence, Crowdsourcing, Curriculum Development, Informal Learning},
location = {Online, USA},
series = {LAK22}
}

@article{10.14778/3415478.3415494,
author = {Fariha, Anna and Brucato, Matteo and Haas, Peter J. and Meliou, Alexandra},
title = {SuDocu: Summarizing Documents by Example},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415494},
doi = {10.14778/3415478.3415494},
abstract = {Text document summarization refers to the task of producing a brief representation of a document for easy human consumption. Existing text summarization techniques mostly focus on generic summarization, but users often require personalized summarization that targets their specific preferences and needs. However, precisely expressing preferences is challenging, and current methods are often ambiguous, outside the user's control, or require costly training data. We propose a novel and effective way to express summarization intent (preferences) via examples: the user provides a few example summaries for a small number of documents in a collection, and the system summarizes the rest. We demonstrate SuDocu, an example-based personalized Document Summarization system. Through a simple interface, SuDocu allows the users to provide example summaries, learns the summarization intent from the examples, and produces summaries for new documents that reflect the user's summarization intent. SuDocu further explains the captured summarization intent in the form of a package query, an extension of a traditional SQL query that handles complex constraints and preferences over answer sets. SuDocu combines topic modeling, semantic similarity discovery, and in-database optimization in a novel way to achieve example-driven document summarization. We demonstrate how SuDocu can detect complex summarization intents from a few example summaries and produce accurate summaries for new documents effectively and efficiently.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2861–2864},
numpages = {4}
}

@inproceedings{10.1145/3378936.3378981,
author = {Ding, Jianwei and Guo, Xiaoyu and Chen, Zhouguo},
title = {Big Data Analyses of ZeroNet Sites for Exploring the New Generation DarkWeb},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378981},
doi = {10.1145/3378936.3378981},
abstract = {ZeroNet is a new generation typical dark web, which uses the Bitcoin encryption algorithm and BitTorrent technology to build a distributed and censored-resistant communication network. Based on our cumulative studies on the onion router, we present a big data analyses framework for automated multi-categorization of ZeroNet websites to facilitate analyst situational awareness of new content that emerges from this dynamic landscape. Over the last two years, our team has developed a distributed crawling infrastructure called ZeroCrawler that automatically crawls and updates ZeroNet websites in realtime. It stores data into a research repository designed to help better understand ZeroNet's hidden service ecosystem. The analysis component of our framework is called Automated Multi-Categorization Labeling (AMCL), which introduces a three-stage thematic labeling strategy: (1) it learns descriptive and discriminative keywords for different categories, and (2) get a probability distribution of the keywords for different categories, and then (3) uses these terms to map ZeroNet website content to several labels. We also present empirical results of AMCL and our ongoing experimentation with it, as we have gained experience applying it to the entirety of our ZeroNet repository, now over 3000 indexed websites. The experimental results show that AMCL can discover categories on previously unlabeled websites, and we discuss applications of AMCL in supporting various analyses and investigations of the ZeroNet websites.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {46–52},
numpages = {7},
keywords = {Dark web conent analysis, ZeroNet, Multi-categarization labels},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/3207677.3278071,
author = {Cui, Liang-Zhong and Guo, Fu-Liang and Liang, Ying-jie},
title = {Research Overview of Educational Recommender Systems},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278071},
doi = {10.1145/3207677.3278071},
abstract = {Recently1, with the thriving development of various educational resources and platforms, the research hotspot of modernization of education is now changing towards providing personalized educational recommendation services to corresponding learners. Firstly, starting from basic definitions and contents of educational recommender systems, we put forward an analysis of the fundamental structure of educational recommender systems while emphatically expounding different research approaches including user modeling, recommended object modeling, recommendation algorithm design and recommendation effect evaluation, etc. Secondly, with deep going analysis, we further introduced frequently used recommendation approaches including content-based recommendation, collaborative filtering-based recommendation, association rule-based recommendation and hybrid recommendation. Finally, in the last part of the article, by combining problems existing in contemporary educational recommender systems as well as comparing both advantages and disadvantages of the four recommendation approaches, we discussed the directions of research and development of educational recommender systems in the future.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {155},
numpages = {7},
keywords = {ACM proceedings, text tagging Recommendation algorithm, personalized recommendation, recommendation evaluation, learner modeling, educational recommender system},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/3265689.3265707,
author = {Liu, Zhishuo and Shen, Qianhui and Ma, Jingmiao},
title = {Extracting Implicit Features Based on Association Rules},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265707},
doi = {10.1145/3265689.3265707},
abstract = {Product reviews in the network shopping platform provide references to customs' purchase decision. However, existing researches on opinion objects mainly focus on explicit features, and few of scholars take implicit features into consideration. In this paper, based on Chinese online comments data preprocessing. We proposed a Fuzzy C-means algorithm based on Simulated Annealing (SA-FCM) to cluster the explicit comment sentences into 9 classes. And put each class of comment sentences into a document set. Then association rules between opinion words and opinion objects in every document set are mined and build an association rules table among classes, opinion targets and opinion words. The implicit features are discovered according to the opinion words in the association rule table. Finally, the implicit features excavate method proposed in this paper can effectively improve the accuracy of the extraction effect through an experiment verification.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {18},
numpages = {7},
keywords = {Implicit Features, Opinion Mining, Association Rule, SA-FCM},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1145/3184558.3186342,
author = {Cheng, Yihang and Zhang, Xi and Wang, Hao and Jiang, Shang},
title = {How to Improve the Answering Effectiveness in Pay-for-Knowledge Community: An Exploratory Application of Intelligent QA System},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186342},
doi = {10.1145/3184558.3186342},
abstract = {Community Question Answering (CQA) has emerged recently and it becomes popular among people. During the process of the communication, different knowledge can be merged. Recently, several vendors use the business model of paying for knowledge to make these knowledge to the monetary benefits, then the Pay-for-Knowledge Communities (PKC) have been applied. Even PKC has interesting business model, there are several problems to be solved, and one of the most salient problem is that questioners may takes too long time to choose the most valuable answers, leading to questioners not able to pay for suitable answerers and many problems about platforms' operation There are several previous research has focused on this problem but still have not found satisfactory solutions as questions and answers are more and more complex in PKC platforms. With the development of cognitive computing techniques, applying an intelligent QA system in PKC to improve the answering effectiveness may be possible. In this paper, we tried to investigate how to apply the intelligent QA system into PKC platform to improve the answering effectiveness. For solving the problems of matching complex questions and answers, we present a Four Module QA Model based on the normal intelligent QA System. Compared to normal intelligent QA System, our model uses categories to classify the questions with traditional machine learning methods. We use answers in each category of corresponding questions as one dataset, answers in each entity of corresponding question as the other dataset, finally, these two datasets make up the document database. Then we got the best answer among past answers through comparing the TF-IDF weighted bag-of word vectors of two datasets or the new answer including key words through Long Short-Term Memory (LSTM) algorithm with PKC's features composed of centrality and money. Experiments were developed on a dataset with 1222 users' QA sites collected from a QA community. The model we proposed is expected to increase QA's effectiveness and improve the business model of Pay-for-Knowledge Communities.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {313–317},
numpages = {5},
keywords = {intelligent QA system, pay-for-knowledge community (PKC)},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3185089.3185130,
author = {Ma, Jian-Hong and Wang, Ning-Ning and Yao, Shuang and Wei, Zi-Mo and Jin, Shuai},
title = {Similar Patent Search Method Based on a Functional Information Fusion},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185130},
doi = {10.1145/3185089.3185130},
abstract = {The most fundamental innovation of products is the innovation of functional principle. Learning from the functional information of existing patents plays an important role in the design of product innovation and patent circumvention. This paper proposed a similar patent search method of fusion function information. The method firstly screening candidate words, the candidate word is the word which is of great contribution to judge whether a sentence contains functional information; the short text classification method based on the candidate words was proposed to judge the functional sentences, and the functional expression rules were combined to determine the patent's functional phrase. Finally, based on the functional expression sentences, this paper proposed a sentence similarity computing method based on word2vec and syntactic dependency to search for the patents which have similar functional information. Experiments show that the proposed method has a certain accuracy and validity in the automatic extraction of functional information and search for similar patent.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {217–222},
numpages = {6},
keywords = {Functional information; mutual information; dependency analysis; word2vec; similarity calculation},
location = {Kuantan, Malaysia},
series = {ICSCA 2018}
}

@inproceedings{10.1145/3106426.3106479,
author = {Amami, Maha and Faiz, Rim and Stella, Fabio and Pasi, Gabriella},
title = {A Graph Based Approach to Scientific Paper Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106479},
doi = {10.1145/3106426.3106479},
abstract = {When looking for recently published scientific papers, a researcher usually focuses on the topics related to her/his scientific interests. The task of a recommender system is to provide a list of unseen papers that match these topics. The core idea of this paper is to leverage the latent topics of interest in the publications of the researchers, and to take advantage of the social structure of the researchers (relations among researchers in the same field) as reliable sources of knowledge to improve the recommendation effectiveness. In particular, we introduce a hybrid approach to the task of scientific papers recommendation, which combines content analysis based on probabilistic topic modeling and ideas from collaborative filtering based on a relevance-based language model. We conducted an experimental study on DBLP, which demonstrates that our approach is promising.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {777–782},
numpages = {6},
keywords = {scientific paper recommendation, hybrid approaches, language modeling, LDA},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3377713.3377803,
author = {Wei, Chao and Zhu, Lijun and Wang, Juncheng and Shi, Jiaoxiang and Wang, Zheng and Chen, Liang},
title = {Extracting Word Embeddings via Joint Learning of Syntagmatic and Paradigmatic Structure},
year = {2019},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377803},
doi = {10.1145/3377713.3377803},
abstract = {In this work, we explore the latent distributional semantic between each given word and its context in a way of reconstruction, and propose an Auto-encoder architecture to encourage the model pay attention to the interactions of Syntagmatic and Paradigmatic structure of context (SPC). In particular, SPC represent the syntagmatic words of the context through a vocabulary co-occurrence matrix, and extract a likely embedding for a better joint predicting of all paradigmatic words to fill in the input context. During the error back propagation, our model focus on the salient statistical structure by allowing a sub-network to approximate the nonzero sub-matrix. Finally, we trained our model on a public Wikipedia corpus and evaluated on word analogy, word similarity tasks and documents clustering and classification. The results show that our method yields almost 4% improvement on word analogy and nearly 3% improvement on word similarity tasks compared to state-of-the-art methods. The evidences demonstrate that SPC outperform baseline and the state-of-the-art methods on all tasks.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {526–532},
numpages = {7},
keywords = {Word Analogy and Similarity, Auto-encoder, Syntagmatic and Paradigmatic structure, Word embeddings},
location = {Sanya, China},
series = {ACAI 2019}
}

@inproceedings{10.1145/3220199.3220203,
author = {Wang, Hengyi and Niu, Zhendong},
title = {Climate Event Detection Algorithm Based on Climate Category Word Embedding},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220203},
doi = {10.1145/3220199.3220203},
abstract = {Detecting climate events efficiently and accurately is important in traffic warning, disaster warning, and disease prevention. Given that ordinary event detection algorithms are ignored in climate domain's knowledge, the results of climate domain's event detection are not satisfactory. This paper proposes a climate event detection algorithm based on climate category word embedding(CEDCWE). This method combines the climate category word embedding and typical factors of climate events as a document representation model to express detailed information of climate documents and detect climate events efficiently and accurately. Compared with other methods, the CEDCWE algorithm can generate better climate document representations and climate event detection results. In the experiments, we acquire the datasets by a web crawler and evaluate our CEDCWE on real-world climate event detection tasks. Experimental results show that our CEDCWE is effective in climate document representation and outperforms typical methods.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {71–77},
numpages = {7},
keywords = {category word embedding, climate domain, event detection},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@article{10.1145/3397162,
author = {Cao, Bin and Wu, Jiawei and Wang, Sichao and Gao, Honghao and Fan, Jing and Deng, Shuiguang and Yin, Jianwei and Liu, Xuan},
title = {Unsupervised Derivation of Keyword Summary for Short Texts},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3397162},
doi = {10.1145/3397162},
abstract = {Automatically summarizing a group of short texts that mainly share one topic is a fundamental task in many applications, e.g., summarizing the main symptoms for a disease based on a group of medical texts that are usually short, i.e., tens of words. Conventional unsupervised short text summarization techniques tend to find the most representative short text document. However, they may cause privacy issues, e.g., personal information in the medical texts may be exposed. Moreover, compared with the complete short text where some unimportant words may exist, a summary consisting of only a few keywords is more preferable by the user due to its clear and concise form. Due to the above reasons, in this article, we aim to solve the problem of unsupervised derivation of keyword summary for short texts. Existing keyword extraction methods such as Latent Dirichlet Allocation cannot be applied to solve this problem, since (1)&nbsp;the ordering relations among the extracted keywords are ignored, which causes troubles for people to capture the main idea of the event, and (2)&nbsp;short texts contain limited context, which makes it hard to find the optimal words for semantic coverage. Hence, we propose a simple but yet effective method named Frequent Closed Wordsets Ranking (FCWRank) to derive the keyword summary from a short text cluster. FCWRank is an unsupervised method that builds on the idea of frequent closed itemset mining in transaction database. FCWRank first mines all frequent closed wordsets from a cluster of short texts and then selects the most important wordset based on an importance model where the similarity between closed wordsets and the relation between the closed wordset and the short text document are considered simultaneously. To make the keywords within the wordset more understandable, FCWRank further unfolds the semantics behind them by sorting them. Experiments on real-world short text collections show that FCWRank outperforms the state-of-the-art baselines in terms of Recall-Oriented Understudy for Gisting Evaluation-Longest common subsequence F1, precision and recall scores.},
journal = {ACM Trans. Internet Technol.},
month = {jun},
articleno = {45},
numpages = {23},
keywords = {short texts, unsupervised, Keyword summary}
}

@article{10.14778/3192965.3192972,
author = {Chen, Jianfei and Zhu, Jun and Lu, Jie and Liu, Shixia},
title = {Scalable Training of Hierarchical Topic Models},
year = {2018},
issue_date = {March 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3192965.3192972},
doi = {10.14778/3192965.3192972},
abstract = {Large-scale topic models serve as basic tools for feature extraction and dimensionality reduction in many practical applications. As a natural extension of flat topic models, hierarchical topic models (HTMs) are able to learn topics of different levels of abstraction, which lead to deeper understanding and better generalization than their flat counterparts. However, existing scalable systems for flat topic models cannot handle HTMs, due to their complicated data structures such as trees and concurrent dynamically growing matrices, as well as their susceptibility to local optima.In this paper, we study the hierarchical latent Dirichlet allocation (hLDA) model which is a powerful nonparametric Bayesian HTM. We propose an efficient partially collapsed Gibbs sampling algorithm for hLDA, as well as an initialization strategy to deal with local optima introduced by tree-structured models. We also identify new system challenges in building scalable systems for HTMs, and propose efficient data layout for vectorizing HTM as well as distributed data structures including dynamic matrices and trees. Empirical studies show that our system is 87 times more efficient than the previous open-source implementation for hLDA, and can scale to thousands of CPU cores. We demonstrate our scalability on a 131-million-document corpus with 28 billion tokens, which is 4--5 orders of magnitude larger than previously used corpus. Our distributed implementation can extract 1,722 topics from the corpus with 50 machines in just 7 hours.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {826–839},
numpages = {14}
}

@article{10.1145/2888403,
author = {Johnson, Daniel and Ventura, Dan},
title = {Musical Motif Discovery from Non-Musical Inspiration Sources},
year = {2017},
issue_date = {Summer 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
url = {https://doi.org/10.1145/2888403},
doi = {10.1145/2888403},
abstract = {Many music composition algorithms attempt to compose music in a particular style. The resulting music is often impressive and indistinguishable from the style of the training data, but it tends to lack significant innovation. In an effort to increase innovation in the selection of pitches and note durations, we present a system that discovers musical motifs by coupling machine-learning techniques with an inspirational component. Unlike many generative models, the inspirational component allows the composition process to originate outside of what is learned from the training data. Candidate motifs are extracted from non-musical data such as audio, images, and sleep signals. Machine-learning algorithms select the motifs that most resemble the training data. We find that the inspirational motif discovery process is more efficient than random generation. We also extract motifs from real music scores, identify themes in the piece according to a theme database, and measure the probability of discovering thematic motifs verses non-thematic motifs. We examine the information content of the motifs by comparing the entropy of the discovered motifs, candidate motifs, and training data. We measure innovation by comparing the probability of the training data and the probability of the discovered motifs given the model.},
journal = {Comput. Entertain.},
month = {jan},
articleno = {7},
numpages = {22},
keywords = {machine learning, Music composition}
}

@inproceedings{10.1145/3531232.3531246,
author = {Fang, Xin and Li, Fan},
title = {CS-GAN: Centrosymmetric Generative Adversarial Network for Cross-Media Retrieval},
year = {2022},
isbn = {9781450387415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531232.3531246},
doi = {10.1145/3531232.3531246},
abstract = {Cross-Media retrieval has become necessary to satisfy the urgent needs of multimedia economic and cultural value. The heterogeneous gap makes it difficult to directly retrieve media of different modalities. Existing common space-based Cross-Media retrieval methods usually focus on increasing the inter-class distance while paying little attention to decreasing the intra- class distance, they neglect to correlate the mapping process from original space to common space, which limits the narrowing of intra-class distance. To address this problem, we propose Centrosymmetric Generative Adversarial Network (CS-GAN) for Cross-Media retrieval, which consists of two generative models and our proposed Heterogeneous Input Discriminative Model (HIDM), forming a centrosymmetric structure. Our proposed HIDM conjoins the mapping processes of each media type by dimensionality reduction and knowledge transfer, which ensures the unification of internal processing and well retains the underlying manifold structure information. Besides, it discriminates the distribution of common representations for both image and text at the same time, which can interrelate the generative mapping, preserve the distribution of the original features and narrow the intra-class distances of common embeddings. The experimental results on PKU XMediaNet and Pascal Sentences show the effectiveness of our proposed CS-GAN.},
booktitle = {2022 4th International Conference on Image, Video and Signal Processing},
pages = {100–106},
numpages = {7},
keywords = {Intra-class distance, Cross-Media retrieval, Generative adversarial networks},
location = {Singapore, Singapore},
series = {IVSP 2022}
}

@inproceedings{10.1145/3487664.3487693,
author = {Yahyaoui, Hamdi and Almulla, Mohamed and Boujarwah, Eiman},
title = {Measuring Semantic Similarity between Services Using Hypergraphs},
year = {2021},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487693},
doi = {10.1145/3487664.3487693},
abstract = {We propose in this paper a new HyperGraph-based Similarity Algorithm&nbsp;(HGSA) to measure the semantic similarity between services with focus on Web services. HGSA&nbsp;relies on a term-similarity function that considers several parameters such as node’s local density, depth, number of relation factors, and link strength. This function is leveraged to build a similarity matrix between Web services. Computing the similarity between Web services is reduced to applying the Hungarian method to this matrix. We study the complexity of the proposed algorithm and provide experiments that show that it outperforms other related approaches in terms of accuracy.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {205–211},
numpages = {7},
keywords = {Semantic similarity, Hypergraph, WordNet, Web services, WSDL},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3472163.3472275,
author = {Saeki, Yui and Toyama, Motomichi},
title = {An In-Browser Collaborative System for Functional Annotations},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472275},
doi = {10.1145/3472163.3472275},
abstract = {In this work, we used the Web IndeX system, which converts words into hyperlinks in arbitrary Web pages, to implement a system for sharing annotations registered on keywords within a limited group. This system allows group members to view all the written annotations by simply mousing over the keyword when it appears on a web page, facilitating sharing information and awareness in collaborative research and work. In this study, we define our system as a sticky note type annotation sharing system. In contrast to the sticky note type, our system is positioned as a functional type, but it may display annotations that are not necessary since it allows browsing on any page. To improve this point, we propose a usability judgment method that shows only the most useful posts.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {297–303},
numpages = {7},
keywords = {Computer supported cooperative work, Web IndeX, Annotation},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3184558.3191629,
author = {Li, Changzhou and Lu, Yao and Wu, Junfeng and Zhang, Yongrui and Xia, Zhongzhou and Wang, Tianchen and Yu, Dantian and Chen, Xurui and Liu, Peidong and Guo, Junyu},
title = {LDA Meets Word2Vec: A Novel Model for Academic Abstract Clustering},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191629},
doi = {10.1145/3184558.3191629},
abstract = {Clustering narrow-domain short texts, such as academic abstracts, is an extremely difficult clustering problem. Firstly, short texts lead to low frequency and sparseness of words, making clustering results highly unstable and inaccurate; Secondly, narrow domain leads to great overlapping of insignificant words and makes it hard to distinguish between sub-domains, or fine-grained clusters. The vocabulary size is also too small to construct a good word bag needed by traditional clustering algorithms like LDA to give a meaningful topic distribution. A novel clustering model, Partitioned Word2Vec-LDA (PW-LDA), is proposed in this paper to tackle the described problems. Since the purpose sentences of an abstract contain crucial information about the topic of the paper, we firstly implement a novel algorithm to extract them from the abstracts according to its structural features. Then high-frequency words are removed from those purpose sentences to get a purified-purpose corpus and LDA and Word2Vec models are trained. After combining the results of both models, we can cluster the abstracts more precisely. Our model uses abstract text instead of keywords to cluster because keywords may be ambiguous and cause unsatisfied clustering results shown by previous work. Experimental results show that the clustering results of PW-LDA are much more accurate and stable than state-of-the-art techniques.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1699–1706},
numpages = {8},
keywords = {lda, abstract, word2vec, short text clustering, document clustering},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1145/3120996,
author = {Li, Xiaopeng and Cheung, Ming and She, James},
title = {A Distributed Streaming Framework for Connection Discovery Using Shared Videos},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3120996},
doi = {10.1145/3120996},
abstract = {With the advances in mobile devices and the popularity of social networks, users can share multimedia content anytime, anywhere. One of the most important types of emerging content is video, which is commonly shared on platforms such as Instagram and Facebook. User connections, which indicate whether two users are follower/followee or have the same interests, are essential to improve services and information relevant to users for many social media applications. But they are normally hidden due to users’ privacy concerns or are kept confidential by social media sites. Using user-shared content is an alternative way to discover user connections. This article proposes to use user-shared videos for connection discovery with the Bag of Feature Tagging method and proposes a distributed streaming computation framework to facilitate the analytics. Exploiting the uniqueness of shared videos, the proposed framework is divided into Streaming processing and Online and Offline Computation. With experiments using a dataset from Twitter, it has been proved that the proposed method using user-shared videos for connection discovery is feasible. And the proposed computation framework significantly accelerates the analytics, reducing the processing time to only 32% for follower/followee recommendation. It has also been proved that comparable performance can be achieved with only partial data for each video and leads to more efficient computation.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {59},
numpages = {24},
keywords = {user shared videos, connection discovery, streaming, Social networks, computation framework, bag-of-features tagging}
}

@article{10.1145/3417989,
author = {Santhosh, K. K. and Dogra, D. P. and Roy, P. P.},
title = {Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3417989},
doi = {10.1145/3417989},
abstract = {Computer vision has evolved in the last decade as a key technology for numerous applications replacing human supervision. Timely detection of traffic violations and abnormal behavior of pedestrians at public places through computer vision and visual surveillance can be highly effective for maintaining traffic order in cities. However, despite a handful of computer vision–based techniques proposed in recent times to understand the traffic violations or other types of on-road anomalies, no methodological survey is available that provides a detailed insight into the classification techniques, learning methods, datasets, and application contexts. Thus, this study aims to investigate the recent visual surveillance–related research on anomaly detection in public places, particularly on road. The study analyzes various vision-guided anomaly detection techniques using a generic framework such that the key technical components can be easily understood. Our survey includes definitions of related terminologies and concepts, judicious classifications of the vision-guided anomaly detection approaches, detailed analysis of anomaly detection methods including deep learning–based methods, descriptions of the relevant datasets with environmental conditions, and types of anomalies. The study also reveals vital gaps in the available datasets and anomaly detection capability in various contexts, and thus gives future directions to the computer vision–guided anomaly detection research. As anomaly detection is an important step in automatic road traffic surveillance, this survey can be a useful resource for interested researchers working on solving various issues of Intelligent Transportation Systems (ITS).},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {119},
numpages = {26},
keywords = {road traffic analysis, Learning methods, classification}
}

@article{10.1162/coli_a_00338,
author = {Cocarascu, Oana and Toni, Francesca},
title = {Combining Deep Learning and Argumentative Reasoning for the Analysis of Social Media Textual Content Using Small Data Sets},
year = {2018},
issue_date = {December 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {44},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00338},
doi = {10.1162/coli_a_00338},
abstract = {The use of social media has become a regular habit for many and has changed the way people interact with each other. In this article, we focus on analyzing whether news headlines support tweets and whether reviews are deceptive by analyzing the interaction or the influence that these texts have on the others, thus exploiting contextual information. Concretely, we define a deep learning method for relation-based argument mining to extract argumentative relations of attack and support. We then use this method for determining whether news articles support tweets, a useful task in fact-checking settings, where determining agreement toward a statement is a useful step toward determining its truthfulness. Furthermore, we use our method for extracting bipolar argumentation frameworks from reviews to help detect whether they are deceptive. We show experimentally that our method performs well in both settings. In particular, in the case of deception detection, our method contributes a novel argumentative feature that, when used in combination with other features in standard supervised classifiers, outperforms the latter even on small data sets.},
journal = {Comput. Linguist.},
month = {dec},
pages = {833–858},
numpages = {26}
}

@inproceedings{10.1145/3340531.3411956,
author = {Lu, Chien and Peltonen, Jaakko and Nummenmaa, Jyrki and J\"{a}rvelin, Kalervo},
title = {Probabilistic Dynamic Non-Negative Group Factor Model for Multi-Source Text Mining},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411956},
doi = {10.1145/3340531.3411956},
abstract = {Nonnegative matrix factorization (NMF) is a popular approach to model data, however, most models are unable to flexibly take into account multiple matrices across sources and time or apply only to integer-valued data. We introduce a probabilistic, Gaussian Process-based, more inclusive NMF-based model which jointly analyzes nonnegative data such as text data word content from multiple sources in a temporal dynamic manner. The model collectively models observed matrix data, source-wise latent variables, and their dependencies and temporal evolution with a full-fledged hierarchical approach including flexible nonparametric temporal dynamics. Experiments on simulated data and real data show the model out-performs, comparable models. A case study on social media and news demonstrates the model discovers semantically meaningful topical factors and their evolution},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1035–1043},
numpages = {9},
keywords = {gaussian process, nonnegative matrix factorization, multiple sources},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3290688.3290720,
author = {Pokharel, Rashmi and Haghighi, Pari Delir and Jayaraman, Prem Prakash and Georgakopoulos, Dimitrios},
title = {Analysing Emerging Topics across Multiple Social Media Platforms},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290720},
doi = {10.1145/3290688.3290720},
abstract = {The ability to compose emerging topics from the data collected from multiple social media platforms can help individuals and organisations meet their business goals and improve decision-making, as such information can provide more complete and accurate information. However, existing research has mainly focused on analysing emerging topics from the posts and related data collected from a single social media platform. In this paper, we propose a framework referred to as Multi-source Social Topic Media Analysis (xSMA) framework to model, rank and semantically analyse emerging topics across various social media platforms. The implementation and evaluation of the xSMA framework using real-world datasets obtained from Twitter and Reddit are also described.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {16},
numpages = {9},
keywords = {Social media, topic ranking, topic modelling, topic similarity},
location = {Sydney, NSW, Australia},
series = {ACSW 2019}
}

@inproceedings{10.1145/3302425.3302470,
author = {Bi, Liyang and Wang, Yongji and Qu, Dacheng and Guan, Bei},
title = {A Hybrid Recommendation Method with Multilayer Perception Applied on Real World Data},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302470},
doi = {10.1145/3302425.3302470},
abstract = {Collaborative filtering (CF) is an approach widely used in recommender systems (RS). Traditional CF-based methods simply utilize user-item rating matrix which implies interactions between users and items to make recommendation. However, rating matrix is often very sparse in real world, resulting in these methods a significant degradation in recommendation performance. Therefore, some employ side information of users and items to address the sparse problem. Nevertheless, when it comes to the interactions between user and item latent factors, they still utilize linear inner product. Neural Collaborative Filtering (NeuMF) is an appealing recent method employing Multilayer Perception (MLP) to learn interaction function between user and item latent factors. However, this method does not integrate side information of users and items. To tackle problems above, we generalize effective learning ability of multilayer perception and propose a hybrid recommendation method with multilayer perception which jointly learns deep representations of users and items from side information as well as rating matrix and interaction function between user and item latent factors. We also propose to add a data normalization layer after combining two vectors coming from different magnitudes. Extensive experiments on two real world datasets show that our model outperforms state-of-the-art algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {20},
numpages = {7},
keywords = {Side Information, Matrix Factorization, Multilayer Perception},
location = {Sanya, China},
series = {ACAI 2018}
}

@inproceedings{10.1145/3268866.3268873,
author = {Duan, Jianyong and Song, Yadi and Zhang, Yongmei and Wu, Mingli and Wang, Hao},
title = {Query Recommendation Using Topic Modeling and Word Embeddings},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268873},
doi = {10.1145/3268866.3268873},
abstract = {Query recommendation plays an important role in improving users' search experience. Traditional ways most mine recommended words from log information. However, in user logs, sessions are difficult to divide. At the same time, click results are with bias and noise, and many queries lack clicks, it will make useful information be sparse. In this paper, we present a novel method based on local documents. Different from the traditional query recommendation, this method recommends related terminology according to the meaning of the query. We extract terminology documents from the pseudo-related feedback documents, then model topics of the terminology documents and use the inference strategies to infer the topic of the query to solve the problem of theme drift. In addition, to bring better recommendation results, we fuse supervised and unsupervised methods to mine semantic concept relations between query words and recommended words. Finally, the words with semantic concepts relation are recommended to the user. Experimental results show that our method can meet the user's search needs better. Compared with traditional query recommendation, users prefer the query recommendation way that we propose.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {86–92},
numpages = {7},
keywords = {Terminology recommendation, Query recommendation, Semantic concepts relation identification},
location = {Beijing, China},
series = {AIPR 2018}
}

@inproceedings{10.1145/3127404.3127426,
author = {Shao, Bin and Yan, Jiafei},
title = {Recommending Answerers for Stack Overflow with LDA Model},
year = {2017},
isbn = {9781450353526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127404.3127426},
doi = {10.1145/3127404.3127426},
abstract = {Stack Overflow is the largest Community-based Question Answering (CQA) site for software developers. Its popularity is mainly attributed to the timely answers provided by a great number of developers. When having problems in learning and using new technologies, developers resort to Stack Overflow for help. However, it is difficult to recommend questions to the potential answerers due to the huge numbers of questions. In order to improve the accuracy of question recommending, we need to find out the members who are interested in the fields related to the questions and match the ability of developers with the difficulty of questions. To do so, we need to pay close attention to the behavior of developers. This paper presents a model with two prediction approaches, namely, the traditional feature-based approach and LDA (Latent Dirichlet Allocation) based approach. When a new question arrives, this model will use LDA method to label the question and indicate the proper category to which the question belongs according to latent semantic feature and content feature. Then, with the traditional features of the question and the asker information, the model will recommend the appropriate developers to answer this new question.},
booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
pages = {80–86},
numpages = {7},
keywords = {Stack Overflow, LDA, classifier, recommender system},
location = {Chongqing, China},
series = {ChineseCSCW '17}
}

@article{10.1145/3450445,
author = {Roy, Soumyadeep and Sural, Shamik and Chhaya, Niyati and Natarajan, Anandhavelu and Ganguly, Niloy},
title = {An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis, and Recommendation},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3450445},
doi = {10.1145/3450445},
abstract = {A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the brand personality of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality.},
journal = {ACM Trans. Web},
month = {may},
articleno = {9},
numpages = {25},
keywords = {sentence ranking, text classification, online reputation management, Brand personality}
}

@inproceedings{10.1145/3328778.3366858,
author = {Mar\c{c}al, Ingrid and Garcia, Rog\'{e}rio Eduardo and Eler, Danilo and Correia, Ronaldo Celso Messias},
title = {A Strategy to Enhance Computer Science Teaching Material Using Topic Modelling: Towards Overcoming The Gap Between College And Workplace Skills},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366858},
doi = {10.1145/3328778.3366858},
abstract = {Computer Science teaching materials are biased towards concepts and theoretical aspects. One may consider it difficult to relate concepts to concrete problems. Consequently, it increases the chances of a student not recognizing the relevance of the subject, becoming unmotivated and unprepared to solve practical problems or coping with workplace needs after college. This paper shows the use of social media data as an alternative to minimize the skill gap between what the student learns in college and the skills required in the workplace. The proposed strategy consists of extracting topics from Stack Overflow questions to identify concepts generally unknown or misunderstood and concepts that their practical application represents a challenge. The concepts covered in Stack Overflow questions provide strong cues about how professors and instructors can improve teaching material with useful content for their students, as well as increase their motivation -- since the teaching material becomes clearly related to practical issues in Computer Science. We show, by an example, how to use the proposed strategy to improve teaching material from the generated topics. Also, we demonstrate that the application of topic modeling in Stack Overflow content is promising as a professor support tool to enhance teaching material relevance in Computer Science courses and diminish the college-workplace skill gap.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {366–371},
numpages = {6},
keywords = {transformative higher education, topic modelling, curriculum improvement, computer science education, college-workplace gap},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@article{10.1145/3530901,
author = {Bin Zia, Haris and Raman, Aravindh and Castro, Ignacio and Hassan Anaobi, Ishaku and De Cristofaro, Emiliano and Sastry, Nishanth and Tyson, Gareth},
title = {Toxicity in the Decentralized Web and the Potential for Model Sharing},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3530901},
doi = {10.1145/3530901},
abstract = {The "Decentralised Web" (DW) is an evolving concept, which encompasses technologies aimed at providing greater transparency and openness on the web. The DW relies on independent servers (aka instances) that mesh together in a peer-to-peer fashion to deliver a range of services (e.g. micro-blogs, image sharing, video streaming). However, toxic content moderation in this decentralised context is challenging. This is because there is no central entity that can define toxicity, nor a large central pool of data that can be used to build universal classifiers. It is therefore unsurprising that there have been several high-profile cases of the DW being misused to coordinate and disseminate harmful material. Using a dataset of 9.9M posts from 117K users on Pleroma (a popular DW microblogging service), we quantify the presence of toxic content. We find that toxic content is prevalent and spreads rapidly between instances. We show that automating per-instance content moderation is challenging due to the lack of sufficient training data available and the effort required in labelling. We therefore propose and evaluate ModPair, a model sharing system that effectively detects toxic content, gaining an average per-instance macro-F1 score 0.89.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {jun},
articleno = {35},
numpages = {25},
keywords = {pleroma, decentralised web, content moderation, toxicity analysis}
}

@inproceedings{10.1145/3491204.3543506,
author = {Lambion, Danielle and Schmitz, Robert and Cordingly, Robert and Heydari, Navid and Lloyd, Wes},
title = {Characterizing X86 and ARM Serverless Performance Variation: A Natural Language Processing Case Study},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3543506},
doi = {10.1145/3491204.3543506},
abstract = {In this paper, we leverage a Natural Language Processing (NLP) pipeline for topic modeling consisting of three functions for data preprocessing, model training, and inferencing to analyze serverless platform performance variation. Specifically, we investigated performance using x86_64 and ARM64 processors over a 24-hour day starting at midnight local time on four cloud regions across three continents on AWS Lambda. We identified public cloud resource contention by leveraging the CPU steal metric, and examined relationships to NLP pipeline runtime. Intel x86_64 Xeon processors at the same clock rate as ARM64 processors (Graviton 2) were more than 23% faster for model training, but ARM64 processors were faster for data preprocessing and inferencing. Use of the Intel x86_64 architecture for the NLP pipeline was up to 33.4% more expensive than ARM64 as a result of incentivized pricing from the cloud provider and slower pipeline runtime due to greater resource contention for Intel processors.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {69–75},
numpages = {7},
keywords = {function-as-a-service, topic modeling, resource contention, serverless computing, performance variation},
location = {Bejing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3502718.3524807,
author = {Goupil, Francois and Laskov, Pavel and Pekaric, Irdin and Felderer, Michael and D\"{u}rr, Alexander and Thiesse, Frederic},
title = {Towards Understanding the Skill Gap in Cybersecurity},
year = {2022},
isbn = {9781450392013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502718.3524807},
doi = {10.1145/3502718.3524807},
abstract = {Given the ongoing "arms race" in cybersecurity, the shortage of skilled professionals in this field is one of the strongest in computer science. The currently unmet staffing demand in cybersecurity is estimated at over 3 million jobs worldwide. Furthermore, the qualifications of the existing workforce are largely believed to be insufficient. We attempt to gain deeper insights into the nature of the current skill gap in cybersecurity. To this end, we correlate data from job ads and academic curricula using two kinds of skill characterizations: manual definitions from established skill frameworks as well as "skill topics" automatically derived by text mining tools. Our analysis shows a strong agreement between these two analysis techniques and reveals a substantial undersupply in several crucial skill categories, e.g., software and application security, security management, requirements engineering, compliance and certification. Based on the results of our analysis, we provide recommendations for future curricula development in cybersecurity so as to decrease the identified skill gaps.},
booktitle = {Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 1},
pages = {477–483},
numpages = {7},
keywords = {skill taxonomy, curricula development, text mining, skill gap},
location = {Dublin, Ireland},
series = {ITiCSE '22}
}

@inproceedings{10.1145/3498851.3498971,
author = {Yao, Lizhong and He, Tiantian},
title = {Fuzzy Community Detection with Multi-View Correlated Topics},
year = {2021},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498971},
doi = {10.1145/3498851.3498971},
abstract = {In this paper, we present a novel fuzzy framework, dubbed as Fuzzy Multi-View Featured Network Clustering (FMVFNC), for effectively uncovering overlapping communities in social network data. Unlike most previous efforts which utilize only edge structure and single view of vertex features to perform the community discovery task, the proposed FMVFNC is able to take advantage of both edge structure and correlated vertex features which may be collected from multiple views. As the uncovered social communities are described by both network structure and semantically correlated features from diverse modalities, their practical significance can be well revealed. We innovatively design a unified fuzzy objective for FMVFNC to perform the task. We then derive an iterative algorithm for the proposed framework to optimize the formulated objective function. FMVFNC has been tested with a number of well-established datasets and has been compared with a number of state-of-the-art baselines for community detection. The notable results obtained may validate the effectiveness of FMVFNC.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {307–313},
numpages = {7},
keywords = {Fuzzy clustering, Graph clustering, Community detection, Matrix factorization},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1145/3469725,
author = {Xiang, Yan and Yu, Zhengtao and Guo, Junjun and Huang, Yuxin and Xian, Yantuan},
title = {Event Graph Neural Network for Opinion Target Classification of Microblog Comments},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3469725},
doi = {10.1145/3469725},
abstract = {Opinion target classification of microblog comments is one of the most important tasks for public opinion analysis about an event. Due to the high cost of manual labeling, opinion target classification is generally considered as a weak-supervised task. This article attempts to address the opinion target classification of microblog comments through an event graph convolution network (EventGCN) in a weak-supervised manner. Specifically, we take microblog contents and comments as document nodes, and construct an event graph with three typical relationships of event microblogs, including the co-occurrence relationship of event keywords extracted from microblogs, the reply relationship of comments, and the document similarity. Finally, under the supervision of a small number of labels, both word features and comment features can be represented well to complete the classification. The experimental results on two event microblog datasets show that EventGCN can significantly improve the classification performance compared with other baseline models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {17},
numpages = {13},
keywords = {social media analysis, weak-supervised classification, Opinion target, text classification, graph neural network}
}

@inproceedings{10.1145/3267471.3267484,
author = {Monti, Diego and Palumbo, Enrico and Rizzo, Giuseppe and Lisena, Pasquale and Troncy, Rapha\"{e}l and Fell, Michael and Cabrio, Elena and Morisio, Maurizio},
title = {An Ensemble Approach of Recurrent Neural Networks Using Pre-Trained Embeddings for Playlist Completion},
year = {2018},
isbn = {9781450365864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267471.3267484},
doi = {10.1145/3267471.3267484},
abstract = {This paper describes the approach of the D2KLab team to the RecSys Challenge 2018 that focuses on the task of playlist completion. We propose an ensemble strategy of different recurrent neural networks leveraging pre-trained embeddings representing tracks, artists, albums, and titles as inputs. We also use lyrics from which we extract semantic and stylistic features that we fed into the network for the creative track. The RNN learns a probabilistic model from the sequences of items in the playlist, which is then used to predict the most likely tracks to be added to the playlist. Concerning the playlists without tracks, we implemented a fall-back strategy called Title2Rec that generates recommendations using only the playlist title. We optimized the RNN, Title2Rec, and the ensemble approach on a validation set, tuning hyper-parameters such as the optimizer algorithm, the learning rate, and the generation strategy. This approach is effective in predicting tracks for a playlist and flexible to include diverse types of inputs, but it is also computationally demanding in the training phase.},
booktitle = {Proceedings of the ACM Recommender Systems Challenge 2018},
articleno = {13},
numpages = {6},
location = {Vancouver, BC, Canada},
series = {RecSys Challenge '18}
}

@inproceedings{10.1109/ESEM.2017.52,
author = {Munezero, Myriam and Kojo, Tero and M\"{a}nnist\"{o}, Tomi},
title = {An Exploratory Analysis of a Hybrid OSS Company's Forum in Search of Sales Leads},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.52},
doi = {10.1109/ESEM.2017.52},
abstract = {Background: Online forums are instruments through which information or problems are shared and discussed, including expressions of interests and intentions.Objective: In this paper, we present ongoing work aimed at analyzing the content of forum posts of a hybrid open source company that offers both free and commercial licenses, in order to help its community manager gain improved understanding of the forum discussions and sentiments and automatically discover new opportunities such as sales leads, i.e., people who are interested in buying a license. These leads can then be forwarded to the sales team for follow-up and can result in them potentially making a sale, thus increasing company revenue.Method: For the analysis of the forums, an untapped channel for sales leads by the company, text analysis techniques are utilized to identify potential sales leads and the discussion topics and sentiments in those leads.Results: Results of our preliminary work make a positive contribution in lessening the community manager's work in understanding the sentiment and discussion topics in the hybrid open source forum community, as well as make it easier and faster to identify potential future customers.Conclusion: We believe that the results will positively contribute to improving the sales of licenses for the hybrid open source company.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {442–447},
numpages = {6},
keywords = {sales lead identification, hybrid OSS company, online forums, topic modeling, text analysis, sentiment analysis},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3325112.3325240,
author = {Vogl, Thomas and Seidelin, Cathrine and Ganesh, Bharath and Bright, Jonathan},
title = {Algorithmic Bureaucracy},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325240},
doi = {10.1145/3325112.3325240},
abstract = {In recent years, local government has been undergoing changes which are strongly influenced by the growing digitization of governmental operations. In this paper, we expand on the concepts of Digital Era Governance and its successor, Essentially Digital Government, by introducing the concept of Algorithmic Bureaucracy, which looks at the impacts of artificial intelligence on the socio-technical nature of public administration. We report on a mixed-method study, which focused on how the growth of data science is changing the ways that local government works in the United Kingdom. Under Algorithmic Bureaucracy, the direct and indirect effects of public administrative changes on the level of social problem solving may become positive in two cases: 1) where through artificial intelligence and isocratic administration the explainability of algorithmic processes increases individual and staff competence, and 2) where algorithms take on some of the role of processing institutional and policy complexity much more effectively than humans.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {148–153},
numpages = {6},
keywords = {Data Science, Digital Era Governance, Socio-technical systems, Algorithmic Bureaucracy, Local Government},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/3293663.3293667,
author = {Feng, Xingjie and Zeng, Yunze and Xu, Yixiong},
title = {Recommendation Algorithm for Federated User Reviews and Item Reviews},
year = {2018},
isbn = {9781450366410},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293663.3293667},
doi = {10.1145/3293663.3293667},
abstract = {The recommendation model based on scoring matrix is widely used. Although it has achieved certain recommendation accuracy, it ignores the large amount of semantic information available in the reviews that reflects the user's interests, and the data sparsity problem still exists. In response to the above problems, a two-channel CNN recommendation algorithm (C-DCNN, Combine-Double CNN) that combines user reviews and item reviews is proposed. First, the user and item review texts are vectorized into word vectors, and then the features of users and the items are extracted by using two CNN networks respectively. Finally, the abstract features are mapped to the same feature space through the dot product in the shared layer which aims at predicting the user's rating for a particular item. Experiments on the public datasets of Amazon, Yelp, and Beer show that the C-DCNN model makes full use of reviews to characterize the deep features of users and items. The MSE of the model on different datasets is smaller than other benchmark algorithms. And C-DCNN effectively alleviates the problem of data sparsity.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Virtual Reality},
pages = {97–103},
numpages = {7},
keywords = {Word Vector, Convolutional Neural Network, Dual Channel, Data Sparsity, Recommendation System},
location = {Nagoya, Japan},
series = {AIVR 2018}
}

@inproceedings{10.1145/3178158.3178181,
author = {Clarizia, Fabio and Colace, Francesco and De Santo, Massimo and Lombardi, Marco and Pascale, Francesco and Pietrosanto, Antonio},
title = {E-Learning and Sentiment Analysis: A Case Study},
year = {2018},
isbn = {9781450353595},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178158.3178181},
doi = {10.1145/3178158.3178181},
abstract = {E-Learning is becoming one of the most effective training approaches. In particular, the blended learning is considered a useful methodology for supporting and understanding students and their learning issues. Thanks to e-Learning platforms and their collaborative tools, students can interact with other students and share doubts on certain topics. However, teachers often remain outside of this process and do not understand the learning problems that are in their classrooms. A solution for ensuring the privacy of communication among students could be the adoption of a Sentiment Analysis methodology for the detection of the classroom mood during the learning process. In this paper, we investigate the adoption of a probabilistic approach based on the Latent Dirichlet Allocation (LDA) as Sentiment Grabber. The proposed approach can detect the mood of students on the various topics and teacher can better tune his/her teaching approach. The proposed method has been tested in real cases with effective and satisfactory results.},
booktitle = {Proceedings of the 6th International Conference on Information and Education Technology},
pages = {111–118},
numpages = {8},
keywords = {e-learning, collaborative learning approach, sentiment analysis},
location = {Osaka, Japan},
series = {ICIET '18}
}

@inproceedings{10.1145/3430665.3456378,
author = {Deng, Yuli and Zeng, Zhen and Huang, Dijiang},
title = {NeoCyberKG: Enhancing Cybersecurity Laboratories with a Machine Learning-Enabled Knowledge Graph},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456378},
doi = {10.1145/3430665.3456378},
abstract = {The hands-on lab is a critical component of cybersecurity education. There lacks of a coherent way to manage existing labs to provide a practical learning plan for learners in the cybersecurity area. Previous studies utilized the word embedding technologies to construct a knowledge graph and adopt it as a learning guide for students, but this approach has its limitations. In this paper, we present a new approach based on latent semantic analysis (LSA) method to replace word embedding in previous studies as it is more appropriate in a small-size corpus, and it is also able to create a mapping that connects both the topic of each lab and concepts contained in each lab. We use LSA to identify relevant semantic relations, extract relevant lab problems, and construct knowledge graphs from lab contents related to cybersecurity topics. We utilize the output of this study by establishing a web-based lab environment for students that: 1. providing lab index and searching, which contains concepts and knowledge extract from each lab. 2.building a recommendation/guidance system for cybersecurity labs and suggesting more relevant labs based on users learning preferences and past lab history to maximize learning outcomes. To measure the effectiveness of the proposed solution, we conducted a use case study and collected survey data from a graduate-level cybersecurity class at a public university. Our study shows that users tend to gain enhanced learning outcomes and express more interest in the cybersecurity area by leveraging the knowledge graph as a learning guide.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {310–316},
numpages = {7},
keywords = {cybersecurity, knowledge graph, laboratory},
location = {Virtual Event, Germany},
series = {ITiCSE '21}
}

@inproceedings{10.1145/3375708.3380315,
author = {AlEroud, Ahmed and Karabatis, George},
title = {Bypassing Detection of URL-Based Phishing Attacks Using Generative Adversarial Deep Neural Networks},
year = {2020},
isbn = {9781450371155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375708.3380315},
doi = {10.1145/3375708.3380315},
abstract = {The URL components of web addresses are frequently used in creating phishing detection techniques. Typically, machine learning techniques are widely used to identify anomalous patterns in URLs as signs of possible phishing. However, adversaries may have enough knowledge and motivation to bypass URL classification algorithms by creating examples that evade classification algorithms. This paper proposes an approach that generates URL-based phishing examples using Generative Adversarial Networks. The created examples can fool Blackbox phishing detectors even when those detectors are created using sophisticated approaches such as those relying on intra-URL similarities. These created instances are used to deceive Blackbox machine learning-based phishing detection models. We tested our approach using actual phishing datasets. The results show that GAN networks are very effective in creating adversarial phishing examples that can fool both simple and sophisticated machine learning phishing detection models.},
booktitle = {Proceedings of the Sixth International Workshop on Security and Privacy Analytics},
pages = {53–60},
numpages = {8},
keywords = {deep learning, generative adversarial networks, phishing, url classification},
location = {New Orleans, LA, USA},
series = {IWSPA '20}
}

@inproceedings{10.1145/3341161.3343695,
author = {Bagavathi, Arunkumar and Bashiri, Pedram and Reid, Shannon and Phillips, Matthew and Krishnan, Siddharth},
title = {Examining Untempered Social Media: Analyzing Cascades of Polarized Conversations},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343695},
doi = {10.1145/3341161.3343695},
abstract = {Online social media, periodically serves as a platform for cascading polarizing topics of conversation. The inherent community structure present in online social networks (homophily) and the advent of fringe outlets like Gab have created online "echo chambers" that amplify the effects of polarization, which fuels detrimental behavior. Recently, in October 2018, Gab made headlines when it was revealed that Robert Bowers, the individual behind the Pittsburgh Synagogue massacre, was an active member of this social media site and used it to express his anti-Semitic views and discuss conspiracy theories. Thus to address the need of automated data-driven analyses of such fringe outlets, this research proposes novel methods to discover topics that are prevalent in Gab and how they cascade within the network. Specifically, using approximately 34 million posts, and 3.7 million cascading conversation threads with close to 300k users; we demonstrate that there are essentially five cascading patterns that manifest in Gab and the most "viral" ones begin with an echo-chamber pattern and grow out to the entire network. Also, we empirically show, through two models viz. Susceptible-Infected and Bass, how the cascades structurally evolve from one of the five patterns to the other based on the topic of the conversation with upto 84% accuracy.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {625–632},
numpages = {8},
keywords = {cascade evolution models, conversation cascades, polarized conversations, conversation topics},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3444757.3485110,
author = {Khemiri, Ahmed and Drissi, Amani and Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
title = {Learn2Construct: An Automatic Ontology Construction Based on LDA from Texual Data},
year = {2021},
isbn = {9781450383141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444757.3485110},
doi = {10.1145/3444757.3485110},
abstract = {In recent years, the research on Ontology Learning has become a hot topic among researchers because of the exponential increase of the number of documents and textual data not only on the web but also in digital libraries. This has participated to the emergence of new computational tools and methods to deal with the automatic organization, representation, retrieval and exploration of large corpus in order to have a good way of organizing and managing huge volumes of data. LDA-based approaches have proven to provide the best result [18][16] [4]. However, they suffers to several limitations related to concept and relation extraction, as well as coping with the corpus evolution. In order to cope with these problems, we propose here a new solution named Learn2Construct which is an automatic ontology construction method based on topic modeling. Experiments have been conducted to measure the effectiveness of our solution and compare it to existing ones. The results obtained are more than satisfactory.},
booktitle = {Proceedings of the 13th International Conference on Management of Digital EcoSystems},
pages = {49–56},
numpages = {8},
keywords = {Ontology based on topic modeling, LDA, Ontology Learning, Text classification, Topic modeling, Latent Dirichlet Allocation},
location = {Virtual Event, Tunisia},
series = {MEDES '21}
}

@inproceedings{10.1145/3360901.3364444,
author = {Badenes-Olmedo, Carlos and Redondo-Garc\'{\i}a, Jos\'{e} Luis and Corcho, Oscar},
title = {Scalable Cross-Lingual Document Similarity through Language-Specific Concept Hierarchies},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364444},
doi = {10.1145/3360901.3364444},
abstract = {With the ongoing growth in number of digital articles in a wider set of languages and the expanding use of different languages, we need annotation methods that enable browsing multi-lingual corpora. Multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine learning models that can be used to perform thematic explorations on collections of texts in multiple languages. However, these approaches require theme-aligned training data to create a language-independent space. This constraint limits the amount of scenarios that this technique can offer solutions to train and makes it difficult to scale up to situations where a huge collection of multi-lingual documents are required during the training phase. This paper presents an unsupervised document similarity algorithm that does not require parallel or comparable corpora, or any other type of translation resource. The algorithm annotates topics automatically created from documents in a single language with cross-lingual labels and describes documents by hierarchies of multi-lingual concepts from independently-trained models. Experiments performed on the English, Spanish and French editions of JCR-Acquis corpora reveal promising results on classifying and sorting documents by similar content.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {147–153},
numpages = {7},
keywords = {large-scale text analysis, cross-lingual semantic similarity, topic models},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3482632.3484057,
author = {Liu, Junli and Xu, Jinbao},
title = {An Interest Drift Model Based on Markov Chain and Its Application in Micro-Blog Recommendation},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484057},
doi = {10.1145/3482632.3484057},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1865–1870},
numpages = {6},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3084226.3084268,
author = {Williams, Ashley and Rainer, Austen},
title = {Toward the Use of Blog Articles as a Source of Evidence for Software Engineering Research},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084268},
doi = {10.1145/3084226.3084268},
abstract = {Background: Blog articles have potential value as a source of practitioner-generated evidence to complement already accepted sources of evidence in software engineering research e.g. interviews and surveys. To be valuable to research, a method for extracting the high quality articles from the vast quantity available needs to be developed. Objective: To better define the benefits and challenges, scope the problem, develop a set of criteria for evaluating blog articles to be used in the method, and propose research questions. Method: We conducted a two-phase pilot study, using a preliminary set of criteria, to explore the challenges of classifying blog articles. We analyse credibility criteria that have been used in previous research, and cross reference those criteria with previous research in evidence-based software engineering. Results: Based on our analysis, we decide that blog articles need to be rigorous, relevant, well written and experience based for them to be considered credible to researchers. Conclusion: Our work provides an overview of the problem domain, as well as presenting criteria and suggested measurements for these criteria. These can be used by others to find blog articles of potential value to their research.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {280–285},
numpages = {6},
keywords = {Evidence, Credibility, Evidence based software engineering, Blogs, Argumentation},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@inproceedings{10.1145/3436369.3437406,
author = {Yin, Fulian and She, Yuwei and Xiong, Rui and Wang, Yanyan},
title = {A Sentiment Analysis Algorithm of Danmaku Based on Building a Mixed Fine-Grained Sentiment Lexicon},
year = {2020},
isbn = {9781450387835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436369.3437406},
doi = {10.1145/3436369.3437406},
abstract = {The Danmaku is a form of instant video text commentary that reflects the viewer's sentiment orientation. Currently, most of sentiment analysis algorithms based on the sentiment lexicon are using manual construction of the lexicon. However, this kind of method has strong subjectivity and low accuracy, and there is no mature sentiment analysis algorithm for the linguistic characteristics of the Danmaku. In order to solve these problems, we propose a sentiment analysis algorithm of Danmaku based on the construction of a mixed fine-grained sentiment lexicon. This paper realizes the sentiment analysis in three parts: the construction of a basic lexicon, the construction of a fine-grained sentiment lexicon, and the calculation of sentiment values. The basic lexicon includes a stopword list with Danmakus' characteristics and a link-word lexicon, while the fine-grained sentiment lexicon is constructed by using the way of combining word embedding and word co-occurrence. The final experimental results show that our method proposed in this paper is effective in analyzing the sentiment of the Danmaku texts.},
booktitle = {Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition},
pages = {424–430},
numpages = {7},
keywords = {finegrained, sentiment analysis, danmaku, word co-occurrence, Word embedding, sentiment lexicon},
location = {Xiamen, China},
series = {ICCPR 2020}
}

@inproceedings{10.1145/3282373.3282386,
author = {Bo\v{z}i\'{c}, Bojan and R\'{\i}os, Andr\'{e} and Delany, Sarah Jane},
title = {Validation of Tagging Suggestion Models for a Hotel Ticketing Corpus},
year = {2018},
isbn = {9781450364799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282373.3282386},
doi = {10.1145/3282373.3282386},
abstract = {This paper investigates methods for the prediction of tags on a textual corpus that describes hotel staff inputs in a ticketing system. The aim is to improve the tagging process and find the most suitable method for suggesting tags for a new text entry. The paper consists of two parts: (i) exploration of existing sample data, which includes statistical analysis and visualisation of the data to provide an overview, and (ii) evaluation of tag prediction approaches. We have included different approaches from different research fields in order to cover a broad spectrum of possible solutions. As a result, we have tested a machine learning model for multi-label classification (using gradient boosting), a statistical approach (using frequency heuristics), and two simple similarity-based classification approaches (Nearest Centroid and k-Nearest Neighbours). The experiment which compares the approaches uses recall to measure the quality of results. Finally, we provide a recommendation of the modelling approach which produces the best accuracy in terms of tag prediction on the sample data.},
booktitle = {Proceedings of the 20th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {15–23},
numpages = {9},
keywords = {Multi-label Classification, Tag Prediction, Natural Language Processing, k-Nearest Neighbour},
location = {Yogyakarta, Indonesia},
series = {iiWAS2018}
}

@inproceedings{10.1145/3152465.3152468,
author = {Zhai, Weixin and Thill, Jean-Claude},
title = {Social Media Discourse in Disaster Situations: A Study of the Deadly July 21, 2012 Beijing Rainstorm},
year = {2017},
isbn = {9781450354936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152465.3152468},
doi = {10.1145/3152465.3152468},
abstract = {The July 21, 2012 Beijing rainstorm was a devastating catastrophe that caused 79 deaths, raising a great deal of attention all over the world. This research aims to explore emotions, attitudes, and views of citizens during the period surrounding this major rainstorm from a social media perspective. The results show that first the rainstorm-related posts of micro-bloggers in Beijing outnumbered that in Hebei, which also outnumbered the other provinces of China. Second, within the most reposted Weibo posts, a strong proportion is storm related and private bloggers are found to be more influential than government agencies. Third, the four district groups in the Beijing metropolitan region expressed a high-low-high sentiment before, during and after the rainstorm, respectively, and a quite uneven trend was observed across the different groups. Topics extracted through LDA modeling also exhibited a striking space-time pattern.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Emergency Management Using},
articleno = {3},
numpages = {7},
keywords = {Weibo, disasters, content analysis, sentiment analysis, rainstorm},
location = {Redondo Beach, CA, USA},
series = {EM-GIS'17}
}

@inproceedings{10.1145/3487664.3487673,
author = {Kreutz, Christin Katharina and Schenkel, Ralf},
title = {RevASIDE: Assignment of Suitable Reviewer Sets for Publications from Fixed Candidate Pools},
year = {2021},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487673},
doi = {10.1145/3487664.3487673},
abstract = {Scientific publishing heavily relies on the assessment of quality of submitted manuscripts by peer reviewers. Assigning a set of matching reviewers to a submission is a highly complex task which can be performed only by domain experts. We introduce RevASIDE, a reviewer recommendation system that assigns suitable sets of complementing reviewers from a predefined candidate pool without requiring manually defined reviewer profiles. Here, suitability includes not only reviewers’ expertise, but also their authority in the target domain, their diversity in their areas of expertise and experience, and their interest in the topics of the manuscript. We present three new data sets for the expert search and reviewer set assignment tasks and compare the usefulness of simple text similarity methods to document embeddings for expert search. Furthermore, an quantitative evaluation demonstrates significantly better results in reviewer set assignment compared to baselines. A qualitative evaluation also shows their superior perceived quality.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {57–68},
numpages = {12},
keywords = {recommendation system, expertise modelling, reviewer assignment},
location = {Linz, Austria},
series = {iiWAS2021}
}

@article{10.1145/3108413,
author = {Li, Liangda and Zha, Hongyuan},
title = {Energy Usage Behavior Modeling in Energy Disaggregation via Hawkes Processes},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3108413},
doi = {10.1145/3108413},
abstract = {Energy disaggregation, the task of taking a whole home electricity signal and decomposing it into its component appliances, has been proved to be essential in energy conservation research. One powerful cue for breaking down the entire household’s energy consumption is user’s daily energy usage behavior, which has so far received little attention: existing works on energy disaggregation mostly ignored the relationship between the energy usages of various appliances by householders across different time slots. The major challenge in modeling such a relationship in that, with ambiguous appliance usage membership of householders, we find it difficult to appropriately model the influence between appliances, since such influence is determined by human behaviors in energy usage. To address this problem, we propose to model the influence between householders’ energy usage behaviors directly through a novel probabilistic model, which combines topic models with the Hawkes processes. The proposed model simultaneously disaggregates the whole home electricity signal into each component appliance and infers the appliance usage membership of household members and enables those two tasks to mutually benefit each other. Experimental results on both synthetic data and four real-world data sets demonstrate the effectiveness of our model, which outperforms state-of-the-art approaches in not only decomposing the entire consumed energy to each appliance in houses but also the inference of household structures. We further analyze the inferred appliance-householder assignment and the corresponding influence within the appliance usage of each householder and across different householders, which provides insight into appealing human behavior patterns in appliance usage.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {36},
numpages = {22},
keywords = {Energy disaggregation, Hawkes process, energy usage behavior, latent dirichlet allocation, household structure analysis}
}

@article{10.1145/3507782,
author = {Deng, Yang and Li, Yaliang and Zhang, Wenxuan and Ding, Bolin and Lam, Wai},
title = {Toward Personalized Answer Generation in E-Commerce via Multi-Perspective Preference Modeling},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3507782},
doi = {10.1145/3507782},
abstract = {Recently, Product Question Answering (PQA) on E-Commerce platforms has attracted increasing attention as it can act as an intelligent online shopping assistant and improve the customer shopping experience. Its key function, automatic answer generation for product-related questions, has been studied by aiming to generate content-preserving while question-related answers. However, an important characteristic of PQA, i.e., personalization, is neglected by existing methods. It is insufficient to provide the same “completely summarized” answer to all customers, since many customers are more willing to see personalized answers with customized information only for themselves, by taking into consideration their own preferences toward product aspects or information needs. To tackle this challenge, we propose a novel Personalized Answer GEneration method with multi-perspective preference modeling, which explores historical user-generated contents to model user preference for generating personalized answers in PQA. Specifically, we first retrieve question-related user history as external knowledge to model knowledge-level user preference. Then, we leverage the Gaussian Softmax distribution model to capture latent aspect-level user preference. Finally, we develop a persona-aware pointer network to generate personalized answers in terms of both content and style by utilizing personal user preference and dynamic user vocabulary. Experimental results on real-world E-Commerce QA datasets demonstrate that the proposed method outperforms existing methods by generating informative and customized answers and show that answer generation in E-Commerce can benefit from personalization.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {87},
numpages = {28},
keywords = {personalization, product question answering, E-Commerce, Answer generation}
}

@article{10.1145/3375394,
author = {Yan, Xiaoqiang and Lou, Zhengzheng and Hu, Shizhe and Ye, Yangdong},
title = {Multi-Task Information Bottleneck Co-Clustering for Unsupervised Cross-View Human Action Categorization},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3375394},
doi = {10.1145/3375394},
abstract = {The widespread adoption of low-cost cameras generates massive amounts of videos recorded from different viewpoints every day. To cope with this vast amount of unlabeled and heterogeneous data, a new multi-task information bottleneck co-clustering (MIBC) approach is proposed to automatically categorize human actions in collections of unlabeled cross-view videos. Our motivation is that, if a learning action category from each view is seen as a single task, it is reasonable to assume that the tasks of learning action patterns from the videos recorded by multiple cameras are dependent and inter-related, since the actions of the same subjects synchronously recorded from different camera viewpoints are complementary to each other. MIBC aims to transfer the shared view knowledge across multiple tasks (i.e., camera viewpoints) to boost the performance of each task. Specifically, MIBC involves the following two parts: (1) extracting action categories for each task by independently maintaining its own relevant information, and (2) allowing the feature representations of all tasks to be compressed into a common feature space, which is utilized to capture the relatedness of multiple tasks and transfer the shared knowledge across different camera viewpoints. These two parts of MIBC work simultaneously and can be solved in a novel co-clustering mechanism. Our experimental evaluation on several cross-view action collections shows that the MIBC algorithm outperforms the existing state-of-the-art baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {15},
numpages = {23},
keywords = {multi-task clustering, view knowledge transfer, Cross-view action categorization, information bottleneck}
}

@article{10.1145/3375548,
author = {Li, Guohui and Chen, Qi and Zheng, Bolong and Hung, Nguyen Quoc Viet and Zhou, Pan and Liu, Guanfeng},
title = {Time-Aspect-Sentiment Recommendation Models Based on Novel Similarity Measure Methods},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3375548},
doi = {10.1145/3375548},
abstract = {The explosive growth of e-commerce has led to the development of the recommendation system. The recommendation system aims to provide a set of items that meet users’ personalized needs through analyzing users’ consumption records. However, the timeliness of purchasing data and the implicity of feedback data pose severe challenges for the existing recommendation methods. To alleviate these challenges, we exploit the user’s consumption records from the perspectives of user and item, by modeling the data on both item and user level, where the item-level value reflects the grade of item, and the user-level value reflects the user’s purchase intention. In this article, we collect the description information and the reviews of the items from public websites, then adopt sentiment analysis techniques to model the similarities on user level and item level, respectively. In particular, we extend the traditional latent factor model and propose two novel methods—Item Level Similarity Matrix Factorization (ILMF) and User Level Similarity Matrix Factorization (ULMF)—by introducing two novel similarity measure methods. In ILMF and ULMF, the consistency between latent factors and explicit aspects is naturally incorporated into learning latent factors of the users and items, such that we can predict the users’ preferences on different items more accurately. Moreover, we propose Item-User Level Similarity Matrix Factorization (IULMF), which combines these two methods to study their contributions on the final performance. Experimental evaluations on the real datasets show that our methods outperform the baseline approaches in terms of both the precision and NDCG.},
journal = {ACM Trans. Web},
month = {feb},
articleno = {5},
numpages = {26},
keywords = {time, matrix factorization, Recommendation system, sentiment analysis, aspect}
}

@article{10.1145/3375197,
author = {Juneja, Prerna and Rama Subramanian, Deepika and Mitra, Tanushree},
title = {Through the Looking Glass: Study of Transparency in Reddit's Moderation Practices},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {GROUP},
url = {https://doi.org/10.1145/3375197},
doi = {10.1145/3375197},
abstract = {Transparency in moderation practices is crucial to the success of an online community. To meet the growing demands of transparency and accountability, several academics came together and proposed the Santa Clara Principles on Transparency and Accountability in Content Moderation (SCP). In 2018, Reddit, home to uniquely moderated communities called subreddits, announced in its transparency report that the company is aligning its content moderation practices with the SCP. But do the moderators of subreddit communities follow these guidelines too? In this paper, we answer this question by employing a mixed-methods approach on public moderation logs collected from 204 subreddits over a period of five months, containing more than 0.5M instances of removals by both human moderators and AutoModerator. Our results reveal a lack of transparency in moderation practices. We find that while subreddits often rely on AutoModerator to sanction newcomer posts based on karma requirements and moderate uncivil content based on automated keyword lists, users are neither notified of these sanctions, nor are these practices formally stated in any of the subreddits' rules. We interviewed 13 Reddit moderators to hear their views on different facets of transparency and to determine why a lack of transparency is a widespread phenomenon. The interviews reveal that moderators' stance on transparency is divided, there is a lack of standardized process to appeal against content removal and Reddit's app and platform design often impede moderators' ability to be transparent in their moderation practices.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {17},
numpages = {35},
keywords = {online communities, rules, content moderation, transparency, norms, mixed methods}
}

@article{10.1145/3324473,
author = {Liao, Xiaofeng and Zhao, Zhiming},
title = {Unsupervised Approaches for Textual Semantic Annotation, A Survey},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3324473},
doi = {10.1145/3324473},
abstract = {Semantic annotation is a crucial part of achieving the vision of the Semantic Web and has long been a research topic among various communities. The most challenging problem in reaching the Semantic Web’s real potential is the gap between a large amount of unlabeled existing/new data and the limited annotation capability available. To resolve this problem, numerous works have been carried out to increase the degree of automation of semantic annotation from manual to semi-automatic to fully automatic. The richness of these works has been well-investigated by numerous surveys focusing on different aspects of the problem. However, a comprehensive survey targeting unsupervised approaches for semantic annotation is still missing and is urgently needed. To better understand the state-of-the-art of semantic annotation in the textual domain adopting unsupervised approaches, this article investigates existing literature and presents a survey to answer three research questions: (1) To what extent can semantic annotation be performed in a fully automatic manner by using an unsupervised way? (2) What kind of unsupervised approaches for semantic annotation already exist in literature? (3) What characteristics and relationships do these approaches have?In contrast to existing surveys, this article helps the reader get an insight into the state-of-art of semantic annotation using unsupervised approaches. While examining the literature, this article also addresses the inconsistency in the terminology used in the literature to describe the various semantic annotation tools’ degree of automation and provides more consistent terminology. Based on this, a uniform summary of the degree of automation of the many semantic annotation tools that were previously investigated can now be presented.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {66},
numpages = {45},
keywords = {relation extraction, entity recognition, machine learning, unsupervised, information extraction, entity linking, Semantic annotation}
}

@article{10.1145/3478282,
author = {Fresneda, Jorge and Hui, Jeremy and Hill, Chelsey},
title = {Market Segmentation in the Emoji Era},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3478282},
doi = {10.1145/3478282},
abstract = {A better understanding and use of unstructured data can improve customer loyalty and relationships for most organizations.},
journal = {Commun. ACM},
month = {mar},
pages = {105–112},
numpages = {8}
}

@article{10.1145/3233231,
author = {Lipton, Zachary C.},
title = {The Mythos of Model Interpretability},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3233231},
doi = {10.1145/3233231},
abstract = {In machine learning, the concept of interpretability is both important and slippery.},
journal = {Commun. ACM},
month = {sep},
pages = {36–43},
numpages = {8}
}

@inproceedings{10.5555/3382225.3382455,
author = {Ajao, Oluwaseun and Bhowmik, Deepayan and Zargari, Shahrzad},
title = {Content-Aware Tweet Location Inference Using Quadtree Spatial Partitioning and Jaccard-Cosine Word Embedding},
year = {2018},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Inferring locations from user texts on social media platforms is a non-trivial and challenging problem relating to public safety. We propose a novel non-uniform grid-based approach for location inference from Twitter messages using Quadtree spatial partitions. The proposed algorithm uses natural language processing (NLP) for semantic understanding and incorporates Cosine similarity and Jaccard similarity measures for feature vector extraction and dimensionality reduction. We chose Twitter as our experimental social media platform due to its popularity and effectiveness for the dissemination of news and stories about recent events happening around the world. Our approach is the first of its kind to make location inference from tweets using Quadtree spatial partitions and NLP, in hybrid word-vector representations. The proposed algorithm achieved significant classification accuracy and outperformed state-of-the-art grid-based content-only location inference methods by up to 24% in correctly predicting tweet locations within a 161km radius and by 300km in median error distance on benchmark datasets.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1116–1123},
numpages = {8},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1145/3517239,
author = {Liao, Chengwu and Chen, Chao and Guo, Suiming and Wang, Zhu and Liu, Yaxiao and Xu, Ke and Zhang, Daqing},
title = {Wheels Know Why You Travel: Predicting Trip Purpose via a Dual-Attention Graph Embedding Network},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3517239},
doi = {10.1145/3517239},
abstract = {Trip purpose - i.e., why people travel - is an important yet challenging research topic in travel behavior analysis. Generally, the key to this problem is understanding the activity semantics from trip contexts. However, most existing methods rely on passengers' sensitive information - e.g., daily travel log or home address from surveys - to achieve accurate results, and could thus be hardly applied in real-life scenarios. In this paper, we aim to predict the passenger's trip purpose in the scenarios of door-to-door ride services (e.g., taxi trips) by only using the vehicle's GPS trajectory on roads, for which "wheels" is used as a metaphor. Specifically, we propose a novel dual-attention graph embedding model based on the vehicle's trajectory and public POI check-in data. Firstly, both data are aggregated to augment the activity semantics of trip contexts, including the spatiotemporal context and POI contexts at the origin and destination, which are important clues. Based on that, graph attention networks and soft-attention are employed to model the dependency of different contexts on the trip purpose, so as to obtain the trip's comprehensive activity semantics for the final prediction. Extensive experiments are conducted based on the large-scale labeled datasets in Beijing. The prediction results show a considerable improvement compared to state-of-the-arts. A case study demonstrates the feasibility of our study.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {22},
numpages = {22},
keywords = {POI check-in data, trip purpose, GPS trajectory, graph embedding, attention mechanism}
}

@article{10.1145/3329713,
author = {Bakhshaei, Somayeh and Safabakhsh, Reza and Khadivi, Shahram},
title = {Matching Graph, a Method for Extracting Parallel Information from Comparable Corpora},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3329713},
doi = {10.1145/3329713},
abstract = {Comparable corpora are valuable alternatives for the expensive parallel corpora. They comprise informative parallel fragments that are useful resources for different natural language processing tasks. In this work, a generative model is proposed for efficient extraction of parallel fragments from a pair of comparable documents. The core of the proposed model is a graph called the Matching Graph. The ability of the Matching Graph to be trained on a small initial seed makes it a proper model for language pairs suffering from the scarce resource problem. Experiments show that the Matching Graph performs significantly better than other recently published models. According to the experiments on English-Persian and Arabic-Persian language pairs, the extracted parallel fragments can be used instead of parallel data for training statistical machine translation systems. Results reveal that the extracted fragments in the best case are able to retrieve about 90% of the information of a statistical machine translation system that is trained on a parallel corpus. Moreover, it is shown that using the extracted fragments as additional information for training statistical machine translation systems leads to an improvement of about 2% for English-Persian and about 1% for Arabic-Persian translation on BLEU score.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {11},
numpages = {29},
keywords = {comparable corpora, and Arabic languages, Persian, parallel fragments, natural language processing, Information extraction, English, statistical machine translation, generative model}
}

@inproceedings{10.1145/3485557.3485560,
author = {El Akrouchi, Manal and Benbrahim, Houda and Kassou, Ismail},
title = {Review on Adopting Concept Extraction in Weak Signals Detection in Competitive Intelligence},
year = {2021},
isbn = {9781450384186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485557.3485560},
doi = {10.1145/3485557.3485560},
abstract = {The dynamic nature of competition in the business environment makes a company’s ability to secure future change more critical to its survival. Consequently, efficient exploitation of valuable intel is globally acknowledged as an essential foundation of competitive advantage, leading to Competitive Intelligence. Besides, one of the crucial keys to successful competitive information securing is studying the future. Thus, predicting what may happen in the uncertain future is a leading-edge technology leading to an extensive need for foresight analysis. Foresight study uses various methods to recognize future developments and make plans that anticipate possible future changes. One of the leading techniques used in foresight is detecting and understanding Weak Signals. But knowing the nature of these signals, automatically scanning them is still considered a difficult task. For this, we examine the Concept Extraction technique as a main step to detect weak signals from documents automatically. In this paper, we will explain the concept extraction methods used so far, and we present in detail all the main methods and approaches and their application in detecting weak signals.},
booktitle = {The 7th Annual International Conference on Arab Women in Computing in Conjunction with the 2nd Forum of Women in Research},
articleno = {3},
numpages = {8},
keywords = {Weak Signals Detection, Concept Extraction, Topic Modeling, Competitive Intelligence},
location = {Sharjah, United Arab Emirates},
series = {ArabWIC 2021}
}

@inproceedings{10.1145/3341161.3344828,
author = {Praznik, Logan and Srivastava, Gautam and Mendhe, Chetan and Mago, Vijay},
title = {Vertex-Weighted Measures for Link Prediction in Hashtag Graphs},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3344828},
doi = {10.1145/3341161.3344828},
abstract = {Communications on the popular social networking platform, Twitter, can be mapped in terms of a hashtag graph, where vertices correspond to hashtags, and edges correspond to co-occurrences of hashtags within the same distinct tweet. Furthermore, a vertex in hashtag graphs can be weighted with the number of tweets a hashtag has occurred in, and edges can be weighted with the number of tweets both hashtags have co-occurred in. In this paper, we describe additions to some well-known link prediction methods that allow the weights of both vertices and edges in a weighted hashtag graph to be taken into account. We base our novel predictive additions on the assumption that more popular hashtags have a higher probability to appear with other hashtags in the future. We then apply these improved methods to 3 sets of Twitter data with the intent of predicting hashtags co-occurences in the future. Experimental results on real-life data sets consisting of over 3,000,000 combined unique Tweets and over 250, 000 unique hashtags show the effectiveness of the proposed models and algorithms on weighted hashtag graphs.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1034–1041},
numpages = {8},
keywords = {social networks, popularity analysis, link analysis, link prediction, hashtag graph, hashtags, Twitter},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3498851.3498993,
author = {Zhang, Wen and Zhao, Jiangpeng and Wang, Song},
title = {SusTriage: Sustainable Bug Triage with Multi-Modal Ensemble Learning},
year = {2021},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498993},
doi = {10.1145/3498851.3498993},
abstract = {Bug report triage refers to the process of selecting appropriate developers to resolve newly submitted bug reports. Most existing methods primarily focus on improving the recommendation accuracy. Such methods suffer serious popularity bias favoring more experienced developers, while ignoring the implications of recommendation results to long-term, noteworthy sustainability of the open source communities. To alleviate the problem, this paper proposes SusTriage to assign bug reports targeting at balancing both accuracy and sustainability of developer recommendation, based on multi-modal deep learning and ensemble learning. When a new bug report is submitted, we firstly match it with all the developers who have experience in the bug resolution of corresponding product. These developers are divided into three groups and we build multi-modal deep learning scoring model for each group separately, i.e., core, active and peripheral developers. Secondly, the textual and meta information of the bug report with a developer are fed into a specified scoring model according to the developer type. Thirdly, we adopt ensemble learning weight to adjust the output score of each developer, and recommend top Q developers for the bug resolution. We conduct experiments on Eclipse and Mozilla projects and the results demonstrate that SusTriage outperforms state-of-the-art automatic bug report triage methods on both recommendation performance and sustainability of bug resolution.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {441–448},
numpages = {8},
keywords = {Sustainability, Multi-modal learning, Ensemble learning, Bug triage},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3371158.3371169,
author = {Kumar, Himanshu and Manwani, Naresh and Sastry, P. S.},
title = {Robust Learning of Multi-Label Classifiers under Label Noise},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371169},
doi = {10.1145/3371158.3371169},
abstract = {In this paper, we address the problem of robust learning of multi-label classifiers when the training data has label noise. We consider learning algorithms in the risk-minimization framework. We define what we call symmetric label noise in multi-label settings which is a useful noise model for many random errors in the labeling of data. We prove that risk minimization is robust to symmetric label noise if the loss function satisfies some conditions. We show that Hamming loss and a surrogate of Hamming loss satisfy these sufficient conditions and hence are robust. By learning feedforward neural networks on some benchmark multi-label datasets, we provide empirical evidence to illustrate our theoretical results on the robust learning of multi-label classifiers under label noise.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {90–97},
numpages = {8},
keywords = {multi-label, label noise, neural networks, robust losses},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/3338533.3366601,
author = {Sun, Wenjin and Wang, Yuhang and Gao, Yuqi and Li, Zesong and Sang, Jitao and Yu, Jian},
title = {Comprehensive Event Storyline Generation from Microblogs},
year = {2019},
isbn = {9781450368414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338533.3366601},
doi = {10.1145/3338533.3366601},
abstract = {Microblogging data contains a wealth of information of trending events and has gained increased attention among users, organizations, and research scholars for social media mining in different disciplines. Event storyline generation is one typical task of social media mining, whose goal is to extract the development stages with associated description of events. Existing storyline generation methods either generate storyline with less integrity or fail to guarantee the coherence between the discovered stages. Secondly, there are no scientific method to evaluate the quality of the storyline. In this paper, we propose a comprehensive storyline generation framework to address the above disadvantages. Given Microblogging data related to the specified event, we first propose Hot-Word-Based stage detection algorithm to identify the potential stages of event, which can effectively avoid ignoring important stages and preventing inconsistent sequence between stages. Community detection algorithm is applied then to select representative data for each stage. Finally, we conduct graph optimization algorithm to generate the logically coherent storylines of the event. We also introduce a new evaluation metric, SLEU, to emphasize the importance of the integrity and coherence of the generated storyline. Extensive experiments on real-world Chinese microblogging data demonstrate the effectiveness of the proposed methods in each module and the overall framework.},
booktitle = {Proceedings of the ACM Multimedia Asia},
articleno = {48},
numpages = {7},
keywords = {event detecting, microblog, Social media, graph optimization, storyline generation, community detection},
location = {Beijing, China},
series = {MMAsia '19}
}

@inproceedings{10.1145/3323503.3360291,
author = {Menezes, Alice A. F. and Figueiredo, Carlos M. S.},
title = {A Ranking Method for Location-Based Categorical Data in Smart Cities},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360291},
doi = {10.1145/3323503.3360291},
abstract = {With the arising of Smart Cities and the amount of useful information that is provided by citizens, government, and companies nowadays, several interdisciplinary studies in Urban Computing became achievable. The literature present applications and analysis of cities dynamics, citizens mobility, and others. In this work, we propose a ranking method to detect different functional regions through virtual sensors using location-based categorical data of multiple sources. Also, we computed a diversity index according to the categories contained in regions of a study area. As a result, we found patterns associated with the characteristics of these regions, that influenced the inferring of functional regions by the ranking method in temporal analyzes. We evaluated our method using real-world datasets from Foursquare (200,339 check-ins) and NYPD Motor Vehicle Collision (209,908 occurrences) in New York City in 2017. The aim was to evaluate the outcomes of the proposed method concerning different types of categorical data. Also, we presented a case study considering the integration of these data. The generated results can be used in applications related to urban planning and can benefit citizens, entrepreneurs, and the government.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {453–460},
numpages = {8},
keywords = {location-based data, functional regions, smart cities, virtual sensors},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@article{10.1145/3359190,
author = {Antoniak, Maria and Mimno, David and Levy, Karen},
title = {Narrative Paths and Negotiation of Power in Birth Stories},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359190},
doi = {10.1145/3359190},
abstract = {Birth stories have become increasingly common on the internet, but they have received little attention as a computational dataset. These unsolicited, publicly posted stories provide rich descriptions of decisions, emotions, and relationships during a common but sometimes traumatic medical experience. These personal details can be illuminating for medical practitioners, and due to their shared structures, birth stories are also an ideal testing ground for narrative analysis techniques. We present an analysis of 2,847 birth stories from an online forum and demonstrate the utility of these stories for computational work. We discover clear sentiment, topic and persona-based patterns that both model the expected narrative event sequences of birth stories and highlight diverging pathways and exceptions to narrative norms. The authors' motivation to publicly post these personal stories can be a way to regain power after a surveilled and disempowering experience, and we explore power relationships between the personas in the stories, showing that these dynamics can vary with the type of birth (e.g., medicated vs unmedicated). Finally, birth stories exist in a space that is both public and deeply personal. This liminality poses a challenge for analysis and presentation, and we discuss tradeoffs and ethical practices for this collection. WARNING: This paper includes detailed narratives of pregnancy and birth.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {88},
numpages = {27},
keywords = {power, narrative, natural language processing, birth stories}
}

@article{10.14778/3055540.3055547,
author = {Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
title = {Truth Inference in Crowdsourcing: Is the Problem Solved?},
year = {2017},
issue_date = {January 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3055540.3055547},
doi = {10.14778/3055540.3055547},
abstract = {Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {541–552},
numpages = {12}
}

@inproceedings{10.5555/3382225.3382257,
author = {Zhang, Daniel (Yue) and Badilla, Jose and Zhang, Yang and Wang, Dong},
title = {Towards Reliable Missing Truth Discovery in Online Social Media Sensing Applications},
year = {2018},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Social media sensing has emerged as a new application paradigm to collect observations from online social media users about the physical environment. A fundamental problem in social media sensing applications lies in estimating the evolving truth of the measured variables and the reliability of data sources without knowing either of them a priori. This problem is referred to as dynamic truth discovery. Two major limitations exist in current truth discovery solutions: i) existing solutions cannot effectively address the missing truth problem where the measured variables do not have any reported measurements from the data sources; ii) the latent correlations among the measured variables were not fully captured and utilized in current solutions. In this paper, we proposed a Reliable Missing Truth Finder (RMTF) to address the above limitations in social media sensing applications. In particular, we develop a novel data-driven technique to identify the lagged and latent correlations among measured variables, and incorporate such correlation information into a holistic spatiotemporal inference model to infer the missing truth. We evaluated the RMTF using the real-world Twitter data feeds. The results show that the RMTF scheme significantly outperforms the state-of-the-art truth discovery solutions by correctly inferring the missing truth of the measured variables.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {143–150},
numpages = {8},
keywords = {spatiotemporal inference, social media sensing, missing truth discovery},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3308560.3317585,
author = {Sarne, David and Schler, Jonathan and Singer, Alon and Sela, Ayelet and Bar Siman Tov, Ittai},
title = {Unsupervised Topic Extraction from Privacy Policies},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317585},
doi = {10.1145/3308560.3317585},
abstract = {This paper suggests the use of automatic topic modeling for large-scale corpora of privacy policies using unsupervised learning techniques. The advantages of using unsupervised learning for this task are numerous. The primary advantages include the ability to analyze any new corpus with a fraction of the effort required by supervised learning, the ability to study changes in topics of interest along time, and the ability to identify finer-grained topics of interest in these privacy policies. Based on general principles of document analysis we synthesize a cohesive framework for privacy policy topic modeling and apply it over a corpus of 4,982 privacy policies of mobile applications crawled from the Google Play Store. The results demonstrate that even with this relatively moderate-size corpus quite comprehensive insights can be attained regarding the focus and scope of current privacy policy documents. The topics extracted, their structure and the applicability of the unsupervised approach for that matter are validated through an extensive comparison to similar findings reported in prior work that uses supervised learning (which heavily depends on manual annotation of experts). The comparison suggests a substantial overlap between the topics found and those reported in prior work, and also unveils some new topics of interest.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {563–568},
numpages = {6},
keywords = {Topic modeling, unsuprevised learning, privacy policies},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3195106.3195126,
author = {Zhu, Yinbo and Wang, Zhenyu and Wu, Yiqun and Huang, Zhenhua and Li, Min and Zeng, Rong},
title = {Tweets Ranking Considering Dynamic Social Influence and Personal Interests},
year = {2018},
isbn = {9781450363532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195106.3195126},
doi = {10.1145/3195106.3195126},
abstract = {Social networks play important roles in information propagation and interactions among friends. Billions of information stream are created by users in Twitter and diffuse among relationship networks. Users may receive mass tweets posted by their followees every week. Thus, how to rank and display the tweets users are interested in and willing to interact with, is necessary to improve user's experience. Information propagation is usually driven by user interaction behaviors including retweet and comment. While these behaviors are mainly affected by the social influence and user interest, which dynamically change with time. In this paper, we propose a tweets ranking model to predict information diffusion based on dynamic social influence and personal interests. Our model combines these factors together by exploiting listwise algorithm such as ListNet. Experiments show that our model performs better than several baseline models, and both social influence and personal interests play important roles in tweets ranking.},
booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
pages = {276–282},
numpages = {7},
keywords = {dynamic social influence, dynamic personal interests, Information propagation, user friendships, social networks},
location = {Macau, China},
series = {ICMLC 2018}
}

@inproceedings{10.1145/3488933.3488970,
author = {Jinbao, Teng and Weiwei, Kong and Yidan, Chang and Qiaoxin, Tian and Chenyuan, Shi and Long, Li},
title = {Text Classification Method Based on BiGRU-Attention and CNN Hybrid Model},
year = {2021},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3488970},
doi = {10.1145/3488933.3488970},
abstract = {Aiming at the problem that traditional Gated Recurrent Unit (GRU) and Convolution Neural Network (CNN) can not reflect the importance of each word in the text when extracting features, a text classification method based on BiGRU Attention and CNN is proposed. Firstly, CNN was used to extract the local information of the text, and then the full-text semantics was integrated. Secondly, BiGRU was used to extract the context features of the text, and attention mechanism was used after BiGRU to extract the attention score of the output information. Finally, the output of BiGRU attention was fused with the output of CNN to realize the effective extraction of text features and focused on the important content words. Experimental results on three public datasets showed that the proposed model was better than GRU, CNN and other models, which can effectively improve the effect of text classification.},
booktitle = {2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {614–622},
numpages = {9},
keywords = {Feature Fusion, Convolution Neural Network, Bi-directional Gated Recurrent Unit, Text Classification, Attention Mechanism},
location = {Xiamen, China},
series = {AIPR 2021}
}

@inproceedings{10.1145/3281375.3281406,
author = {Shimizu, Shota and Takama, Yasufumi},
title = {Preliminary Investigation on Quantitative Evaluation Method of Scientific Papers Based on Text Analysis},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281406},
doi = {10.1145/3281375.3281406},
abstract = {Recently, there are many resources of scientific research on the Web. Because the gap between the amount of academic information available on the Web and human processing abilities becomes large, several problems have arisen: (1) losing opportunities of research presentation, (2) loosing opportunities of gathering research information, (3) increasing burden of peer review, (4) difficulty in selecting papers to read. In order to solve these problems, quantitative evaluation index of a paper as a selection criterion is needed.This paper proposes quantitative evaluation methods of scientific papers on the basis of text analysis. The journal similarity of a target journal to an authoritative journal is defined with using distributed representations of papers. When the similarity of a target journal is high, its quality in terms of writing and organization is expected to be high. This paper also proposes an evaluation method using ROUGE (Recall-Oriented Understudy for Gisting Evaluation).Proposed evaluation methods are evaluated by experiments. Experiments results show that the journal similarity has rough correspondence to Scimago Journal Rank (SJR). The result also implies the possibility of evaluating journals that have not yet been indexed in some authoritative journal indices using the proposed methods. The evaluation method using ROUGE is shown to have the possibility of evaluating the consistency of papers.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {39–46},
numpages = {8},
keywords = {research evaluation, ROUGE, similarity, open access journal},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3229607.3229608,
author = {Otomo, Kazuki and Kobayashi, Satoru and Fukuda, Kensuke and Esaki, Hiroshi},
title = {Finding Anomalies in Network System Logs with Latent Variables},
year = {2018},
isbn = {9781450359047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229607.3229608},
doi = {10.1145/3229607.3229608},
abstract = {System logs are useful to understand the status of and detect faults in large scale networks. However, due to their diversity and volume of these logs, log analysis requires much time and effort. In this paper, we propose a log event anomaly detection method for large-scale networks without pre-processing and feature extraction. The key idea is to embed a large amount of diverse data into hidden states by using latent variables. We evaluate our method with 15 months of system logs obtained from a nation-wide academic network in Japan. Through comparisons with Kleinberg's univariate burst detection and a traditional multivariate analysis (i.e., PCA), we demonstrate that our proposed method detects anomalies and ease troubleshooting of network system faults.},
booktitle = {Proceedings of the 2018 Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {8–14},
numpages = {7},
keywords = {Variational autoencoder, Network log analysis, Latent variable analysis},
location = {Budapest, Hungary},
series = {Big-DAMA '18}
}

@inproceedings{10.1145/3513130.3558979,
author = {Sherrill, John Timothy},
title = {Mapping Storytelling on Etsy.Com as Hyper-Differentiation Strategy, and Considerations of Scope},
year = {2022},
isbn = {9781450392464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3513130.3558979},
doi = {10.1145/3513130.3558979},
abstract = {This paper presents findings from a qualitative and quantitative analysis of 100 Etsy shop descriptions. Etsy.com is situated as a site of sellers documenting technical expertise and also marketing handmade goods through storytelling and narratives. Shop “About” sections were analyzed for their narrative structure in an attempt to map common narrative features on the site, as narrative and storytelling function as a hyper-differentiation strategy in post-industrial markets where customization and bespoke products are commonplace. The study found that “About” sections consistently followed the narrative structures suggested by the Etsy seller handbook. It suggests that technical communication researchers must be particularly aware of the scope of analysis, sampling methods, and rhetorical complexity of variables when analyzing hyper-differentiation strategies in order for relatively subtle differences to be identifiable.},
booktitle = {Proceedings of the 40th ACM International Conference on Design of Communication},
pages = {63–68},
numpages = {6},
keywords = {Hyper-differentiation, Storytelling, DIY, Narrative},
location = {Boston, MA, USA},
series = {SIGDOC '22}
}

@inproceedings{10.1145/3366423.3380052,
author = {Kim, Seungbae and Jiang, Jyun-Yu and Nakada, Masaki and Han, Jinyoung and Wang, Wei},
title = {Multimodal Post Attentive Profiling for Influencer Marketing},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380052},
doi = {10.1145/3366423.3380052},
abstract = {Influencer marketing has become a key marketing method for brands in recent years. Hence, brands have been increasingly utilizing influencers’ social networks to reach niche markets, and researchers have been studying various aspects of influencer marketing. However, brands have often suffered from searching and hiring the right influencers with specific interests/topics for their marketing due to a lack of available influencer data and/or limited capacity of marketing agencies. This paper proposes a multimodal deep learning model that uses text and image information from social media posts (i) to classify influencers into specific interests/topics (e.g., fashion, beauty) and (ii) to classify their posts into certain categories. We use the attention mechanism to select the posts that are more relevant to the topics of influencers, thereby generating useful influencer representations. We conduct experiments on the dataset crawled from Instagram, which is the most popular social media for influencer marketing. The experimental results show that our proposed model significantly outperforms existing user profiling methods by achieving 98% and 96% accuracy in classifying influencers and their posts, respectively. We release our influencer dataset of 33,935 influencers labeled with specific topics based on 10,180,500 posts to facilitate future research.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2878–2884},
numpages = {7},
keywords = {Multimodal neural network, User profiling, Multi-task learning, Social media, Influencer profiling, Influencer marketing},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3152178.3152185,
author = {Knoblock, Craig A. and Joshi, Aparna R. and Megotia, Abhishek and Pham, Minh and Ursaner, Chelsea},
title = {Automatic Spatio-Temporal Indexing to Integrate and Analyze the Data of an Organization},
year = {2017},
isbn = {9781450354950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152178.3152185},
doi = {10.1145/3152178.3152185},
abstract = {Organizations are awash in data. In many cases, they do not know what data exists within the organization and much information is not available when needed, or worse, information gets recreated from other sources. In this paper, we present an automatic approach to spatio-temporal indexing of the datasets within an organization. The indexing process automatically identifies the spatial and temporal fields, normalizes and cleans those fields, and then loads them into a big data store where the information can be efficiently searched, queried, and analyzed. We evaluated our approach on 600 datasets published by the City of Los Angeles and show that we can automatically process their data and can efficiently access and analyze the indexed data.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
articleno = {7},
numpages = {8},
keywords = {large-scale integration, spatio-temporal indexing, urban data, efficient querying and analysis, data cleaning},
location = {Redondo Beach, CA, USA},
series = {UrbanGIS'17}
}

@inproceedings{10.1145/3463274.3463358,
author = {Kaplan, Angelika and Keim, Jan},
title = {Towards an Automated Classification Approach for Software Engineering Research},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463358},
doi = {10.1145/3463274.3463358},
abstract = {The rapid growth of software engineering research publications forces an amount of scholarly knowledge that needs to be managed, organized and communicated in digital libraries and scientific search engines. Thus, there is a need for classified papers to accomplish these tasks, but the classification process is cumbersome. Moreover, in case of new schemas, one would need to reclassify previously published research. We propose to automate the classification and present different possible techniques for doing so: Using natural language models, a rule-based approach, or an approach based on topic-labeling. In this proposal paper, we initially implemented a prototype for text classification of software engineering research papers.},
booktitle = {Evaluation and Assessment in Software Engineering},
pages = {347–352},
numpages = {6},
keywords = {information extraction, neural machine learning, scholarly knowledge communication, NLP, text classification, Research knowledge organization and management},
location = {Trondheim, Norway},
series = {EASE 2021}
}

@inproceedings{10.1145/3400806.3400815,
author = {Hazel Kwon, K. and Shao, Chun},
title = {Communicative Constitution of Illicit Online Trade Collectives: An Exploration of Darkweb Market Subreddits},
year = {2020},
isbn = {9781450376884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400806.3400815},
doi = {10.1145/3400806.3400815},
abstract = {This study aims to understand communicative activities of an illicit cyber-trade community on an anonymous social platform, Reddit. Based on the communication-as-constitutive of organization (CCO) perspective, the study identifies participants with different levels of engagement, and examines how their discursive engagement collectively reflects the ways in which illicit market users co-orient themselves to respond to a crisis event (i.e., market shutdown). The empirical case for this study is a subreddit channel dedicated to what was once the largest dark web market: r/AlphabayMarket. We examine two month period's posting activities until market was permanently shut down in July 2017. Our analysis comprises three parts. First, a social network analysis was conducted to identify key and non-key players in the community. Second, a structural topic modeling was computed to inductively infer topic clusters. Third, posts were manually reviewed to articulate the process of co-orientation manifest in the results of topic modeling.},
booktitle = {International Conference on Social Media and Society},
pages = {65–72},
numpages = {8},
keywords = {Dark web, communicative constitution of organization, cryptomarket, cyber markets, cybercrime, Reddit, illicit online communities},
location = {Toronto, ON, Canada},
series = {SMSociety'20}
}

@article{10.1145/3466876,
author = {Cantini, Riccardo and Marozzo, Fabrizio and Bruno, Giovanni and Trunfio, Paolo},
title = {Learning Sentence-to-Hashtags Semantic Mapping for Hashtag Recommendation on Microblogs},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3466876},
doi = {10.1145/3466876},
abstract = {The growing use of microblogging platforms is generating a huge amount of posts that need effective methods to be classified and searched. In Twitter and other social media platforms, hashtags are exploited by users to facilitate the search, categorization, and spread of posts. Choosing the appropriate hashtags for a post is not always easy for users, and therefore posts are often published without hashtags or with hashtags not well defined. To deal with this issue, we propose a new model, called HASHET (HAshtag recommendation using Sentence-to-Hashtag Embedding Translation), aimed at suggesting a relevant set of hashtags for a given post. HASHET is based on two independent latent spaces for embedding the text of a post and the hashtags it contains. A mapping process based on a multi-layer perceptron is then used for learning a translation from the semantic features of the text to the latent representation of its hashtags. We evaluated the effectiveness of two language representation models for sentence embedding and tested different search strategies for semantic expansion, finding out that the combined use of BERT (Bidirectional Encoder Representation from Transformer) and a global expansion strategy leads to the best recommendation results. HASHET has been evaluated on two real-world case studies related to the 2016 United States presidential election and COVID-19 pandemic. The results reveal the effectiveness of HASHET in predicting one or more correct hashtags, with an average F-score up to 0.82 and a recommendation hit-rate up to 0.92. Our approach has been compared to the most relevant techniques used in the literature (generative models, unsupervised models, and attention-based supervised models) by achieving up to 15% improvement in F-score for the hashtag recommendation task and 9% for the topic discovery task.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {sep},
articleno = {32},
numpages = {26},
keywords = {social media, sentence embedding, Deep neural networks, word embedding, hashtag recommendation}
}

@inproceedings{10.1145/3184558.3186336,
author = {Mahapatra, Debabrata and Mariappan, Ragunathan and Rajan, Vaibhav},
title = {Automatic Hierarchical Table of Contents Generation for Educational Videos},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186336},
doi = {10.1145/3184558.3186336},
abstract = {The number of freely available online educational videos from universities and other organizations is growing rapidly. Accurate indexing and summarization are essential for efficient search, recommendation and effective consumption of videos. In this paper, we describe a new method of automatically creating a hierarchical table of contents for a video. It provides a summary of the video content along with a textbook--like facility for nonlinear navigation and search through the video. Our multimodal approach combines new methods for shot level video segmentation and for hierarchical summarization. Empirical results demonstrate the efficacy of our approach on many educational videos.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {267–274},
numpages = {8},
keywords = {table of contents, tree knapsack, text summarization, shot segmentation},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3041021.3053053,
author = {Jin, Jian and Geng, Qian and Zhao, Qian and Zhang, Lixue},
title = {Integrating the Trend of Research Interest for Reviewer Assignment},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053053},
doi = {10.1145/3041021.3053053},
abstract = {Reviewer assignment problem in the research field usually refers to invite experts for comments on the quality of papers, projects, etc. Different factors in conventional approaches are reckoned to choose appropriate reviewers, such as the relevance between reviewer candidates and submissions, the diversity of candidates. However, many studies ignore the temporal changes of reviewer interest and the stability of reviewers' interest trend. Accordingly, in this research, three indispensable aspects are analyzed, including the relevance between reviewer candidates and submissions, the interest trend of candidates as well as the authority of candidates. Next, with extracted aspects, the reviewer assignment is formulated as an integer linear programming problem. Finally, categories of comparative experiments are conducted with two large datasets that are built from WANFANG and ArnetMiner, which shows the availability of the proposed approach in modeling the temporal changes of reviewers' research interest. Also, it demonstrates the effectiveness of the proposed approach for the reviewer assignment problem.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1233–1241},
numpages = {9},
keywords = {research interest trend, reviewer assignment, expert recommendation},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1145/3412326,
author = {Oliveira, Lucas Santos De and Vaz-de-Melo, Pedro O. S. and Amaral, Marcelo S. and Pinho, Jos\'{e} Ant\^{o}nio G.},
title = {Do Politicians Talk about Politics? Assessing Online Communication Patterns of Brazilian Politicians},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2469-7818},
url = {https://doi.org/10.1145/3412326},
doi = {10.1145/3412326},
abstract = {Politicians need to decide how to communicate with their voters to build their reputations. This problem is especially complicated during important political events such as the elections when politicians must decide whether to confront and share their thoughts about controversial topics or to simply communicate non-political messages. Aware of these communication behaviors, our goal is to analyze how politicians present themselves in the digital environment and how the public reacts to them. We also investigate whether they change their communication and if there is a typical pattern that is chosen by the majority of politicians over time. To address these problems, we collected 751,117 public tweets of 692 Brazilian deputies from October 2013 to October 2015. Furthermore, we propose a methodology for identifying Twitter messages about political issues at a large scale. We use this methodology to characterize the communication behavior of Brazilian congresspeople in a 2-year span. We found that Brazilian congresspeople changed their communication behavior as the election approached and as they were elected or not. Moreover, we showed that although most of the politicians increased the number of non-political messages during elections, the audience tends to favorite and retweet political messages more.},
journal = {Trans. Soc. Comput.},
month = {sep},
articleno = {19},
numpages = {28},
keywords = {social media, Twitter, Brazil, congresspeople, politics}
}

@inproceedings{10.1145/3437984.3458835,
author = {Goodman, Daniel and Pocock, Adam and Peck, Jason and Steele, Guy},
title = {Vate: Runtime Adaptable Probabilistic Programming for Java},
year = {2021},
isbn = {9781450382984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437984.3458835},
doi = {10.1145/3437984.3458835},
abstract = {Inspired by earlier work on Augur, Vate is a probabilistic programming language for the construction of JVM based probabilistic models with an Object-Oriented interface. As a compiled language it is able to examine the dependency graph of the model to produce optimised code that can be dynamically targeted to different platforms. Using Gibbs Sampling, Metropolis-Hastings and variable marginalisation it can handle a range of model types and is able to efficiently infer values, estimate probabilities, and execute models.},
booktitle = {Proceedings of the 1st Workshop on Machine Learning and Systems},
pages = {62–69},
numpages = {8},
keywords = {Probabilistic Programming, Java, Augur, Parallel Programming, MCMC},
location = {Online, United Kingdom},
series = {EuroMLSys '21}
}

@inproceedings{10.1145/3292500.3330737,
author = {Lu, John and Sridhar, Sumati and Pandey, Ritika and Hasan, Mohammad Al and Mohler, Georege},
title = {Investigate Transitions into Drug Addiction through Text Mining of Reddit Data},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330737},
doi = {10.1145/3292500.3330737},
abstract = {Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2367–2375},
numpages = {9},
keywords = {text mining, reddit forum, drug addiction and recovery, cox regression},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3243082.3243103,
author = {de Souza, Paulo Roberto and Dur\~{a}o, Frederico Ara\'{u}jo},
title = {RecTwitter: A Semantic-Based Recommender System for Twitter Users},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3243103},
doi = {10.1145/3243082.3243103},
abstract = {Twitter is a microblog which contains large amounts of users who contribute with messages for a wide variety of real-world events. It is possible to identify users who share interests using the messages published in their timeline. However, this task is an exhausting process because the algorithm has to analyze all users' messages. In this project, we propose a semantic recommendation system based on SWRL rules to recommend accounts to be followed or unfollowed. In order to evaluate the recommendations, we conducted an experiment with real users. The results show that 80% of the recommendations were generated to unfollow and 20% to follow some account.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {371–378},
numpages = {8},
keywords = {Recommendation, Twitter, Information Overload, Semantic Web},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3234944.3234955,
author = {Gollapalli, Sujatha Das and Li, Xiao-li},
title = {Using PageRank for Characterizing Topic Quality in LDA},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234955},
doi = {10.1145/3234944.3234955},
abstract = {Topic models based on Latent Dirichlet Allocation (LDA) are employed effectively in various information retrieval and data mining tasks. Despite their popularity and wide-spread application, the question of assessing the quality of topics extracted by LDA models is still not completely resolved. While various measures have been proposed to quantify the thematic coherence and interpretability of a topic extracted by LDA, they do not address this problem sufficiently. We observe that existing quality measures select top topic words based on their topic-word co-occurrence without considering word co-occurrences within the same context. We incorporate precisely this information by constructing topic-specific graphs capturing neighborhood of words in an LDA modeled corpus. Next, the PageRank algorithm is applied on these graphs to assign word importance scores based on centrality. We propose two measures to compute topic quality: (1) the Aggregate PageRank of Top-words of a topic and (2) the PageRank Centralization Index of a topic-specific word graph. Our experiments across three datasets show that unlike existing quality measures, our proposed measures are able to identify topics that are discriminative as well as interpretable and yield superior performance on both classification and intruder word identification tasks.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {115–122},
numpages = {8},
keywords = {topic quality measures, topic modeling, pagerank},
location = {Tianjin, China},
series = {ICTIR '18}
}

@inproceedings{10.1145/3532213.3532261,
author = {Xue, Bingxin and Zhu, Cui and Wang, Xuan and Zhu, Wenjun},
title = {The Study on the Text Classification Based on Graph Convolutional Network and BiLSTM},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532261},
doi = {10.1145/3532213.3532261},
abstract = {Recently, Graph Convolutional Neural Network (GCN) is widely used in text classification tasks. And it has been effectively used to accomplish tasks that are thought to have a rich relational structure. However, due to the sparse adjacency matrix constructed by GCN, GCN cannot make full use of context-dependent information in text classification, and it is not good at capturing local information. The Bidirectional Encoder Representation from Transformers (BERT) has the ability to capture contextual information in sentences or documents, but it is limited in capturing global information about vocabulary in a language, which is the advantage of GCN. Therefore, this paper proposes an improved model named Improved Mutual Graph Convolution Networks (IMGCN) to solve the above problems. The original GCN uses word co-occurrence relationships to build text graphs. Word connections are not rich enough and cannot capture context dependencies well, so we introduce semantic dictionary (WordNet) and dependencies. While the model enhances the ability to capture contextual dependencies, it lacks the ability to capture sequences. Therefore, we introduced BERT and Bi-directional Long Short-Term Memory (BiLSTM) Network to perform deeper learning on the features of text, thereby improving the classification effect of the model. The experimental results show that our model is more effective than previous research reports on four text classification datasets.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {323–331},
numpages = {9},
keywords = {dependencies, graph convolutional network, Bi-directional Long Short-Term Memory, Text classification, ResNet},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3463274.3463330,
author = {Williams, Ashley and Shardlow, Matthew and Rainer, Austen},
title = {Towards a Corpus for Credibility Assessment in Software Practitioner Blog Articles},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463330},
doi = {10.1145/3463274.3463330},
abstract = {Background: Blogs are a source of grey literature which are widely adopted by software practitioners for disseminating opinion and experience. Analysing such articles can provide useful insights into the state–of–practice for software engineering research. However, there are challenges in identifying higher quality content from the large quantity of articles available. Credibility assessment can help in identifying quality content, though there is a lack of existing corpora. Credibility is typically measured through a series of conceptual criteria, with ’argumentation’ and ’evidence’ being two important criteria. Objective: We create a corpus labelled for argumentation and evidence that can aid the credibility community. The corpus consists of articles from the blog of a single software practitioner and is publicly available. Method: Three annotators label the corpus with a series of conceptual credibility criteria, reaching an agreement of 0.82 (Fleiss’ Kappa). We present preliminary analysis of the corpus by using it to investigate the identification of claim sentences (one of our ten labels). Results: We train four systems (Bert, KNN, Decision Tree and SVM) using three feature sets (Bag of Words, Topic Modelling and InferSent), achieving an F1 score of 0.64 using InferSent and a Linear SVM. Conclusions: Our preliminary results are promising, indicating that the corpus can help future studies in detecting the credibility of grey literature. Future research will investigate the degree to which the sentence level annotations can infer the credibility of the overall document.},
booktitle = {Evaluation and Assessment in Software Engineering},
pages = {100–108},
numpages = {9},
keywords = {experience mining, argumentation mining, text mining, credibility assessment},
location = {Trondheim, Norway},
series = {EASE 2021}
}

@inproceedings{10.1145/3109761.3158403,
author = {Razavi, Seyyed Aref and Asadpour, Masoud},
title = {Word Embedding-Based Approach to Aspect Detection for Aspect-Based Summarization of Persian Customer Reviews},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3158403},
doi = {10.1145/3109761.3158403},
abstract = {Many1 ecommerce websites provide the customers with the ability to share their opinions about the products. These opinions can assist other customers to purchase wisely and manufacturers to improve their products and services. Due to a huge volume of product reviews in the online websites and hence difficulty of perusing all of them, it is essential to produce a concise summary of the reviews about different aspects of the products. Aspect detection is a vital step of aspect-based summarization, aiming at identifying the most important product aspects about which users express their opinions. In this paper, we propose a novel unsupervised approach to aspect detection employing word embedding techniques to identify relevant aspects and their semantically related words, called aspect keywords and categorize aspects into semantic categories. The main purpose of our method is to use semantic and syntactic relationships in word embedding vectors in order to improve extraction of multiword aspects and distinguishing explicit and implicit aspects from their keywords. Our experimental results indicate the improvements.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {33},
numpages = {10},
keywords = {product aspect detection, word embedding, aspect categorization, persian reviews mining, aspect extraction},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@article{10.1145/3186729,
author = {Xiao, Ping and Toivonen, Hannu and Gross, Oskar and Cardoso, Am\'{\i}lcar and Correia, Jo\~{a}o and Machado, Penousal and Martins, Pedro and Oliveira, Hugo Goncalo and Sharma, Rahul and Pinto, Alexandre Miguel and D\'{\i}az, Alberto and Francisco, Virginia and Gerv\'{a}s, Pablo and Herv\'{a}s, Raquel and Le\'{o}n, Carlos and Forth, Jamie and Purver, Matthew and Wiggins, Geraint A. and Miljkovi\'{c}, Dragana and Podpe\v{c}an, Vid and Pollak, Senja and Kralj, Jan and \v{Z}nidar\v{s}i\v{c}, Martin and Bohanec, Marko and Lavra\v{c}, Nada and Urban\v{c}i\v{c}, Tanja and Velde, Frank Van Der and Battersby, Stuart},
title = {Conceptual Representations for Computational Concept Creation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3186729},
doi = {10.1145/3186729},
abstract = {Computational creativity seeks to understand computational mechanisms that can be characterized as creative. The creation of new concepts is a central challenge for any creative system. In this article, we outline different approaches to computational concept creation and then review conceptual representations relevant to concept creation, and therefore to computational creativity. The conceptual representations are organized in accordance with two important perspectives on the distinctions between them. One distinction is between symbolic, spatial and connectionist representations. The other is between descriptive and procedural representations. Additionally, conceptual representations used in particular creative domains, such as language, music, image and emotion, are reviewed separately. For every representation reviewed, we cover the inference it affords, the computational means of building it, and its application in concept creation.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {9},
numpages = {33},
keywords = {procedural representation, conceptual representation, concept, concept creation, Computational creativity}
}

@article{10.1145/3441454,
author = {Cheng, Lin and Shi, Yuliang and Zhang, Kun and Wang, Xinjun and Chen, Zhiyong},
title = {GGATB-LSTM: Grouping and Global Attention-Based Time-Aware Bidirectional LSTM Medical Treatment Behavior Prediction},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441454},
doi = {10.1145/3441454},
abstract = {In China, with the continuous development of national health insurance policies, more and more people have joined the health insurance. How to accurately predict patients future medical treatment behavior becomes a hotspot issue. The biggest challenge in this issue is how to improve the prediction performance by modeling health insurance data with high-dimensional time characteristics. At present, most of the research is to solve this issue by using Recurrent Neural Networks (RNNs) to construct an overall prediction model for the medical visit sequences. However, RNNs can not effectively solve the long-term dependence, and RNNs ignores the importance of time interval of the medical visit sequence. Additionally, the global model may lose some important content to different groups. In order to solve these problems, we propose a Grouping and Global Attention based Time-aware Bidirectional Long Short-Term Memory (GGATB-LSTM) model to achieve medical treatment behavior prediction. The model first constructs a heterogeneous information network based on health insurance data, and uses a tensor CANDECOMP/PARAFAC decomposition method to achieve similarity grouping. In terms of group prediction, a global attention and time factor are introduced to extend the bidirectional LSTM. Finally, the proposed model is evaluated by using real dataset, and conclude that GGATB-LSTM is better than other methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {may},
articleno = {45},
numpages = {16},
keywords = {medical treatment behavior prediction, medical visit sequence, similarity grouping, Health insurance}
}

@inproceedings{10.1145/3290688.3290712,
author = {Xie, Simon and Li, Man and Li, Jianxin},
title = {Sentiment Correlation Discovery From Social Media to Share Market},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290712},
doi = {10.1145/3290688.3290712},
abstract = {Social media data analytics have been successfully applied in many real applications such as product recommendation, target advertisement. In recent years, it also attracted lots of attention from the financial researchers to analyse the financial trending or stock marketing prediction. In this paper, our goal is to investigate the meaningful way of uncovering the correlation between the stock share price change and the social media data usage. In this work, we first provide a mechanism to collect Twitter data, use Latent Dirichlet Allocation for topic modelling, then perform the sentiment analysis based on topics, and finally discover the correlation between social media and share price. Based on our empirical results, we find that the correlation could be impacted by the popularity of discussion as well as the valence of community, which represents the happiness to the target companies to be analysed and predicted. This could be built up by exploring the market and crisis resolution. The influence of online social users also plays a significant role in the correlation, which is a factor of manipulation that the influential users should be considered by measuring the responsibility of their social media account.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {52},
numpages = {8},
keywords = {Share Price, Social Media, Sentiment Analysis},
location = {Sydney, NSW, Australia},
series = {ACSW 2019}
}

@inproceedings{10.1145/3209542.3209558,
author = {Aipe, Alan and Gadiraju, Ujwal},
title = {SimilarHITs: Revealing the Role of Task Similarity in Microtask Crowdsourcing},
year = {2018},
isbn = {9781450354271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209542.3209558},
doi = {10.1145/3209542.3209558},
abstract = {Workers in microtask crowdsourcing systems typically consume different types of tasks. Task consumption is driven by the self-selection of workers in the most popular platforms such as Amazon Mechanical Turk and CrowdFlower. Workers typically complete tasks one after another in a chain. Prior works have revealed the impact of ordering tasks while considering aspects such as task complexity. However, little is understood about the benefits of considering task similarity in microtask chains. In this paper, we investigate the role of task similarity in microtask crowdsourcing and how it affects market dynamics. We identified different dimensions that affect the perception of task similarity among workers, and propose a supervised machine learning model to predict the overall task similarity of a task pair. Leveraging task similarity, we studied the effects of similarity on worker retention, satisfaction, boredom and fatigue. We reveal the impact of chaining tasks according to their similarity on worker accuracy and their task completion time. Our findings enrich the current understanding of crowd work and bear important implications on structuring workflow.},
booktitle = {Proceedings of the 29th on Hypertext and Social Media},
pages = {115–122},
numpages = {8},
keywords = {performance, microtasks, crowdsourcing, workers, task similarity},
location = {Baltimore, MD, USA},
series = {HT '18}
}

@article{10.1145/3372406,
author = {Xu, Yanan and Shen, Yanyan and Zhu, Yanmin and Yu, Jiadi},
title = {AR2Net: An Attentive Neural Approach for Business Location Selection with Satellite Data and Urban Data},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372406},
doi = {10.1145/3372406},
abstract = {Business location selection is crucial to the success of businesses. Traditional approaches like manual survey investigate multiple factors, such as foot traffic, neighborhood structure, and available workforce, which are typically hard to measure. In this article, we propose to explore both satellite data (e.g., satellite images and nighttime light data) and urban data for business location selection tasks of various businesses. We extract discriminative features from the two kinds of data and perform empirical analysis to evaluate the correlation between extracted features and the business popularity of locations. A novel neural network approach named R2Net is proposed to learn deep interactions among features and predict the business popularity of locations. The proposed approach is trained with a regression-and-ranking combined loss function to preserve accurate popularity estimation and the ranking order of locations simultaneously. To support the location selection for multiple businesses, we propose an approach named AR2Net with three attention modules, which enable the approach to focus on different latent features according to business types. Comprehensive experiments on a real-world dataset demonstrate that the satellite features are effective and our models outperform the state-of-the-art methods in terms of four metrics.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {20},
numpages = {28},
keywords = {satellite images, business location selection, nighttime light, Satellite data}
}

@article{10.1145/3300199,
author = {Likhyani, Ankita and Bedathur, Srikanta and P., Deepak},
title = {Location-Specific Influence Quantification in Location-Based Social Networks},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3300199},
doi = {10.1145/3300199},
abstract = {Location-based social networks (LBSNs) such as Foursquare offer a platform for users to share and be aware of each other’s physical movements. As a result of such a sharing of check-in information with each other, users can be influenced to visit (or check-in) at the locations visited by their friends. Quantifying such influences in these LBSNs is useful in various settings such as location promotion, personalized recommendations, mobility pattern prediction, and so forth. In this article, we develop a model to quantify the influence specific to a location between a pair of users. Specifically, we develop a framework called LoCaTe, that combines (a) a user mobility model based on kernel density estimates; (b) a model of the semantics of the location using topic models; and (c) a user correlation model that uses an exponential distribution. We further develop LoCaTe+, an advanced model within the same framework where user correlation is quantified using a Mutually Exciting Hawkes Process. We show the applicability of LoCaTe and LoCaTe+ for location promotion and location recommendation tasks using LBSNs. Our models are validated using a long-term crawl of Foursquare data collected between January 2015 and February 2016, as well as other publicly available LBSN datasets. Our experiments demonstrate the efficacy of the LoCaTe framework in capturing location-specific influence between users. We also show that our models improve over state-of-the-art models for the task of location promotion as well as location recommendation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {23},
numpages = {28},
keywords = {influence quantification, Location-based social networks}
}

@inproceedings{10.1145/3503161.3548248,
author = {Huang, Shudong and Liu, Yixi and Ren, Yazhou and Tsang, Ivor W. and Xu, Zenglin and Lv, Jiancheng},
title = {Learning Smooth Representation for Multi-View Subspace Clustering},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548248},
doi = {10.1145/3503161.3548248},
abstract = {Multi-view subspace clustering aims to exploit data correlation consensus among multiple views, which essentially can be treated as graph-based approach. However, existing methods usually suffer from suboptimal solution as the raw data might not be separable into subspaces. In this paper, we propose to achieve a smooth representation for each view and thus facilitate the downstream clustering task. It is based on a assumption that a graph signal is smooth if nearby nodes on the graph have similar features representations. Specifically, our mode is able to retain the graph geometric features by applying a low-pass filter to extract the smooth representations of multiple views. Besides, our method achieves the smooth representation learning as well as multi-view clustering interactively in a unified framework, hence it is an end-to-end single-stage learning problem. Substantial experiments on benchmark multi-view datasets are performed to validate the effectiveness of the proposed method, compared to the state-of-the-arts over the clustering performance.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3421–3429},
numpages = {9},
keywords = {subspace learning, multi-view learning, clustering, graph filtering},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3503161.3548248,
author = {Huang, Shudong and Liu, Yixi and Ren, Yazhou and Tsang, Ivor W. and Xu, Zenglin and Lv, Jiancheng},
title = {Learning Smooth Representation for Multi-View Subspace Clustering},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548248},
doi = {10.1145/3503161.3548248},
abstract = {Multi-view subspace clustering aims to exploit data correlation consensus among multiple views, which essentially can be treated as graph-based approach. However, existing methods usually suffer from suboptimal solution as the raw data might not be separable into subspaces. In this paper, we propose to achieve a smooth representation for each view and thus facilitate the downstream clustering task. It is based on a assumption that a graph signal is smooth if nearby nodes on the graph have similar features representations. Specifically, our mode is able to retain the graph geometric features by applying a low-pass filter to extract the smooth representations of multiple views. Besides, our method achieves the smooth representation learning as well as multi-view clustering interactively in a unified framework, hence it is an end-to-end single-stage learning problem. Substantial experiments on benchmark multi-view datasets are performed to validate the effectiveness of the proposed method, compared to the state-of-the-arts over the clustering performance.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3421–3429},
numpages = {9},
keywords = {subspace learning, multi-view learning, clustering, graph filtering},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1145/3178456,
author = {She, Xiaohan and Jian, Ping and Zhang, Pengcheng and Huang, Heyan},
title = {Leveraging Hierarchical Deep Semantics to Classify Implicit Discourse Relations via a Mutual Learning Method},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3178456},
doi = {10.1145/3178456},
abstract = {This article presents a mutual learning method using hierarchical deep semantics for the classification of implicit discourse relations in English. With the absence of explicit discourse markers, traditional discourse techniques mainly concentrate on discrete linguistic features in this task, which always leads to a data sparseness problem. To relieve this problem, we propose a mutual learning neural model that makes use of multilevel semantic information together, including the distribution of implicit discourse relations, the semantics of arguments, and the co-occurrence of phrases and words. During the training process, the predicting targets of the model, which are the probability of the discourse relation type and the distributed representation of semantic components, are learned jointly and optimized mutually. The experimental results show that this method outperforms the previous works, especially in multiclass identification attributed to the hierarchical semantic representations and the mutual learning strategy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {feb},
articleno = {21},
numpages = {12},
keywords = {Hierarchical deep semantics, Implicit discourse relation classification, Mutual learning neural network}
}

@article{10.1145/3236386.3241340,
author = {Lipton, Zachary C.},
title = {The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability is Both Important and Slippery.},
year = {2018},
issue_date = {May-June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/3236386.3241340},
doi = {10.1145/3236386.3241340},
abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
journal = {Queue},
month = {jun},
pages = {31–57},
numpages = {27}
}

@inproceedings{10.1145/3176349.3176385,
author = {Dinneen, Jesse David and Asadi, Banafsheh and Frissen, Ilja and Shu, Fei and Julien, Charles-Antoine},
title = {Improving Exploration of Topic Hierarchies: Comparative Testing of Simplified Library of Congress Subject Heading Structures},
year = {2018},
isbn = {9781450349253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176349.3176385},
doi = {10.1145/3176349.3176385},
abstract = {Many large digital collections are organized by sorting their items into topics and arranging these topics hierarchically, such as those displayed in a tree view. The resulting information organization structures mitigate some of the challenges of searching digital information realms; however, the topic hierarchies are often large and complex, and thus difficult to navigate. Automated techniques have been shown to produce significantly smaller, simplified versions of existing topic hierarchies while preserving access to the majority of the collection, but these simplified topic hierarchies have never been tested with human participants, and so it is not clear what effect simplification would have on the exploration and use of such structures for browsing and retrieval. This study partly addresses this gap by performing a comparative test with three groups of university students (N=62) performing ten topic hierarchy exploration tasks using one of three versions of the Library of Congress Subject Headings (LCSH) hierarchy: 1) the original LCSH hierarchy, acting as a baseline, 2) a shallower version of 1), and 3) a narrower version of 2). A quantitative analysis of measures of accuracy, time, and browsing shows that participants using the simplified trees were significantly more accurate and faster than those using the unmodified tree, and the narrower, balanced tree was also faster than the shallower tree. These results show that automated topic hierarchy simplification can facilitate the use of such hierarchies, which has implications for the development of information organization theory and human-information interaction techniques for similar information structures.},
booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
pages = {102–109},
numpages = {8},
keywords = {comparative testing, topic hierarchies, trees, library of congress subject headings (lcsh), organization of information, browsing},
location = {New Brunswick, NJ, USA},
series = {CHIIR '18}
}

@inproceedings{10.1145/3477495.3532084,
author = {Yang, Haitian and Zhao, Xuan and Wang, Yan and Li, Min and Chen, Wei and Huang, Weiqing},
title = {DGQAN: Dual Graph Question-Answer Attention Networks for Answer Selection},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532084},
doi = {10.1145/3477495.3532084},
abstract = {Community question answering (CQA) becomes increasingly prevalent in recent years, providing platforms for users with various backgrounds to obtain information and share knowledge. However, the redundancy and lengthiness issues of crowd-sourced answers limit the performance of answer selection, thus leading to difficulties in reading or even misunderstandings for community users. To solve these problems, we propose the dual graph question-answer attention networks (DGQAN) for answer selection task. Aims to fully understand the internal structure of the question and the corresponding answer, firstly, we construct a dual-CQA concept graph with graph convolution networks using the original question and answer text. Specifically, our CQA concept graph exploits the correlation information between question-answer pairs to construct two sub-graphs (QSubject-Answer and QBody-Answer), respectively. Further, a novel dual attention mechanism is incorporated to model both the internal and external semantic relations among questions and answers. More importantly, we conduct experiment to investigate the impact of each layer in the BERT model. The experimental results show that DGQAN model achieves state-of-the-art performance on three datasets (SemEval-2015, 2016, and 2017), outperforming all the baseline models.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1230–1239},
numpages = {10},
keywords = {answer selection, community question answering, dual graph attention},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3341161.3342881,
author = {Kwon, Young D. and Mogavi, Reza Hadi and Haq, Ehsan Ul and Kwon, Youngjin and Ma, Xiaojuan and Hui, Pan},
title = {Effects of Ego Networks and Communities on Self-Disclosure in an Online Social Network},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3342881},
doi = {10.1145/3341161.3342881},
abstract = {Understanding how much users disclose personal information in Online Social Networks (OSN) has served various scenarios such as maintaining social relationships and customer segmentation. Prior studies on self-disclosure have relied on surveys or users' direct social networks. These approaches, however, cannot represent the whole population nor consider user dynamics at the community level.In this paper, we conduct a quantitative study at different granularities of networks (ego networks and user communities) to understand users' self-disclosing behaviors better. As our first contribution, we characterize users into three types (open, closed, and moderate) based on the Communication Privacy Management theory and extend the analysis of the self-disclosure of users to a large-scale OSN dataset which could represent the entire network structure. As our second contribution, we show that our proposed features of ego networks and positional and structural properties of communities significantly affect self-disclosing behavior. Based on these insights, we present the possible relation between the propensity of the self-disclosure of users and the sociological theory of structural holes, i.e., users at a bridge position can leverage advantages among distinct groups. To the best of our knowledge, our study provides the first attempt to shed light on the self-disclosure of users using the whole network structure, which paves the way to a better understanding of users' self-disclosing behaviors and their relations with overall network structures.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {17–24},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@article{10.1145/3291044,
author = {Xuan, Junyu and Lu, Jie and Zhang, Guangquan},
title = {A Survey on Bayesian Nonparametric Learning},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291044},
doi = {10.1145/3291044},
abstract = {Bayesian (machine) learning has been playing a significant role in machine learning for a long time due to its particular ability to embrace uncertainty, encode prior knowledge, and endow interpretability. On the back of Bayesian learning’s great success, Bayesian nonparametric learning (BNL) has emerged as a force for further advances in this field due to its greater modelling flexibility and representation power. Instead of playing with the fixed-dimensional probabilistic distributions of Bayesian learning, BNL creates a new “game” with infinite-dimensional stochastic processes. BNL has long been recognised as a research subject in statistics, and, to date, several state-of-the-art pilot studies have demonstrated that BNL has a great deal of potential to solve real-world machine-learning tasks. However, despite these promising results, BNL has not created a huge wave in the machine-learning community. Esotericism may account for this. The books and surveys on BNL written by statisticians are overcomplicated and filled with tedious theories and proofs. Each is certainly meaningful but may scare away new researchers, especially those with computer science backgrounds. Hence, the aim of this article is to provide a plain-spoken, yet comprehensive, theoretical survey of BNL in terms that researchers in the machine-learning community can understand. It is hoped this survey will serve as a starting point for understanding and exploiting the benefits of BNL in our current scholarly endeavours. To achieve this goal, we have collated the extant studies in this field and aligned them with the steps of a standard BNL procedure—from selecting the appropriate stochastic processes through manipulation to executing the model inference algorithms. At each step, past efforts have been thoroughly summarised and discussed. In addition, we have reviewed the common methods for implementing BNL in various machine-learning tasks along with its diverse applications in the real world as examples to motivate future studies.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {13},
numpages = {36},
keywords = {machine-learning, Bayesian-learning, Data science}
}

@inproceedings{10.1145/3487553.3524925,
author = {Yang, Puyu and Colavizza, Giovanni},
title = {A Map of Science in Wikipedia},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524925},
doi = {10.1145/3487553.3524925},
abstract = {In recent decades, the rapid growth of Internet adoption is offering opportunities for convenient and inexpensive access to scientific information. Wikipedia, one of the largest encyclopedias worldwide, has become a reference in this respect, and has attracted widespread attention from scholars. However, a clear understanding of the scientific sources underpinning Wikipedia’s contents remains elusive. In this work, we rely on an open dataset of citations from Wikipedia to map the relationship between Wikipedia articles and scientific journal articles. We find that most journal articles cited from Wikipedia belong to STEM fields, in particular biology and medicine (47.6% of citations; 46.1% of cited articles). Furthermore, Wikipedia’s biographies play an important role in connecting STEM fields with the humanities, especially history. These results contribute to our understanding of Wikipedia’s reliance on scientific sources, and its role as knowledge broker to the public.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1289–1300},
numpages = {12},
keywords = {bibliometrics, altmetrics, network analysis, citation networks, Wikipedia},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3534678.3539107,
author = {He, Zhicheng and Xia, Wei and Dong, Kai and Guo, Huifeng and Tang, Ruiming and Xia, Dingyin and Zhang, Rui},
title = {Unsupervised Learning Style Classification for Learning Path Generation in Online Education Platforms},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539107},
doi = {10.1145/3534678.3539107},
abstract = {Online education, which educates students that cannot be present at school, has become an important supplement to traditional education. Without the direct supervision and instruction of teachers, online education is always concerned with potential distractions and misunderstandings. Learning Style Classification (LSC) is proposed to analyze the learning behavior patterns of online learning users, based on which personalized learning paths are generated to help them learn and maintain their interests.Existing LSC studies rely on expert-labored labeling, which is infeasible in large-scale applications, so we resort to unsupervised classification techniques. However, current unsupervised classification methods are not applicable due to two important challenges: C1) the unawareness of the LSC problem formulation and pedagogy domain knowledge; C2) the absence of any supervision signals. In this paper, we give a formal definition of the unsupervised LSC problem and summarize the domain knowledge into problem-solving heuristics (which addresses C1). A rule-based approach is first designed to provide a tentative solution in a principled manner (which addresses C2). On top of that, a novel Deep Unsupervised Classifier with domain Knowledge (DUCK) is proposed to convert the discovered conclusions and domain knowledge into learnable model components (which addresses both C1 and C2), which significantly improves the effectiveness, efficiency, and robustness. Extensive offline experiments on both public and industrial datasets demonstrate the superiority of our proposed methods. Moreover, the proposed methods are now deployed in the Huawei Education Center, and the ongoing A/B testing results verify the effectiveness of the methods.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2997–3006},
numpages = {10},
keywords = {user behavior analysis, unsupervised classification, learning style classification, educational data mining, deep clustering},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3493700.3493724,
author = {Saxena, Rohan and Chaudhary, Maheep and Maurya, Chandresh Kumar and Prasad, Shitala},
title = {An Intelligent Recommendation-Cum-Reminder System},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493724},
doi = {10.1145/3493700.3493724},
abstract = {Intelligent recommendation and reminder systems are the need of the fast-pacing life. Current intelligent systems such as Siri, Google Assistant, Microsoft Cortona, etc., have limited capability. For example, if you want to wake up at 6 am because you have an upcoming trip, you have to set the alarm manually. Besides, these systems do not recommend or remind what else to carry, such as carrying an umbrella during a likely rain. The present work proposes a system that takes an email as input and returns a recommendation-cum-reminder list. As a first step, we parse the emails, recognize the entities using named entity recognition (NER). In the second step, information retrieval over the web is done to identify nearby places, climatic conditions, etc. Imperative sentences from the reviews of all places are extracted and passed to the object extraction module. The main challenge lies in extracting the objects (items) of interest from the review. To solve it, a modified Machine Reading Comprehension-NER (MRC-NER) model is trained to tag objects of interest by formulating annotation rules as a query. The objects so found are recommended to the user one day in advance. The final reminder list of objects is pruned by our proposed model for tracking objects kept during the ”packing activity.” Eventually, when the user leaves for the event/trip, an alert is sent containing the reminding list items. Our approach achieves superior performance compared to several baselines by as much as 30% on recall and 10% on precision.},
booktitle = {5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {169–177},
numpages = {9},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.1145/3411564.3411572,
author = {Campos, Rodrigo and dos Santos, Rodrigo Pereira and Oliveira, Jonice},
title = {A Recommendation System Based on Knowledge Gap Identification in MOOCs Ecosystems},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411572},
doi = {10.1145/3411564.3411572},
abstract = {The consolidation of recommendation systems in a big data era brings opportunities in different scenarios to customize methods that recommend data. In the scenarios of the Massive Open Online Courses (MOOCs) ecosystems, these recommenders mainly support students in choosing the best courses from the platforms. However, the expansion of course platforms and the scarcity of student data increases the difficulty in finding courses, or even part of courses, that fill a given knowledge gap. In this paper, we propose a recommendation system to support students in finding the best modules or courses in these ecosystems. First, topic modeling techniques were implemented with Non-negative Matrix Factorization (NMF) to find similarities between multiple MOOCs providers. Then, a content-based recommendation provides recommendations to a user interested in acquiring new knowledge, based on a history extraction on those platforms. We evaluate our approach through an experiment with real data collected in multiple MOOCs providers. In addition, by comparing the NMF approach with a baseline Latent Dirichlet Allocation (LDA) technique, we verify the model effectiveness and show that our system is useful to this context.},
booktitle = {XVI Brazilian Symposium on Information Systems},
articleno = {2},
numpages = {8},
keywords = {recommendation system, MOOCs ecosystems, non-negative matrix factorization, topic modeling},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI'20}
}

@inproceedings{10.1145/3038912.3052575,
author = {Wei, Xiaokai and Xu, Linchuan and Cao, Bokai and Yu, Philip S.},
title = {Cross View Link Prediction by Learning Noise-Resilient Representation Consensus},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052575},
doi = {10.1145/3038912.3052575},
abstract = {Link Prediction has been an important task for social and information networks. Existing approaches usually assume the completeness of network structure. However, in many real-world networks, the links and node attributes can usually be partially observable. In this paper, we study the problem of Cross View Link Prediction (CVLP) on partially observable networks, where the focus is to recommend nodes with only links to nodes with only attributes (or vice versa). We aim to bridge the information gap by learning a robust consensus for link-based and attribute-based representations so that nodes become comparable in the latent space. Also, the link-based and attribute-based representations can lend strength to each other via this consensus learning. Moreover, attribute selection is performed jointly with the representation learning to alleviate the effect of noisy high-dimensional attributes. We present two instantiations of this framework with different loss functions and develop an alternating optimization framework to solve the problem. Experimental results on four real-world datasets show the proposed algorithm outperforms the baseline methods significantly for cross-view link prediction.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1611–1619},
numpages = {9},
keywords = {partially observable network, social network, link prediction, information network},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3502223.3502233,
author = {Chen, Mingyang and Zhang, Wen and Yuan, Zonggang and Jia, Yantao and Chen, Huajun},
title = {FedE: Embedding Knowledge Graphs in Federated Setting},
year = {2021},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502233},
doi = {10.1145/3502223.3502233},
abstract = {Knowledge graphs (KGs) become widespread and many organizations construct as well as maintain their own knowledge graphs. Same as the data isolation which has been a long-standing problem, knowledge graph isolation is common in real knowledge graph applications. Since the incompleteness of knowledge graphs obtained by different owners, they need to take advantage of other knowledge graphs to complete their own knowledge graphs, without exposing knowledge graphs explicitly since the consideration of data privacy, commercial interests and so on. Knowledge graph embedding (KGE) methods represent components of a knowledge graph as vectors in continuous vector spaces (i.e., embeddings) and proved to be effective in conducting knowledge graph completion. However, current knowledge graph embedding methods focus on the scenario with only a single knowledge graph. To solve this problem, we introduce the federated setting for knowledge graphs and apply it in knowledge graph embedding. We propose a Federated Knowledge Graph Embedding framework, FedE, focusing on learning knowledge graph embeddings by aggregating locally-computed updates. In this framework, there is a client for each knowledge graph and a server for coordinating embedding aggregation. Specifically, entity embeddings are locally learned in clients and the server is responsible for aggregating entity embeddings from clients. Furthermore, a model fusion procedure blends the capability of learned embeddings based only on one client without using the federated setting and embeddings based on all the clients in the federated setting. Finally, we conduct extensive experiments on datasets derived from KGE benchmark datasets, and results show the effectiveness of our proposed FedE.},
booktitle = {The 10th International Joint Conference on Knowledge Graphs},
pages = {80–88},
numpages = {9},
keywords = {Representation Learning, Knowledge Graph, Federated Learning, Knowledge Graph Embedding},
location = {Virtual Event, Thailand},
series = {IJCKG'21}
}

@inproceedings{10.1145/3341161.3344379,
author = {Bose, Avishek and Behzadan, Vahid and Aguirre, Carlos and Hsu, William H.},
title = {A Novel Approach for Detection and Ranking of Trendy and Emerging Cyber Threat Events in Twitter Streams},
year = {2019},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3344379},
doi = {10.1145/3341161.3344379},
abstract = {We present a new machine learning and text information extraction approach to detection of cyber threat events in Twitter that are novel (previously non-extant) and developing (marked by significance with respect to similarity with a previously detected event). While some existing approaches to event detection measure novelty and trendiness, typically as independent criteria and occasionally as a holistic measure, this work focuses on detecting both novel and developing events using an unsupervised machine learning approach. Furthermore, our proposed approach enables the ranking of cyber threat events based on an importance score by extracting the tweet terms that are characterized as named entities, keywords, or both. We also impute influence to users in order to assign a weighted score to noun phrases in proportion to user influence and the corresponding event scores for named entities and keywords. To evaluate the performance of our proposed approach, we measure the efficiency and detection error rate for events over a specified time interval, relative to human annotator ground truth.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {871–878},
numpages = {8},
keywords = {user influence, event detection, novelty detection, emerging topics, tweet analysis, named entity recognition, threat intelligence},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3123266.3123314,
author = {Song, Xuemeng and Feng, Fuli and Liu, Jinhuan and Li, Zekun and Nie, Liqiang and Ma, Jun},
title = {NeuroStylist: Neural Compatibility Modeling for Clothing Matching},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123314},
doi = {10.1145/3123266.3123314},
abstract = {Nowadays, as a beauty-enhancing product, clothing plays an important role in human's social life. In fact, the key to a proper outfit usually lies in the harmonious clothing matching. Nevertheless, not everyone is good at clothing matching. Fortunately, with the proliferation of fashion-oriented online communities, fashion experts can publicly share their fashion tips by showcasing their outfit compositions, where each fashion item (e.g., a top or bottom) usually has an image and context metadata (e.g., title and category). Such rich fashion data offer us a new opportunity to investigate the code in clothing matching. However, challenges co-exist with opportunities. The first challenge lies in the complicated factors, such as color, material and shape, that affect the compatibility of fashion items. Second, as each fashion item involves multiple modalities (i.e., image and text), how to cope with the heterogeneous multi-modal data also poses a great challenge. Third, our pilot study shows that the composition relation between fashion items is rather sparse, which makes traditional matrix factorization methods not applicable. Towards this end, in this work, we propose a content-based neural scheme to model the compatibility between fashion items based on the Bayesian personalized ranking (BPR) framework. The scheme is able to jointly model the coherent relation between modalities of items and their implicit matching preference. Experiments verify the effectiveness of our scheme, and we deliver deep insights that can benefit future research.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {753–761},
numpages = {9},
keywords = {multi-modal, fashion analysis, compatibility modeling},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3487351.3488341,
author = {Dubey, Manisha and Srijith, P. K. and Desarkar, Maunendra Sankar},
title = {Multi-View Hypergraph Convolution Network for Semantic Annotation in LBSNs},
year = {2021},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3488341},
doi = {10.1145/3487351.3488341},
abstract = {Semantic characterization of the Point-of-Interest (POI) plays an important role for modeling location-based social networks and various related applications like POI recommendation, link prediction etc. However, semantic categories are not available for many POIs which makes this characterization difficult. Semantic annotation aims to predict such missing categories of POIs. Existing approaches learn a representation of POIs using graph neural networks to predict semantic categories. However, LBSNs involve complex and higher order mobility dynamics. These higher order relations can be captured effectively by employing hypergraphs. Moreover, visits to POIs can be attributed to various reasons like temporal characteristics, spatial context etc. Hence, we propose a Multi-view Hypergraph Convolution Network (Multi-HGCN) where we learn POI representations by considering multiple hypergraphs across multiple views of the data. We build a comprehensive model to learn the POI representation capturing temporal, spatial and trajectory-based patterns among POIs by employing hypergraphs. We use hypergraph convolution to learn better POI representation by using spectral properties of hypergraph. Experiments conducted on three real-world datasets show that the proposed approach outperforms the state-of-the-art approaches.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {219–227},
numpages = {9},
keywords = {semantic annotation, hypergraphs, location-based social networks},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/3336191.3371827,
author = {Gu, Yulong and Ding, Zhuoye and Wang, Shuaiqiang and Yin, Dawei},
title = {Hierarchical User Profiling for E-Commerce Recommender Systems},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371827},
doi = {10.1145/3336191.3371827},
abstract = {Hierarchical user profiling that aims to model users' real-time interests in different granularity is an essential issue for personalized recommendations in E-commerce. On one hand, items (i.e. products) are usually organized hierarchically in categories, and correspondingly users' interests are naturally hierarchical on different granularity of items and categories. On the other hand, multiple granularity oriented recommendations become very popular in E-commerce sites, which require hierarchical user profiling in different granularity as well. In this paper, we propose HUP, a Hierarchical User Profiling framework to solve the hierarchical user profiling problem in E-commerce recommender systems. In HUP, we provide a Pyramid Recurrent Neural Networks, equipped with Behavior-LSTM to formulate users' hierarchical real-time interests at multiple scales. Furthermore, instead of simply utilizing users' item-level behaviors (e.g., ratings or clicks) in conventional methods, HUP harvests the sequential information of users' temporal finely-granular interactions (micro-behaviors, e.g., clicks on components of items like pictures or comments, browses with navigation of the search engines or recommendations) for modeling. Extensive experiments on two real-world E-commerce datasets demonstrate the significant performance gains of the HUP against state-of-the-art methods for the hierarchical user profiling and recommendation problems. We release the codes and datasets at https://github.com/guyulongcs/WSDM2020_HUP.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {223–231},
numpages = {9},
keywords = {hierarchical user profiling, user profiling, recommender systems, pyramid recurrent neural networks, e-commerce},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3350546.3352518,
author = {Comito, Carmela and Forestiero, Agostino and Pizzuti, Clara},
title = {Word Embedding Based Clustering to Detect Topics in Social Media},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352518},
doi = {10.1145/3350546.3352518},
abstract = {Social media are playing an increasingly important role in reporting major events happening in the world. However, detecting events and topics of interest from social media is a challenging task due to the huge magnitude of the data and the complex semantics of the language being processed. The paper proposes an online algorithm to discover topics that incrementally groups short text by incorporating the textual content with latent feature vector representations of words appearing in the text, trained on very large corpora to improve the check-in topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, the approach obtains significant improvements with respect to classical topic detection methods.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {192–199},
numpages = {8},
keywords = {Social Media, Word Embedding, Clustering, Topic Detection},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1145/3148055.3148078,
author = {Hatua, Amartya and Nguyen, Trung T. and Sung, Andrew H.},
title = {Information Diffusion on Twitter: Pattern Recognition and Prediction of Volume, Sentiment, and Influence},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148078},
doi = {10.1145/3148055.3148078},
abstract = {Characterizing, predicting, and quantifying the impact of postings, tweets, messages, etc. on social media platforms is a topic of growing interest due to the increasing reliance on using social media as a means for various purposes by individuals and organizations alike. In this paper, we describe an information diffusion model on the social network of Twitter. The model treats information diffusion on social media as a multivariate time series problem and deals mainly with three different dimensions of Twitter data and the different patterns of information diffusion. These dimensions are the volume of tweets, the sentiment of tweets and influence of tweets. To discover different patterns of information diffusion on Twitter, time series clustering is used where Dynamic Time Warping distance is adopted as the distance measure. To predict different parameters of each of the three dimensions, the linear time series model of Autoregressive Integrated Moving Average (ARIMA) and the non-linear time series model of Long Short-Term Memory (LSTM) Recurrent Neural Networks are used and their performance is compared. Results indicate that LSTM models achieve far better performance and hold great potential to be utilized for real-world applications.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {157–167},
numpages = {11},
keywords = {time series clustering, twitter, information diffusion, dtw, arima, rnn, lstm, time series forecasting},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3121050.3121057,
author = {Rafailidis, Dimitrios and Crestani, Fabio},
title = {Recommendation with Social Relationships via Deep Learning},
year = {2017},
isbn = {9781450344906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121050.3121057},
doi = {10.1145/3121050.3121057},
abstract = {While users trust the selections of their social friends in recommendation systems, the preferences of friends do not necessarily match. In this study, we introduce a deep learning approach to learn both about user preferences and the social influence of friends when generating recommendations. In our model we design a deep learning architecture by stacking multiple marginalized Denoising Autoencoders. We define a joint objective function to enforce the latent representation of social relationships in the Autoencoder's hidden layer to be as close as possible to the users' latent representation when factorizing the user-item matrix. We formulate a joint objective function as a minimization problem to learn both user preferences and friends' social influence and we present an optimization algorithm to solve the joint minimization problem. Our experiments on four benchmark datasets show that the proposed approach achieves high recommendation accuracy, compared to other state-of-the-art methods.},
booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {151–158},
numpages = {8},
keywords = {deep learning, social relationships, recommendation systems, matrix factorization, denoising autoencoders},
location = {Amsterdam, The Netherlands},
series = {ICTIR '17}
}

@inproceedings{10.1145/3041021.3054156,
author = {Wu, Runze and Xu, Guandong and Chen, Enhong and Liu, Qi and Ng, Wan},
title = {Knowledge or Gaming? Cognitive Modelling Based on Multiple-Attempt Response},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054156},
doi = {10.1145/3041021.3054156},
abstract = {Recent decades have witnessed the rapid growth of intelligent tutoring systems (ITS), in which personalized adaptive techniques are successfully employed to improve the learning of each individual student. However, the problem of using cognitive analysis to distill the knowledge and gaming factor from students learning history is still underexplored. To this end, we propose a Knowledge Plus Gaming Response Model (KPGRM) based on multiple-attempt responses. Specifically, we first measure the explicit gaming factor in each multiple-attempt response. Next, we utilise collaborative filtering methods to infer the implicit gaming factor of one-attempt responses. Then we model student learning cognitively by considering both gaming and knowledge factors simultaneously based on a signal detection model. Extensive experiments on two real-world datasets prove that KPGRM can model student learning more effectively as well as obtain a more reasonable analysis.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {321–329},
numpages = {9},
keywords = {cognitive analysis, educational data analytics, context-aware web-based learning, intelligent tutoring systems, gaming the system},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3543434.3543484,
author = {Ledwaba, Mashadi and Marivate, Vukosi},
title = {Semi-Supervised Learning Approaches for Predicting South African Political Sentiment for Local Government Elections},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543484},
doi = {10.1145/3543434.3543484},
abstract = {This study aims to understand the South African political context by analysing the sentiments shared on Twitter during the local government elections. An emphasis on the analysis was placed on understanding the discussions led around four predominant political parties – ANC, DA, EFF and ActionSA. A semi-supervised approach by means of a graph-based technique to label the vast accessible Twitter data for the classification of tweets into negative and positive sentiment was used. The tweets expressing negative sentiment were further analysed through latent topic extraction to uncover hidden topics of concern associated with each of the political parties. Our findings demonstrated that the general sentiment across South African Twitter users is negative towards all four predominant parties with the worst negative sentiment among users projected towards the current ruling party, ANC, relating to concerns centered around corruption, incompetence and loadshedding.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {129–137},
numpages = {9},
keywords = {semi-supervised learning, sentiment analysis, topic modelling, local government elections},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@inproceedings{10.1145/3472163.3472169,
author = {Comito, Carmela},
title = {COVID-19 Concerns in US: Topic Detection in Twitter},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472169},
doi = {10.1145/3472163.3472169},
abstract = {COVID-19 pandemic is affecting the lives of the citizens worldwide. Epidemiologists, policy makers and clinicians need to understand public concerns and sentiment to make informed decisions and adopt preventive and corrective measures to avoid critical situations. In the last few years, social media become a tool for spreading the news, discussing ideas and comments on world events. In this context, social media plays a key role since represents one of the main source to extract insight into public opinion and sentiment. In particular, Twitter has been already recognized as an important source of health-related information, given the amount of news, opinions and information that is shared by both citizens and official sources. However, it is a challenging issue identifying interesting and useful content from large and noisy text-streams. The study proposed in the paper aims to extract insight from Twitter by detecting the most discussed topics regarding COVID-19. The proposed approach combines peak detection and clustering techniques. Tweets features are first modeled as time series. After that, peaks are detected from the time series, and peaks of textual features are clustered based on the co-occurrence in the tweets. Results, performed over real-world datasets of tweets related to COVID-19 in US, show that the proposed approach is able to accurately detect several relevant topics of interest, spanning from health status and symptoms, to government policy, economic crisis, COVID-19-related updates, prevention, vaccines and treatments.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {103–110},
numpages = {8},
keywords = {Social Media Data, COVID-19, Topic Modeling},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3184558.3191659,
author = {Halder, Kishaloy and Poddar, Lahari and Kan, Min-Yen},
title = {Cold Start Thread Recommendation as Extreme Multi-Label Classification},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191659},
doi = {10.1145/3184558.3191659},
abstract = {In public online discussion forums, the large user base and frequent posts can create challenges for recommending threads to users. Importantly, traditional recommender systems, based on collaborative filtering, are not capable of handlingnever-seen-before items (threads). We can view this task as a form of Extreme Multi-label Classification (XMLC), where for a newly-posted thread, we predict the set of users (labels) who will want to respond to it. Selecting a subset of users from the set of all users in the community poses significant challenges due to scalability, and sparsity. We propose a neural network architecture to solve thisnew thread recommendation task. Our architecture uses stacked bi-directional Gated Recurrent Units (GRU) for text encoding along with cluster sensitive attention for exploiting correlations among the large label space. Experimental evaluation with four datasets from different domains show that our model outperforms both the state-of-the-art recommendation systems as well as other XMLC approaches for this task in terms of MRR, Recall, and NDCG.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1911–1918},
numpages = {8},
keywords = {discussion forum, extreme multi-label classification, neural network, recommendation system, cold start},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3534678.3539474,
author = {Tran, Nhu-Thuat and Lauw, Hady W.},
title = {Aligning Dual Disentangled User Representations from Ratings and Textual Content},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539474},
doi = {10.1145/3534678.3539474},
abstract = {Classical recommendation methods typically render user representation as a single vector in latent space. Oftentimes, a user's interactions with items are influenced by several hidden factors. To better uncover these hidden factors, we seek disentangled representations. Existing disentanglement methods for recommendations are mainly concerned with user-item interactions alone. To further improve not only the effectiveness of recommendations but also the interpretability of the representations, we propose to learn a second set of disentangled user representations from textual content and to align the two sets of representations with one another. The purpose of this coupling is two-fold. For one benefit, we leverage textual content to resolve sparsity of user-item interactions, leading to higher recommendation accuracy. For another benefit, by regularizing factors learned from user-item interactions with factors learned from textual content, we map uninterpretable dimensions from user representation into words. An attention-based alignment is introduced to align and enrich hidden factors representations. A series of experiments conducted on four real-world datasets show the efficacy of our methods in improving recommendation quality.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1798–1806},
numpages = {9},
keywords = {user preferences interpretation, textual content-aware recommender systems, disentangled representation},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3343031.3350909,
author = {Wang, Xin and Wu, Bo and Zhong, Yueqi},
title = {Outfit Compatibility Prediction and Diagnosis with Multi-Layered Comparison Network},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350909},
doi = {10.1145/3343031.3350909},
abstract = {Existing works about fashion outfit compatibility focus on predicting the overall compatibility of a set of fashion items with their information from different modalities. However, there are few works explore how to explain the prediction, which limits the persuasiveness and effectiveness of the model. In this work, we propose an approach to not only predict but also diagnose the outfit compatibility. We introduce an end-to-end framework for this goal, which features for: (1) The overall compatibility is learned from all type-specified pairwise similarities between items, and the backpropagation gradients are used to diagnose the incompatible factors. (2) We leverage the hierarchy of CNN and compare the features at different layers to take into account the compatibilities of different aspects from the low level (such as color, texture) to the high level (such as style). To support the proposed method, we build a new type-specified outfit dataset named Polyvore-T based on Polyvore dataset. We compare our method with the prior state-of-the-art in two tasks: outfit compatibility prediction and fill-in-the-blank. Experiments show that our approach has advantages in both prediction performance and diagnosis ability.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {329–337},
numpages = {9},
keywords = {fashion recommendation, deep learning, outfit compatibility},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3498851.3498940,
author = {Ueda, Kentaro and Sasaki, Kodai and Suwa, Hirohiko and Ogawa, Yuki and Umehara, Eiichi and Yamashita, Tatsuo and Tsubouchi, Kota and Yasumoto, Keiichi},
title = {Prediction of Nikkei VI Increase for Reducing Investment Risk Using Yahoo! JAPAN Stock BBS},
year = {2021},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498940},
doi = {10.1145/3498851.3498940},
abstract = {In stock investment, it is important to predict future market fluctuations in order to reduce risk. The Nikkei 225 Volatility Index (VI) is a measure of the expectations of the investors of the future of the Japanese market. A rise in this index indicates that investors are concerned about the future of the market, and predicting this rise may be used to reduce investment risk. Social media posts contain the opinions and feelings of the posters. In the present study, we proposed a means of predicting the increase in the Nikkei 225 VI by analyzing the social media of the largest stock trading website in Japan, ”Yahoo! Japan Stock Message Board,” and capturing changes in the topics of discussion. As a result of evaluation over a long validation period, we developed a prediction model with an F1-measure of 0.26.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {126–133},
numpages = {8},
keywords = {Natural Language Processing, Machine Learning, Social Media Analysis, Stock BBS, Volatility Index},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@article{10.1145/3492844,
author = {Gauthier, Robert P. and Wallace, James R.},
title = {The Computational Thematic Analysis Toolkit},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492844},
doi = {10.1145/3492844},
abstract = {As online communities have grown, Computational Social Science has rapidly developed new techniques to study them. However, these techniques require researchers to become experts in a wide variety of tools in addition to qualitative and computational research methods. Studying online communities also requires researchers to constantly navigate highly contextual ethical and transparency considerations when engaging with data, such as respecting their members' privacy when discussing sensitive or stigmatized topics. To overcome these challenges, we developed the Computational Thematic Analysis Toolkit, a modular software package that supports analysis of online communities by combining aspects of reflexive thematic analysis with computational techniques. Our toolkit demonstrates how common analysis tasks like data collection, cleaning and filtering, modelling and sampling, and coding can be implemented within a single visual interface, and how that interface can encourage researchers to manage ethical and transparency considerations throughout their research process.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {25},
numpages = {15},
keywords = {thematic analysis, computational methods, design, methodology}
}

@inproceedings{10.1145/3394171.3413761,
author = {Zhan, Li-Ming and Liu, Bo and Fan, Lu and Chen, Jiaxin and Wu, Xiao-Ming},
title = {Medical Visual Question Answering via Conditional Reasoning},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413761},
doi = {10.1145/3394171.3413761},
abstract = {Medical visual question answering (Med-VQA) aims to accurately answer a clinical question presented with a medical image. Despite its enormous potential in healthcare industry and services, the technology is still in its infancy and is far from practical use. Med-VQA tasks are highly challenging due to the massive diversity of clinical questions and the disparity of required visual reasoning skills for different types of questions. In this paper, we propose a novel conditional reasoning framework for Med-VQA, aiming to automatically learn effective reasoning skills for various Med-VQA tasks. Particularly, we develop a question-conditioned reasoning module to guide the importance selection over multimodal fusion features. Considering the different nature of closed-ended and open-ended Med-VQA tasks, we further propose a type-conditioned reasoning module to learn a different set of reasoning skills for the two types of tasks separately. Our conditional reasoning framework can be easily applied to existing Med-VQA systems to bring performance gains. In the experiments, we build our system on top of a recent state-of-the-art Med-VQA model and evaluate it on the VQA-RAD benchmark [23]. Remarkably, our system achieves significantly increased accuracy in predicting answers to both closed-ended and open-ended questions, especially for open-ended questions, where a 10.8% increase in absolute accuracy is obtained. The source code can be downloaded from https://github.com/awenbocc/med-vqa.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2345–2354},
numpages = {10},
keywords = {medical visual question answering, conditional reasoning, attention mechanism},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3365109.3368792,
author = {Cherrington, Marianne and Airehrour, David and Lu, Joan and Xu, Qiang and Wade, Stephen and Madanian, Samaneh},
title = {Feature Selection Methods for Linked Data: Limitations, Capabilities and Potentials},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368792},
doi = {10.1145/3365109.3368792},
abstract = {Feature selection is an important pre-processing, data mining, and knowledge discovery tool for data analysis. By eliminating redundant and irrelevant features from high-dimensional data, feature selection diminishes the 'curse of dimensionality' to improve performance. Data are becoming increasingly complex; heterogeneous data may often be viewed as natural collections of linked objects. Linked data are structured data that are connected with other data sources through the use of semantic queries. It is increasingly prevalent in social media websites and biological networks. Many feature selection methods assume independent and identically distributed data (IID), a condition violated with linked data. In this paper, a review of current feature selection techniques for linked data is presented. Several approaches are examined in various contexts so that performance issues and ongoing challenges can be assessed. The major contribution of this paper is to underscore contemporary uses and limitations of linked data feature selection techniques with the purpose of informing existing capabilities and current potentials for key areas of future research and application.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {103–112},
numpages = {10},
keywords = {feature selection (fs), high-dimensional data (hdd), linked data (ld), dimensionality reduction, heterogeneous data},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@article{10.1613/jair.1.11640,
author = {Ruder, Sebastian and Vuli\'{c}, Ivan and S\o{}gaard, Anders},
title = {A Survey of Cross-Lingual Word Embedding Models},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11640},
doi = {10.1613/jair.1.11640},
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {569–630},
numpages = {62}
}

@inproceedings{10.1145/3485447.3512120,
author = {Cho, Yoon-Sik and Oh, Min-hwan},
title = {Stochastic-Expert Variational Autoencoder for Collaborative Filtering},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512120},
doi = {10.1145/3485447.3512120},
abstract = {Motivated by the recent successes of deep generative models used for collaborative filtering, we propose a novel framework of VAE for collaborative filtering using multiple experts and stochastic expert selection, which allows the model to learn a richer and more complex latent representation of user preferences. In our method, individual experts are sampled stochastically at each user-item interaction which can effectively utilize the variability among multiple experts. While we propose this framework in the context of collaborative filtering, the proposed stochastic expert technique can be used to enhance VAEs in general beyond the application of collaborative filtering. Hence, this novel technique can be of independent interest. We comprehensively evaluate our proposed method, Stochastic-Expert Variational Autoencoder (SE-VAE) on numerical experiments on the real-world benchmark datasets from MovieLens and Netflix and show that it consistently outperforms the existing state-of-the-art methods across all metrics. Our proposed stochastic expert framework is generic and adaptable to any VAE architecture. The experimental results show that the adaptations to various architectures provided performance gains over the existing methods.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2482–2490},
numpages = {9},
keywords = {Recommender Systems, Variational Autoencoder, Deep Generative Models, Neural Networks, Collaborative Filtering, Variational Inference},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1109/ESEM.2017.20,
author = {Kabeer, Shaikh Jeeshan and Nayebi, Maleknaz and Ruhe, Guenther and Carlson, Chris and Chew, Francis},
title = {Predicting the Vector Impact of Change: An Industrial Case Study at Brightsquid},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.20},
doi = {10.1109/ESEM.2017.20},
abstract = {Background: Understanding and controlling the impact of change decides about the success or failure of evolving products. The problem magnifies for start-ups operating with limited resources. Their usual focus is on Minimum Viable Product (MVP's) providing specialized functionality, thus have little expense available for handling changes. Aims: Change Impact Analysis (CIA) refers to the identification of source code files impacted when implementing a change request. We extend this question to predict not only affected files, but also the effort needed for implementing the change, and the duration necessary for that. Method: This study evaluates the performance of three textual similarity techniques for CIA based on Bag of words in combination with either topic modeling or file coupling. Results: The approaches are applied on data from two industrial projects. The data comes as part of an industrial collaboration project with Brightsquid, a Canadian start-up company specializing in secure communication solutions. Performance analysis shows that combining textual similarity with file coupling improves impact prediction, resulting in Recall of 67%. Effort and duration can be predicted with 84% and 72% accuracy using textual similarity only. Conclusions: The relative effort invested into CIA for predicting impacted files can be reduced by extending its applicability to multiple dimensions which include impacted files, effort, and duration.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {131–140},
numpages = {10},
keywords = {case study, effort estimation, bag of words, change impact analysis, topic modeling, software repository},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3126858.3126892,
author = {Amancio, Leandro and Dorneles, Carina Friedrich},
title = {Towards Recency Ranking in Community Question Answering: A Case Study of Stack Overflow},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126892},
doi = {10.1145/3126858.3126892},
abstract = {In Community Question Answering, recency ranking refers to put the freshness answers with high quality in top positions of a ranking. Freshness is not related to how recent is the answer creation date, but to how up-to-date is the answer content. This is extremely important because the users need to get best answers quickly to solve their questions and, usually, they expect up-to-date solutions. In this paper, we propose a new approach to provide recency ranking in these environments and present a set of experiments that show the effectiveness of our proposal.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {173–180},
numpages = {8},
keywords = {recency ranking, community question answering},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1145/3488560.3498499,
author = {Lin, Lu and Blaser, Ethan and Wang, Hongning},
title = {Graph Embedding with Hierarchical Attentive Membership},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498499},
doi = {10.1145/3488560.3498499},
abstract = {This paper studies a remarkable property of graphs which is the latent hierarchical grouping of nodes, where each node manifests its membership to a specific group based on the context composed by its neighboring nodes. When modeling the neighborhood structure for graph representation learning, most prior works ignore such latent groups and nodes' membership to different groups, not to mention the hierarchy. Thus, they fall short of delivering a comprehensive understanding of the nodes under different contexts in a graph. In this paper, we propose a novel hierarchical attentive membership model for graph embedding, where the latent memberships for each node are dynamically discovered based on its neighboring context. Both group-level and individual-level attentions are performed when aggregating neighboring states to generate node embeddings. We introduce structural constraints to explicitly regularize the inferred memberships of each node, such that a well-defined hierarchical grouping structure is captured. The proposed model outperformed a set of state-of-the-art graph embedding solutions on node classification and link prediction tasks in a variety of graphs including citation networks and social networks. Qualitative evaluations visualize the learned node embeddings along with the inferred memberships, which proved the concept of membership hierarchy and enables explainable embedding learning in graphs.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {582–590},
numpages = {9},
keywords = {graph neural network, representation learning, graph embedding, mixed membership block models},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3341981.3344229,
author = {Mishra, Rahul and Setty, Vinay},
title = {SADHAN: Hierarchical Attention Networks to Learn Latent Aspect Embeddings for Fake News Detection},
year = {2019},
isbn = {9781450368810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341981.3344229},
doi = {10.1145/3341981.3344229},
abstract = {Recently false claims and misinformation have become rampant in the web, affecting election outcomes, societies and economies. Consequently, fact checking websites such as snopes.com and politifact.com are becoming popular. However, these websites require expert analysis which is slow and not scalable. Many recent works try to solve these challenges using machine learning models trained on a variety of features and a rich lexicon or more recently, deep neural networks to avoid feature engineering. In this paper, we propose hierarchical deep attention networks to learn embeddings for various latent aspects of news. Contrary to existing solutions which only apply word-level self-attention, our model jointly learns the latent aspect embeddings for classifying false claims by applying hierarchical attention. Using several manually annotated high quality datasets such as Politifact, Snopes and Fever we show that these learned aspect embeddings are strong predictors of false claims. We show that latent aspect embeddings learned from attention mechanisms improve the accuracy of false claim detection by up to 13.5% in terms of Macro F1 compared to a state-of-the-art attention mechanism guided by claim-text DeClarE. We also extract and visualize the evidence from the external articles which supports or disproves the claims},
booktitle = {Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {197–204},
numpages = {8},
keywords = {fake news detection, latent aspect embeddings, hierarchical attention},
location = {Santa Clara, CA, USA},
series = {ICTIR '19}
}

@inproceedings{10.1145/3159652.3159688,
author = {Zhang, Yan and Yin, Hongzhi and Huang, Zi and Du, Xingzhong and Yang, Guowu and Lian, Defu},
title = {Discrete Deep Learning for Fast Content-Aware Recommendation},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159688},
doi = {10.1145/3159652.3159688},
abstract = {Cold-start problem and recommendation efficiency have been regarded as two crucial challenges in the recommender system. In this paper, we propose a hashing based deep learning framework called Discrete Deep Learning (DDL), to map users and items to Hamming space, where a user»s preference for an item can be efficiently calculated by Hamming distance, and this computation scheme significantly improves the efficiency of online recommendation. Besides, DDL unifies the user-item interaction information and the item content information to overcome the issues of data sparsity and cold-start. To be more specific, to integrate content information into our DDL framework, a deep learning model, Deep Belief Network (DBN), is applied to extract effective item representation from the item content information. Besides, the framework imposes balance and irrelevant constraints on binary codes to derive compact but informative binary codes. Due to the discrete constraints in DDL, we propose an efficient alternating optimization method consisting of iteratively solving a series of mixed-integer programming subproblems. Extensive experiments have been conducted to evaluate the performance of our DDL framework on two different Amazon datasets, and the experimental results demonstrate the superiority of DDL over the state-of-the-art methods regarding online recommendation efficiency and cold-start recommendation accuracy.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {717–726},
numpages = {10},
keywords = {cold-start, hash code, deep learning, recommender system},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{10.1145/3123266.3123454,
author = {Jin, Zhiwei and Cao, Juan and Guo, Han and Zhang, Yongdong and Luo, Jiebo},
title = {Multimodal Fusion with Recurrent Neural Networks for Rumor Detection on Microblogs},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123454},
doi = {10.1145/3123266.3123454},
abstract = {Microblogs have become popular media for news propagation in recent years. Meanwhile, numerous rumors and fake news also bloom and spread wildly on the open social media platforms. Without verification, they could seriously jeopardize the credibility of microblogs. We observe that an increasing number of users are using images and videos to post news in addition to texts. Tweets or microblogs are commonly composed of text, image and social context. In this paper, we propose a novel Recurrent Neural Network with an attention mechanism (att-RNN) to fuse multimodal features for effective rumor detection. In this end-to-end network, image features are incorporated into the joint features of text and social context, which are obtained with an LSTM (Long-Short Term Memory) network, to produce a reliable fused classification. The neural attention from the outputs of the LSTM is utilized when fusing with the visual features. Extensive experiments are conducted on two multimedia rumor datasets collected from Weibo and Twitter. The results demonstrate the effectiveness of the proposed end-to-end att-RNN in detecting rumors with multimodal contents.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {795–816},
numpages = {22},
keywords = {lstm, multimodal fusion, rumor detection, attention mechanism, microblog},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3240508.3240665,
author = {Semedo, David and Magalhaes, Joao},
title = {Temporal Cross-Media Retrieval with Soft-Smoothing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240665},
doi = {10.1145/3240508.3240665},
abstract = {Multimedia information have strong temporal correlations that shape the way modalities co-occur over time. In this paper we study the dynamic nature of multimedia and social-media information, where the temporal dimension emerges as a strong source of evidence for learning the temporal correlations across visual and textual modalities. So far, cross-media retrieval models, explored the correlations between different modalities (e.g. text and image) to learn a common subspace, in which semantically similar instances lie in the same neighbourhood. Building on such knowledge, we propose a novel temporal cross-media neural architecture, that departs from standard cross-media methods, by explicitly accounting for the temporal dimension through temporal subspace learning. The model is softly-constrained with temporal and inter-modality constraints that guide the new subspace learning task by favouring temporal correlations between semantically similar and temporally close instances. Experiments on three distinct datasets show that accounting for time turns out to be important for cross-media retrieval. Namely, the proposed method outperforms a set of baselines on the task of temporal cross-media retrieval, demonstrating its effectiveness for performing temporal subspace learning.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1038–1046},
numpages = {9},
keywords = {temporal smoothing, multimedia retrieval, cross-media, temporal cross-media},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1145/3393880,
author = {Guo, Bin and Ding, Yasan and Yao, Lina and Liang, Yunji and Yu, Zhiwen},
title = {The Future of False Information Detection on Social Media: New Perspectives and Trends},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3393880},
doi = {10.1145/3393880},
abstract = {The massive spread of false information on social media has become a global risk, implicitly influencing public opinion and threatening social/political development. False information detection (FID) has thus become a surging research topic in recent years. As a promising and rapidly developing research field, we find that much effort has been paid to new research problems and approaches of FID. Therefore, it is necessary to give a comprehensive review of the new research trends of FID. We first give a brief review of the literature history of FID, based on which we present several new research challenges and techniques of it, including early detection, detection by multimodal data fusion, and explanatory detection. We further investigate the extraction and usage of various crowd intelligence in FID, which paves a promising way to tackle FID challenges. Finally, we give our views on the open issues and future research directions of FID, such as model adaptivity/generality to new events, embracing of novel machine learning models, aggregation of crowd wisdom, adversarial attack and defense in detection models, and so on.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {68},
numpages = {36},
keywords = {fake news, explanatory detection, False information detection, social media, crowd intelligence}
}

@inproceedings{10.1145/3545258.3545288,
author = {Huang, Zijie and Shao, Zhiqing and Fan, Guisheng and Yu, Huiqun and Yang, Kang and Zhou, Ziyi},
title = {Bug Report Priority Prediction Using Developer-Oriented Socio-Technical Features},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545288},
doi = {10.1145/3545258.3545288},
abstract = {Software stakeholders report bugs in Issue Tracking System (ITS) with manually labeled priorities. However, the lack of knowledge and standard for prioritization may cause stakeholders to mislabel the priorities. In response, priority predictors are actively developed to support them. Prior studies trained machine learners based on textual similarity, categorical, and numeric technical features of bug reports. Most models were validated by time-insensitive approaches, and they were producing sub-optimal results for practical usage. Moreover, they tend to ignore the developer and social aspects of ITS. Since ITS bridges users and developers, we integrate their sentiment- and community-oriented socio-technical features to perform 2- and multi-classed bug priority prediction and validate our model in within-project, cross-project, and time-wise scenarios. The proposed model outperforms the 2 baselines by up to 10% in AUC-ROC and 13% in MCC, and the significance of improvement is statistically confirmed. We reveal involving assignee and reporter features from socio-technical perspectives such as sentiment could boost prediction performance. Finally, we test statistically the mean and distribution of the features that reflect the differences in socio-technical aspects (e.g., quality of communication and resource distribution) between high and low priority reports. In conclusion, we suggest researchers should involve contributors’ experience and sentiments in bug report priority prediction.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {202–211},
numpages = {10},
keywords = {empirical software engineering, socio-technical analysis, developer sentiment, issue tracking system, bug report priority},
location = {Hohhot, China},
series = {Internetware '22}
}

@inproceedings{10.1145/3383219.3383224,
author = {Zahedi, Mansooreh and Rajapakse, Roshan Namal and Babar, Muhammad Ali},
title = {Mining Questions Asked about Continuous Software Engineering: A Case Study of Stack Overflow},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383224},
doi = {10.1145/3383219.3383224},
abstract = {Context: With the growing popularity of rapid software delivery and deployment, the methods, practices and technologies of Continuous Software Engineering (CSE) are evolving steadily. This creates the need for understanding the recent trends of the technologies, practitioners' challenges and views in this domain. Objective: In this paper, we present an empirical study aimed at exploring CSE from the practitioners' perspective by mining discussions from Q&amp;A websites. Method: We have analyzed 12,989 questions and answers posted on Stack Overflow. Topic modelling is conducted to derive the dominant topics in this domain. Further, a qualitative analysis was conducted to identify the key challenges discussed. Findings: Whilst the trend of posted questions is sharply increasing, the questions are becoming more specific to technologies and more difficult to attract answers. We identified 32 topics of discussions, among which "Error messages in Continuous Integration/Deployment" and "Continuous Integration concepts" are the most dominant. We also present the most challenging areas in this domain from the practitioners' perspectives.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {41–50},
numpages = {10},
keywords = {topic modelling, continuous integration, stack overflow, mining software repositories, qualitative analysis, continuous software engineering, continuous deployment},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3358502.3361270,
author = {Mattis, Toni and Rein, Patrick and Hirschfeld, Robert},
title = {Ambiguous, Informal, and Unsound: Metaprogramming for Naturalness},
year = {2019},
isbn = {9781450369855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358502.3361270},
doi = {10.1145/3358502.3361270},
abstract = {Program code needs to be understood by both machines and programmers. While the goal of executing programs requires the unambiguity of a formal language, programmers use natural language within these formal constraints to explain implemented concepts to each other. This so called naturalness – the property of programs to resemble human communication – motivated many statistical and machine learning (ML) approaches with the goal to improve software engineering activities. The metaprogramming facilities of most programming environments model the formal elements of a program (meta-objects). If ML is used to support engineering or analysis tasks, complex infrastructure needs to bridge the gap between meta-objects and ML models, changes are not reflected in the ML model, and the mapping from an ML output back into the program’s meta-object domain is laborious. In the scope of this work, we propose to extend metaprogramming facilities to give tool developers access to the representations of program elements within an exchangeable ML model. We demonstrate the usefulness of this abstraction in two case studies on test prioritization and refactoring. We conclude that aligning ML representations with the program’s formal structure lowers the entry barrier to exploit statistical properties in tool development.},
booktitle = {Proceedings of the 4th ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection},
pages = {1–10},
numpages = {10},
keywords = {naturalness, metaprogramming, meta-objects, machine learning},
location = {Athens, Greece},
series = {META 2019}
}

@inproceedings{10.1145/3447548.3467302,
author = {Chu, Zhixuan and Rathbun, Stephen L. and Li, Sheng},
title = {Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467302},
doi = {10.1145/3447548.3467302},
abstract = {Treatment effect estimation from observational data is a critical research topic across many domains. The foremost challenge in treatment effect estimation is how to capture hidden confounders. Recently, the growing availability of networked observational data offers a new opportunity to deal with the issue of hidden confounders. Unlike networked data in traditional graph learning tasks, such as node classification and link detection, the networked data under the causal inference problem has its particularity, i.e., imbalanced network structure. In this paper, we propose a Graph Infomax Adversarial Learning (GIAL) model for treatment effect estimation, which makes full use of the network structure to capture more information by recognizing the imbalance in network structure. We evaluate the performance of our GIAL model on two benchmark datasets, and the results demonstrate superiority over the state-of-the-art methods.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {176–184},
numpages = {9},
keywords = {graph mining, social network analysis, causal inference},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3442381.3450132,
author = {Zhang, Yanzhao and Zhang, Richong and Kim, Jaein and Liu, Xudong and Mao, Yongyi},
title = {Unsupervised Semantic Association Learning with Latent Label Inference},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450132},
doi = {10.1145/3442381.3450132},
abstract = {In this paper, we unify a diverse set of learning tasks in NLP, semantic retrieval and related areas, under a common umbrella, which we call unsupervised semantic association learning (USAL). Examples of this generic task include word sense disambiguation, answer selection and question retrieval. We then present a novel modeling framework to tackle such tasks. The framework introduces, under the deep learning paradigm, a latent label indexing the true target in the candidate target set. An EM algorithm is then developed for learning the deep model and inferring the latent variables, principled under variational techniques and noise contrastive estimation. We apply the model and algorithm to several semantic retrieval benchmark tasks and the superior performance of the proposed approach is demonstrated via empirical studies.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {4010–4019},
numpages = {10},
keywords = {Question Retrieval, Word Sense Disambiguation, Answer Selection, Semantic retrieval},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3343031.3350905,
author = {Dong, Xue and Song, Xuemeng and Feng, Fuli and Jing, Peiguang and Xu, Xin-Shun and Nie, Liqiang},
title = {Personalized Capsule Wardrobe Creation with Garment and User Modeling},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350905},
doi = {10.1145/3343031.3350905},
abstract = {Recent years have witnessed a growing trend of building the capsule wardrobe by minimizing and diversifying the garments in their messy wardrobes. Thanks to the recent advances in multimedia techniques, many researches have promoted the automatic creation of capsule wardrobes by the garment modeling. Nevertheless, most capsule wardrobes generated by existing methods fail to consider the user profile, including the user preferences, body shapes and consumption habits, which indeed largely affects the wardrobe creation. To this end, we introduce a combinatorial optimization-based personalized capsule wardrobe creation framework, named PCW-DC, which jointly integrates both garment modeling (textiti.e., wardrobe compatibility) and user modeling (textiti.e., preferences, body shapes). To justify our model, we construct a dataset, named bodyFashion, which consists of $116,532$ user-item purchase records on Amazon involving 11,784 users and 75,695 fashion items. Extensive experiments on bodyFashion have demonstrated the effectiveness of our proposed model. As a byproduct, we have released the codes and the data to facilitate the research community.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {302–310},
numpages = {9},
keywords = {user modeling, fashion analysis, compatibility learning},
location = {Nice, France},
series = {MM '19}
}

@article{10.1145/3290768.3290775,
author = {Zhao, Pengfei and Ma, Jian and Hua, Zhongsheng and Fang, Shijian},
title = {Academic Social Network-Based Recommendation Approach for Knowledge Sharing},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/3290768.3290775},
doi = {10.1145/3290768.3290775},
abstract = {Academic information overload has brought researchers great difficulty due to the rapid growth of scientific articles. Methods have been proposed to help professional readers find relevant articles on the basis of their publications. Although effectively sharing publications is essential to spreading knowledge and ideas, few studies have focused on knowledge sharing from an author perspective. This study leverages the online academic social network to propose a recommendation approach for knowledge sharing. In our approach, we integrate researcher-level and document-level analyses in the same model. Our model works in two stages: 1) researcher-level analysis and 2) document-level analysis. The former combines research topic relevance, social relations, and research quality dimension, and the latter uses the machine learning method to learn the vector representation for each word. Online social behavior information is also leveraged to enhance readers' short-term interests. Our approach is deployed in ScholarMate, a prevalent academic social network. Compared with other baseline methods (CB, LDA, and part of the proposed approach), our approach significantly improves the accuracy of recommendations. Moreover, our method can disseminate papers efficiently to readers who have no publications.},
journal = {SIGMIS Database},
month = {nov},
pages = {78–91},
numpages = {14},
keywords = {recommender systems, academic social network, knowledge sharing}
}

@inproceedings{10.1145/3240323.3240346,
author = {Tondulkar, Rohan and Dubey, Manisha and Desarkar, Maunendra Sankar},
title = {Get Me the Best: Predicting Best Answerers in Community Question Answering Sites},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240346},
doi = {10.1145/3240323.3240346},
abstract = {There has been a massive rise in the use of Community Question and Answering (CQA) forums to get solutions to various technical and non-technical queries. One common problem faced in CQA is the small number of experts, which leaves many questions unanswered. This paper addresses the challenging problem of predicting the best answerer for a new question and thereby recommending the best expert for the same. Although there are work in the literature that aim to find possible answerers for questions posted in CQA, very few algorithms exist for finding the best answerer whose answer will satisfy the information need of the original Poster. For finding answerers, existing approaches mostly use features based on content and tags associated with the questions. There are few approaches that additionally consider the users' history. In this paper, we propose an approach that considers a comprehensive set of features including but not limited to text representation, tag based similarity as well as multiple user-based features that target users' availability, agility as well as expertise for predicting the best answerer for a given question. We also include features that give incentives to users who answer less but more important questions over those who answer a lot of questions of less importance. A learning to rank algorithm is used to find the weight of each feature. Experiments conducted on a real dataset from Stack Exchange show the efficacy of the proposed method in terms of multiple evaluation metrics for accuracy, robustness and real time performance.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {251–259},
numpages = {9},
keywords = {learning to rank, expert recommendation, community question answering},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.1145/3038912.3052710,
author = {Wang, Yashen and Huang, Heyan and Feng, Chong},
title = {Query Expansion Based on a Feedback Concept Model for Microblog Retrieval},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052710},
doi = {10.1145/3038912.3052710},
abstract = {We tackle the problem of improving microblog retrieval algorithms by proposing a Feedback Concept Model for query expansion. In particular, we expand the query using knowledge information derived from Probase so that the expanded one could better reflect users' search intent, which allows for microblog retrieval at a concept-level, rather than term-level. In the proposed feedback concept model: (i) we mine the concept information implicit in short-texts based on the external knowledge bases; (ii) with the relevant concepts associated with short-texts, a mixture model is generated to estimate a concept language model; (iii) finally, we utilize the concept language model for query expansion. Moreover, we incorporate temporal prior into the proposed query expansion method to satisfy real-time information need. Finally, we test the generalization power of the feedback concept model on the TREC Microblog corpora. The experimental results demonstrate that the proposed model outperforms the previous methods for microblog retrieval significantly.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {559–568},
numpages = {10},
keywords = {microblog retrieval, short-text conceptualization, pseudo-relevance feedback, query expansion},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3442442.3452347,
author = {Johnson, Isaac and Gerlach, Martin and S\'{a}ez-Trumper, Diego},
title = {Language-Agnostic Topic Classification for Wikipedia},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452347},
doi = {10.1145/3442442.3452347},
abstract = {A major challenge for many analyses of Wikipedia dynamics—e.g., imbalances in content quality, geographic differences in what content is popular, what types of articles attract more editor discussion—is grouping the very diverse range of Wikipedia articles into coherent, consistent topics. This problem has been addressed using various approaches based on Wikipedia’s category network, WikiProjects, and external taxonomies. However, these approaches have always been limited in their coverage: typically, only a small subset of articles can be classified, or the method cannot be applied across (the more than 300) languages on Wikipedia. In this paper, we propose a language-agnostic approach based on the links in an article for classifying articles into a taxonomy of topics that can be easily applied to (almost) any language and article on Wikipedia. We show that it matches the performance of a language-dependent approach while being simpler and having much greater coverage.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {594–601},
numpages = {8},
keywords = {topic classification, Wikipedia, language-agnostic},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3240323.3240346,
author = {Tondulkar, Rohan and Dubey, Manisha and Desarkar, Maunendra Sankar},
title = {Get Me the Best: Predicting Best Answerers in Community Question Answering Sites},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240346},
doi = {10.1145/3240323.3240346},
abstract = {There has been a massive rise in the use of Community Question and Answering (CQA) forums to get solutions to various technical and non-technical queries. One common problem faced in CQA is the small number of experts, which leaves many questions unanswered. This paper addresses the challenging problem of predicting the best answerer for a new question and thereby recommending the best expert for the same. Although there are work in the literature that aim to find possible answerers for questions posted in CQA, very few algorithms exist for finding the best answerer whose answer will satisfy the information need of the original Poster. For finding answerers, existing approaches mostly use features based on content and tags associated with the questions. There are few approaches that additionally consider the users' history. In this paper, we propose an approach that considers a comprehensive set of features including but not limited to text representation, tag based similarity as well as multiple user-based features that target users' availability, agility as well as expertise for predicting the best answerer for a given question. We also include features that give incentives to users who answer less but more important questions over those who answer a lot of questions of less importance. A learning to rank algorithm is used to find the weight of each feature. Experiments conducted on a real dataset from Stack Exchange show the efficacy of the proposed method in terms of multiple evaluation metrics for accuracy, robustness and real time performance.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {251–259},
numpages = {9},
keywords = {learning to rank, expert recommendation, community question answering},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.1145/3038912.3052710,
author = {Wang, Yashen and Huang, Heyan and Feng, Chong},
title = {Query Expansion Based on a Feedback Concept Model for Microblog Retrieval},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052710},
doi = {10.1145/3038912.3052710},
abstract = {We tackle the problem of improving microblog retrieval algorithms by proposing a Feedback Concept Model for query expansion. In particular, we expand the query using knowledge information derived from Probase so that the expanded one could better reflect users' search intent, which allows for microblog retrieval at a concept-level, rather than term-level. In the proposed feedback concept model: (i) we mine the concept information implicit in short-texts based on the external knowledge bases; (ii) with the relevant concepts associated with short-texts, a mixture model is generated to estimate a concept language model; (iii) finally, we utilize the concept language model for query expansion. Moreover, we incorporate temporal prior into the proposed query expansion method to satisfy real-time information need. Finally, we test the generalization power of the feedback concept model on the TREC Microblog corpora. The experimental results demonstrate that the proposed model outperforms the previous methods for microblog retrieval significantly.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {559–568},
numpages = {10},
keywords = {microblog retrieval, short-text conceptualization, pseudo-relevance feedback, query expansion},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3474085.3475568,
author = {Shen, Lei and Zhan, Haolan and Shen, Xin and Song, Yonghao and Zhao, Xiaofang},
title = {Text is NOT Enough: Integrating Visual Impressions into Open-Domain Dialogue Generation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475568},
doi = {10.1145/3474085.3475568},
abstract = {Open-domain dialogue generation in natural language processing (NLP) is by default a pure-language task, which aims to satisfy human need for daily communication on open-ended topics by producing related and informative responses. In this paper, we point out that hidden images, named as visual impressions (VIs), can be explored from the text-only data to enhance dialogue understanding and help generate better responses. Besides, the semantic dependency between an dialogue post and its response is complicated, e.g., few word alignments and some topic transitions. Therefore, the visual impressions of them are not shared, and it is more reasonable to integrate the response visual impressions (RVIs) into the decoder, rather than the post visual impressions (PVIs). However, both the response and its RVIs are not given directly in the test process. To handle the above issues, we propose a framework to explicitly construct VIs based on pure-language dialogue datasets and utilize them for better dialogue understanding and generation. Specifically, we obtain a group of images (PVIs) for each post based on a pre-trained word-image mapping model. These PVIs are used in a co-attention encoder to get a post representation with both visual and textual information. Since the RVIs are not provided during testing, we design a cascade decoder that consists of two sub-decoders. The first sub-decoder predicts the content words in response, and applies the word-image mapping model to get corresponding RVIs. Then, the second sub-decoder generates the response based on the post and RVIs. Experimental results on two open-domain dialogue datasets show that our proposed approach achieves superior performance over competitive baselines in terms of fluency, relatedness, and diversity.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4287–4296},
numpages = {10},
keywords = {open-domain dialogue, visual impressions, dialogue generation},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3331184.3331228,
author = {Liang, Di and Zhang, Fubao and Zhang, Weidong and Zhang, Qi and Fu, Jinlan and Peng, Minlong and Gui, Tao and Huang, Xuanjing},
title = {Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331228},
doi = {10.1145/3331184.3331228},
abstract = {Community-based question answering (CQA), which provides a platform for people with diverse backgrounds to share information and knowledge, has become increasingly popular. With the accumulation of site data, methods to detect duplicate questions in CQA sites have attracted considerable attention. Existing methods typically use only questions to complete the task. However, the paired answers may also provide valuable information. In this paper, we propose an answer information- enhanced adaptive multi-attention network (AMAN) to perform this task. AMAN takes full advantage of the semantic information in the paired answers while alleviating the noise problem caused by adding the answers. To evaluate the proposed method, we use a CQADupStack set and the Quora question-pair dataset expanded with paired answers. Experimental results demonstrate that the proposed model can achieve state-of-the-art performance on the above two data sets.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {95–104},
numpages = {10},
keywords = {community-based question answering, duplicate question detection, adaptive multi-attention},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1109/TASLP.2018.2872106,
author = {Kamper, Herman and Shakhnarovich, Gregory and Livescu, Karen},
title = {Semantic Speech Retrieval With a Visually Grounded Model of Untranscribed Speech},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2872106},
doi = {10.1109/TASLP.2018.2872106},
abstract = {There is a growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here, we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to semantic keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {89–98},
numpages = {10}
}

@inproceedings{10.1145/3131151.3131157,
author = {Villanes, Isabel K. and Ascate, Silvia M. and Gomes, Josias and Dias-Neto, Arilo Claudio},
title = {What Are Software Engineers Asking about Android Testing on Stack Overflow?},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131157},
doi = {10.1145/3131151.3131157},
abstract = {Software testing represents an important activity to achieve quality during mobile application development. The constant evolution of mobile applications in previous years relating to size and complexity entails the need to improve testing techniques and tools. In this context, developers/testers often resort to specialized communities or Question &amp; Answer repositories to clarity doubts regarding testing in a practical and efficient way. Thus, these repositories become a popular source of data to understand the current context of software testing practices. In this paper, we present a study using the Stack Overflow repository for analyzing and clustering the main topics on Android testing. We employed the LDA algorithm to summarize the mobile testing related questions. Our findings show that topics such as testing tools, functional testing, and unit testing are often discussed when compared to other topics. We also analyzed the evolution of the interest of Android testing tools. Results show that developers are more interested in Appium, Espresso, Monkey, and Robotium tools.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {104–113},
numpages = {10},
keywords = {Mobile Testing, Stack Overflow, Model Topics},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/3110025.3123028,
author = {Yazdavar, Amir Hossein and Al-Olimat, Hussein S. and Ebrahimi, Monireh and Bajaj, Goonmeet and Banerjee, Tanvi and Thirunarayan, Krishnaprasad and Pathak, Jyotishman and Sheth, Amit},
title = {Semi-Supervised Approach to Monitoring Clinical Depressive Symptoms in Social Media},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3123028},
doi = {10.1145/3110025.3123028},
abstract = {With the rise of social media, millions of people are routinely expressing their moods, feelings, and daily struggles with mental health issues on social media platforms like Twitter. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of clinical depression from tweets obtained unobtrusively. Based on the analysis of tweets crawled from users with self-reported depressive symptoms in their Twitter profiles, we demonstrate the potential for detecting clinical depression symptoms which emulate the PHQ-9 questionnaire clinicians use today. Our study uses a semi-supervised statistical model to evaluate how the duration of these symptoms and their expression on Twitter (in terms of word usage patterns and topical preferences) align with the medical findings reported via the PHQ-9. Our proactive and automatic screening tool is able to identify clinical depressive symptoms with an accuracy of 68% and precision of 72%.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1191–1198},
numpages = {8},
keywords = {Mental Health, Natural Language Processing, Semi-supervised Machine Learning, Social Media},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@article{10.1145/3437259,
author = {R\'{\i}ssola, Esteban A. and Losada, David E. and Crestani, Fabio},
title = {A Survey of Computational Methods for Online Mental State Assessment on Social Media},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1957},
url = {https://doi.org/10.1145/3437259},
doi = {10.1145/3437259},
abstract = {Mental state assessment by analysing user-generated content is a field that has recently attracted considerable attention. Today, many people are increasingly utilising online social media platforms to share their feelings and moods. This provides a unique opportunity for researchers and health practitioners to proactively identify linguistic markers or patterns that correlate with mental disorders such as depression, schizophrenia or suicide behaviour. This survey describes and reviews the approaches that have been proposed for mental state assessment and identification of disorders using online digital records. The presented studies are organised according to the assessment technology and the feature extraction process conducted. We also present a series of studies which explore different aspects of the language and behaviour of individuals suffering from mental disorders, and discuss various aspects related to the development of experimental frameworks. Furthermore, ethical considerations regarding the treatment of individuals’ data are outlined. The main contributions of this survey are a comprehensive analysis of the proposed approaches for online mental state assessment on social media, a structured categorisation of the methods according to their design principles, lessons learnt over the years and a discussion on possible avenues for future research.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {17},
numpages = {31},
keywords = {social media, data mining, Online mental state assessment}
}

@article{10.1145/3470659,
author = {Zhang, Hangbin and Wong, Raymond K. and Chu, Victor W.},
title = {Hybrid Variational Autoencoder for Recommender Systems},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3470659},
doi = {10.1145/3470659},
abstract = {E-commerce platforms heavily rely on automatic personalized recommender systems, e.g., collaborative filtering models, to improve customer experience. Some hybrid models have been proposed recently to address the deficiency of existing models. However, their performances drop significantly when the dataset is sparse. Most of the recent works failed to fully address this shortcoming. At most, some of them only tried to alleviate the problem by considering either user side or item side content information. In this article, we propose a novel recommender model called Hybrid Variational Autoencoder (HVAE) to improve the performance on sparse datasets. Different from the existing approaches, we encode both user and item information into a latent space for semantic relevance measurement. In parallel, we utilize collaborative filtering to find the implicit factors of users and items, and combine their outputs to deliver a hybrid solution. In addition, we compare the performance of Gaussian distribution and multinomial distribution in learning the representations of the textual data. Our experiment results show that HVAE is able to significantly outperform state-of-the-art models with robust performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {sep},
articleno = {37},
numpages = {37},
keywords = {hybrid filtering, Recommender systems}
}

@article{10.1145/3359786,
author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
title = {Techniques for Interpretable Machine Learning},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3359786},
doi = {10.1145/3359786},
abstract = {Uncovering the mysterious ways machine learning models make decisions.},
journal = {Commun. ACM},
month = {dec},
pages = {68–77},
numpages = {10}
}

@article{10.1109/TCBB.2018.2817488,
author = {Li, Min and Fei, Zhihui and Zeng, Min and Wu, Fang-Xiang and Li, Yaohang and Pan, Yi and Wang, Jianxin},
title = {Automated ICD-9 Coding via A Deep Learning Approach},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2817488},
doi = {10.1109/TCBB.2018.2817488},
abstract = {ICD-9 the Ninth Revision of International Classification of Diseases is widely used to describe a patient's diagnosis. Accurate automated ICD-9 coding is important because manual coding is expensive, time-consuming, and inefficient. Inspired by the recent successes of deep learning, in this study, we present a deep learning framework called DeepLabeler to automatically assign ICD-9 codes. DeepLabeler combines the convolutional neural network with the 'Document to Vector' technique to extract and encode local and global features. Our proposed DeepLabeler demonstrates its effectiveness by achieving state-of-the-art performance, i.e., 0.335 micro F-measure on MIMIC-II dataset and 0.408 micro F-measure on MIMIC-III dataset. It outperforms classical hierarchy-based SVM and flat-SVM both on these two datasets by at least 14 percent. Furthermore, we analyze the deep neural network structure to discover the vital elements in the success of DeepLabeler. We find that the convolutional neural network is the most effective component in our network and the 'Document to Vector' technique is also necessary for enhancing classification performance since it extracts well-recognized global features. Extensive experimental results demonstrate that the great promise of deep learning techniques in the field of text multi-label classification and automated medical coding.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {1193–1202},
numpages = {10}
}

@inproceedings{10.1145/3240508.3240649,
author = {Li, Liang and Wang, Shuhui and Jiang, Shuqiang and Huang, Qingming},
title = {Attentive Recurrent Neural Network for Weak-Supervised Multi-Label Image Classification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240649},
doi = {10.1145/3240508.3240649},
abstract = {Multi-label image classification is a fundamental and challenging task in computer vision, and recently achieved significant progress by exploiting semantic relations among labels. However, the spatial positions of labels for multi-labels images are usually not provided in real scenarios, which brings insuperable barrier to conventional models. In this paper, we propose an end-to-end attentive recurrent neural network for multi-label image classification under only image-level supervision, which learns the discriminative feature representations and models the label relations simultaneously. First, inspired by attention mechanism, we propose a recurrent highlight network (RHN) which focuses on the most related regions in the image to learn the discriminative feature representations for different objects in an iterative manner. Second, we develop a gated recurrent relation extractor (GRRE) to model the label relations using multiplicative gates in a recurrent fashion, which learns to decide how multiple labels of the image influence the relation extraction. Extensive experiments on three benchmark datasets show that our model outperforms the state-of-the-arts, and performs better on small-object categories and under the scenario with large number of labels.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1092–1100},
numpages = {9},
keywords = {attentive recurrent neural network, multi-label image classification, label relations modeling, reinforcement learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3110025.3110150,
author = {Jia, Haofeng and Saule, Erik},
title = {An Analysis of Citation Recommender Systems: Beyond the Obvious},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110150},
doi = {10.1145/3110025.3110150},
abstract = {As science advances, the academic community has published millions of research papers. Researchers devote time and effort to search relevant manuscripts when writing a paper or simply to keep up with current research. In this paper, we consider the problem of citation recommendation by extending a set of known-to-be-relevant references. Our analysis shows the degrees of cited papers in the subgraph induced by the citations of a paper, called projection graph, follow a power law distribution. Existing popular methods are only good at finding the long tail papers, the ones that are highly connected to others. In other words, the majority of cited papers are loosely connected in the projection graph but they are not going to be found by existing methods. To address this problem, we propose to combine author, venue and keyword information to interpret the citation behavior behind those loosely connected papers. Results show that different methods are finding cited papers with widely different properties. We suggest multiple recommended lists by different algorithms could satisfy various users for a real citation recommendation system.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {216–223},
numpages = {8},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1109/ICPC.2017.36,
author = {Lin, Bin and Ponzanelli, Luca and Mocci, Andrea and Bavota, Gabriele and Lanza, Michele},
title = {On the Uniqueness of Code Redundancies},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.36},
doi = {10.1109/ICPC.2017.36},
abstract = {Code redundancy widely occurs in software projects. Researchers have investigated the existence, causes, and impacts of code redundancy, showing that it can be put to good use, for example in the context of code completion. When analyzing source code redundancy, previous studies considered software projects as sequences of tokens, neglecting the role of the syntactic structures enforced by programming languages. However, differences in the redundancy of such structures may jeopardize the performance of applications leveraging code redundancy.We present a study of the redundancy of several types of code constructs in a large-scale dataset of active Java projects mined from GitHub, unveiling that redundancy is not uniform and mainly resides in specific code constructs. We further investigate the implications of the locality of redundancy by analyzing the performance of language models when applied to code completion. Our study discloses the perils of exploiting code redundancy without taking into account its strong locality in specific code constructs.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {121–131},
numpages = {11},
keywords = {code completion, code redundancy, empirical study},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@article{10.1145/3015462,
author = {Liu, Xuanzhe and Ai, Wei and Li, Huoran and Tang, Jian and Huang, Gang and Feng, Feng and Mei, Qiaozhu},
title = {Deriving User Preferences of Mobile Apps from Their Management Activities},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3015462},
doi = {10.1145/3015462},
abstract = {App marketplaces host millions of mobile apps that are downloaded billions of times. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behavior and preferences of mobile device users, infer the quality of apps, and improve user experience. Existing literature provides very limited knowledge about app management activities, due to the lack of app usage data at scale. This article takes the initiative to analyze a very large app management log collected through a leading Android app marketplace. The dataset covers 5 months of detailed downloading, updating, and uninstallation activities, which involve 17 million anonymized users and 1 million apps. We present a surprising finding that the metrics commonly used to rank apps in app stores do not truly reflect the users’ real attitudes. We then identify behavioral patterns from the app management activities that more accurately indicate user preferences of an app even when no explicit rating is available. A systematic statistical analysis is designed to evaluate machine learning models that are trained to predict user preferences using these behavioral patterns, which features an inverse probability weighting method to correct the selection biases in the training process.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
articleno = {39},
numpages = {32},
keywords = {app management activities, Mobile apps, behavior analysis}
}

@article{10.1145/3487066,
author = {Zorrilla, Asier L\'{o}pez and Torres, M. In\'{e}s},
title = {A Multilingual Neural Coaching Model with Enhanced Long-Term Dialogue Structure},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3487066},
doi = {10.1145/3487066},
abstract = {In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jul},
articleno = {16},
numpages = {47},
keywords = {multilingual, coaching, explainable artificial intelligence, transfer learning, Dialogue system}
}

@inproceedings{10.1145/3428658.3430977,
author = {Santos, Bonny K. S. dos and de A. Cysneiros Filho, Gilberto A. and Lacerda, Yuri Almeida},
title = {An Approach to Recommendation Systems Oriented towards the Perspective of Tourist Experiences},
year = {2020},
isbn = {9781450381963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428658.3430977},
doi = {10.1145/3428658.3430977},
abstract = {The Community-Contributed Geotagged Photos has widely contributed to the construction of tourism recommendation systems that facilitate the task of choosing points of interest (POIs) to visit, organizing itineraries, managing activities and improving tourist experiences when they are visiting an unfamiliar city. Tourists have driven by their aspirations, desires, and preferences. There is a new kind of traveler who instead to explore the most popular places in a city; they would like to have contact with local people and culture, exploring areas where the local people usually visit considering temporal issues, such as: parts of day, day of week, vacations, events, holidays, etc. This work presents the new recommendation model, a tourist recommender system that makes POIs recommendations considering the different interaction of tourists and residents around a city over time. This new model was constructed using Naive Bayes classifier and Collaborative Filtering. The experiments used data of 83.302 photos taken in the city of Rio de Janeiro published on Flickr and 242 locations identified through OpenStreetMap. The results demonstrated that this approach could make predictions based on the temporal context and considering differences between resident and tourist perspective being this work one of the first to consider the data yielded by residents like relevant to the build of recommendations.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {201–208},
numpages = {8},
keywords = {bayesian probabilities, tourist recommendation systems, collaborative filtering, geotagged photos},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {WebMedia '20}
}

@inproceedings{10.1145/3372923.3404783,
author = {Gangireddy, Siva Charan Reddy and P, Deepak and Long, Cheng and Chakraborty, Tanmoy},
title = {Unsupervised Fake News Detection: A Graph-Based Approach},
year = {2020},
isbn = {9781450370981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372923.3404783},
doi = {10.1145/3372923.3404783},
abstract = {Fake news has become more prevalent than ever, correlating with the rise of social media that allows every user to rapidly publish their views or hearsay. Today, fake news spans almost every realm of human activity, across diverse fields such as politics and healthcare. Most existing methods for fake news detection leverage supervised learning methods and expect a large labelled corpus of articles and social media user engagement information, which are often hard, time-consuming and costly to procure. In this paper, we consider the task of unsupervised fake news detection, which considers fake news detection in the absence of labelled historical data. We develop GTUT, a graph-based approach for the task which operates in three phases. Starting off with identifying a seed set of fake and legitimate articles exploiting high-level observations on inter-user behavior in fake news propagation, it progressively expands the labelling to all articles in the dataset. Our technique draws upon graph-based methods such as biclique identification, graph-based feature vector learning and label spreading. Through an extensive empirical evaluation over multiple real-world datasets, we establish the improved effectiveness of our method over state-of-the-art techniques for the task.},
booktitle = {Proceedings of the 31st ACM Conference on Hypertext and Social Media},
pages = {75–83},
numpages = {9},
keywords = {graph mining, unsupervised learning, fake news detection, social media},
location = {Virtual Event, USA},
series = {HT '20}
}

@inproceedings{10.1109/ICSSP.2019.00025,
author = {Nistala, Padmalata and Nori, Kesav Vithal and Reddy, Raghu},
title = {Software Quality Models: A Systematic Mapping Study},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00025},
doi = {10.1109/ICSSP.2019.00025},
abstract = {Quality Models play a critical role in assuring quality and have evolved over 40+ years. They provide support for defining quality attributes, building and measuring the quality of the resulting product. Each quality model adopts a critical view on quality in terms of a set of model elements and relationships between them. This study aims to provide an overview of the state-of-the-art research on quality models with a focus on encompassing model elements and their support to architecting quality. The study was conducted using systematic mapping as the research methodology. A total of 238 primary papers were classified based on the type of research, standards usage, and publication trends. We identified that 17% (40) of papers belong to quality models. These 40 models were analyzed for the underlying meta-model elements and their support for a quality architecture using Bayer's reference architecture framework. The architecture phase mapping analysis shows that quality planning phase is 100% supported, quality assessment is 75% supported, quality documentation is included in 40% models and quality realization aspect is barely considered in 13% models. Quality realization happens through software processes and patterns, and it is necessary to evolve quality models and software process architectures that correlate quality definitions and quality realization mechanisms. Future research is expected in this direction.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {125–134},
numpages = {10},
keywords = {quality pattern, quality characteristic, quality meta model, software quality, quality architecture, product quality, software process, reference architecture, quality model, quality realization},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@article{10.1109/TASLP.2016.2637280,
author = {Wang, Zhongqing and Lee, Sophia Yat Mei and Li, Shoushan and Zhou, Guodong and Zhongqing Wang and Mei Lee, Sophia Yat and Shoushan Li and Guodong Zhou},
title = {Emotion Analysis in Code-Switching Text With Joint Factor Graph Model},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2637280},
doi = {10.1109/TASLP.2016.2637280},
abstract = {Previous research on emotions analysis has placed much emphasis in monolingual instead of bilingual text. However, emotions on social media platforms are often found in bilingual or code-switching posts. Different from monolingual text, emotions in code-switching text can be expressed in both monolingual and bilingual forms. Moreover, more than one emotion can be expressed within a single post; yet they tend to be related in some ways which offers some implications. It is thus necessary to consider the correlation between different emotions. In this paper, a joint factor graph model is proposed to address this issue. In particular, attribute functions of the factor graph model are utilized to learn both monolingual and bilingual information from each post, factor functions are used to explore the relationship among different emotions, and a belief propagation algorithm is employed to learn and predict the model. Empirical studies demonstrate the importance of emotion analysis in code-switching text and the effectiveness of our proposed joint learning model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {469–480},
numpages = {12}
}

@article{10.1145/3486250,
author = {Guo, Jiafeng and Cai, Yinqiong and Fan, Yixing and Sun, Fei and Zhang, Ruqing and Cheng, Xueqi},
title = {Semantic Models for the First-Stage Retrieval: A Comprehensive Review},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3486250},
doi = {10.1145/3486250},
abstract = {Multi-stage ranking pipelines have been a practical solution in modern search systems, where the first-stage retrieval is to return a subset of candidate documents and latter stages attempt to re-rank those candidates. Unlike re-ranking stages going through quick technique shifts over the past decades, the first-stage retrieval has long been dominated by classical term-based models. Unfortunately, these models suffer from the vocabulary mismatch problem, which may block re-ranking stages from relevant documents at the very beginning. Therefore, it has been a long-term desire to build semantic models for the first-stage retrieval that can achieve high recall efficiently. Recently, we have witnessed an explosive growth of research interests on the first-stage semantic retrieval models. We believe it is the right time to survey current status, learn from existing methods, and gain some insights for future development. In this article, we describe the current landscape of the first-stage retrieval models under a unified framework to clarify the connection between classical term-based retrieval methods, early semantic retrieval methods, and neural semantic retrieval methods. Moreover, we identify some open challenges and envision some future directions, with the hope of inspiring more research on these important yet less investigated topics.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {66},
numpages = {42},
keywords = {survey, Semantic retrieval models, information retrieval}
}

@article{10.1145/3447239,
author = {Cheng, Wen-Huang and Song, Sijie and Chen, Chieh-Yun and Hidayati, Shintami Chusnul and Liu, Jiaying},
title = {Fashion Meets Computer Vision: A Survey},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447239},
doi = {10.1145/3447239},
abstract = {Fashion is the way we present ourselves to the world and has become one of the world’s largest industries. Fashion, mainly conveyed by vision, has thus attracted much attention from computer vision researchers in recent years. Given the rapid development, this article provides a comprehensive survey of more than 200 major fashion-related works covering four main aspects for enabling intelligent fashion: (1) Fashion detection includes landmark detection, fashion parsing, and item retrieval; (2) Fashion analysis contains attribute recognition, style learning, and popularity prediction; (3) Fashion synthesis involves style transfer, pose transformation, and physical simulation; and (4) Fashion recommendation comprises fashion compatibility, outfit matching, and hairstyle suggestion. For each task, the benchmark datasets and the evaluation protocols are summarized. Furthermore, we highlight promising directions for future research.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {72},
numpages = {41},
keywords = {fashion detection, fashion recommendation, fashion synthesis, Intelligent fashion, fashion analysis}
}

@article{10.1145/3419972,
author = {Liu, Bulou and Li, Chenliang and Zhou, Wei and Ji, Feng and Duan, Yu and Chen, Haiqing},
title = {An Attention-Based Deep Relevance Model for Few-Shot Document Filtering},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3419972},
doi = {10.1145/3419972},
abstract = {With the large quantity of textual information produced on the Internet, a critical necessity is to filter out the irrelevant information and organize the rest into categories of interest (e.g., an emerging event). However, supervised-learning document filtering methods heavily rely on a large number of labeled documents for model training. Manually identifying plenty of positive examples for each category is expensive and time-consuming. Also, it is unrealistic to cover all the categories from an evolving text source that covers diverse kinds of events, user opinions, and daily life activities. In this article, we propose a novel attention-based deep relevance model for few-shot document filtering (named ADRM), inspired by the relevance feedback methodology proposed for ad hoc retrieval. ADRM calculates the relevance score between a document and a category by taking a set of seed words and a few seed documents relevant to the category. It constructs the category-specific conceptual representation of the document based on the corresponding seed words and seed documents. Specifically, to filter irrelevant yet noisy information in the seed documents, ADRM employs two types of attention mechanisms (namely whole-match attention and max-match attention) and generates category-specific representations for them. Then ADRM is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process, a self-attention layer, and a relevance aggregation layer. Extensive experiments on three real-world datasets show that ADRM consistently outperforms the existing technical alternatives, including the conventional classification and retrieval baselines, and the state-of-the-art deep relevance ranking models for few-shot document filtering. We also perform an ablation study to demonstrate that each component in ADRM is effective for enhancing filtering performance. Further analysis shows that ADRM is robust under varying parameter settings.},
journal = {ACM Trans. Inf. Syst.},
month = {oct},
articleno = {6},
numpages = {35},
keywords = {deep learning, Few-shot learning, document filtering}
}

@article{10.1145/3299986.3299990,
author = {Karmaker Santu, Shubhra Kanti and Geigle, Chase and Ferguson, Duncan and Cope, William and Kalantzis, Mary and Searsmith, Duane and Zhai, Chengxiang},
title = {SOFSAT: Towards a Setlike Operator Based Framework for Semantic Analysis of Text},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3299986.3299990},
doi = {10.1145/3299986.3299990},
abstract = {As data reported by humans about our world, text data play a very important role in all data mining applications, yet how to develop a general text analysis system to sup- port all text mining applications is a difficult challenge. In this position paper, we introduce SOFSAT, a new frame- work that can support set-like operators for semantic analy- sis of natural text data with variable text representations. It includes three basic set-like operators|TextIntersect, Tex- tUnion, and TextDi erence|that are analogous to the cor- responding set operators intersection, union, and di erence, respectively, which can be applied to any representation of text data, and di erent representations can be combined via transformation functions that map text to and from any rep- resentation. Just as the set operators can be exibly com- bined iteratively to construct arbitrary subsets or supersets based on some given sets, we show that the correspond- ing text analysis operators can also be combined exibly to support a wide range of analysis tasks that may require di erent work ows, thus enabling an application developer to program" a text mining application by using SOFSAT as an application programming language for text analysis. We discuss instantiations and implementation strategies of the framework with some speci c examples, present ideas about how the framework can be implemented by exploit- ing/extending existing techniques, and provide a roadmap for future research in this new direction.},
journal = {SIGKDD Explor. Newsl.},
month = {dec},
pages = {21–30},
numpages = {10},
keywords = {Semantic Analysis, Semantic Operator for Text, Intelligent Text Analysis, Text Mining}
}

@inproceedings{10.1145/3132847.3132911,
author = {Xiao, Lin and Min, Zhang and Yongfeng, Zhang and Yiqun, Liu and Shaoping, Ma},
title = {Learning and Transferring Social and Item Visibilities for Personalized Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132911},
doi = {10.1145/3132847.3132911},
abstract = {User feedback in the form of movie-watching history, item ratings, or product consumption is very helpful in training recommender systems. However, relatively few interactions between items and users can be observed. Instances of missing user--item entries are caused by the user not seeing the item (although the actual preference to the item could still be positive) or the user seeing the item but not liking it. Separating these two cases enables missing interactions to be modeled with finer granularity, and thus reflects user preferences more accurately. However, most previous studies on the modeling of missing instances have not fully considered the case where the user has not seen the item. Social connections are known to be helpful for modeling users' potential preferences more extensively, although a similar visibility problem exists in accurately identifying social relationships. That is, when two users are unaware of each other's existence, they have no opportunity to connect. In this paper, we propose a novel user preference model for recommender systems that considers the visibility of both items and social relationships. Furthermore, the two kinds of information are coordinated in a unified model inspired by the idea of transfer learning. Extensive experiments have been conducted on three real-world datasets in comparison with five state-of-the-art approaches. The encouraging performance of the proposed system verifies the effectiveness of social knowledge transfer and the modeling of both item and social visibilities.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {337–346},
numpages = {10},
keywords = {recommender system, implicit feedback, social network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3131151.3131167,
author = {Font\~{a}o, Awdren and Lima, Fabricio and \'{A}bia, Bruno and dos Santos, Rodrigo Pereira and Dias-Neto, Arilo Claudio},
title = {Hearing the Voice of Developers in Mobile Software Ecosystems},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131167},
doi = {10.1145/3131151.3131167},
abstract = {In a Mobile Software Ecosystem (MSECO), there is no direct communication between the organizations that maintain mobile platforms (e.g. Apple, Google, and Microsoft) and developers to solve technical questions. Thus, Q&amp;A repositories can serve as a mechanism to understand and define strategies to support developers. In this paper, we mined 13,515,636 posts from Stack Overflow by identifying 1,568,377 questions related to Android, iOS, and Windows Phone platforms. Next, we performed comparisons among those three MSECOs regarding: (i) developers' activity intensity, (ii) hot-topics (using Latent Dirichlet allocation algorithm) in all and more commented/viewed questions, and (iii) relationship among questions and official developer events. From the results, we identified four key insights: recruiting, educating, and monitoring strategies; barrier reduction; management of technology insertion; and fostering of relationships.},
booktitle = {Proceedings of the XXXI Brazilian Symposium on Software Engineering},
pages = {4–13},
numpages = {10},
keywords = {mining software repository, Software ecosystems, mobile application development, Stack Overflow},
location = {Fortaleza, CE, Brazil},
series = {SBES '17}
}

@inproceedings{10.1145/3025453.3025977,
author = {Padilla, Stefano and Methven, Thomas S. and Robb, David A. and Chantler, Mike J.},
title = {Understanding Concept Maps: A Closer Look at How People Organise Ideas},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025977},
doi = {10.1145/3025453.3025977},
abstract = {Research into creating visualisations that organise ideas into concise concept maps often focuses on implicit mathematical and statistical theories which are built around algorithmic efficacy or visual complexity. Although there are multiple techniques which attempt to mathematically optimise this multi-dimensional problem, it is still unknown how to create concept maps that are immediately understandable to people. In this paper, we present an in-depth qualitative study observing the behaviour and discussing the strategy used by non-expert participants to create, interact, update and communicate a concept map that represents a collection of research ideas. Our results show non-expert individuals create concept maps differently to visualisation algorithms. We found that our participants prioritised narrative, landmarks, abstraction, clarity, and simplicity. Finally, we derive design recommendations from our results which we hope will inspire future algorithms that automatically create more usable and compelling concept maps better suited to the natural behaviours and needs of users.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {815–827},
numpages = {13},
keywords = {concept maps, knowledge, information, design, visualisation, interfaces, data, organisation., interaction},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@article{10.14778/3514061.3514074,
author = {Cheng, Kewei and Li, Xian and Xu, Yifan Ethan and Dong, Xin Luna and Sun, Yizhou},
title = {PGE: Robust Product Graph Embedding Learning for Error Detection},
year = {2022},
issue_date = {February 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3514061.3514074},
doi = {10.14778/3514061.3514074},
abstract = {Although product graphs (PGs) have gained increasing attentions in recent years for their successful applications in product search and recommendations, the extensive power of PGs can be limited by the inevitable involvement of various kinds of errors. Thus, it is critical to validate the correctness of triples in PGs to improve their reliability. Knowledge graph (KG) embedding methods have strong error detection abilities. Yet, existing KG embedding methods may not be directly applicable to a PG due to its distinct characteristics: (1) PG contains rich textual signals, which necessitates a joint exploration of both text information and graph structure; (2) PG contains a large number of attribute triples, in which attribute values are represented by free texts. Since free texts are too flexible to define entities in KGs, traditional way to map entities to their embeddings using ids is no longer appropriate for attribute value representation; (3) Noisy triples in a PG mislead the embedding learning and significantly hurt the performance of error detection. To address the aforementioned challenges, we propose an end-to-end noise-tolerant embedding learning framework, PGE, to jointly leverage both text information and graph structure in PG to learn embeddings for error detection. Experimental results on real-world product graph demonstrate the effectiveness of the proposed framework comparing with the state-of-the-art approaches.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {1288–1296},
numpages = {9}
}

@article{10.1145/3442199,
author = {Wang, Wei and Xia, Feng and Wu, Jian and Gong, Zhiguo and Tong, Hanghang and Davison, Brian D.},
title = {Scholar2vec: Vector Representation of Scholars for Lifetime Collaborator Prediction},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3442199},
doi = {10.1145/3442199},
abstract = {While scientific collaboration is critical for a scholar, some collaborators can be more significant than others, e.g., lifetime collaborators. It has been shown that lifetime collaborators are more influential on a scholar’s academic performance. However, little research has been done on investigating predicting such special relationships in academic networks. To this end, we propose Scholar2vec, a novel neural network embedding for representing scholar profiles. First, our approach creates scholars’ research interest vector from textual information, such as demographics, research, and influence. After bridging research interests with a collaboration network, vector representations of scholars can be gained with graph learning. Meanwhile, since scholars are occupied with various attributes, we propose to incorporate four types of scholar attributes for learning scholar vectors. Finally, the early-stage similarity sequence based on Scholar2vec is used to predict lifetime collaborators with machine learning methods. Extensive experiments on two real-world datasets show that Scholar2vec outperforms state-of-the-art methods in lifetime collaborator prediction. Our work presents a new way to measure the similarity between two scholars by vector representation, which tackles the knowledge between network embedding and academic relationship mining.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {40},
numpages = {19},
keywords = {academic information retrieval, scientific collaboration, graph learning, Network embedding}
}

@inproceedings{10.1145/3394171.3413886,
author = {Li, Jiacheng and Tang, Siliang and Li, Juncheng and Xiao, Jun and Wu, Fei and Pu, Shiliang and Zhuang, Yueting},
title = {Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413886},
doi = {10.1145/3394171.3413886},
abstract = {Visual Storytelling~(VIST) is a task to tell a narrative story about a certain topic according to the given photo stream. The existing studies focus on designing complex models, which rely on a huge amount of human-annotated data. However, the annotation of VIST is extremely costly and many topics cannot be covered in the training dataset due to the long-tail topic distribution. In this paper, we focus on enhancing the generalization ability of the VIST model by considering the few-shot setting. Inspired by the way humans tell a story, we propose a topic adaptive storyteller to model the ability of inter-topic generalization. In practice, we apply the gradient-based meta-learning algorithm on multi-modal seq2seq models to endow the model the ability to adapt quickly from topic to topic. Besides, We further propose a prototype encoding structure to model the ability of intra-topic derivation. Specifically, we encode and restore the few training story text to serve as a reference to guide the generation at inference time. Experimental results show that topic adaptation and prototype encoding structure mutually bring benefit to the few-shot model on BLEU and METEOR metric. The further case study shows that the stories generated after few-shot adaptation are more relative and expressive.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4208–4216},
numpages = {9},
keywords = {meta-learning, prototype, visual storytelling},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3397271.3401150,
author = {Yang, Xin and Song, Xuemeng and Han, Xianjing and Wen, Haokun and Nie, Jie and Nie, Liqiang},
title = {Generative Attribute Manipulation Scheme for Flexible Fashion Search},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401150},
doi = {10.1145/3397271.3401150},
abstract = {In this work, we aim to investigate the practical task of flexible fashion search with attribute manipulation, where users can retrieve the target fashion items by replacing the unwanted attributes of an available query image with the desired ones (e.g., changing the collar attribute from v-neck to round). Although several pioneer efforts have been dedicated to fulfilling the task, they mainly ignore the potential of generative models in enhancing the visual understanding of target fashion items. To this end, we propose an end-to-end generative attribute manipulation scheme, which consists of a generator and a discriminator. The generator works on producing the prototype image that meets the user's requirement of attribute manipulation over the query image with the regularization of visual-semantic consistency and pixel-wise consistency. Besides, the discriminator aims to jointly fulfill the semantic learning towards correct attribute manipulation and adversarial metric learning for fashion search. Pertaining to the adversarial metric learning, we provide two general paradigms: the pair-based scheme and the triplet-based scheme, where the fake generated prototype images that closely resemble the ground truth images of target items are incorporated as hard negative samples to boost the model performance. Extensive experiments on two real-world datasets verify the effectiveness of our scheme.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {941–950},
numpages = {10},
keywords = {generative adversarial networks, fashion search, attribute manipulation, deep metric learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3343031.3350953,
author = {Liu, Fan and Cheng, Zhiyong and Sun, Changchang and Wang, Yinglong and Nie, Liqiang and Kankanhalli, Mohan},
title = {User Diverse Preference Modeling by Multimodal Attentive Metric Learning},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350953},
doi = {10.1145/3343031.3350953},
abstract = {Most existing recommender systems represent a user's preference with a feature vector, which is assumed to be fixed when predicting this user's preferences for different items. However, the same vector cannot accurately capture a user's varying preferences on all items, especially when considering the diverse characteristics of various items. To tackle this problem, in this paper, we propose a novel Multimodal Attentive Metric Learning (MAML) method to model user diverse preferences for various items. In particular, for each user-item pair, we propose an attention neural network, which exploits the item's multimodal features to estimate the user's special attention to different aspects of this item. The obtained attention is then integrated into a metric-based learning method to predict the user preference on this item. The advantage of metric learning is that it can naturally overcome the problem of dot product similarity, which is adopted by matrix factorization (MF) based recommendation models but does not satisfy the triangle inequality property. In addition, it is worth mentioning that the attention mechanism cannot only help model user's diverse preferences towards different items, but also overcome the geometrically restrictive problem caused by collaborative metric learning. Extensive experiments on large-scale real-world datasets show that our model can substantially outperform the state-of-the-art baselines, demonstrating the potential of modeling user diverse preference for recommendation.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1526–1534},
numpages = {9},
keywords = {personalized recommendation, multimodal information, attention mechanism, metric learning},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3320435.3320465,
author = {Tsai, Chun-Hua and Brusilovsky, Peter},
title = {Evaluating Visual Explanations for Similarity-Based Recommendations: User Perception and Performance},
year = {2019},
isbn = {9781450360210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320435.3320465},
doi = {10.1145/3320435.3320465},
abstract = {Recommender system helps users to reduce information overload. In recent years, enhancing explainability in recommender systems has drawn more and more attention in the field of Human-Computer Interaction (HCI). However, it is not clear whether a user-preferred explanation interface can maintain the same level of performance while the users are exploring or comparing the recommendations. In this paper, we introduced a participatory process of designing explanation interfaces with multiple explanatory goals for three similarity-based recommendation models. We investigate the relations of user perception and performance with two user studies. In the first study (N=15), we conducted card-sorting and semi-interview to identify the user preferred interfaces. In the second study (N=18), we carry out a performance-focused evaluation of six explanation interfaces. The result suggests that the user-preferred interface may not guarantee the same level of performance.},
booktitle = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {22–30},
numpages = {9},
keywords = {recommendation, similarity-based, visual explanation},
location = {Larnaca, Cyprus},
series = {UMAP '19}
}

@inproceedings{10.1145/3097983.3098009,
author = {Xun, Guangxu and Li, Yaliang and Gao, Jing and Zhang, Aidong},
title = {Collaboratively Improving Topic Discovery and Word Embeddings by Coordinating Global and Local Contexts},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098009},
doi = {10.1145/3097983.3098009},
abstract = {A text corpus typically contains two types of context information -- global context and local context. Global context carries topical information which can be utilized by topic models to discover topic structures from the text corpus, while local context can train word embeddings to capture semantic regularities reflected in the text corpus. This encourages us to exploit the useful information in both the global and the local context information. In this paper, we propose a unified language model based on matrix factorization techniques which 1) takes the complementary global and local context information into consideration simultaneously, and 2) models topics and learns word embeddings collaboratively. We empirically show that by incorporating both global and local context, this collaborative model can not only significantly improve the performance of topic discovery over the baseline topic models, but also learn better word embeddings than the baseline word embedding models. We also provide qualitative analysis that explains how the cooperation of global and local context information can result in better topic structures and word embeddings.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {535–543},
numpages = {9},
keywords = {unified language model, topic modeling, local context, word embeddings, global context},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3041021.3054149,
author = {Chen, Huijun and Li, Xin and Rao, Yanghui and Xie, Haoran and Wang, Fu Lee and Wong, Tak-Lam},
title = {Sentiment Strength Prediction Using Auxiliary Features},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054149},
doi = {10.1145/3041021.3054149},
abstract = {With an increasingly large amount of sentimental information embedded in online documents, sentiment analysis is quite valuable to product recommendation, opinion summarization, and so forth. Different from most works on identifying documents' qualitative affective information, this research focuses on the measurement of users' intensity over each sentimental category. Affect indicates positive or negative sentiment, while cognition includes certainty and tentative. Thus, our research can help bridge the cognitive and affective gaps between users and documents. The contributions of this study are twofold: (i) we proposed a neural network-based framework to sentiment strength prediction by convolving hybrid vectors, and (ii) we considered words jointly with a set of linguistic features for enhancing model robustness and adaptiveness. By exploiting the auxiliary features of sentiments from the corpus, the proposed model did not rely on well-established lexicons, and showed its robustness over sparse words. Experiments on six corpora validated the effectiveness of our sentiment strength prediction method.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {5–14},
numpages = {10},
keywords = {convolutional neural network, sentiment strength, hybrid features},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3038912.3052671,
author = {Lee, Kathy and Qadir, Ashequl and Hasan, Sadid A. and Datla, Vivek and Prakash, Aaditya and Liu, Joey and Farri, Oladimeji},
title = {Adverse Drug Event Detection in Tweets with Semi-Supervised Convolutional Neural Networks},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052671},
doi = {10.1145/3038912.3052671},
abstract = {Current Adverse Drug Events (ADE) surveillance systems are often associated with a sizable time lag before such events are published. Online social media such as Twitter could describe adverse drug events in real-time, prior to official reporting. Deep learning has significantly improved text classification performance in recent years and can potentially enhance ADE classification in tweets. However, these models typically require large corpora with human expert-derived labels, and such resources are very expensive to generate and are hardly available. Semi-supervised deep learning models, which offer a plausible alternative to fully supervised models, involve the use of a small set of labeled data and a relatively larger collection of unlabeled data for training. Traditionally, these models are trained on labeled and unlabeled data from similar topics or domains. In reality, millions of tweets generated daily often focus on disparate topics, and this could present a challenge for building deep learning models for ADE classification with random Twitter stream as unlabeled training data. In this work, we build several semi-supervised convolutional neural network (CNN) models for ADE classification in tweets, specifically leveraging different types of unlabeled data in developing the models to address the problem. We demonstrate that, with the selective use of a variety of unlabeled data, our semi-supervised CNN models outperform a strong state-of-the-art supervised classification model by +9.9% F1-score. We evaluated our models on the Twitter data set used in the PSB 2016 Social Media Shared Task. Our results present the new state-of-the-art for this data set.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {705–714},
numpages = {10},
keywords = {pharmacovigilance, healthcare, semi-supervised convolutional neural networks, text classification, social media, adverse drug events},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/3203246,
author = {Toth, Edward and Chawla, Sanjay},
title = {Group Deviation Detection Methods: A Survey},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3203246},
doi = {10.1145/3203246},
abstract = {Pointwise anomaly detection and change detection focus on the study of individual data instances; however, an emerging area of research involves groups or collections of observations. From applications of high-energy particle physics to health care collusion, group deviation detection techniques result in novel research discoveries, mitigation of risks, prevention of malicious collaborative activities, and other interesting explanatory insights. In particular, static group anomaly detection is the process of identifying groups that are not consistent with regular group patterns, while dynamic group change detection assesses significant differences in the state of a group over a period of time. Since both group anomaly detection and group change detection share fundamental ideas, this survey article provides a clearer and deeper understanding of group deviation detection research in static and dynamic situations.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {77},
numpages = {38},
keywords = {generative models, machine learning, discriminative methods, group change detection, hypothesis testing, group anomaly detection, Group deviation detection}
}

@inproceedings{10.1145/3539637.3557052,
author = {J\'{u}nior, Ant\^{o}nio Pereira De Souza and Cecilio, Pablo and Viegas, Felipe and Cunha, Washington and Albergaria, Elisa Tuler De and Rocha, Leonardo Chaves Dutra Da},
title = {Evaluating Topic Modeling Pre-Processing Pipelines for Portuguese Texts},
year = {2022},
isbn = {9781450394093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539637.3557052},
doi = {10.1145/3539637.3557052},
abstract = {Topic Modeling (TM) is among the most exploited approaches to extracting and organizing information from large amounts of data. Basically, these approaches aim to find semantic topics from textual documents (e.g., product reviews, tweets). Despite the good results of these approaches in English texts, we do not observe the same semantic quality when applied in Portuguese Texts since they are more verbose, presenting varied and complex verb conjugations and many homonyms, among other specific particularities. This work intends to fill this scientific gap by exploiting and evaluating different Topic Modeling Pre-processing Pipelines for Portuguese texts, which correspond to sequences of tasks that needed to be performed before the TM strategies. More specifically, we evaluate different pre-processing pipeline configurations using different semantic data representations to overcome the challenges faced by TM strategies in Portuguese Text. In our experimentation evaluation, considering two datasets collected from Twitter and Reddit related to Brazilian political discussion, we show that our proposed extended pre-processing pipeline, especially considering semantic representations, can achieve significant gains in effectiveness when compared to the TM approaches originally proposed for English texts (up to 9x better).},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {191–201},
numpages = {11},
keywords = {Pre-processing Pipeline, Portuguese Text, Topic Modeling, Semantic Data Representation},
location = {Curitiba, Brazil},
series = {WebMedia '22}
}

@inproceedings{10.1145/3442381.3449821,
author = {Liu, Zhijun and Huang, Chao and Yu, Yanwei and Dong, Junyu},
title = {Motif-Preserving Dynamic Attributed Network Embedding},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449821},
doi = {10.1145/3442381.3449821},
abstract = {Network embedding has emerged as a new learning paradigm to embed complex network into a low-dimensional vector space while preserving node proximities in both network structures and properties. It advances various network mining tasks, ranging from link prediction to node classification. However, most existing works primarily focus on static networks while many networks in real-life evolve over time with addition/deletion of links and nodes, naturally with associated attribute evolution. In this work, we present Motif-preserving Temporal Shift Network (MTSN), a novel dynamic network embedding framework that simultaneously models the local high-order structures and temporal evolution for dynamic attributed networks. Specifically, MTSN learns node representations by stacking the proposed TIME module to capture both local high-order structural proximities and node attributes by motif-preserving encoder and temporal dynamics by temporal shift operation in a dynamic attributed network. Finally, we perform extensive experiments on four real-world network datasets to demonstrate the superiority of MTSN against state-of-the-art network embedding baselines in terms of both effectiveness and efficiency. The source code of our method is available at: https://github.com/ZhijunLiu95/MTSN.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1629–1638},
numpages = {10},
keywords = {dynamic networks, graph neural networks, Network embedding},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3336191.3371856,
author = {Feyisetan, Oluwaseyi and Balle, Borja and Drake, Thomas and Diethe, Tom},
title = {Privacy- and Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371856},
doi = {10.1145/3336191.3371856},
abstract = {Accurately learning from user data while providing quantifiable privacy guarantees provides an opportunity to build better ML models while maintaining user trust. This paper presents a formal approach to carrying out privacy preserving text perturbation using the notion of d_χ-privacy designed to achieve geo-indistinguishability in location data. Our approach applies carefully calibrated noise to vector representation of words in a high dimension space as defined by word embedding models. We present a privacy proof that satisfies d_χ-privacy where the privacy parameter $varepsilon$ provides guarantees with respect to a distance metric defined by the word embedding space. We demonstrate how $varepsilon$ can be selected by analyzing plausible deniability statistics backed up by large scale analysis on GloVe and fastText embeddings. We conduct privacy audit experiments against $2$ baseline models and utility experiments on 3 datasets to demonstrate the tradeoff between privacy and utility for varying values of varepsilon on different task types. Our results demonstrate practical utility (&lt; 2% utility loss for training binary classifiers) while providing better privacy guarantees than baseline models.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {178–186},
numpages = {9},
keywords = {differential privacy, plausible deniability, privacy},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3292500.3330924,
author = {He, Zhicheng and Liu, Jie and Li, Na and Huang, Yalou},
title = {Learning Network-to-Network Model for Content-Rich Network Embedding},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330924},
doi = {10.1145/3292500.3330924},
abstract = {Recently, network embedding (NE) has achieved great successes in learning low dimensional representations for network nodes and has been increasingly applied to various network analytic tasks. In this paper, we consider the representation learning problem for content-rich networks whose nodes are associated with rich content information. Content-rich network embedding is challenging in fusing the complex structural dependencies and the rich contents. To tackle the challenges, we propose a generative model, Network-to-Network Network Embedding (Net2Net-NE) model, which can effectively fuse the structure and content information into one continuous embedding vector for each node. Specifically, we regard the content-rich network as a pair of networks with different modalities, i.e., content network and node network. By exploiting the strong correlation between the focal node and the nodes to whom it is connected to, a multilayer recursively composable encoder is proposed to fuse the structure and content information of the entire ego network into the egocentric node embedding. Moreover, a cross-modal decoder is deployed to mapping the egocentric node embeddings into node identities in an interconnected network. By learning the identity of each node according to its content, the mapping from content network to node network is learned in a generative manner. Hence the latent encoding vectors learned by the Net2Net-NE can be used as effective node embeddings. Extensive experimental results on three real-world networks demonstrate the superiority of Net2Net-NE over state-of-the-art methods.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1037–1045},
numpages = {9},
keywords = {network embedding, network representation learning, network to network, egocentric embedding},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1109/TCBB.2017.2705686,
author = {Chen, Xiaojun and Huang, Joshua Z. and Wu, Qingyao and Yang, Min},
title = {Subspace Weighting Co-Clustering of Gene Expression Data},
year = {2019},
issue_date = {March-April 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2017.2705686},
doi = {10.1109/TCBB.2017.2705686},
abstract = {Microarray technology enables the collection of vast amounts of gene expression data from biological experiments. Clustering algorithms have been successfully applied to exploring the gene expression data. Since a set of genes may be only correlated to a subset of samples, it is useful to use co-clustering to recover co-clusters in the gene expression data. In this paper, we propose a novel algorithm, called Subspace Weighting Co-Clustering (SWCC), for high dimensional gene expression data. In SWCC, a gene subspace weight matrix is introduced to identify the contribution of gene objects in distinguishing different sample clusters. We design a new co-clustering objective function to recover the co-clusters in the gene expression data, in which the subspace weight matrix is introduced. An iterative algorithm is developed to solve the objective function, in which the subspace weight matrix is automatically computed during the iterative co-clustering process. Our empirical study shows encouraging results of the proposed algorithm in comparison with six state-of-the-art clustering algorithms on ten gene expression data sets. We also propose to use SWCC for gene clustering and selection. The experimental results show that the selected genes can improve the classification performance of Random Forests.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {352–364},
numpages = {13}
}

@inproceedings{10.1145/3097983.3098077,
author = {Li, Xiaopeng and She, James},
title = {Collaborative Variational Autoencoder for Recommender Systems},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098077},
doi = {10.1145/3097983.3098077},
abstract = {Modern recommender systems usually employ collaborative filtering with rating information to recommend items to users due to its successful performance. However, because of the drawbacks of collaborative-based methods such as sparsity, cold start, etc., more attention has been drawn to hybrid methods that consider both the rating and content information. Most of the previous works in this area cannot learn a good representation from content for recommendation task or consider only text modality of the content, thus their methods are very limited in current multimedia scenario. This paper proposes a Bayesian generative model called collaborative variational autoencoder (CVAE) that considers both rating and content for recommendation in multimedia scenario. The model learns deep latent representations from content data in an unsupervised manner and also learns implicit relationships between items and users from both content and rating. Unlike previous works with denoising criteria, the proposed CVAE learns a latent distribution for content in latent space instead of observation space through an inference network and can be easily extended to other multimedia modalities other than text. Experiments show that CVAE is able to significantly outperform the state-of-the-art recommendation methods with more robust performance.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {305–314},
numpages = {10},
keywords = {variational inference, generative models, recommender systems, bayesian, deep learning, autoencoder},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3505711.3505729,
author = {Zhao, Jinjin and Thille, Candace and Zimmaro, Dawn},
title = {Data Mining for Discovering Cognitive Models of Learning},
year = {2021},
isbn = {9781450390699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505711.3505729},
doi = {10.1145/3505711.3505729},
abstract = {A cognitive model is a descriptive account or computational representation of human thinking about a given concept, skill, or domain. A cognitive model of learning, includes both a way of organizing knowledge within a subject area and an account of how humans develop accurate and complete knowledge of that subject area. Learning designers engage in a variety of practices to unpack knowledge from subject matter experts and novices to develop cognitive models of learning and use those models to guide the design of instruction or instructional technologies. Traditional approaches to eliciting and organizing knowledge, such as conducting a cognitive task analysis (CTA) [14] with experts and novices, are labor-intensive and require specific expertise that many learning designers do not have. However, learning data generated from learners’ interaction with courses, can provide insight into how humans think and develop knowledge. As a continued effort, we extend the framework presented in our earlier work [17] to discover and refine cognitive models of learning with learning data. The framework includes 1. a Variational Autoencoder (VAE) and a Gaussian Mixture Model (GMM) that models and clusters cognitive learning patterns; 2. a multidimensional measure that quantifies validity and reliability of the discovered cognitive models of learning; 3. a topic-based solution that interprets the cognitive models from a linguistic perspective; and 4. a simulation-based analysis for both accuracy measures and course refinement insights. We demonstrate the end-to-end solution with two applications and four case studies that are deployed in an openly navigated learning system in a workforce learning environment. We also report the usefulness of the discovered cognitive models of learning with subject matter expert evaluation.},
booktitle = {2021 The 5th International Conference on Advances in Artificial Intelligence (ICAAI)},
pages = {130–139},
numpages = {10},
keywords = {natural language processing, cognitive model of learning, behavior modeling, human-computer interaction, knowledge tracing},
location = {Virtual Event, United Kingdom},
series = {ICAAI 2021}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Requirements Engineering, Natural Language Processing, Software Product Line, Machine Learning, Feature Model Validation},
location = {Krems, Austria},
series = {VaMoS'21}
}

@inproceedings{10.1145/3383219.3383227,
author = {Di Sipio, Claudio and Rubei, Riccardo and Di Ruscio, Davide and Nguyen, Phuong T.},
title = {A Multinomial Na\"{\i}ve Bayesian (MNB) Network to Automatically Recommend Topics for GitHub Repositories},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383227},
doi = {10.1145/3383219.3383227},
abstract = {GitHub has become a precious service for storing and managing software source code. Over the last year, 10M new developers have joined the GitHub community, contributing to more than 44M repositories. In order to help developers increase the reachability of their repositories, in 2017 GitHub introduced the possibility to classify them by means of topics. However, assigning wrong topics to a given repository can compromise the possibility of helping other developers approach it, and thus preventing them from contributing to its development.In this paper we investigate the application of Multinomial Na\"{\i}ve Bayesian (MNB) networks to automatically classify GitHub repositories. By analyzing the README file(s) of the repository to be classified and the source code implementing it, the conceived approach is able to recommend GitHub topics. To the best of our knowledge, this is the first supervised approach addressing the considered problem. Consequently, since there exists no suitable baseline for the comparison, we validated the approach by considering different metrics, aiming to study various quality aspects.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {71–80},
numpages = {10},
keywords = {Multinomial Na\"{\i}ve Bayesian network, Recommender systems, GitHub topics},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3292500.3330698,
author = {Zhou, Jingbo and Gou, Shan and Hu, Renjun and Zhang, Dongxiang and Xu, Jin and Jiang, Airong and Li, Ying and Xiong, Hui},
title = {A Collaborative Learning Framework to Tag Refinement for Points of Interest},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330698},
doi = {10.1145/3292500.3330698},
abstract = {Tags of a Point of Interest (POI) can facilitate location-based services from many aspects like location search and place recommendation. However, many POI tags are often incomplete or imprecise, which may lead to performance degradation of tag-dependent applications. In this paper, we study the POI tag refinement problem which aims to automatically fill in the missing tags as well as correct noisy tags for POIs. We propose a tri-adaptive collaborative learning framework to search for an optimal POI-tag score matrix. The framework integrates three components to collaboratively (i) model the similarity matching between POI and tag, (ii) recover the POI-tag pattern via matrix factorization and (iii) learn to infer the most possible tags by maximum likelihood estimation. We devise an adaptively joint training process to optimize the model and regularize each component simultaneously. And the final refinement results are the consensus of multiple views from different components. We also discuss how to utilize various data sources to construct features for tag refinement, including user profile data, query data on Baidu Maps and basic properties of POIs. Finally, we conduct extensive experiments to demonstrate the effectiveness of our framework. And we further present a case study of the deployment of our framework on Baidu Maps.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1752–1761},
numpages = {10},
keywords = {point of interest, collaborative learning, tag refinement, tag mining, location based service},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3219819.3219985,
author = {Wang, Pengyang and Fu, Yanjie and Zhang, Jiawei and Wang, Pengfei and Zheng, Yu and Aggarwal, Charu},
title = {You Are How You Drive: Peer and Temporal-Aware Representation Learning for Driving Behavior Analysis},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219985},
doi = {10.1145/3219819.3219985},
abstract = {Driving is a complex activity that requires multi-level skilled operations (e.g., acceleration, braking, turning). Analyzing driving behavior can help us assess driver performances, improve traffic safety, and, ultimately, promote the development of intelligent and resilient transportation systems. While some efforts have been made for analyzing driving behavior, existing methods can be improved via representation learning by jointly exploring the peer and temporal dependencies of driving behavior. To that end, in this paper, we develop a Peer and Temporal-Aware Representation Learning based framework (PTARL) for driving behavior analysis with GPS trajectory data. Specifically, we first detect the driving operations and states of each driver from GPS traces. Then, we derive a sequence of multi-view driving state transition graphs from the driving state sequences, in order to characterize a driver's driving behavior that varies over time. In addition, we develop a peer and temporal-aware representation learning method to learn a sequence of time-varying yet relational vectorized representations from the driving state transition graphs. The proposed method can simultaneously model both the graph-graph peer dependency and the current-past temporal dependency in a unified optimization framework. Also, we provide effective solutions for the optimization problem. Moreover, we exploit the learned representations of driving behavior to score driving performances and detect dangerous regions. Finally, extensive experimental results with big trajectory data demonstrate the enhanced performance of the proposed method for driving behavior analysis.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2457–2466},
numpages = {10},
keywords = {driving behavior analysis, representation learning, spatio-temporal graphs},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/2987379,
author = {Tao, Dapeng and Tao, Dacheng and Li, Xuelong and Gao, Xinbo},
title = {Large Sparse Cone Non-Negative Matrix Factorization for Image Annotation},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2987379},
doi = {10.1145/2987379},
abstract = {Image annotation assigns relevant tags to query images based on their semantic contents. Since Non-negative Matrix Factorization (NMF) has the strong ability to learn parts-based representations, recently, a number of algorithms based on NMF have been proposed for image annotation and have achieved good performance. However, most of the efforts have focused on the representations of images and annotations. The properties of the semantic parts have not been well studied. In this article, we revisit the sparseness-constrained NMF (sNMF) proposed by Hoyer [2004]. By endowing the sparseness constraint with a geometric interpretation and sNMF with theoretical analyses of the generalization ability, we show that NMF with such a sparseness constraint has three advantages for image annotation tasks: (i) The sparseness constraint is more ℓ0-norm oriented than the ℓ1-norm-based sparseness, which significantly enhances the ability of NMF to robustly learn semantic parts. (ii) The sparseness constraint has a large cone interpretation and thus allows the reconstruction error of NMF to be smaller, which means that the learned semantic parts are more powerful to represent images for tagging. (iii) The learned semantic parts are less correlated, which increases the discriminative ability for annotating images. Moreover, we present a new efficient large sparse cone NMF (LsCNMF) algorithm to optimize the sNMF problem by employing the Nesterov’s optimal gradient method. We conducted experiments on the PASCAL VOC07 dataset and demonstrated the effectiveness of LsCNMF for image annotation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {37},
numpages = {21},
keywords = {Non-negative matrix factorization, Nesterovs optimal gradient, image annotation, sparseness constraint}
}

@inproceedings{10.1145/3336191.3371816,
author = {Guo, Ruocheng and Li, Jundong and Liu, Huan},
title = {Learning Individual Causal Effects from Networked Observational Data},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371816},
doi = {10.1145/3336191.3371816},
abstract = {The convenient access to observational data enables us to learn causal effects without randomized experiments. This research direction draws increasing attention in research areas such as economics, healthcare, and education. For example, we can study how a medicine (the treatment) causally affects the health condition (the outcome) of a patient using existing electronic health records. To validate causal effects learned from observational data, we have to control confounding bias -- the influence of variables which causally influence both the treatment and the outcome. Existing work along this line overwhelmingly relies on the unconfoundedness assumption that there do not exist unobserved confounders. However, this assumption is untestable and can even be untenable. In fact, an important fact ignored by the majority of previous work is that observational data can come with network information that can be utilized to infer hidden confounders. For example, in an observational study of the individual-level treatment effect of a medicine, instead of randomized experiments, the medicine is often assigned to each individual based on a series of factors. Some of the factors (e.g., socioeconomic status) can be challenging to measure and therefore become hidden confounders. Fortunately, the socioeconomic status of an individual can be reflected by whom she is connected in social networks. With this fact in mind, we aim to exploit the network information to recognize patterns of hidden confounders which would further allow us to learn valid individual causal effects from observational data. In this work, we propose a novel causal inference framework, the network deconfounder, which learns representations to unravel patterns of hidden confounders from the network information. Empirically, we perform extensive experiments to validate the effectiveness of the network deconfounder on various datasets.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {232–240},
numpages = {9},
keywords = {individual treatment effect, causal inference, networked observational data},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3505711.3505729,
author = {Zhao, Jinjin and Thille, Candace and Zimmaro, Dawn},
title = {Data Mining for Discovering Cognitive Models of Learning},
year = {2021},
isbn = {9781450390699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505711.3505729},
doi = {10.1145/3505711.3505729},
abstract = {A cognitive model is a descriptive account or computational representation of human thinking about a given concept, skill, or domain. A cognitive model of learning, includes both a way of organizing knowledge within a subject area and an account of how humans develop accurate and complete knowledge of that subject area. Learning designers engage in a variety of practices to unpack knowledge from subject matter experts and novices to develop cognitive models of learning and use those models to guide the design of instruction or instructional technologies. Traditional approaches to eliciting and organizing knowledge, such as conducting a cognitive task analysis (CTA) [14] with experts and novices, are labor-intensive and require specific expertise that many learning designers do not have. However, learning data generated from learners’ interaction with courses, can provide insight into how humans think and develop knowledge. As a continued effort, we extend the framework presented in our earlier work [17] to discover and refine cognitive models of learning with learning data. The framework includes 1. a Variational Autoencoder (VAE) and a Gaussian Mixture Model (GMM) that models and clusters cognitive learning patterns; 2. a multidimensional measure that quantifies validity and reliability of the discovered cognitive models of learning; 3. a topic-based solution that interprets the cognitive models from a linguistic perspective; and 4. a simulation-based analysis for both accuracy measures and course refinement insights. We demonstrate the end-to-end solution with two applications and four case studies that are deployed in an openly navigated learning system in a workforce learning environment. We also report the usefulness of the discovered cognitive models of learning with subject matter expert evaluation.},
booktitle = {2021 The 5th International Conference on Advances in Artificial Intelligence (ICAAI)},
pages = {130–139},
numpages = {10},
keywords = {natural language processing, cognitive model of learning, behavior modeling, human-computer interaction, knowledge tracing},
location = {Virtual Event, United Kingdom},
series = {ICAAI 2021}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Requirements Engineering, Natural Language Processing, Software Product Line, Machine Learning, Feature Model Validation},
location = {Krems, Austria},
series = {VaMoS'21}
}

@inproceedings{10.1145/3383219.3383227,
author = {Di Sipio, Claudio and Rubei, Riccardo and Di Ruscio, Davide and Nguyen, Phuong T.},
title = {A Multinomial Na\"{\i}ve Bayesian (MNB) Network to Automatically Recommend Topics for GitHub Repositories},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383227},
doi = {10.1145/3383219.3383227},
abstract = {GitHub has become a precious service for storing and managing software source code. Over the last year, 10M new developers have joined the GitHub community, contributing to more than 44M repositories. In order to help developers increase the reachability of their repositories, in 2017 GitHub introduced the possibility to classify them by means of topics. However, assigning wrong topics to a given repository can compromise the possibility of helping other developers approach it, and thus preventing them from contributing to its development.In this paper we investigate the application of Multinomial Na\"{\i}ve Bayesian (MNB) networks to automatically classify GitHub repositories. By analyzing the README file(s) of the repository to be classified and the source code implementing it, the conceived approach is able to recommend GitHub topics. To the best of our knowledge, this is the first supervised approach addressing the considered problem. Consequently, since there exists no suitable baseline for the comparison, we validated the approach by considering different metrics, aiming to study various quality aspects.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {71–80},
numpages = {10},
keywords = {Multinomial Na\"{\i}ve Bayesian network, Recommender systems, GitHub topics},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3292500.3330698,
author = {Zhou, Jingbo and Gou, Shan and Hu, Renjun and Zhang, Dongxiang and Xu, Jin and Jiang, Airong and Li, Ying and Xiong, Hui},
title = {A Collaborative Learning Framework to Tag Refinement for Points of Interest},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330698},
doi = {10.1145/3292500.3330698},
abstract = {Tags of a Point of Interest (POI) can facilitate location-based services from many aspects like location search and place recommendation. However, many POI tags are often incomplete or imprecise, which may lead to performance degradation of tag-dependent applications. In this paper, we study the POI tag refinement problem which aims to automatically fill in the missing tags as well as correct noisy tags for POIs. We propose a tri-adaptive collaborative learning framework to search for an optimal POI-tag score matrix. The framework integrates three components to collaboratively (i) model the similarity matching between POI and tag, (ii) recover the POI-tag pattern via matrix factorization and (iii) learn to infer the most possible tags by maximum likelihood estimation. We devise an adaptively joint training process to optimize the model and regularize each component simultaneously. And the final refinement results are the consensus of multiple views from different components. We also discuss how to utilize various data sources to construct features for tag refinement, including user profile data, query data on Baidu Maps and basic properties of POIs. Finally, we conduct extensive experiments to demonstrate the effectiveness of our framework. And we further present a case study of the deployment of our framework on Baidu Maps.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1752–1761},
numpages = {10},
keywords = {point of interest, collaborative learning, tag refinement, tag mining, location based service},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3219819.3219985,
author = {Wang, Pengyang and Fu, Yanjie and Zhang, Jiawei and Wang, Pengfei and Zheng, Yu and Aggarwal, Charu},
title = {You Are How You Drive: Peer and Temporal-Aware Representation Learning for Driving Behavior Analysis},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219985},
doi = {10.1145/3219819.3219985},
abstract = {Driving is a complex activity that requires multi-level skilled operations (e.g., acceleration, braking, turning). Analyzing driving behavior can help us assess driver performances, improve traffic safety, and, ultimately, promote the development of intelligent and resilient transportation systems. While some efforts have been made for analyzing driving behavior, existing methods can be improved via representation learning by jointly exploring the peer and temporal dependencies of driving behavior. To that end, in this paper, we develop a Peer and Temporal-Aware Representation Learning based framework (PTARL) for driving behavior analysis with GPS trajectory data. Specifically, we first detect the driving operations and states of each driver from GPS traces. Then, we derive a sequence of multi-view driving state transition graphs from the driving state sequences, in order to characterize a driver's driving behavior that varies over time. In addition, we develop a peer and temporal-aware representation learning method to learn a sequence of time-varying yet relational vectorized representations from the driving state transition graphs. The proposed method can simultaneously model both the graph-graph peer dependency and the current-past temporal dependency in a unified optimization framework. Also, we provide effective solutions for the optimization problem. Moreover, we exploit the learned representations of driving behavior to score driving performances and detect dangerous regions. Finally, extensive experimental results with big trajectory data demonstrate the enhanced performance of the proposed method for driving behavior analysis.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2457–2466},
numpages = {10},
keywords = {driving behavior analysis, representation learning, spatio-temporal graphs},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/2987379,
author = {Tao, Dapeng and Tao, Dacheng and Li, Xuelong and Gao, Xinbo},
title = {Large Sparse Cone Non-Negative Matrix Factorization for Image Annotation},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2987379},
doi = {10.1145/2987379},
abstract = {Image annotation assigns relevant tags to query images based on their semantic contents. Since Non-negative Matrix Factorization (NMF) has the strong ability to learn parts-based representations, recently, a number of algorithms based on NMF have been proposed for image annotation and have achieved good performance. However, most of the efforts have focused on the representations of images and annotations. The properties of the semantic parts have not been well studied. In this article, we revisit the sparseness-constrained NMF (sNMF) proposed by Hoyer [2004]. By endowing the sparseness constraint with a geometric interpretation and sNMF with theoretical analyses of the generalization ability, we show that NMF with such a sparseness constraint has three advantages for image annotation tasks: (i) The sparseness constraint is more ℓ0-norm oriented than the ℓ1-norm-based sparseness, which significantly enhances the ability of NMF to robustly learn semantic parts. (ii) The sparseness constraint has a large cone interpretation and thus allows the reconstruction error of NMF to be smaller, which means that the learned semantic parts are more powerful to represent images for tagging. (iii) The learned semantic parts are less correlated, which increases the discriminative ability for annotating images. Moreover, we present a new efficient large sparse cone NMF (LsCNMF) algorithm to optimize the sNMF problem by employing the Nesterov’s optimal gradient method. We conducted experiments on the PASCAL VOC07 dataset and demonstrated the effectiveness of LsCNMF for image annotation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {37},
numpages = {21},
keywords = {Non-negative matrix factorization, Nesterovs optimal gradient, image annotation, sparseness constraint}
}

@inproceedings{10.1145/3535511.3535519,
author = {Cruz, Jadna Almeida da and Oliveira, Amanda Chagas and Silva, Diego Corr\^{e}a da and Dur\~{a}o, Frederico Ara\'{u}jo},
title = {GRSPOI: A Point-of-Interest Recommender Systems for Groups Using Diversification},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535519},
doi = {10.1145/3535511.3535519},
abstract = {Context: With the massive availability and usage of the Internet, the search for Points of Interest is becoming an arduous task. Thus, Points of Interest Recommender Systems arise to help users in the search. These systems traditionally recommend points of interest to individual users, however, there are scenarios in which individuals gather, therefore creating the need to recommend items to groups. Problem: The problem is that users’ location is not always considered, only their preferences. Hence, there are studies indicating the greater is users commuting, the less POIs relevance appears to them. Furthermore, the recommendations belong to the same category, without diversity. Solution: Develop a Points of Interest Recommendation System for a group using a diversity algorithm, based on members’ preferences and their locations. IS Theory: This work was conceived in the light of the General Theory of Systems, in particular open systems as they undergo interactions with the environment where they can be inserted. Recommender systems depend on a continuous exchange of information with the external environment. Method: The research is based on the literature, and its evaluation was carried out through an online experiment with real users. The analysis of the results was carried out with a qualitative approach. Summary of Results: Precision metrics were used in the evaluation, and it was observed that the level at which the results are analyzed is relevant. For the top-3, recommendations without diversity performed better, but at the top-5 and top-10 levels, diversification had a positive impact on the results. Contributions and Impact in the IS area: A recommendation system for groups that considers the geographic location of users, their preferences and the diversity of recommendations. In addition, we provide the community with a dataset with user ratings of points of interest and geolocation information.},
booktitle = {XVIII Brazilian Symposium on Information Systems},
articleno = {8},
numpages = {8},
keywords = {Recommendation for Groups, Recommendation System, Points of Interest.},
location = {Curitiba, Brazil},
series = {SBSI}
}

@article{10.1109/TASLP.2018.2852492,
author = {Jang, Youngsoo and Ham, Jiyeon and Lee, Byung-Jun and Kim, Kee-Eung},
title = {Cross-Language Neural Dialog State Tracker for Large Ontologies Using Hierarchical Attention},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852492},
doi = {10.1109/TASLP.2018.2852492},
abstract = {Dialog state tracking, which refers to identifying the user intent from utterances, is one of the most important tasks in dialog management. In this paper, we present our dialog state tracker developed for the fifth dialog state tracking challenge, which focused on cross-language adaptation using a very scarce machine-translated training data when compared to the size of the ontology. Our dialog state tracker is based on the bi-directional long short-term memory network with a hierarchical attention mechanism in order to spot important words in user utterances. The user intent is predicted by finding the closest keyword in the ontology to the attention-weighted word vector. With the suggested methodology, our tracker can overcome various difficulties due to the scarce training data that existing machine learning-based trackers had, such as predicting user intents they have not seen before. We show that our tracker outperforms other trackers submitted to the challenge with respect to most of the performance measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2072–2082},
numpages = {11}
}

@inproceedings{10.1145/3197026.3197036,
author = {Backes, Tobias},
title = {Effective Unsupervised Author Disambiguation with Relative Frequencies},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197036},
doi = {10.1145/3197026.3197036},
abstract = {This work addresses the problem of author name homonymy in the Web of Science. Aiming for an efficient, simple and straightforward solution, we introduce a novel probabilistic similarity measure for author name disambiguation based on feature overlap. Using the researcher-ID available for a subset of the Web of Science, we evaluate the application of this measure in the context of agglomeratively clustering author mentions. We focus on a concise evaluation that shows clearly for which problem setups and at which time during the clustering process our approach works best. In contrast to most other works in this field, we are skeptical towards the performance of author name disambiguation methods in general and compare our approach to the trivial single-cluster baseline. Our results are presented separately for each correct clustering size as we can explain that, when treating all cases together, the trivial baseline and more sophisticated approaches are hardly distinguishable in terms of evaluation results. Our model shows state-of-the-art performance for all correct clustering sizes without any discriminative training and with tuning only one convergence parameter.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {203–212},
numpages = {10},
keywords = {author disambiguation, probabilities, agglomerative clustering},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3132847.3132971,
author = {Li, Jiyi and Baba, Yukino and Kashima, Hisashi},
title = {Hyper Questions: Unsupervised Targeting of a Few Experts in Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132971},
doi = {10.1145/3132847.3132971},
abstract = {Quality control is one of the major problems in crowdsourcing. One of the primary approaches to rectify this issue is to assign the same task to different workers and then aggregate their answers to obtain a reliable answer. In addition to simple aggregation approaches such as majority voting, various sophisticated probabilistic models have been proposed. However, given that most of the existing methods operate by strengthening the opinions of the majority, these models often fail when the tasks require highly specialized knowledge and the ability of a large majority of the workers is inadequate. In this paper, we focus on an important class of answer aggregation problems in which majority voting fails and propose the concept of hyper questions to devise effective aggregation methods. A hyper question is a set of single questions, and our key idea is that experts are more likely to provide correct answers to all of the single questions included in a hyper question than non-experts. Thus, experts are more likely to reach consensus on the hyper questions than non-experts, which strengthen their influences. We incorporate the concept of hyper questions into existing answer aggregation methods. The results of our experiments conducted using both synthetic datasets and real datasets demonstrate that our simple and easily usable approach works effectively in cases where only a few experts are available.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1069–1078},
numpages = {10},
keywords = {answer aggregation, crowdsourcing, hyper question, heterogeneous-answer multiple-choice questions},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3018661.3018710,
author = {Yu, Hai-Tao and Jatowt, Adam and Blanco, Roi and Joho, Hideo and Jose, Joemon and Chen, Long and Yuan, Fajie},
title = {A Concise Integer Linear Programming Formulation for Implicit Search Result Diversification},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018710},
doi = {10.1145/3018661.3018710},
abstract = {To cope with ambiguous and/or underspecified queries, search result diversification (SRD) is a key technique that has attracted a lot of attention. This paper focuses on implicit SRD, where the possible subtopics underlying a query are unknown beforehand. We formulate implicit SRD as a process of selecting and ranking k exemplar documents that utilizes integer linear programming (ILP). Unlike the common practice of relying on approximate methods, this formulation enables us to obtain the optimal solution of the objective function. Based on four benchmark collections, our extensive empirical experiments reveal that: (1) The factors, such as different initial runs, the number of input documents, query types and the ways of computing document similarity significantly affect the performance of diversification models. Careful examinations of these factors are highly recommended in the development of implicit SRD methods. (2) The proposed method can achieve substantially improved performance over the state-of-the-art unsupervised methods for implicit SRD.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {191–200},
numpages = {10},
keywords = {implicit srd, cluster-based ir, integer linear programming},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3506860.3506914,
author = {Khalil, Mohammad and Wong, Jacqueline and Er, Erkan and Heitmann, Martin and Belokrys, Gleb},
title = {Tweetology of Learning Analytics: What Does Twitter Tell Us about the Trends and Development of the Field?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506914},
doi = {10.1145/3506860.3506914},
abstract = {Twitter is a very popular microblogging platform that has been actively used by scientific communities to exchange scientific information and to promote scholarly discussions. The present study aimed to leverage the tweet data to provide valuable insights into the development of the learning analytics field since its initial days. Descriptive analysis, geocoding analysis, and topic modeling were performed on over 1.6 million tweets related to learning analytics posted between 2010-2021. The descriptive analysis reveals an increasing popularity of the field on the Twittersphere in terms of number of users, twitter posts, and hashtags emergence. The topic modeling analysis uncovers new insights of the major topics in the field of learning analytics. Emergent themes in the field were identified, and the increasing (e.g., Artificial Intelligence) and decreasing&nbsp;(e.g., Education) trends were shared. Finally, the geocoding analysis indicates an increasing participation in the field from more diverse countries all around the world. Further findings are discussed in the paper.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {347–357},
numpages = {11},
keywords = {geospatial analysis, learning analytics, Twitter, topic modeling, Twitter analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3442381.3449931,
author = {Zhao, Mingjun and Wu, Haijiang and Niu, Di and Wang, Zixuan and Wang, Xiaoli},
title = {Verdi: Quality Estimation and Error Detection for Bilingual Corpora},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449931},
doi = {10.1145/3442381.3449931},
abstract = {Translation Quality Estimation is critical to reducing post-editing efforts in machine translation and to cross-lingual corpus cleaning. As a research problem, quality estimation (QE) aims to directly estimate the quality of translation in a given pair of source and target sentences, and highlight the words that need corrections, without referencing to golden translations. In this paper, we propose Verdi, a novel framework for word-level and sentence-level post-editing effort estimation for bilingual corpora. Verdi adopts two word predictors to enable diverse features to be extracted from a pair of sentences for subsequent quality estimation, including a transformer-based neural machine translation (NMT) model and a pre-trained cross-lingual language model (XLM). We exploit the symmetric nature of bilingual corpora and apply model-level dual learning in the NMT predictor, which handles a primal task and a dual task simultaneously with weight sharing, leading to stronger context prediction ability than single-direction NMT models. By taking advantage of the dual learning scheme, we further design a novel feature to directly encode the translated target information without relying on the source context. Extensive experiments conducted on WMT20 QE tasks demonstrate that our method beats the winner of the competition and outperforms other baseline methods by a great margin. We further use the sentence-level scores provided by Verdi to clean a parallel corpus and observe benefits on both model performance and training efficiency.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3023–3031},
numpages = {9},
keywords = {Machine Translation, Bilingual Corpus Filtering, Quality Estimation, Model-level Dual Learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3331184.3331191,
author = {Lin, Tzu-Heng and Gao, Chen and Li, Yong},
title = {CROSS: Cross-Platform Recommendation for Social E-Commerce},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331191},
doi = {10.1145/3331184.3331191},
abstract = {Social e-commerce, as a new concept of e-commerce, uses social media as a new prevalent platform for online shopping. Users are now able to view, add to cart, and buy products within a single social media app. In this paper, we address the problem of cross-platform recommendation for social e-commerce, i.e., recommending products to users when they are shopping through social media. To the best of our knowledge, this is a new and important problem for all e-commerce companies (e.g. Amazon, Alibaba), but has never been studied before.Existing cross-platform and social related recommendation methods cannot be applied directly for this problem since they do not co-consider the social information and the cross-platform characteristics together. To study this problem, we first investigate the heterogeneous shopping behaviors between traditional e-commerce app and social media. Based on these observations from data, we propose CROSS (Cross-platform Recommendation for Online Shopping in Social Media), a recommendation model utilizing not only user-item interaction data on both platforms, but also social relation data on social media. Extensive experiments on real-world online shopping dataset demonstrate that our proposed CROSS significantly outperforms existing state-of-the-art methods.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {515–524},
numpages = {10},
keywords = {recommender systems, social e-commerce, social media, collaborative filtering},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1109/TCBB.2016.2591523,
author = {Stojanovic, Jelena and Gligorijevic, Djordje and Radosavljevic, Vladan and Djuric, Nemanja and Grbovic, Mihajlo and Obradovic, Zoran},
title = {Modeling Healthcare Quality via Compact Representations of Electronic Health Records},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2591523},
doi = {10.1109/TCBB.2016.2591523},
abstract = {Increased availability of Electronic Health Record EHR data provides unique opportunities for improving the quality of health services. In this study, we couple EHRs with the advanced machine learning tools to predict three important parameters of healthcare quality. More specifically, we describe how to learn low-dimensional vector representations of patient conditions and clinical procedures in an unsupervised manner, and generate feature vectors of hospitalized patients useful for predicting their length of stay, total incurred charges, and mortality rates. In order to learn vector representations, we propose to employ state-of-the-art language models specifically designed for modeling co-occurrence of diseases and applied clinical procedures. The proposed model is trained on a large-scale EHR database comprising more than 35 million hospitalizations in California over a period of nine years. We compared the proposed approach to several alternatives and evaluated their effectiveness by measuring accuracy of regression and classification models used for three predictive tasks considered in this study. Our model outperformed the baseline models on all tasks, indicating a strong potential of the proposed approach for advancing quality of the healthcare system.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {may},
pages = {545–554},
numpages = {10}
}

@article{10.1145/3469028,
author = {Tian, Haiman and Presa-Reyes, Maria and Tao, Yudong and Wang, Tianyi and Pouyanfar, Samira and Miguel, Alonso and Luis, Steven and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja Sitharama},
title = {Data Analytics for Air Travel Data: A Survey and New Perspectives},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469028},
doi = {10.1145/3469028},
abstract = {From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {167},
numpages = {35},
keywords = {revenue management, Airline, big data}
}

@article{10.1145/3341702,
author = {Walia, Rajan and Narayanan, Praveen and Carette, Jacques and Tobin-Hochstadt, Sam and Shan, Chung-chieh},
title = {From High-Level Inference Algorithms to Efficient Code},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {ICFP},
url = {https://doi.org/10.1145/3341702},
doi = {10.1145/3341702},
abstract = {Probabilistic programming languages are valuable because they allow domain experts to express probabilistic models and inference algorithms without worrying about irrelevant details. However, for decades there remained an important and popular class of probabilistic inference algorithms whose efficient implementation required manual low-level coding that is tedious and error-prone. They are algorithms whose idiomatic expression requires random array variables that are latent or whose likelihood is conjugate. Although that is how practitioners communicate and compose these algorithms on paper, executing such expressions requires eliminating the latent variables and recognizing the conjugacy by symbolic mathematics. Moreover, matching the performance of handwritten code requires speeding up loops by more than a constant factor. We show how probabilistic programs that directly and concisely express these desired inference algorithms can be compiled while maintaining efficiency. We introduce new transformations that turn high-level probabilistic programs with arrays into pure loop code. We then make great use of domain-specific invariants and norms to optimize the code, and to specialize and JIT-compile the code per execution. The resulting performance is competitive with manual implementations.},
journal = {Proc. ACM Program. Lang.},
month = {jul},
articleno = {98},
numpages = {30},
keywords = {loop optimization, probabilistic programs, arrays, map-reduce, collapsed Gibbs sampling, conjugacy, multidimensional distributions, plates, marginalization}
}

@article{10.1145/3183370,
author = {Tan, Jiwei and Wan, Xiaojun and Liu, Hui and Xiao, Jianguo},
title = {QuoteRec: Toward Quote Recommendation for Writing},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3183370},
doi = {10.1145/3183370},
abstract = {Quote is a language phenomenon of transcribing the statement of someone else, such as a proverb and a famous saying. An appropriate usage of quote usually equips the expression with more elegance and credibility. However, there are times when we are eager to stress our idea by citing a quote, while nothing relevant comes to mind. Therefore, it is exciting to have a recommender system which provides quote recommendations while we are writing. This article extends previous study of quote recommendation, the task that recommends the appropriate quote according to the context (i.e., the content occurring before and after the quote). In this article, a quote recommender system called QuoteRec is presented to tackle the task. We investigate two models to learn the vector representations of quotes and contexts, and then rank the candidate quotes based on the representations. The first model learns the quote representation according to the contexts of a quote. The second model is an extension of the neural network model in previous study, which learns the representation of a quote by concerning both its content and contexts. Experimental results demonstrate the effectiveness of the two models in learning the semantic representations of quotes, and the neural network model achieves state-of-the-art results on the quote recommendation task.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {34},
numpages = {36},
keywords = {quote recommendation, Deep learning, LSTM, document recommendation}
}

@article{10.1145/3441691,
author = {Maruf, Sameen and Saleh, Fahimeh and Haffari, Gholamreza},
title = {A Survey on Document-Level Neural Machine Translation: Methods and Evaluation},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3441691},
doi = {10.1145/3441691},
abstract = {Machine translation (MT) is an important task in natural language processing (NLP), as it automates the translation process and reduces the reliance on human translators. With the resurgence of neural networks, the translation quality surpasses that of the translations obtained using statistical techniques for most language-pairs. Up until a few years ago, almost all of the neural translation models translated sentences independently, without incorporating the wider document-context and inter-dependencies among the sentences. The aim of this survey article is to highlight the major works that have been undertaken in the space of document-level machine translation after the neural revolution, so researchers can recognize the current state and future directions of this field. We provide an organization of the literature based on novelties in modelling and architectures as well as training and decoding strategies. In addition, we cover evaluation strategies that have been introduced to account for the improvements in document MT, including automatic metrics and discourse-targeted test sets. We conclude by presenting possible avenues for future exploration in this research field.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {45},
numpages = {36},
keywords = {Context-aware neural machine translation}
}

@article{10.1145/3447687,
author = {Jiang, Di and Tan, Conghui and Peng, Jinhua and Chen, Chaotao and Wu, Xueyang and Zhao, Weiwei and Song, Yuanfeng and Tong, Yongxin and Liu, Chang and Xu, Qian and Yang, Qiang and Deng, Li},
title = {A GDPR-Compliant Ecosystem for Speech Recognition with Transfer, Federated, and Evolutionary Learning},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3447687},
doi = {10.1145/3447687},
abstract = {Automatic Speech Recognition (ASR) is playing a vital role in a wide range of real-world applications. However, Commercial ASR solutions are typically “one-size-fits-all” products and clients are inevitably faced with the risk of severe performance degradation in field test. Meanwhile, with new data regulations such as the European Union’s General Data Protection Regulation (GDPR) coming into force, ASR vendors, which traditionally utilize the speech training data in a centralized approach, are becoming increasingly helpless to solve this problem, since accessing clients’ speech data is prohibited. Here, we show that by seamlessly integrating three machine learning paradigms (i.e., Transfer learning, Federated learning, and Evolutionary learning (TFE)), we can successfully build a win-win ecosystem for ASR clients and vendors and solve all the aforementioned problems plaguing them. Through large-scale quantitative experiments, we show that with TFE, the clients can enjoy far better ASR solutions than the “one-size-fits-all” counterpart, and the vendors can exploit the abundance of clients’ data to effectively refine their own ASR products.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {may},
articleno = {30},
numpages = {19},
keywords = {transfer learning, evolutionary learning, Speech recognition, federated learning}
}

@inproceedings{10.1145/3437963.3441719,
author = {Cheng, Lu and Guo, Ruocheng and Liu, Huan},
title = {Long-Term Effect Estimation with Surrogate Representation},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441719},
doi = {10.1145/3437963.3441719},
abstract = {There are many scenarios where short- and long-term causal effects of an intervention are different. For example, low-quality ads may increase short-term ad clicks but decrease the long-term revenue via reduced clicks. This work, therefore, studies the the problem of long-term effect where the outcome of primary interest, orprimary outcome, takes months or even years to accumulate. The observational study of long-term effect presents unique challenges. First, the confounding bias causes large estimation error and variance, which can further accumulate towards the prediction of primary outcomes. Second, short-term outcomes are often directly used as the proxy of the primary outcome, i.e., thesurrogate. Nevertheless, this method entails the strong surrogacy assumption that is often impractical. To tackle these challenges, we propose to build connections between long-term causal inference and sequential models in machine learning. This enables us to learnsurrogate representations that account for thetemporal unconfoundedness and circumvent the stringent surrogacy assumption by conditioning on the inferred time-varying confounders. Experimental results show that the proposed framework outperforms the state-of-the-art.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {274–282},
numpages = {9},
keywords = {long-term effect, surrogates, sequential models, representation learning},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3340531.3411933,
author = {Li, Mingda and Gao, Weiting and Chen, Yi},
title = {A Topic and Concept Integrated Model for Thread Recommendation in Online Health Communities},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411933},
doi = {10.1145/3340531.3411933},
abstract = {Online health communities (OHCs) provide a popular channel for users to seek information, suggestions and support during their medical treatment and recovery processes. To help users find relevant information easily, we present CLIR, an effective system for recommending relevant discussion threads to users in OHCs. We identify that thread content and user interests can be categorized in two dimensions: topics and concepts. CLIR leverages Latent Dirichlet Allocation model to summarize the topic dimension and uses Convolutional Neural Network to encode the concept dimension. It then builds a thread neural network to capture thread characteristics and builds a user neural network to capture user interests by integrating these two dimensions and their interactions. Finally, it matches the target thread's characteristics with candidate users' interests to make recommendations. Experimental evaluation with multiple OHC datasets demonstrates the performance advantage of CLIR over the state-of-the-art recommender systems on various evaluation metrics.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {765–774},
numpages = {10},
keywords = {online health community, discussion forum, neural network, thread recommendation, latent dirichlet allocation, recommender systems},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3077136.3080784,
author = {Zhang, Yuan and Lyu, Tianshu and Zhang, Yan},
title = {Hierarchical Community-Level Information Diffusion Modeling in Social Networks},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080784},
doi = {10.1145/3077136.3080784},
abstract = {Recently, online social networks are becoming increasingly popular platforms for social interactions. Understanding how information propagates in such networks is important for personalization and recommendation in social search.In this paper, we propose a Hierarchical Community-level Information Diffusion (HCID) model to capture the information diffusion process in social networks. We introduce the notion of users' topic popularity as to enable our model to depict the information diffusion process which is both topic-aware (which topic the information is concerned with) and source-aware (where the information comes from). Instead of assuming homogeneity of social communities, we propose the notion of community hierarchy, where information diffusion across inter-level communities is uni-directional from the higher levels to the lower ones.We design a Gibbs sampling algorithm to infer model parameters and propose prediction methods for two information diffusion prediction tasks, the retweet prediction and the cascade prediction. Comparison experiments are conducted on two real datasets. Results show that our model achieves substantial improvement compared with the existing work.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {753–762},
numpages = {10},
keywords = {information diffusion, communities, social networks},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1145/3095808,
author = {Maquil, Valerie and Tobias, Eric and Anastasiou, Dimitra and Mayer, H\'{e}l\`{e}ne and Latour, Thibaud},
title = {COPSE: Rapidly Instantiating Problem Solving Activities Based on Tangible Tabletop Interfaces},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {EICS},
url = {https://doi.org/10.1145/3095808},
doi = {10.1145/3095808},
abstract = {Collaborative problem solving is a skill that has become very important in our everyday lives and is constantly gaining attention in educational settings. In this paper, we present COPSE: a novel and unique software framework for instantiating Microworlds as collaborative problem solving activities on tangible tabletop interfaces. The framework provides three types of building blocks: widgets (provide input and localized feedback), equations (define the model), and scenes (visualize feedback), which can be specified in the form of structured text. Aim of COPSE is to simplify processes of creating, adjusting, and reusing custom Microworlds scenarios. We describe the structure of the framework, provide an example of a scenario, and report on a case study where we have used COPSE together with 33 teachers to build new scenarios on the fly.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {6},
numpages = {16},
keywords = {software framework, tabletop interfaces, tui, end-user development, tangible user interfaces, microworlds, collaborative problem solving}
}

@inproceedings{10.1145/3292500.3330721,
author = {Zhe, Chen and Sun, Aixin and Xiao, Xiaokui},
title = {Community Detection on Large Complex Attribute Network},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330721},
doi = {10.1145/3292500.3330721},
abstract = {A large payment network contains millions of merchants and billions of transactions, and the merchants are described in a large number of attributes with incomplete values. Understanding its community structures is crucial to ensure its sustainable and long lasting. Knowing a merchant's community is also important from many applications - risk management, compliance, legal and marketing. To detect communities, an algorithm has to take advances from both attribute and topological information. Further, the method has to be able to handle incomplete and complex attributes. In this paper, we propose a framework named AGGMMR to effectively address the challenges come from scalability, mixed attributes, and incomplete value. We evaluate our proposed framework on four benchmark datasets against five strong baselines. More importantly, we provide a case study of running AGGMMR on a large network from PayPal which contains $100 million$ merchants with $1.5 billion$ transactions. The results demonstrate AGGMMR's effectiveness and practicability.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2041–2049},
numpages = {9},
keywords = {large attributed network, community detection, complex attributes},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3322276.3322354,
author = {Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and Dow, Steven P.},
title = {The Civic Data Deluge: Understanding the Challenges of Analyzing Large-Scale Community Input},
year = {2019},
isbn = {9781450358507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322354},
doi = {10.1145/3322276.3322354},
abstract = {Advancements in digital civics have enabled leaders to engage and gather input from a broader spectrum of the public. However, less is known about the analysis process around community input and the challenges faced by civic leaders as engagement practices scale up. To understand these challenges, we conducted 21 interviews with leaders on civic-oriented projects. We found that at a small-scale, civic leaders manage to facilitate sensemaking through collaborative or individual approaches. However, as civic leaders scale engagement practices to account for more diverse perspectives, making sense of the large quantity of qualitative data becomes a challenge. Civic leaders could benefit from training in qualitative data analysis and simple, scalable collaborative analysis tools that would help the community form a shared understanding. Drawing from these insights, we discuss opportunities for designing tools that could improve civic leaders' ability to utilize and reflect public input in decisions.},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
pages = {1171–1181},
numpages = {11},
keywords = {digital civics, qualitative dataanalysis, public inpu, community engagement},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@inproceedings{10.1145/3097983.3098063,
author = {Zhao, Huan and Yao, Quanming and Li, Jianda and Song, Yangqiu and Lee, Dik Lun},
title = {Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098063},
doi = {10.1145/3097983.3098063},
abstract = {Heterogeneous Information Network (HIN) is a natural and general representation of data in modern large commercial recommender systems which involve heterogeneous types of data. HIN based recommenders face two problems: how to represent the high-level semantics of recommendations and how to fuse the heterogeneous information to make recommendations. In this paper, we solve the two problems by first introducing the concept of meta-graph to HIN-based recommendation, and then solving the information fusion problem with a "matrix factorization (MF) + factorization machine (FM)" approach. For the similarities generated by each meta-graph, we perform standard MF to generate latent features for both users and items. With different meta-graph based features, we propose to use FM with Group lasso (FMG) to automatically learn from the observed ratings to effectively select useful meta-graph based features. Experimental results on two real-world datasets, Amazon and Yelp, show the effectiveness of our approach compared to state-of-the-art FM and other HIN-based recommendation algorithms.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {635–644},
numpages = {10},
keywords = {recommendation system, heterogeneous information networks, factorization machine, collaborative filtering},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3397271.3401047,
author = {Dong, Xue and Wu, Jianlong and Song, Xuemeng and Dai, Hongjun and Nie, Liqiang},
title = {Fashion Compatibility Modeling through a Multi-Modal Try-on-Guided Scheme},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401047},
doi = {10.1145/3397271.3401047},
abstract = {Recent years have witnessed a growing trend of fashion compatibility modeling, which scores the matching degree of the given outfit and then provides people with some dressing advice. Existing methods have primarily solved this problem by analyzing the discrete interaction among multiple complementary items. However, the fashion items would present certain occlusion and deformation when they are worn on the body. Therefore, the discrete item interaction cannot capture the fashion compatibility in a combined manner due to the neglect of a crucial factor: the overall try-on appearance. In light of this, we propose a multi-modal try-on-guided compatibility modeling scheme to jointly characterize the discrete interaction and try-on appearance of the outfit. In particular, we first propose a multi-modal try-on template generator to automatically generate a try-on template from the visual and textual information of the outfit, depicting the overall look of its composing fashion items. Then, we introduce a new compatibility modeling scheme which integrates the outfit try-on appearance into the traditional discrete item interaction modeling. To fulfill the proposal, we construct a large-scale real-world dataset from SSENSE, named FOTOS, consisting of 11,000 well-matched outfits and their corresponding realistic try-on images. Extensive experiments have demonstrated its superiority to state-of-the-arts.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {771–780},
numpages = {10},
keywords = {fashion analysis, try-on-guided scheme, compatibility modeling},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1109/ICSE.2019.00069,
author = {Cui, Di and Liu, Ting and Cai, Yuanfang and Zheng, Qinghua and Feng, Qiong and Jin, Wuxia and Guo, Jiaqi and Qu, Yu},
title = {Investigating the Impact of Multiple Dependency Structures on Software Defects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00069},
doi = {10.1109/ICSE.2019.00069},
abstract = {Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {584–595},
numpages = {12},
keywords = {software structure, software quality, software maintenance},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3132847.3133023,
author = {Krishnan, Adit and Sankar, Aravind and Zhi, Shi and Han, Jiawei},
title = {Unsupervised Concept Categorization and Extraction from Scientific Document Titles},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133023},
doi = {10.1145/3132847.3133023},
abstract = {This paper studies the automated categorization and extraction of scientific concepts from titles of scientific articles, in order to gain a deeper understanding of their key contributions and facilitate the construction of a generic academic knowledgebase. Towards this goal, we propose an unsupervised, domain-independent, and scalable two-phase algorithm to type and extract key concept mentions into aspects of interest (e.g., Techniques, Applications, etc.). In the first phase of our algorithm we proposePhraseType, a probabilistic generative model which exploits textual features and limited POS tags to broadly segment text snippets into aspect-typed phrases. We extend this model to simultaneously learn aspect-specific features and identify academic domains in multi-domain corpora, since the two tasks mutually enhance each other. In the second phase, we propose an approach based on adaptor grammars to extract fine grained concept mentions from the aspect-typed phrases without the need for any external resources or human effort, in a purely data-driven manner. We apply our technique to study literature from diverse scientific domains and show significant gains over state-of-the-art concept extraction techniques. We also present a qualitative analysis of the results obtained.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1339–1348},
numpages = {10},
keywords = {probabilistic model, adaptor grammar, concept extraction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3485447.3512018,
author = {Truong, Quoc-Tuan and Zhao, Tong and Yuan, Changhe and Li, Jin and Chan, Jim and Pantel, Soo-Min and Lauw, Hady W.},
title = {AmpSum: Adaptive Multiple-Product Summarization towards Improving Recommendation Captions},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512018},
doi = {10.1145/3485447.3512018},
abstract = {In e-commerce websites, multiple related product recommendations are usually organized into “widgets”, each given a name, as a recommendation caption, to describe the products within. These recommendation captions are usually manually crafted and generic in nature, making it difficult to attach meaningful and informative names at scale. As a result, the captions are inadequate in helping customers to better understand the connection between the multiple recommendations and make faster product discovery. We propose an Adaptive Multiple-Product Summarization framework (AmpSum) that automatically and adaptively generates widget captions based on different recommended products. The multiplicity of products to be summarized in a widget caption is particularly novel. The lack of well-developed labels motivates us to design a weakly supervised learning approach with distant supervision to bootstrap the model learning from pseudo labels, and then fine-tune the model with a small amount of manual labels. To validate the efficacy of this method, we conduct extensive experiments on several product categories of Amazon data. The results demonstrate that our proposed framework consistently outperforms state-of-the-art baselines over 9.47-29.14% on ROUGE and 27.31% on METEOR. With case studies, we illustrate how AmpSum could adaptively generate summarization based on different product recommendations.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2978–2988},
numpages = {11},
keywords = {Multiple-Product Summarization, Product Summarization, Recommendation Captions},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3423455.3430305,
author = {Pesavento, John and Chen, Andy and Yu, Rayan and Kim, Joon-Seok and Kavak, Hamdi and Anderson, Taylor and Z\"{u}fle, Andreas},
title = {Data-Driven Mobility Models for COVID-19 Simulation},
year = {2020},
isbn = {9781450381659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423455.3430305},
doi = {10.1145/3423455.3430305},
abstract = {Agent-based models (ABM) play a prominent role in guiding critical decision-making and supporting the development of effective policies for better urban resilience and response to the COVID-19 pandemic. However, many ABMs lack realistic representations of human mobility, a key process that leads to physical interaction and subsequent spread of disease. Therefore, we propose the application of Latent Dirichlet Allocation (LDA), a topic modeling technique, to foot-traffic data to develop a realistic model of human mobility in an ABM that simulates the spread of COVID-19. In our novel approach, LDA treats POIs as "words" and agent home census block groups (CBGs) as "documents" to extract "topics" of POIs that frequently appear together in CBG visits. These topics allow us to simulate agent mobility based on the LDA topic distribution of their home CBG. We compare the LDA based mobility model with competitor approaches including a naive mobility model that assumes visits to POIs are random. We find that the naive mobility model is unable to facilitate the spread of COVID-19 at all. Using the LDA informed mobility model, we simulate the spread of COVID-19 and test the effect of changes to the number of topics, various parameters, and public health interventions. By examining the simulated number of cases over time, we find that the number of topics does indeed impact disease spread dynamics, but only in terms of the outbreak's timing. Further analysis of simulation results is needed to better understand the impact of topics on simulated COVID-19 spread. This study contributes to strengthening human mobility representations in ABMs of disease spread.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
pages = {29–38},
numpages = {10},
keywords = {agent-based modeling, simulation, latent dirichlet allocation topic modeling, mobility modeling, policy interventions, COVID-19},
location = {Seattle, Washington},
series = {ARIC '20}
}

@inproceedings{10.1145/3411408.3411414,
author = {Mylonas, Nikolaos and Karlos, Stamatis and Tsoumakas, Grigorios},
title = {Zero-Shot Classification of Biomedical Articles with Emerging MeSH Descriptors},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411414},
doi = {10.1145/3411408.3411414},
abstract = {Although numerous applications that have been developed during the last years produce vast amounts of data, the inability to obtain their ground truth target values has triggered the appearance of several new machine learning (ML) variants that tackle such phenomena. The main reasons why this happens are the evolutionary nature that characterizes the majority of real-world problems, highly hindering the conventional approaches to be applied because of incompatibility, as well as the noisy sources of data or even the shortage of available training data to produce robust predictive models. The objective of this work is to provide a new ML approach in the field of zero-shot classification, focused on classifying abstracts that come from PubMed, a well-known resource of publications from the biomedical field. The proposed approach differs in that it uses bioBERT embeddings for transforming the textual data into a new semantic space exploiting them on sentence-level, instead of adopting the usual n-grams solution. Moreover, its asset of constructing a learning model without demanding any collected training data leads to an instance-based approach, while at the same time, it can be used as an internal mechanism for assigning labels to collected unlabeled training data, creating appropriate weakly supervised learning batch-based variants. Our evaluations over 3 different MeSH terms highlights the usefulness of these approaches against a state-of-the-art approach and a well-defined baseline, respectively.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {175–184},
numpages = {10},
keywords = {semantic similarity, Zero-Shot Classification, MeSH indexing},
location = {Athens, Greece},
series = {SETN 2020}
}

@inproceedings{10.1145/3357384.3358050,
author = {Sun, Ke and Qian, Tieyun and Yin, Hongzhi and Chen, Tong and Chen, Yiqi and Chen, Ling},
title = {What Can History Tell Us?},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358050},
doi = {10.1145/3357384.3358050},
abstract = {Recommendation systems have been widely applied to many E-commerce and online social media platforms. Recently, sequential item recommendation, especially session-based recommendation, has aroused wide research interests. However, existing sequential recommendation approaches either ignore the historical sessions or consider all historical sessions without any distinction that whether the historical sessions are relevant or not to the current session, which motivates us to distinguish the effect of each historical session and identify relevant historical sessions for recommendation. In light of this, we propose a novel deep learning based sequential recommender framework for session-based recommendation, which takes Nonlocal Neural Network and Recurrent Neural Network as the main building blocks. Specifically, we design a two-layer nonlocal architecture to identify historical sessions that are relevant to the current session and learn the long-term user preferences mostly from these relevant sessions. Besides, we also design a gated recurrent unit (GRU) enhanced by the nonlocal structure to learn the short-term user preferences from the current session. Finally, we propose a novel approach to integrate both long-term and short-term user preferences in a unified way to facilitate training the whole recommender model in an end-to-end manner. We conduct extensive experiments on two widely used real-world datasets, and the experimental results show that our model achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1593–1602},
numpages = {10},
keywords = {deep neural networks, next-item recommendation, session-based recommendation},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3289600.3291026,
author = {Beigi, Ghazaleh and Guo, Ruocheng and Nou, Alexander and Zhang, Yanchao and Liu, Huan},
title = {Protecting User Privacy: An Approach for Untraceable Web Browsing History and Unambiguous User Profiles},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291026},
doi = {10.1145/3289600.3291026},
abstract = {The overturning of the Internet Privacy Rules by the Federal Communications Commissions (FCC) in late March 2017 allows Internet Service Providers (ISPs) to collect, share and sell their customers' Web browsing data without their consent. With third-party trackers embedded on Web pages, this new rule has put user privacy under more risk. The need arises for users on their own to protect their Web browsing history from any potential adversaries. Although some available solutions such as Tor, VPN, and HTTPS can help users conceal their online activities, their use can also significantly hamper personalized online services, i.e., degraded utility. In this paper, we design an effective Web browsing history anonymization scheme, PBooster, aiming to protect users' privacy while retaining the utility of their Web browsing history. The proposed model pollutes users' Web browsing history by automatically inferring how many and what links should be added to the history while addressing the utility-privacy trade-off challenge. We conduct experiments to validate the quality of the manipulated Web browsing history and examine the robustness of the proposed approach for user privacy protection.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {213–221},
numpages = {9},
keywords = {trade-off, privacy, web browsing history anonymization, utility},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3289600.3290956,
author = {Huang, Xiao and Zhang, Jingyuan and Li, Dingcheng and Li, Ping},
title = {Knowledge Graph Embedding Based Question Answering},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3290956},
doi = {10.1145/3289600.3290956},
abstract = {Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {105–113},
numpages = {9},
keywords = {question answering, deep learning, knowledge graph embedding},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3159652.3159653,
author = {Kawamae, Noriaki},
title = {Topic Chronicle Forest for Topic Discovery and Tracking},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159653},
doi = {10.1145/3159652.3159653},
abstract = {To ease comprehension of given time-stamped corpora, we extend topic models to handle both the specificity and temporality of topics; this is a significant advance over previous models which fail to provide both views simultaneously. Our proposed model consists of the Topic Chronicle Forest(TCF) and Thematic Dirichlet Processes(TDP). TCF is a set of Topic Chronicle Trees, where each tree is a hierarchy of topics that becomes more specialized toward the leaves. Only one tree is defined in each time interval, a region, and is used for TDP to generate a document. The advantage of our approach lies in providing more compact topic organization, while preserving both the semantic of a given corpus and the thematic of each document. Experiments show that TCF is a useful extension for longitudinal topic discovery and tracking, and helps us to organize and digest data sets.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {315–323},
numpages = {9},
keywords = {trend analysis, time varying topic models, hierarchical dirichlet processes, hierarchical bayesian nonparametrics},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{10.1145/3501247.3531571,
author = {Childs, Matthew and Buntain, Cody and Z. Trujillo, Milo and D. Horne, Benjamin},
title = {Characterizing YouTube and BitChute Content and Mobilizers During U.S. Election Fraud Discussions on Twitter},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3531571},
doi = {10.1145/3501247.3531571},
abstract = {In this study, we characterize the cross-platform mobilization of YouTube and BitChute videos on Twitter during the 2020 U.S. Election fraud discussions. Specifically, we extend the VoterFraud2020 dataset&nbsp;[1] to describe the prevalence of content supplied by both platforms, the mobilizers of that content, the suppliers of that content, and the content itself. We find that while BitChute videos promoting election fraud claims were linked to and engaged with in the Twitter discussion, they played a relatively small role compared to YouTube videos promoting fraud claims. This core finding points to the continued need for proactive, consistent, and collaborative content moderation solutions rather than the reactive and inconsistent solutions currently being used. Additionally, we find that cross-platform disinformation spread from video platforms was not prominently from bot accounts or political elites, but rather average Twitter users. This finding supports past work arguing that research on disinformation should move beyond a focus on bots and trolls to a focus on participatory disinformation spread.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {250–259},
numpages = {10},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@article{10.5555/3455716.3455945,
author = {Roy, Arkaprava and Dunson, David B},
title = {Nonparametric Graphical Model for Counts},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Although multivariate count data are routinely collected in many application areas, there is surprisingly little work developing flexible models for characterizing their dependence structure. This is particularly true when interest focuses on inferring the conditional independence graph. In this article, we propose a new class of pairwise Markov random field-type models for the joint distribution of a multivariate count vector. By employing a novel type of transformation, we avoid restricting to non-negative dependence structures or inducing other restrictions through truncations. Taking a Bayesian approach to inference, we choose a Dirichlet process prior for the distribution of a random effect to induce great exibility in the specification. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We prove various theoretical properties, including posterior consistency, and show that our COunt Nonparametric Graphical Analysis (CONGA) approach has good performance relative to competitors in simulation studies. The methods are motivated by an application to neuron spike count data in mice.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {229},
numpages = {21},
keywords = {dirichlet process, graphical model, multivariate count data, Markov random field, conditional independence}
}

@inproceedings{10.1145/3394486.3403179,
author = {Li, Shuangli and Zhou, Jingbo and Xu, Tong and Liu, Hao and Lu, Xinjiang and Xiong, Hui},
title = {Competitive Analysis for Points of Interest},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403179},
doi = {10.1145/3394486.3403179},
abstract = {The competitive relationship of Points of Interest (POIs) refers to the degree of competition between two POIs for business opportunities from third parties in an urban area. Existing studies for competitive analysis usually focus on mining competitive relationships of entities, such as companies or products, from textual data. However, there are few studies which have a focus on competitive analysis for POIs. Indeed, the growing availability of user behavior data about POIs, such as POI reviews and human mobility data, enables a new paradigm for understanding the competitive relationships among POIs. To this end, in this paper, we study how to predict the POI competitive relationship. Along this line, a very first challenge is how to integrate heterogeneous user behavior data with the spatial features of POIs. As a solution, we first build a heterogeneous POI information network (HPIN) from POI reviews and map search data. Then, we develop a graph neural network-based deep learning framework, named DeepR, for POI competitive relationship prediction based on HPIN. Specifically, DeepR contains two components: a spatial adaptive graph neural network (SA-GNN) and a POI pairwise knowledge extraction learning (PKE) model. The SA-GNN is a novel GNN architecture with incorporating POI's spatial information and location distribution by a specially designed spatial oriented aggregation layer and spatial-dependency attentive propagation mechanism. In addition, PKE is devised to distill the POI pairwise knowledge in HPIN being useful for relationship prediction into condensate vectors with relational graph convolution and cross attention. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1265–1274},
numpages = {10},
keywords = {point of interest, competitive analysis, heterogeneous information network, graph neural networks},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1109/TASLP.2018.2885775,
author = {Yang, Guang and He, Haibo and Chen, Qian},
title = {Emotion-Semantic-Enhanced Neural Network},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2885775},
doi = {10.1109/TASLP.2018.2885775},
abstract = {Although sentiment analysis on microblog posts has been studied in depth, sentiment analysis of posts is still challenging because of the limited contextual information that they normally contain. In microblog environments, emoticons are frequently used and they have clear emotional meanings. They are important emotional signals for microblog sentimental analysis. Existing studies typically use emoticons as noisy sentiment labels or similar sentiment indicators to effectively train classifier but overlook their emotional potentiality. We address this issue by constructing an emotional space as a feature representation matrix and projecting emoticons and words into the emotional space based on the semantic composition. To improve the performance of sentimental analysis, we propose a new emotion-semantic-enhanced convolutional neural network ECNN model. ECNN can use emoticon embedding as an emotional space projection operator. By projecting emoticons and words into an emoticon space, it can help identify subjectivity, polarity, and emotion in microblog environments. It is more capable of capturing emotion semantic than other models, so it can improve the sentiment analysis performance. The experimental results show that this model consistently outperforms other models on the dataset of several sentiment tasks. This paper provides insights on the design of ECNN for sentimental analysis in other natural language processing tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {531–543},
numpages = {13}
}

@inproceedings{10.1145/3172944.3172966,
author = {Liao, Zhenyu and Xian, Yikun and Yang, Xiao and Zhao, Qinpei and Zhang, Chenxi and Li, Jiangfeng},
title = {TSCSet: A Crowdsourced Time-Sync Comment Dataset for Exploration of User Experience Improvement},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172966},
doi = {10.1145/3172944.3172966},
abstract = {Time-Sync Comment (TSC) is a type of crowdsourced user review embedded in online video websites, which provides better real-time user interaction than traditional user comment type. Various TSC-related problems and approaches have been studied to improve user experience by taking advantage of special characteristics of TSCs such as strong time reliance. However, there are three major drawbacks to these TSC researches. First, they did not explicitly show advantage of TSC features over the traditional features in terms of users' experience. Second, the experiments were conducted on some inconsistent TSC datasets crawled from different source, which makes the effectiveness of their methods less convincing. Third, the methods were manually evaluated by a limited number of so-called "experts" in these experiments, so it is hard for other researchers to obtain the data labels and reproduce the results. In order to overcome these drawbacks, this paper aims to explore the usefulness of TSC data for for the improvement of user experience online by exploiting the TSC pattern inside a new dataset. Specifically, we present a larger-scale TSC dataset with four-level structures and rich self-labeled attributes and formally define a group of TSC-related research problems based on this dataset. The problems are solved by adapted state-of-the-art methods and evaluated through crowdsourced labels in the dataset. The result can be regarded as a baseline for further research.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {641–652},
numpages = {12},
keywords = {storyline prediction, hierarchical structured dataset, crowdsourced time-sync comment, episode representation learning},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3077136.3080822,
author = {Li, Piji and Wang, Zihao and Ren, Zhaochun and Bing, Lidong and Lam, Wai},
title = {Neural Rating Regression with Abstractive Tips Generation for Recommendation},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080822},
doi = {10.1145/3077136.3080822},
abstract = {Recently, some E-commerce sites launch a new interaction box called Tips on their mobile apps. Users can express their experience and feelings or provide suggestions using short texts typically several words or one sentence. In essence, writing some tips and giving a numerical rating are two facets of a user's product assessment action, expressing the user experience and feelings. Jointly modeling these two facets is helpful for designing a better recommendation system. While some existing models integrate text information such as item specifications or user reviews into user and item latent factors for improving the rating prediction, no existing works consider tips for improving recommendation quality. We propose a deep learning based framework named NRT which can simultaneously predict precise ratings and generate abstractive tips with good linguistic quality simulating user experience and feelings. For abstractive tips generation, gated recurrent neural networks are employed to "translate'' user and item latent representations into a concise sentence. Extensive experiments on benchmark datasets from different domains show that NRT achieves significant improvements over the state-of-the-art methods. Moreover, the generated tips can vividly predict the user experience and feelings.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {345–354},
numpages = {10},
keywords = {deep learning, rating prediction, tips generation},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3097983.3098068,
author = {Fox, Ian and Ang, Lynn and Jaiswal, Mamta and Pop-Busui, Rodica and Wiens, Jenna},
title = {Contextual Motifs: Increasing the Utility of Motifs Using Contextual Data},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098068},
doi = {10.1145/3097983.3098068},
abstract = {Motifs are a powerful tool for analyzing physiological waveform data. Standard motif methods, however, ignore important contextual information (e.g., what the patient was doing at the time the data were collected). We hypothesize that these additional contextual data could increase the utility of motifs. Thus, we propose an extension to motifs, contextual motifs, that incorporates context. Recognizing that, oftentimes, context may be unobserved or unavailable, we focus on methods to jointly infer motifs and context. Applied to both simulated and real physiological data, our proposed approach improves upon existing motif methods in terms of the discriminative utility of the discovered motifs. In particular, we discovered contextual motifs in continuous glucose monitor (CGM) data collected from patients with type 1 diabetes. Compared to their contextless counterparts, these contextual motifs led to better predictions of hypo- and hyperglycemic events. Our results suggest that even when inferred, context is useful in both a long- and short-term prediction horizon when processing and interpreting physiological waveform data.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {155–164},
numpages = {10},
keywords = {contextual motifs, motif discovery, blood glucose},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3442381.3449951,
author = {Long, Qingqing and Jin, Yilun and Wu, Yi and Song, Guojie},
title = {Theoretically Improving Graph Neural Networks via Anonymous Walk Graph Kernels},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449951},
doi = {10.1145/3442381.3449951},
abstract = {Graph neural networks (GNNs) have achieved tremendous success in graph mining. However, the inability of GNNs to model substructures in graphs remains a significant drawback. Specifically, message-passing GNNs (MPGNNs), as the prevailing type of GNNs, have been theoretically shown unable to distinguish, detect or count many graph substructures. While efforts have been paid to complement the inability, existing works either rely on pre-defined substructure sets, thus being less flexible, or are lacking in theoretical insights. In this paper, we propose GSKN1, a GNN model with a theoretically stronger ability to distinguish graph structures. Specifically, we design GSKN based on anonymous walks (AWs), flexible substructure units, and derive it upon feature mappings of graph kernels (GKs). We theoretically show that GSKN provably extends the 1-WL test, and hence the maximally powerful MPGNNs from both graph-level and node-level viewpoints. Correspondingly, various experiments are leveraged to evaluate GSKN, where GSKN outperforms a wide range of baselines, endorsing the analysis.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1204–1214},
numpages = {11},
keywords = {Graph Kernels, Structural Patterns, Graph Convolutional Network},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3441486,
author = {Li, Juan-Hui and Huang, Ling and Wang, Chang-Dong and Huang, Dong and Lai, Jian-Huang and Chen, Pei},
title = {Attributed Network Embedding with Micro-Meso Structure},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441486},
doi = {10.1145/3441486},
abstract = {Recently, network embedding has received a large amount of attention in network analysis. Although some network embedding methods have been developed from different perspectives, on one hand, most of the existing methods only focus on leveraging the plain network structure, ignoring the abundant attribute information of nodes. On the other hand, for some methods integrating the attribute information, only the lower-order proximities (e.g., microscopic proximity structure) are taken into account, which may suffer if there exists the sparsity issue and the attribute information is noisy. To overcome this problem, the attribute information and mesoscopic community structure are utilized. In this article, we propose a novel network embedding method termed Attributed Network Embedding with Micro-Meso structure, which is capable of preserving both the attribute information and the structural information including the microscopic proximity structure and mesoscopic community structure. In particular, both the microscopic proximity structure and node attributes are factorized by Nonnegative Matrix Factorization (NMF), from which the low-dimensional node representations can be obtained. For the mesoscopic community structure, a community membership strength matrix is inferred by a generative model (i.e., BigCLAM) or modularity from the linkage structure, which is then factorized by NMF to obtain the low-dimensional node representations. The three components are jointly correlated by the low-dimensional node representations, from which two objective functions (i.e., ANEM_B and ANEM_M) can be defined. Two efficient alternating optimization schemes are proposed to solve the optimization problems. Extensive experiments have been conducted to confirm the superior performance of the proposed models over the state-of-the-art network embedding methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {72},
numpages = {26},
keywords = {node attribute, microscopic proximity structure, mesoscopic community structure, Network embedding}
}

@inproceedings{10.1145/3396956.3396971,
author = {M. Vogl, Thomas},
title = {Artificial Intelligence and Organizational Memory in Government: The Experience of Record Duplication in the Child Welfare Sector in Canada},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3396971},
doi = {10.1145/3396956.3396971},
abstract = {In recent years, the topic of artificial intelligence in government has become a major area of study. Governments have been eager to adopt artificial intelligence for a number of purposes, including for the prediction of risk in social services. Child protection services are exploring predictive analytics for the initial screening of cases. While research identifies data quality issues as a major barrier, little is known about the characteristics of these issues in child protection, their relationship to organizational memory contained in administrative data, and their impact on the ability of an organization to adopt these technologies. This study gained insight into the socio-technical limitations of duplicate records when trying to bring organizational memory to bear in predictive decision support by interviewing and observing staff use of information technology systems. The study's findings suggest that record duplication in case management systems in child protection could pose a significant challenge to the introduction of artificial intelligence technologies such as predictive analytics for decision assistance. There is a need to address foundational information management and system issues before artificial intelligence approaches such as this can be introduced in the child protection sector.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {223–231},
numpages = {9},
keywords = {Artificial Intelligence, Enterprise Case Management, Data Quality, Record Duplication, Digital Government, Child Protection, Unique Identification},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@inproceedings{10.1145/3308558.3313463,
author = {Truong, Quoc-Tuan and Lauw, Hady},
title = {Multimodal Review Generation for Recommender Systems},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313463},
doi = {10.1145/3308558.3313463},
abstract = {Key to recommender systems is learning user preferences, which are expressed through various modalities. In online reviews, for instance, this manifests in numerical rating, textual content, as well as visual images. In this work, we hypothesize that modelling these modalities jointly would result in a more holistic representation of a review towards more accurate recommendations. Therefore, we propose Multimodal Review Generation (MRG), a neural approach that simultaneously models a rating prediction component and a review text generation component. We hypothesize that the shared user and item representations would augment the rating prediction with richer information from review text, while sensitizing the generated review text to sentiment features based on user and item of interest. Moreover, when review photos are available, visual features could inform the review text generation further. Comprehensive experiments on real-life datasets from several major US cities show that the proposed model outperforms comparable multimodal baselines, while an ablation analysis establishes the relative contributions of the respective components of the joint model.},
booktitle = {The World Wide Web Conference},
pages = {1864–1874},
numpages = {11},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3485447.3512126,
author = {Papakyriakopoulos, Orestis and Goodman, Ellen},
title = {The Impact of Twitter Labels on Misinformation Spread and User Engagement: Lessons from Trump’s Election Tweets},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512126},
doi = {10.1145/3485447.3512126},
abstract = {Social media platforms are performing “soft moderation” by attaching warning labels to misinformation to reduce dissemination of, and engagement with, such content. This study investigates the warning labels that Twitter placed on Donald Trump’s false tweets about the 2020 US Presidential election. It specifically studies their relation to misinformation spread, and the magnitude and nature of user engagement. We categorize the warning labels by type –“veracity labels” calling out falsity and “contextual labels” providing more information. In addition, we categorize labels by their rebuttal strength and textual overlap (linguistic, topical) with the underlying tweet. We look at user interactions (liking, retweeting, quote tweeting, and replying), the content of user replies, and the type of user involved (partisanship and Twitter activity level) according to various standard metrics. Using appropriate statistical tools, we find that, overall, label placement did not change the propensity of users to share and engage with labeled content, but the falsity of content did. However, we show that the presence of textual overlap in labels did reduce user interactions, while stronger rebuttals reduced the toxicity in comments. We also find that users were more likely to discuss their positions on the underlying tweets in replies when the labels contained rebuttals. When false content was labeled, results show that liberals engaged more than conservatives. Labels also increased the engagement of more passive Twitter users. This case study has direct implications for the design of effective soft moderation and related policies.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2541–2551},
numpages = {11},
keywords = {warning labels, political discourse, misinformation, content moderation, Trump},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3447548.3467390,
author = {He, Bing and Ahamad, Mustaque and Kumar, Srijan},
title = {PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-Based Classification Models},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467390},
doi = {10.1145/3447548.3467390},
abstract = {What should a malicious user write next to fool a detection model? Identifying malicious users is critical to ensure the safety and integrity of internet platforms. Several deep learning based detection models have been created. However, malicious users can evade deep detection models by manipulating their behavior, rendering these models of little use. The vulnerability of such deep detection models against adversarial attacks is unknown. Here we create a novel adversarial attack model against deep user sequence embedding-based classification models, which use the sequence of user posts to generate user embeddings and detect malicious users. In the attack, the adversary generates a new post to fool the classifier. We propose a novel end-to-end Personalized Text Generation Attack model, called PETGEN, that simultaneously reduces the efficacy of the detection model and generates posts that have several key desirable properties. Specifically, PETGEN generates posts that are personalized to the user's writing style, have knowledge about a given target context, are aware of the user's historical posts on the target context, and encapsulate the user's recent topical interests. We conduct extensive experiments on two real-world datasets (Yelp and Wikipedia, both with ground-truth of malicious users) to show that PETGEN significantly reduces the performance of popular deep user sequence embedding-based classification models. PETGEN outperforms five attack baselines in terms of text quality and attack efficacy in both white-box and black-box classifier settings. Overall, this work paves the path towards the next generation of adversary-aware sequence classification models.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {575–584},
numpages = {10},
keywords = {sequence classification, attack, user classification, deep learning, adversarial text generation},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3361242.3362699,
author = {Cao, Yingkui and Zou, Yanzhen and Xie, Bing},
title = {Extracting Code-Relevant Description Sentences Based on Structural Similarity},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3362699},
doi = {10.1145/3361242.3362699},
abstract = {Software developers often need to read code snippets that are dispersed among different documentation, e.g., Q&amp;A posts, to reuse APIs to complete certain tasks. These code snippets are often surrounded by lengthy context text which are used to describe the functions of code snippets. It will be helpful for code comprehension if we can align a code snippet with its description. In this paper, we propose an approach to extracting code-relevant sentences from its context text. To quantify the relevance between code line and natural language sentence, we represent them with structure trees and calculate their structural similarity. We conduct two experiments to evaluate our approach. In Experiment I, the results show that our approach achieves 83.5% precision and 80.1% recall in aligning Lucene code snippets and corresponding comments. Our approach achieves 27.6% ~ 40.2% improvement in precision compared with existing method, and 33.8% ~ 39.7% improvement in recall. In Experiment II, the results show that our approach achieves 66.4% ~ 93.9% precision to extract code-relevant sentences.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {9},
numpages = {10},
keywords = {code and text alignment, structural similarity, code relevant description, code comprehension},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3331184.3338062,
author = {Ferro, Nicola and Sanderson, Mark},
title = {Improving the Accuracy of System Performance Estimation by Using Shards},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3338062},
doi = {10.1145/3331184.3338062},
abstract = {We improve the measurement accuracy of retrieval system performance by better modeling the noise present in test collection scores. Our technique draws its inspiration from two approaches: one, which exploits the variable measurement accuracy of topics; the other, which randomly splits document collections into shards. We describe and theoretically analyze an ANOVA model able to capture the effects of topics, systems, and document shards as well as their interactions. Using multiple TREC collections, we empirically confirm theoretical results in terms of improved estimation accuracy and robustness of found significant differences. The improvements compared to widely used test collection measurement techniques are substantial. We speculate that our technique works because we do not assume that the topics of a test collection measure performance equally.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {805–814},
numpages = {10},
keywords = {multiple comparison, effectiveness model, anova},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3219819.3219964,
author = {Zhang, Chen and Wang, Yijun and Chen, Can and Du, Changying and Yin, Hongzhi and Wang, Hao},
title = {StockAssIstant: A Stock AI Assistant for Reliability Modeling of Stock Comments},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219964},
doi = {10.1145/3219819.3219964},
abstract = {Stock comments from analysts contain important consulting information for investors to foresee stock volatility and market trends. Existing studies on stock comments usually focused on capturing coarse-grained opinion polarities or understanding market fundamentals. However, investors are often overwhelmed and confused by massive comments with huge noises and ambiguous opinions. Therefore, it is an emerging need to have a fine-grained stock comment analysis tool to identify more reliable stock comments. To this end, this paper provides a solution called StockAssIstant for modeling the reliability of stock comments by considering multiple factors, such as stock price trends, comment content, and the performances of analysts, in a holistic manner. Specifically, we first analyze the pattern of analysts' opinion dynamics from historical comments. Then, we extract key features from the time-series constructed by using the semantic information in comment text, stock prices and the historical behaviors of analysts. Based on these features, we propose an ensemble learning based approach for measuring the reliability of comments. Finally, we conduct extensive experiments and provide a trading simulation on real-world stock data. The experimental results and the profit achieved by the simulated trading in 12-month period clearly validate the effectiveness of our approach for modeling the reliability of stock comments.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2710–2719},
numpages = {10},
keywords = {stock comment, reliability modeling, time-series},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3219819.3219827,
author = {Petroni, Fabio and Raman, Natraj and Nugent, Tim and Nourbakhsh, Armineh and Pani\'{c}, \v{Z}arko and Shah, Sameena and Leidner, Jochen L.},
title = {An Extensible Event Extraction System With Cross-Media Event Resolution},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219827},
doi = {10.1145/3219819.3219827},
abstract = {The automatic extraction of breaking news events from natural language text is a valuable capability for decision support systems. Traditional systems tend to focus on extracting events from a single media source and often ignore cross-media references. Here, we describe a large-scale automated system for extracting natural disasters and critical events from both newswire text and social media. We outline a comprehensive architecture that can identify, categorize and summarize seven different event types - namely floods, storms, fires, armed conflict, terrorism, infrastructure breakdown, and labour unavailability. The system comprises fourteen modules and is equipped with a novel coreference mechanism, capable of linking events extracted from the two complementary data sources. Additionally, the system is easily extensible to accommodate new event types. Our experimental evaluation demonstrates the effectiveness of the system.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {626–635},
numpages = {10},
keywords = {event coreference, first story detection, news analytics, event extraction, information extraction},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3159652.3159655,
author = {Huang, Xiao and Song, Qingquan and Li, Jundong and Hu, Xia},
title = {Exploring Expert Cognition for Attributed Network Embedding},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159655},
doi = {10.1145/3159652.3159655},
abstract = {Attributed network embedding has been widely used in modeling real-world systems. The obtained low-dimensional vector representations of nodes preserve their proximity in terms of both network topology and node attributes, upon which different analysis algorithms can be applied. Recent advances in explanation-based learning and human-in-the-loop models show that by involving experts, the performance of many learning tasks can be enhanced. It is because experts have a better cognition in the latent information such as domain knowledge, conventions, and hidden relations. It motivates us to employ experts to transform their meaningful cognition into concrete data to advance network embedding. However, learning and incorporating the expert cognition into the embedding remains a challenging task. Because expert cognition does not have a concrete form, and is difficult to be measured and laborious to obtain. Also, in a real-world network, there are various types of expert cognition such as the comprehension of word meaning and the discernment of similar nodes. It is nontrivial to identify the types that could lead to a significant improvement in the embedding. In this paper, we study a novel problem of exploring expert cognition for attributed network embedding and propose a principled framework NEEC. We formulate the process of learning expert cognition as a task of asking experts a number of concise and general queries. Guided by the exemplar theory and prototype theory in cognitive science, the queries are systematically selected and can be generalized to various real-world networks. The returned answers from the experts contain their valuable cognition. We model them as new edges and directly add into the attributed network, upon which different embedding methods can be applied towards a more informative embedding representation. Experiments on real-world datasets verify the effectiveness and efficiency of NEEC.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {270–278},
numpages = {9},
keywords = {human cognition, human-in-the-loop, attributed networks, network embedding},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{10.1145/3123266.3123342,
author = {Zhang, Hua and Wang, Rui and Zhang, Changqing and Cao, Xiaochun},
title = {LEAF: Latent Extended Attribute Features Discovery for Visual Classification},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123342},
doi = {10.1145/3123266.3123342},
abstract = {To improve the discrimination of attribute representation, in this paper, we propose to extend the traditional attribute representations via embedding the latent high-order structure between attributes. Specifically, our aim is to construct the Latent Extended Attribute Features (LEAF) for visual classification. Since there only exist weak label for each attribute, we firstly propose a feature selection method to explore the common feature structures across categories. After that, the attribute classifiers are trained based on the selected features. Then, the category specific graph is introduced, which is composed of single attributes and their co-occurrence attribute pairs. This attribute graph is used as the initialized representation of each image. Considering our aim, we should discover the discriminative latent structure between attributes and train the robust category classifiers. To that end, we develop a joint learning objective function which is composed of the high-order representation mining term and the classifier training term. The mining term can both preserve category-specific information and discover the common structure between categories. Based on the discovery representation, the robust visual classifiers could be trained by the classifier term. Finally, an alternating optimization method is designed to seek the optimal solution of our objective function. Experimental results on the challenging datasets demonstrate the advantages of our proposed model over existing work.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {979–987},
numpages = {9},
keywords = {visual classification, semantic feature representation, high-order attribute correlation discovery},
location = {Mountain View, California, USA},
series = {MM '17}
}

@article{10.1145/3522759,
author = {Ma, Wanlun and Hu, Xiangyu and Chen, Chao and Wen, Sheng and Choo, Kkwang Raymond and Xiang, Yang},
title = {Social Media Event Prediction Using DNN with Feedback Mechanism},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3522759},
doi = {10.1145/3522759},
abstract = {Online social networks (OSNs) are a rich source of information, and the data (including user-generated content) can be mined to facilitate real-world event prediction. However, the dynamic nature of OSNs and the fast-pace nature of social events or hot topics compound the challenge of event prediction. This is a key limitation in many existing approaches. For example, our evaluations of six baseline approaches (i.e., logistic regression latent Dirichlet allocation (LDA)-based logistic regression (LR), multi-task learning (MTL), long short-term memory (LSTM) and convolutional neural networks, and transformer-based model) on three datasets collected as part of this research (two from Twitter and one from a news collection site1), reveal that the accuracy of these approaches is between 50% and 60%, and they are not capable of utilizing new events in event predictions. Hence, in this article, we develop a novel DNN-based framework (hereafter referred to as event prediction with feedback mechanism— EPFM. Specifically, EPFM makes use of a feedback mechanism based on emerging events detection to improve the performance of event prediction. The feedback mechanism ensembles three outlier detection processes and returns a list of new events. Some of the events will then be chosen by analysts to feed into the fine-tuning process to update the predictive model. To evaluate EPFM, we conduct a series of experiments on the same three datasets, whose findings show that EPFM achieves 80% accuracy in event detection and outperforms the six baseline approaches.We also validate EPFM’s capability of detecting new events by empirically analyzing the feedback mechanism under different thresholds.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {may},
articleno = {33},
numpages = {24},
keywords = {event prediction, neural networks, Online social networks}
}

@inproceedings{10.1145/3463274.3463342,
author = {Jahanshahi, Hadi and Chhabra, Kritika and Cevik, Mucahit and Ba\th{}ar, Ay\th{}e},
title = {DABT: A Dependency-Aware Bug Triaging Method},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463342},
doi = {10.1145/3463274.3463342},
abstract = {In software engineering practice, fixing a bug promptly reduces the associated costs. On the other hand, the manual bug fixing process can be time-consuming, cumbersome, and error-prone. In this work, we introduce a bug triaging method, called Dependency-aware Bug Triaging (DABT), which leverages natural language processing and integer programming to assign bugs to appropriate developers. Unlike previous works that mainly focus on one aspect of the bug reports, DABT considers the textual information, cost associated with each bug, and dependency among them. Therefore, this comprehensive formulation covers the most important aspect of the previous works while considering the blocking effect of the bugs. We report the performance of the algorithm on three open-source software systems, i.e., EclipseJDT, LibreOffice, and Mozilla. Our result shows that DABT is able to reduce the number of overdue bugs up to 12%. It also decreases the average fixing time of the bugs by half. Moreover, it reduces the complexity of the bug dependency graph by prioritizing blocking bugs.},
booktitle = {Evaluation and Assessment in Software Engineering},
pages = {221–230},
numpages = {10},
keywords = {optimization, bug dependency, software quality, issue tracking system, bug triage, repository mining},
location = {Trondheim, Norway},
series = {EASE 2021}
}

@inproceedings{10.1145/3340531.3411882,
author = {Hui, Bo and Yan, Da and Ku, Wei-Shinn and Wang, Wenlu},
title = {Predicting Economic Growth by Region Embedding: A Multigraph Convolutional Network Approach},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411882},
doi = {10.1145/3340531.3411882},
abstract = {With the rapid progress of global urbanization and function division among different geographical regions, it is of urgent need to develop methods that can find regions of desired future function distributions in applications. For example, a company tends to open a new branch in a region where the growth trend of industrial sectors fits its strategic goals, or is similar to that of an existing company location; while a job hunter tends to search regions where his/her expertise aligns with the industrial growth trend providing sufficient job opportunities to sustain future employment and job-hopping.Our solution is to learn a distribution (aka. embedding) of the growth of various industrial sectors for each region, so that the embeddings of different regions can be searched, or compared for similarity querying. We consider the fine granularity of ZIP code areas as they are usually representative of the regional functions. By effectively utilizing open data on the Internet such as government data (e.g., from US Census Bureau) and third-party data for supervised learning, we propose to first construct a multigraph that captures the various relationships between regions such as direct flight connections and shared school districts, and then learn region embeddings using a novel graph convolutional network architecture. Our multigraph convnet (MGCN) differentiates various feature types such as demographic, social, economic and housing features, and learns different weights on different features and spatial relationships for effective data-driven feature aggregation.While deep learning is known to require large amounts of data to train, our weighted MGCN (WMGCN) is designed to minimize the number of parameters so that it does not underfit on the limited amount of open data. Extensive experiments are conducted to compare our WMGCN model with several competitive baselines to demonstrate the superiority of our WMGCN design.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {555–564},
numpages = {10},
keywords = {economic growth, graph convolutional network, embedding, multigraph, geographical region},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3209978.3210054,
author = {Jiang, Jyun-Yu and Li, Cheng-Te and Chen, Yian and Wang, Wei},
title = {Identifying Users behind Shared Accounts in Online Streaming Services},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210054},
doi = {10.1145/3209978.3210054},
abstract = {Online streaming services are prevalent. Major service providers, such as Netflix (for movies) and Spotify (for music), usually have a large customer base. More often than not, users may share an account. This has attracted increasing attention recently, as account sharing not only compromises the service provider's financial interests but also impairs the performance of recommendation systems and consequently the quality of service provided to the users. To address this issue, this paper focuses on the problem of user identification in shared accounts. Our goal is three-fold: (1) Given an account, along with its historical session logs, we identify a set of users who share such account; (2) Given a new session issued by an account, we find the corresponding user among the identified users of such account; (3) We aim to boost the performance of item recommendation by user identification. While the mapping between users and accounts is unknown, we propose an unsupervised learning-based framework, Session-based Heterogeneous graph Embedding for User Identification (SHE-UI), to differentiate and model the preferences of users in an account, and to group sessions by these users. In SHE-UI, a heterogeneous graph is constructed to represent items such as songs and their available metadata such as artists, genres, and albums. An item-based session embedding technique is proposed using a normalized random walk in the heterogeneous graph. Our experiments conducted on two large-scale music streaming datasets, Last.fm and KKBOX, show that SHE-UI not only accurately identifies users, but also significantly improves the performance of item recommendation over the state-of-the-art methods.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {65–74},
numpages = {10},
keywords = {user session clustering, user identification, shared accounts, recommender systems, heterogeneous graph embedding},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@article{10.1162/coli_a_00383,
author = {Oved, Nadav and Feder, Amir and Reichart, Roi},
title = {Predicting In-Game Actions from Interviews of NBA Players},
year = {2020},
issue_date = {September 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {3},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00383},
doi = {10.1162/coli_a_00383},
abstract = {Sports competitions are widely researched in computer and social science, with the goal of understanding how players act under uncertainty. Although there is an abundance of computational work on player metrics prediction based on past performance, very few attempts to incorporate out-of-game signals have been made. Specifically, it was previously unclear whether linguistic signals gathered from players’ interviews can add information that does not appear in performance metrics. To bridge that gap, we define text classification tasks of predicting deviations from mean in NBA players’ in-game actions, which are associated with strategic choices, player behavior, and risk, using their choice of language prior to the game. We collected a data set of transcripts from key NBA players’ pre-game interviews and their in-game performance metrics, totalling 5,226 interview-metric pairs. We design neural models for players’ action prediction based on increasingly more complex aspects of the language signals in their open-ended interviews. Our models can make their predictions based on the textual signal alone, or on a combination of that signal with signals from past-performance metrics. Our text-based models outperform strong baselines trained on performance metrics only, demonstrating the importance of language usage for action prediction. Moreover, the models that utilize both textual input and past-performance metrics produced the best results. Finally, as neural networks are notoriously difficult to interpret, we propose a method for gaining further insight into what our models have learned. Particularly, we present a latent Dirichlet allocation–based analysis, where we interpret model predictions in terms of correlated topics. We find that our best performing textual model is most associated with topics that are intuitively related to each prediction task and that better models yield higher correlation with more informative topics.1},
journal = {Comput. Linguist.},
month = {nov},
pages = {667–712},
numpages = {46}
}

@inproceedings{10.1145/3442381.3449962,
author = {Zheng, Zhi and Wang, Chao and Xu, Tong and Shen, Dazhong and Qin, Penggang and Huai, Baoxing and Liu, Tongzhu and Chen, Enhong},
title = {Drug Package Recommendation via Interaction-Aware Graph Induction},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449962},
doi = {10.1145/3442381.3449962},
abstract = {Recent years have witnessed the rapid accumulation of massive electronic medical records (EMRs), which highly support the intelligent medical services such as drug recommendation. However, prior arts mainly follow the traditional recommendation strategies like collaborative filtering, which usually treat individual drugs as mutually independent, while the latent interactions among drugs, e.g., synergistic or antagonistic effect, have been largely ignored. To that end, in this paper, we target at developing a new paradigm for drug package recommendation with considering the interaction effect within drugs, in which the interaction effects could be affected by patient conditions. Specifically, we first design a pre-training method based on neural collaborative filtering to get the initial embedding of patients and drugs. Then, the drug interaction graph will be initialized based on medical records and domain knowledge. Along this line, we propose a new Drug Package Recommendation (DPR) framework with two variants, respectively DPR on Weighted Graph (DPR-WG) and DPR on Attributed Graph (DPR-AG) to solve the problem, in which each the interactions will be described as signed weights or attribute vectors. In detail, a mask layer is utilized to capture the impact of patient condition, and graph neural networks (GNNs) are leveraged for the final graph induction task to embed the package. Extensive experiments on a real-world data set from a first-rate hospital demonstrate the effectiveness of our DPR framework compared with several competitive baseline methods, and further support the heuristic study for the drug package generation task with adequate performance.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1284–1295},
numpages = {12},
keywords = {Drug Recommendation, Graph Neural Network, Package Recommendation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3340531.3411906,
author = {Tsurel, David and Doron, Michael and Nus, Alexander and Dagan, Arnon and Guy, Ido and Shahaf, Dafna},
title = {E-Commerce Dispute Resolution Prediction},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411906},
doi = {10.1145/3340531.3411906},
abstract = {E-Commerce marketplaces support millions of daily transactions, and some disagreements between buyers and sellers are unavoidable. Resolving disputes in an accurate, fast, and fair manner is of great importance for maintaining a trustworthy platform. Simple cases can be automated, but intricate cases are not sufficiently addressed by hard-coded rules, and therefore most disputes are currently resolved by people. In this work we take a first step towards automatically assisting human agents in dispute resolution at scale. We construct a large dataset of disputes from the eBay online marketplace, and identify several interesting behavioral and linguistic patterns. We then train classifiers to predict dispute outcomes with high accuracy. We explore the model and the dataset, reporting interesting correlations, important features, and insights.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1465–1474},
numpages = {10},
keywords = {dispute resolution, e-commerce, online transactions},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3397271.3401128,
author = {Tzaban, Hen and Guy, Ido and Greenstein-Messica, Asnat and Dagan, Arnon and Rokach, Lior and Shapira, Bracha},
title = {Product Bundle Identification Using Semi-Supervised Learning},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401128},
doi = {10.1145/3397271.3401128},
abstract = {Many sellers on e-commerce platforms offer buyers product bundles, which package together two or more different items. The identification of such bundles is a necessary step to support a variety of related services, from recommendation to dynamic pricing. In this work, we present a comprehensive study of bundle identification on a large e-commerce website. Our analysis of bundle compared to non-bundle listed items reveals several key differentiating characteristics, spanning the listing's title, image, and attributes. Following, we experiment with a multi-modal classifier, which takes advantage of these characteristics as features. Our analysis also shows that a bundle indicator input by sellers tends to be highly noisy and carries only a weak signal. The bundle identification task therefore faces the challenge of having a small set of manually-labeled clean examples and a larger set of noisy-labeled examples, in conjunction with class imbalance due to the relative scarcity of bundles.Our experiments with basic supervised classifiers, using the manually-labeled and/or the noisy-labeled data for training, demonstrates only moderate performance. We therefore turn to a semisupervised approach and propose GREED, a self-training ensemblebased algorithm with a greedy model selection. Our evaluation over two different meta-categories shows a superior performance of semi-supervised approaches for the bundle identification task, with GREED outperforming several semi-supervised alternatives. The combination of textual, image, and some metadata features is shown to yield the best performance, reaching an AUC of 0.89 and 0.92 for the two meta-categories, respectively},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {791–800},
numpages = {10},
keywords = {electronic commerce, semi-supervised learning, self-training, product bundling, ensemble learning},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1109/MSR.2019.00075,
author = {Chatterjee, Preetha and Damevski, Kostadin and Pollock, Lori and Augustine, Vinay and Kraft, Nicholas A.},
title = {Exploratory Study of Slack Q&amp;A Chats as a Mining Source for Software Engineering Tools},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00075},
doi = {10.1109/MSR.2019.00075},
abstract = {Modern software development communities are increasingly social. Popular chat platforms such as Slack host public chat communities that focus on specific development topics such as Python or Ruby-on-Rails. Conversations in these public chats often follow a Q&amp;A format, with someone seeking information and others providing answers in chat form. In this paper, we describe an exploratory study into the potential usefulness and challenges of mining developer Q&amp;A conversations for supporting software maintenance and evolution tools. We designed the study to investigate the availability of information that has been successfully mined from other developer communications, particularly Stack Overflow. We also analyze characteristics of chat conversations that might inhibit accurate automated analysis. Our results indicate the prevalence of useful information, including API mentions and code snippets with descriptions, and several hurdles that need to be overcome to automate mining that information.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {490–501},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3197026.3197049,
author = {Fafalios, Pavlos and Kasturia, Vaibhav and Nejdl, Wolfgang},
title = {Ranking Archived Documents for Structured Queries on Semantic Layers},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197049},
doi = {10.1145/3197026.3197049},
abstract = {Archived collections of documents (like newspaper and web archives) serve as important information sources in a variety of disciplines, including Digital Humanities, Historical Science, and Journalism. However, the absence of efficient and meaningful exploration methods still remains a major hurdle in the way of turning them into usable sources of information. A semantic layer is an RDF graph that describes metadata and semantic information about a collection of archived documents, which in turn can be queried through a semantic query language (SPARQL). This allows running advanced queries by combining metadata of the documents (like publication date) and content-based semantic information (like entities mentioned in the documents). However, the results returned by such structured queries can be numerous and moreover they all equally match the query. In this paper, we deal with this problem and formalize the task of ranking archived documents for structured queries on semantic layers. Then, we propose two ranking models for the problem at hand which jointly consider: i) the relativeness of documents to entities, ii) the timeliness of documents, and iii) the temporal relations among the entities. The experimental results on a new evaluation dataset show the effectiveness of the proposed models and allow us to understand their limitations.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {155–164},
numpages = {10},
keywords = {semantic layers, probabilistic modeling, ranking, archived documents, stochastic modeling},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3534678.3539393,
author = {Zhang, Xiao and Dai, Sunhao and Xu, Jun and Dong, Zhenhua and Dai, Quanyu and Wen, Ji-Rong},
title = {Counteracting User Attention Bias in Music Streaming Recommendation via Reward Modification},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539393},
doi = {10.1145/3534678.3539393},
abstract = {In streaming media applications, like music Apps, songs are recommended in a continuous way in users' daily life. The recommended songs are played automatically although users may not pay any attention to them, posing a challenge of user attention bias in training recommendation models, i.e., the training instances contain a large number of false-positive labels (users' feedback). Existing approaches either directly use the auto-feedbacks or heuristically delete the potential false-positive labels. Both of the approaches lead to biased results because the false-positive labels cause the shift of training data distribution, hurting the accuracy of the recommendation models. In this paper, we propose a learning-based counterfactual approach to adjusting the user auto-feedbacks and learning the recommendation models using Neural Dueling Bandit algorithm, called NDB. Specifically, NDB maintains two neural networks: a user attention network for computing the importance weights that are used for modifying the original rewards, and another random network trained with dueling bandit for conducting online recommendations based on the modified rewards. Theoretical analysis showed that the modified rewards are statistically unbiased, and the learned bandit policy enjoys a sub-linear regret bound. Experimental results demonstrated that NDB can significantly outperform the state-of-the-art baselines.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2504–2514},
numpages = {11},
keywords = {dueling bandit, streaming recommendation, user attention bias},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3490099.3511116,
author = {Shibata, Ryoichi and Matsumori, Shoya and Fukuchi, Yosuke and Maekawa, Tomoyuki and Kimoto, Mitsuhiko and Imai, Michita},
title = {Utilizing Core-Query for Context-Sensitive Ad Generation Based on Dialogue},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511116},
doi = {10.1145/3490099.3511116},
abstract = {In this work, we present a system that sequentially generates advertisements within the context of a dialogue. Advertisements tailored to the user have long been displayed on the digital signage in stores, on web pages, and on smartphone applications. Advertisements will work more effectively if they are aware of the context of the dialogue between the users. Creating an advertising sentence as a query and searching the web by using that query is one way to present a variety of advertisements, but there is currently no method to create an appropriate search query for the search in accordance with the dialogue context. Therefore, we developed a method called the Conversational Context-sensitive Advertisement generator (CoCoA). The novelty of CoCoA is that advertisers simply need to prepare a few abstract phrases, called Core-Queries, and then CoCoA dynamically transforms the Core-Queries into complete search queries in accordance with the dialogue context. Here, “transforms” means to add words related to the context in the dialogue to the prepared Core-Queries. The transformation is enabled by a masked word prediction technique that predicts a word that is hidden in a sentence. Our attempt is the first to apply masked word prediction to a web information retrieval framework that takes into account the dialogue context. We asked users to evaluate the search query presented by CoCoA against the dialogue text of multiple domains prepared in advance and found that CoCoA could present more contextual and effective advertisements than Google Suggest or a method without the query transformation. In addition, we found that CoCoA generated high-quality advertisements that advertisers had not expected when they created the Core-Queries.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {734–745},
numpages = {12},
keywords = {context, advertisement, dialogue, mask prediction, query generation},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1613/jair.1.12802,
author = {Song, Linfeng and Xin, Chunlei and Lai, Shaopeng and Wang, Ante and Su, Jinsong and Xu, Kun},
title = {CASA: Conversational Aspect Sentiment Analysis for Dialogue Understanding},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12802},
doi = {10.1613/jair.1.12802},
abstract = {Dialogue understanding has always been a bottleneck for many conversational tasks, such as dialogue response generation and conversational question answering. To expedite the progress in this area, we introduce the task of conversational aspect sentiment analysis (CASA) that can provide useful fine-grained sentiment information for dialogue understanding and planning. Overall, this task extends the standard aspect-based sentiment analysis to the conversational scenario with several major adaptations. To aid the training and evaluation of data-driven methods, we annotate 3,000 chit-chat dialogues (27,198 sentences) with fine-grained sentiment information, including all sentiment expressions, their polarities and the corresponding target mentions. We also annotate an out-of-domain test set of 200 dialogues for robustness evaluation. Besides, we develop multiple baselines based on either pretrained BERT or self-attention for preliminary study. Experimental results show that our BERT-based model has strong performances for both in-domain and out-of-domain datasets, and thorough analysis indicates several potential directions for further improvements.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {23},
keywords = {Fine-grained sentiment analysis, Aspect sentiment analysis, Dialogue modeling, Dialogue understanding}
}

@inproceedings{10.1145/3377325.3377503,
author = {Shrivastava, Akshat and Heer, Jeffrey},
title = {ISeqL: Interactive Sequence Learning},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377503},
doi = {10.1145/3377325.3377503},
abstract = {Exploratory analysis of unstructured text is a difficult task, particularly when defining and extracting domain-specific concepts. We present iSeqL, an interactive tool for the rapid construction of customized text mining models through sequence labeling. With iSeqL, analysts engage in an active learning loop, labeling text instances and iteratively assessing trained models by viewing model predictions in the context of both individual text instances and task-specific visualizations of the full dataset. To build suitable models with limited training data, iSeqL leverages transfer learning and pre-trained contextual word embeddings within a recurrent neural architecture. Through case studies and an online experiment, we demonstrate the use of iSeqL to quickly bootstrap models sufficiently accurate to perform in-depth exploratory analysis. With less than an hour of annotation effort, iSeqL users are able to generate stable outputs over custom extracted entities, including context-sensitive discovery of phrases that were never manually labeled.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {43–54},
numpages = {12},
keywords = {natural language processing, exploratory data analysis, interactive machine learning},
location = {Cagliari, Italy},
series = {IUI '20}
}

@inproceedings{10.1109/ASE.2019.00012,
author = {Wan, Yao and Shu, Jingdong and Sui, Yulei and Xu, Guandong and Zhao, Zhou and Wu, Jian and Yu, Philip S.},
title = {Multi-Modal Attention Network Learning for Semantic Source Code Retrieval},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00012},
doi = {10.1109/ASE.2019.00012},
abstract = {Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query (e.g., a short natural language text describing the functionality for retrieving a particular code snippet). Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results.To tackle the two aforementioned issues, this paper proposes MMAN, a novel <u>M</u>ulti-<u>M</u>odal <u>A</u>ttention <u>N</u>etwork for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {13–25},
numpages = {13},
keywords = {multi-modal network, attention mechanism, code retrieval, deep learning},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3308558.3313496,
author = {Li, Piji and Wang, Zihao and Bing, Lidong and Lam, Wai},
title = {Persona-Aware Tips Generation?},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313496},
doi = {10.1145/3308558.3313496},
abstract = {Tips, as a compacted and concise form of reviews, were paid less attention by researchers. In this paper, we investigate the task of tips generation by considering the “persona” information which captures the intrinsic language style of the users or the different characteristics of the product items. In order to exploit the persona information, we propose a framework based on adversarial variational auto-encoders (aVAE) for persona modeling from the historical tips and reviews of users and items. The latent variables from aVAE are regarded as persona embeddings. Besides representing persona using the latent embeddings, we design a persona memory for storing the persona related words for users and items. Pointer Network is used to retrieve persona wordings from the memory when generating tips. Moreover, the persona embeddings are used as latent factors by a rating prediction component to predict the sentiment of a user over an item. Finally, the persona embeddings and the sentiment information are incorporated into a recurrent neural networks based tips generation component. Extensive experimental results are reported and discussed to elaborate the peculiarities of our framework.},
booktitle = {The World Wide Web Conference},
pages = {1006–1016},
numpages = {11},
keywords = {Persona Modeling, Abstractive Tips Generation, Adversarial Variational Auto-Encoders., Rating Prediction},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313439,
author = {Wang, Chengyu and Fan, Yan and He, Xiaofeng and Zhou, Aoying},
title = {A Family of Fuzzy Orthogonal Projection Models for Monolingual and Cross-Lingual Hypernymy Prediction},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313439},
doi = {10.1145/3308558.3313439},
abstract = {Hypernymy is a semantic relation, expressing the “is-a” relation between a concept and its instances. Such relations are building blocks for large-scale taxonomies, ontologies and knowledge graphs. Recently, much progress has been made for hypernymy prediction in English using textual patterns and/or distributional representations. However, applying such techniques to other languages is challenging due to the high language dependency of these methods and the lack of large training datasets of lower-resourced languages. In this work, we present a family of fuzzy orthogonal projection models for both monolingual and cross-lingual hypernymy prediction. For the monolingual task, we propose a Multi-Wahba Projection (MWP) model to distinguish hypernymy vs. non-hypernymy relations based on word embeddings. This model establishes distributional fuzzy mappings from embeddings of a term to those of its hypernyms and non-hypernyms, which consider the complicated linguistic regularities of these relations. For cross-lingual hypernymy prediction, a Transfer MWP (TMWP) model is proposed to transfer the semantic knowledge from the source language to target languages based on neural word translation. Additionally, an Iterative Transfer MWP (ITMWP) model is built upon TMWP, which augments the training sets of target languages when target languages are lower-resourced with limited training data. Experiments show i) MWP outperforms previous methods over two hypernymy prediction tasks for English; and ii) TMWP and ITMWP are effective to predict hypernymy over seven non-English languages.},
booktitle = {The World Wide Web Conference},
pages = {1965–1976},
numpages = {12},
keywords = {cross-lingual transfer learning, hypernymy prediction, Multi-Wahba Projection},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3290605.3300389,
author = {Lambton-Howard, Daniel and Anderson, Robert and Montague, Kyle and Garbett, Andrew and Hazeldine, Shaun and Alvarez, Carlos and Sweeney, John A. and Olivier, Patrick and Kharrufa, Ahmed and Nappey, Tom},
title = {WhatFutures: Designing Large-Scale Engagements on WhatsApp},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300389},
doi = {10.1145/3290605.3300389},
abstract = {WhatsApp, as the world's most popular messaging application, offers significant opportunities for improving the reach and effectiveness of engagement projects. In collaboration with the International Federation of Red Cross and Red Crescent Societies (IFRC) we designed WhatFutures, a collaborative future forecasting engagement for global youth using WhatsApp. WhatFutures was successfully deployed with 487 players across 5 countries (Kenya, Bulgaria, Finland, Australia and Hong Kong) to inform strategic change within the IFRC. Based on our analysis of the activity - including 16,100 messages, 95 multimedia artifacts, and a post-engagement survey - we present a reflection upon the design decisions underpinning WhatFutures and identify how decisions made around group structures, processes and externalization of outputs influenced engagement and data quality. We conclude with the wider implications of our findings for the design of engagements that best utilize the affordances of existing messaging applications.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {whatsapp, engagement, collaborative content creation},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3287560.3287601,
author = {Celis, L. Elisa and Kapoor, Sayash and Salehi, Farnood and Vishnoi, Nisheeth},
title = {Controlling Polarization in Personalization: An Algorithmic Framework},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287601},
doi = {10.1145/3287560.3287601},
abstract = {Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but -- subject to these constraints -- will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {160–169},
numpages = {10},
keywords = {group fairness, diversification, bandit optimization, recommender systems, Personalization, polarization},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{10.1145/3209978.3209985,
author = {Ai, Qingyao and Bi, Keping and Guo, Jiafeng and Croft, W. Bruce},
title = {Learning a Deep Listwise Context Model for Ranking Refinement},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3209985},
doi = {10.1145/3209978.3209985},
abstract = {Learning to rank has been intensively studied and widely applied in information retrieval. Typically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature space. Inspired by the idea of pseudo relevance feedback where top ranked documents, which we refer as the local ranking context, can provide important information about the query's characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list. Specifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results. There are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods. Experimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpora.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {135–144},
numpages = {10},
keywords = {deep neural network, local ranking context, learning to rank},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3173574.3173800,
author = {Concannon, Shauna Julia and Balaam, Madeline and Simpson, Emma and Comber, Rob},
title = {Applying Computational Analysis to Textual Data from the Wild: A Feminist Perspective},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173800},
doi = {10.1145/3173574.3173800},
abstract = {With technologies that afford much larger-scale data collection than previously imagined, new ways of processing and interpreting qualitative textual data are required. HCI researchers use a range of methods for interpreting the 'full range of human experience' from qualitative data, however, such approaches are not always scalable. Feminist geography seeks to explore how diverse and varied accounts of place can be understood and represented, whilst avoiding reductive classification systems. In this paper, we assess the extent to which unsupervised topic models can support such a research agenda. Drawing on literature from Feminist and Critical GIS, we present a case study analysis of a Volunteered Geographic Information dataset of reviews about breastfeeding in public spaces. We demonstrate that topic modelling can offer novel insights and nuanced interpretations of complex concepts such as privacy and be integrated into a critically reflexive feminist data analysis approach that captures and represents diverse experiences of place.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {topic modelling, text analysis, geodata, feminist gis, feminism, gis, critical gis, data analysis, human-data-interaction},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3178876.3186170,
author = {Lin, Yusan and Yin, Peifeng and Lee, Wang-Chien},
title = {Modeling Dynamic Competition on Crowdfunding Markets},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186170},
doi = {10.1145/3178876.3186170},
abstract = {The often fierce competition on crowdfunding markets can significantly affect project success. While various factors have been considered in predicting the success of crowdfunding projects, to the best knowledge of the authors, the phenomenon of competition has not been investigated. In this paper, we study the competition on crowdfunding markets through data analysis, and propose a probabilistic generative model, Dynamic Market Competition (DMC) model, to capture the competitiveness of projects in crowdfunding. Through an empirical evaluation using the pledging history of past crowdfunding projects, our approach has shown to capture the competitiveness of projects very well, and significantly outperforms several baseline approaches in predicting the daily collected funds of crowdfunding projects, reducing errors by 31.73% to 45.14%. In addition, our analyses on the correlations between project competitiveness, project design factors, and project success indicate that highly competitive projects, while being winners under various setting of project design factors, are particularly impressive with high pledging goals and high price rewards, comparing to medium and low competitive projects. Finally, the competitiveness of projects learned by DMC is shown to be very useful in applications of predicting final success and days taken to hit pledging goal, reaching 85% accuracy and error of less than 7 days, respectively, with limited information at early pledging stage.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1815–1824},
numpages = {10},
keywords = {crowdfunding, market competition, business competitiveness, probabilistic generative model},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1145/3423208,
author = {Bailey, Shawn and Zhang, Yue and Ramesh, Arti and Golbeck, Jennifer and Getoor, Lise},
title = {A Structured and Linguistic Approach to Understanding Recovery and Relapse in AA},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3423208},
doi = {10.1145/3423208},
abstract = {Alcoholism, also known as Alcohol Use Disorder (AUD), is a serious problem affecting millions of people worldwide. Recovery from AUD is known to be challenging and often leads to relapse at various points after enrolling in a rehabilitation program such as Alcoholics Anonymous (AA). In this work, we present a structured and linguistic approach using hinge-loss Markov random fields (HL-MRFs) to understand recovery and relapse from AUD using social media data. We evaluate our models on AA-attending users extracted from: (i) the Twitter social network and predict recovery at two different points—90 days and 1 year after the user joins AA, respectively, and (ii) the Reddit AA recovery forums and predict whether the participating user is currently sober. The two datasets present two facets of the same underlying problem of understanding recovery and relapse in AUD users. We flesh out different characteristics in both these datasets: (i) In the Twitter dataset, we focus on the social aspect of the users and the relationship with recovery and relapse, and (ii) in the Reddit dataset, we focus on modeling the linguistic topics and dependency structure to understand users’ recovery journey. We design a unified modeling framework using HL-MRFs that takes the different characteristics of both these platforms into account. Our experiments reveal that our structured and linguistic approach is helpful in predicting recovery in users in both these datasets. We perform extensive quantitative analysis of different groups of features and dependencies among them in both datasets. The interpretable and intuitive nature of our models and analysis is helpful in making meaningful predictions and can potentially be helpful in identifying and preventing relapse early.},
journal = {ACM Trans. Web},
month = {nov},
articleno = {5},
numpages = {35},
keywords = {probabilistic graphical models, Social media analysis, alcoholics anonymous, modeling recovery from alcoholism}
}

@article{10.1145/3409383,
author = {Wang, Hao and Yeung, Dit-Yan},
title = {A Survey on Bayesian Deep Learning},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3409383},
doi = {10.1145/3409383},
abstract = {A comprehensive artificial intelligence system needs to not only perceive the environment with different “senses” (e.g., seeing and hearing) but also infer the world’s conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks, such as visual object recognition and speech recognition, using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models.1 In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and, in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, and so on. We also discuss the relationship and differences between Bayesian deep learning and other related topics, such as Bayesian treatment of neural networks.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {108},
numpages = {37},
keywords = {Deep learning, Bayesian networks, generative models, probabilistic graphical models}
}

@inproceedings{10.1145/3514221.3517906,
author = {Suhara, Yoshihiko and Li, Jinfeng and Li, Yuliang and Zhang, Dan and Demiralp, \c{C}a\u{g}atay and Chen, Chen and Tan, Wang-Chiew},
title = {Annotating Columns with Pre-Trained Language Models},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517906},
doi = {10.1145/3514221.3517906},
abstract = {Inferring meta information about tables, such as column headers or relationships between columns, is an active research topic in data management as we find many tables are missing some of this information. In this paper, we study the problem of annotating table columns (i.e., predicting column types and the relationships between columns) using only information from the table itself. We develop a multi-task learning framework (called Doduo) based on pre-trained language models, which takes the entire table as input and predicts column types/relations using a single model. Experimental results show that Doduo establishes new state-of-the-art performance on two benchmarks for the column type prediction and column relation prediction tasks with up to 4.0% and 11.9% improvements, respectively. We report that Doduo can already outperform the previous state-of-the-art performance with a minimal number of tokens, only 8 tokens per column. We release a toolbox (https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo on a real-world data science problem through a case study.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1493–1503},
numpages = {11},
keywords = {language models, table understanding, multi-task learning},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3382494.3410690,
author = {Di Rocco, Juri and Di Ruscio, Davide and Di Sipio, Claudio and Nguyen, Phuong and Rubei, Riccardo},
title = {TopFilter: An Approach to Recommend Relevant GitHub Topics},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410690},
doi = {10.1145/3382494.3410690},
abstract = {Background: In the context of software development, GitHub has been at the forefront of platforms to store, analyze and maintain a large number of software repositories. Topics have been introduced by GitHub as an effective method to annotate stored repositories. However, labeling GitHub repositories should be carefully conducted to avoid adverse effects on project popularity and reachability. Aims: We present TopFilter, a novel approach to assist open source software developers in selecting suitable topics for GitHub repositories being created. Method: We built a project-topic matrix and applied a syntactic-based similarity function to recommend missing topics by representing repositories and related topics in a graph. The ten-fold cross-validation methodology has been used to assess the performance of TopFilter by considering different metrics, i.e., success rate, precision, recall, and catalog coverage. Result: The results show that TopFilter recommends good topics depending on different factors, i.e., collaborative filtering settings, considered datasets, and pre-processing activities. Moreover, TopFilter can be combined with a state-of-the-art topic recommender system (i.e., MNB network) to improve the overall prediction performance. Conclusion: Our results confirm that collaborative filtering techniques can successfully be used to provide relevant topics for GitHub repositories. Moreover, TopFilter can gain a significant boost in prediction performances by employing the outcomes obtained by the MNB network as its initial set of topics.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {21},
numpages = {11},
keywords = {Recommender systems, Collaborative filtering, GitHub topics recommendation},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3383583.3398542,
author = {Wang, Xinyue and Xie, Zhiwu},
title = {The Case For Alternative Web Archival Formats To Expedite The Data-To-Insight Cycle},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398542},
doi = {10.1145/3383583.3398542},
abstract = {The WARC file format is widely used by web archives to preserve collected web content for future use. With the rapid growth of web archives and the increasing interest to reuse these archives as big data sources for statistical and analytical research, the speed to turn these data into insights becomes critical. In this paper we show that the WARC format carries significant performance penalties for batch processing workload. We trace the root cause of these penalties to its data structure, encoding, and addressing method. We then run controlled experiments to illustrate how severe these problems can be. Indeed, performance gain of one to two orders of magnitude can be achieved simply by reformatting WARC files into Parquet or Avro formats. While these results do not necessarily constitute an endorsement for Avro or Parquet, the time has come for the web archiving community to consider replacing WARC with more efficient web archival formats.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {177–186},
numpages = {10},
keywords = {file format, web archiving, big data analysis, storage management},
location = {Virtual Event, China},
series = {JCDL '20}
}

@article{10.1109/TASLP.2019.2937190,
author = {Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun},
title = {Neural Machine Translation With Sentence-Level Topic Context},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937190},
doi = {10.1109/TASLP.2019.2937190},
abstract = {Traditional neural machine translation NMT methods use the word-level context to predict target language translation while neglecting the sentence-level context, which has been shown to be beneficial for translation prediction in statistical machine translation. This paper represents the sentence-level context as latent topic representations by using a convolution neural network, and designs a topic attention to integrate source sentence-level topic context information into both attention-based and Transformer-based NMT. In particular, our method can improve the performance of NMT by modeling source topics and translations jointly. Experiments on the large-scale LDC Chinese-to-English translation tasks and WMT’14 English-to-German translation tasks show that the proposed approach can achieve significant improvements compared with baseline systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {1970–1984},
numpages = {15}
}

@inproceedings{10.1145/3357384.3358017,
author = {Liu, Huafeng and Wen, Jingxuan and Jing, Liping and Yu, Jian and Zhang, Xiangliang and Zhang, Min},
title = {In2Rec: Influence-Based Interpretable Recommendation},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358017},
doi = {10.1145/3357384.3358017},
abstract = {Interpretability of recommender systems has caused increasing attention due to its promotion of the effectiveness and persuasiveness of recommendation decision, and thus user satisfaction. Most existing methods, such as Matrix Factorization (MF), tend to be black-box machine learning models that lack interpretability and do not provide a straightforward explanation for their outputs. In this paper, we focus on probabilistic factorization model and further assume the absence of any auxiliary information, such as item content or user review. We propose an influence mechanism to evaluate the importance of the users' historical data, so that the most related users and items can be selected to explain each predicted rating. The proposed method is thus called Influencebased Interpretable Recommendation model (In2Rec). To further enhance the recommendation accuracy, we address the important issue of missing not at random, i.e., missing ratings are not independent from the observed and other unobserved ratings, because users tend to only interact what they like. In2Rec models the generative process for both observed and missing data, and integrates the influence mechanism in a Bayesian graphical model. A learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to maximum a posteriori estimation for In2Rec. A series of experiments on four real-world datasets (Movielens 10M, Netflix, Epinions, and Yelp) have been conducted. By comparing with the state-of-the-art recommendation methods, the experimental results have shown that In2Rec can consistently benefit the recommendation system in both rating prediction and ranking estimation tasks, and friendly interpret the recommendation results with the aid of the proposed influence mechanism.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1803–1812},
numpages = {10},
keywords = {recommendation system, collaborative filtering, interpretable recommendation, probabilistic matrix factorization},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3351108.3351134,
author = {Cornelissen, Laurenz A. and Daly, Lucia I. and Sinandile, Qhama and de Lange, Heinrich and Barnett, Richard J.},
title = {A Computational Analysis of News Media Bias: A South African Case Study},
year = {2019},
isbn = {9781450372657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351108.3351134},
doi = {10.1145/3351108.3351134},
abstract = {News media in South Africa is assumed to be unbiased and objective in their reporting of the news. Indeed, editors are required to uphold an objective and balanced view with no favour to external political or corporate interests. This assumption of objectivity is tested on a large scale by computationally analysing 30 000 articles published by five media houses: News24, SABC, EWN, ENCA, and IOL. Using topic modelling, 38 topics are extracted from the corpus, and sentiment is computed for each topic. The study highlights various cases of both over and under-reporting by media houses on particular topics. We also identify various tonality biases by media houses.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019},
articleno = {25},
numpages = {10},
keywords = {south-african politics, NLP, media bias, topic modelling, news},
location = {Skukuza, South Africa},
series = {SAICSIT '19}
}

@inproceedings{10.1145/3196321.3196335,
author = {Zhou, Cheng and Li, Bin and Sun, Xiaobing and Guo, Hongjing},
title = {Recognizing Software Bug-Specific Named Entity in Software Bug Repository},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196335},
doi = {10.1145/3196321.3196335},
abstract = {Software bug issues are unavoidable in software development and maintenance. In order to manage bugs effectively, bug tracking systems are developed to help to record, manage and track the bugs of each project. The rich information in the bug repository provides the possibility of establishment of entity-centric knowledge bases to help understand and fix the bugs. However, existing named entity recognition (NER) systems deal with text that is structured, formal, well written, with a good grammatical structure and few spelling errors, which cannot be directly used for bug-specific named entity recognition. For bug data, they are free-form texts, which include a mixed language studded with code, abbreviations and software-specific vocabularies. In this paper, we summarize the characteristics of bug entities, propose a classification method for bug entities, and build a baseline corpus on two open source projects (Mozilla and Eclipse). On this basis, we propose an approach for bug-specific entity recognition called BNER with the Conditional Random Fields (CRF) model and word embedding technique. An empirical study is conducted to evaluate the accuracy of our BNER technique, and the results show that the two designed baseline corpus are suitable for bug-specific named entity recognition, and our BNER approach is effective on cross-projects NER.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {108–119},
numpages = {12},
keywords = {software bug corpus, software bug, word embedding, CRF model, named entity recognition},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3196321.3196322,
author = {Moran, Kevin and Bernal-C\'{a}rdenas, Carlos and Linares-V\'{a}squez, Mario and Poshyvanyk, Denys},
title = {Overcoming Language Dichotomies: Toward Effective Program Comprehension for Mobile App Development},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196322},
doi = {10.1145/3196321.3196322},
abstract = {Mobile devices and platforms have become an established target for modern software developers due to performant hardware and a large and growing user base numbering in the billions. Despite their popularity, the software development process for mobile apps comes with a set of unique, domain-specific challenges rooted in program comprehension. Many of these challenges stem from developer difficulties in reasoning about different representations of a program, a phenomenon we define as a "language dichotomy". In this paper, we reflect upon the various language dichotomies that contribute to open problems in program comprehension and development for mobile apps. Furthermore, to help guide the research community towards effective solutions for these problems, we provide a roadmap of directions for future work.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {7–18},
numpages = {12},
keywords = {code, program comprehension, Android, natural language, mobile},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3167132.3167274,
author = {Cucchiarelli, Alessandro and Morbidoni, Christian and Stilo, Giovanni and Velardi, Paola},
title = {What to Write and Why: A Recommender for News Media},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167274},
doi = {10.1145/3167132.3167274},
abstract = {The way in which people acquire information on events and form their own opinion on them has changed dramatically with the advent of social media. For many readers, the news gathered from online sources become an opportunity to share points of view and information within micro-blogging platforms such as Twitter, mainly aimed at satisfying their communication needs. Furthermore, the need to deepen the aspects related to news stimulates a demand for additional information which is often met through online encyclopedias, such as Wikipedia. This behavior has also influenced the way in which journalists write their articles, requiring a careful assessment of what actually interests the readers. The goal of this paper is to present a recommender system, What To Write and Why, capable of suggesting to a journalist, for a given event, the aspects still uncovered in news articles on which the readers focus their interest. The basic idea is to characterize an event according to the echo it receives in online news sources and associate it with the corresponding readers' communicative and informative patterns, detected through the analysis of Twitter and Wikipedia, respectively. Our methodology temporally aligns the results of this analysis and recommends the concepts that emerge as topics of interest from Twitter and Wikipedia, either not covered or poorly covered in the published news articles.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1321–1330},
numpages = {10},
keywords = {event detection, recommender systems, social networks, on line news, temporal mining, Twitter, wikipedia},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3168828,
author = {Ros\`{a}, Andrea and Rosales, Eduardo and Binder, Walter},
title = {Analyzing and Optimizing Task Granularity on the JVM},
year = {2018},
isbn = {9781450356176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168828},
doi = {10.1145/3168828},
abstract = {Task granularity, i.e., the amount of work performed by parallel tasks, is a key performance attribute of parallel applications. On the one hand, fine-grained tasks (i.e., small tasks carrying out few computations) may introduce considerable parallelization overheads. On the other hand, coarse-grained tasks (i.e., large tasks performing substantial computations) may not fully utilize the available CPU cores, resulting in missed parallelization opportunities. In this paper, we provide a better understanding of task granularity for applications running on a Java Virtual Machine. We present a novel profiler which measures the granularity of every executed task. Our profiler collects carefully selected metrics from the whole system stack with only little overhead, and helps the developer locate performance problems. We analyze task granularity in the DaCapo and ScalaBench benchmark suites, revealing several inefficiencies related to fine-grained and coarse-grained tasks. We demonstrate that the collected task-granularity profiles are actionable by optimizing task granularity in two benchmarks, achieving speedups up to 1.53x.},
booktitle = {Proceedings of the 2018 International Symposium on Code Generation and Optimization},
pages = {27–37},
numpages = {11},
keywords = {parallel applications, actionable profiler, task granularity, Java Virtual Machine, performance analysis},
location = {Vienna, Austria},
series = {CGO 2018}
}

@inproceedings{10.1145/3132847.3133016,
author = {Proskurnia, Julia and Mavlyutov, Ruslan and Castillo, Carlos and Aberer, Karl and Cudr\'{e}-Mauroux, Philippe},
title = {Efficient Document Filtering Using Vector Space Topic Expansion and Pattern-Mining: The Case of Event Detection in Microposts},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133016},
doi = {10.1145/3132847.3133016},
abstract = {Automatically extracting information from social media is challenging given that social content is often noisy, ambiguous, and inconsistent. However, as many stories break on social channels first before being picked up by mainstream media, developing methods to better handle social content is of utmost importance. In this paper, we propose a robust and effective approach to automatically identify microposts related to a specific topic defined by a small sample of reference documents. Our framework extracts clusters of semantically similar microposts that overlap with the reference documents, by extracting combinations of key features that define those clusters through frequent pattern mining. This allows us to construct compact and interpretable representations of the topic, dramatically decreasing the computational burden compared to classical clustering and k-NN-based machine learning techniques and producing highly-competitive results even with small training sets (less than 1'000 training objects). Our method is efficient and scales gracefully with large sets of incoming microposts. We experimentally validate our approach on a large corpus of over 60M microposts, showing that it significantly outperforms state-of-the-art techniques.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {457–466},
numpages = {10},
keywords = {frequent patterns mining, event detection, semantic attributes, microposts},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3445794,
author = {Zhu, Junnan and Xiang, Lu and Zhou, Yu and Zhang, Jiajun and Zong, Chengqing},
title = {Graph-Based Multimodal Ranking Models for Multimodal Summarization},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3445794},
doi = {10.1145/3445794},
abstract = {Multimodal summarization aims to extract the most important information from the multimedia input. It is becoming increasingly popular due to the rapid growth of multimedia data in recent years. There are various researches focusing on different multimodal summarization tasks. However, the existing methods can only generate single-modal output or multimodal output. In addition, most of them need a lot of annotated samples for training, which makes it difficult to be generalized to other tasks or domains. Motivated by this, we propose a unified framework for multimodal summarization that can cover both single-modal output summarization and multimodal output summarization. In our framework, we consider three different scenarios and propose the respective unsupervised graph-based multimodal summarization models without the requirement of any manually annotated document-summary pairs for training: (1) generic multimodal ranking, (2) modal-dominated multimodal ranking, and (3) non-redundant text-image multimodal ranking. Furthermore, an image-text similarity estimation model is introduced to measure the semantic similarity between image and text. Experiments show that our proposed models outperform the single-modal summarization methods on both automatic and human evaluation metrics. Besides, our models can also improve the single-modal summarization with the guidance of the multimedia information. This study can be applied as the benchmark for further study on multimodal summarization task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {60},
numpages = {21},
keywords = {single-modal, multimodal ranking, Multimodal summarization, unsupervised}
}

@inproceedings{10.1145/3292522.3326031,
author = {Zabihimayvan, Mahdieh and Sadeghi, Reza and Doran, Derek and Allahyari, Mehdi},
title = {A Broad Evaluation of the Tor English Content Ecosystem},
year = {2019},
isbn = {9781450362023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292522.3326031},
doi = {10.1145/3292522.3326031},
abstract = {Tor is among most well-known dark net in the world. It has noble uses, including as a platform for free speech and information dissemination under the guise of true anonymity, but may be culturally better known as a conduit for criminal activity and as a platform to market illicit goods and data. Past studies on the content of Tor support this notion, but were carried out by targeting popular domains likely to contain illicit content. A survey of past studies may thus not yield a complete evaluation of the content and use of Tor. This work addresses this gap by presenting a broad evaluation of the content of the English Tor ecosystem. We perform a comprehensive crawl of the Tor dark web and, through topic and network analysis, characterize the 'types' of information and services hosted across a broad swath of Tor domains and their hyperlink relational structure. We recover nine domain types defined by the information or service they host and, among other findings, unveil how some types of domains intentionally silo themselves from the rest of Tor. We also present measurements that (regrettably) suggest how marketplaces of illegal drugs and services do emerge as the dominant type of Tor domain. Our study is the product of crawling over 1 million pages from 20,000 Tor seed addresses, yielding a collection of over 150,000 Tor pages. The domain structure is publicly available as a dataset at urlhttps://github.com/wsu-wacs/TorEnglishContent.},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {333–342},
numpages = {10},
keywords = {content analysis, tor, structural analysis},
location = {Boston, Massachusetts, USA},
series = {WebSci '19}
}

@inproceedings{10.1145/3209978.3210040,
author = {Wu, Zhuofeng and Li, Cheng and Zhao, Zhe and Wu, Fei and Mei, Qiaozhu},
title = {Identify Shifts of Word Semantics through Bayesian Surprise},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210040},
doi = {10.1145/3209978.3210040},
abstract = {Much work has been done recently on learning word embeddings from large corpora, which attempts to find the coordinates of words in a static and high dimensional semantic space. In reality, such corpora often span a sufficiently long time period, during which the meanings of many words may have changed. The co-evolution of word meanings may also result in a distortion of the semantic space, making these static embeddings unable to accurately represent the dynamics of semantics. In this paper, we present a novel computational method to capture such changes and to model the evolution of word semantics. Distinct from existing approaches that learn word embeddings independently from time periods and then align them, our method explicitly establishes the stable topological structure of word semantics and identifies the surprising changes in the semantic space over time through a principled statistical method. Empirical experiments on large-scale real-world corpora demonstrate the effectiveness of the proposed approach, which outperforms the state-of-the-art by a large margin.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {825–834},
numpages = {10},
keywords = {word embeddings, bayesian surprise, semantic shifts},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@article{10.1145/3152463,
author = {Wen, Jiqing and She, James and Li, Xiaopeng and Mao, Hui},
title = {Visual Background Recommendation for Dance Performances Using Deep Matrix Factorization},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3152463},
doi = {10.1145/3152463},
abstract = {The stage background is one of the most important features for a dance performance, as it helps to create the scene and atmosphere. In conventional dance performances, the background images are usually selected or designed by professional stage designers according to the theme and the style of the dance. In new media dance performances, the stage effects are usually generated by media editing software. Selecting or producing a dance background is quite challenging and is generally carried out by skilled technicians. The goal of the research reported in this article is to ease this process. Instead of searching for background images from the sea of available resources, dancers are recommended images that they are more likely to use. This work proposes the idea of a novel system to recommend images based on content-based social computing. The core part of the system is a probabilistic prediction model to predict a dancer’s interests in candidate images through social platforms. Different from traditional collaborative filtering or content-based models, the model proposed here effectively combines a dancer’s social behaviors (rating action, click action, etc.) with the visual content of images shared by the dancer using deep matrix factorization (DMF). With the help of such a system, dancers can select from the recommended images and set them as the backgrounds of their dance performances through a media editor. According to the experiment results, the proposed DMF model outperforms the previous methods, and when the dataset is very sparse, the proposed DMF model shows more significant results.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {11},
numpages = {19},
keywords = {image recommendation, dance background, content-based social computing, Interactive dance}
}

@inproceedings{10.1145/3078714.3078725,
author = {Duan, Yijun and Jatowt, Adam and Tanaka, Katsumi},
title = {Discovering Typical Histories of Entities by Multi-Timeline Summarization},
year = {2017},
isbn = {9781450347082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078714.3078725},
doi = {10.1145/3078714.3078725},
abstract = {Categorization is a common solution used for organizing entities. For example, there are over 1.13 million categories in Wikipedia which group various types of entities such as persons, locations, etc. What is however often lacking when it comes to understanding categories is a clear information about the common aspects of the entities in a given category, for example, information on their shared histories. We propose in this paper a novel task of automatically creating summaries of typical histories of entities within their categories (e.g., a typical history of a Japanese city). The output summary is in the form of key representative events together with the information on their average dates. We introduce 4 methods for the aforementioned task and evaluate them on Wikipedia categories containing several types of cities and persons. The summaries we generate can provide information on the common evolution of entities falling into the same category as well as they can be compared with the summaries of related categories for providing contrastive type of knowledge.},
booktitle = {Proceedings of the 28th ACM Conference on Hypertext and Social Media},
pages = {105–114},
numpages = {10},
keywords = {wikipedia, digital history, typicality, entity summarization},
location = {Prague, Czech Republic},
series = {HT '17}
}

@article{10.1145/3015463,
author = {Yang, Zhenguo and Li, Qing and Lu, Zheng and Ma, Yun and Gong, Zhiguo and Liu, Wenyin},
title = {Dual Structure Constrained Multimodal Feature Coding for Social Event Detection from Flickr Data},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3015463},
doi = {10.1145/3015463},
abstract = {In this work, a three-stage social event detection (SED) framework is proposed to discover events from Flickr-like data. First, multiple bipartite graphs are constructed for the heterogeneous feature modalities to achieve fused features. Furthermore, considering the geometrical structures of dictionary and data, a dual structure constrained multimodal feature coding model is designed to learn discriminative feature codes by incorporating corresponding regularization terms into the objective. Finally, clustering models utilizing density or label knowledge and data recovery residual models are devised to discover real-world events. The proposed SED approach achieves the highest performance on the MediaEval 2014 SED dataset.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {19},
numpages = {20},
keywords = {Social multimedia analytics, multimedia content analysis, feature coding, multimodal fusion, event detection}
}

@inproceedings{10.1145/3485447.3512084,
author = {Gourru, Antoine and Velcin, Julien and Gravier, Christophe and Jacques, Julien},
title = {Dynamic Gaussian Embedding of Authors},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512084},
doi = {10.1145/3485447.3512084},
abstract = {Authors publish documents in a dynamic manner. Their topic of interest and writing style might shift over time. Tasks such as author classification, author identification or link prediction are difficult to solve in such complex data settings. We propose a new representation learning model, DGEA (for Dynamic Gaussian Embedding of Authors), that is more suited to solve these tasks by capturing this temporal evolution. We formulate a general embedding framework: author representation at time t is a Gaussian distribution that leverages pre-trained document vectors, and that depends on the publications observed until t. The representations should retain some form of multi-topic information and temporal smoothness. We propose two models that fit into this framework. The first one, K-DGEA, uses a first order Markov model optimized with an Expectation Maximization Algorithm with Kalman Equations. The second, R-DGEA, makes use of a Recurrent Neural Network to model the time dependence. We evaluate our method on several quantitative tasks: author identification, classification, and co-authorship prediction, on two datasets written in English. In addition, our model is language agnostic since it only requires pre-trained document embeddings. It outperforms existing baselines by up to 18% on an author classification task on a news articles dataset.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2109–2119},
numpages = {11},
keywords = {Author Embedding, Document Embedding, Representation Learning, Dynamic Gaussian Embedding},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3459637.3482450,
author = {Pang, Liang and Lan, Yanyan and Cheng, Xueqi},
title = {Match-Ignition: Plugging PageRank into Transformer for Long-Form Text Matching},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482450},
doi = {10.1145/3459637.3482450},
abstract = {Neural text matching models have been widely used in community question answering, information retrieval, and dialogue. However, these models designed for short texts cannot well address the long-form text matching problem, because there are many contexts in long-form texts can not be directly aligned with each other, and it is difficult for existing models to capture the key matching signals from such noisy data. Besides, these models are computationally expensive for simply use all textual data indiscriminately. To tackle the effectiveness and efficiency problem, we propose a novel hierarchical noise filtering model, namely Match-Ignition. The main idea is to plug the well-known PageRank algorithm into the Transformer, to identify and filter both sentence and word level noisy information in the matching process. Noisy sentences are usually easy to detect because previous work has shown that their similarity can be explicitly evaluated by the word overlapping, so we directly use PageRank to filter such information based on a sentence similarity graph. Unlike sentences, words rely on their contexts to express concrete meanings, so we propose to jointly learn the filtering and matching process, to well capture the critical word-level matching signals. Specifically, a word graph is first built based on the attention scores in each self-attention block of Transformer, and key words are then selected by applying PageRank on this graph. In this way, noisy words will be filtered out layer by layer in the matching process. Experimental results show that Match-Ignition outperforms both SOTA short text matching models and recent long-form text matching models. We also conduct detailed analysis to show that Match-Ignition efficiently captures important sentences and words, to facilitate the long-form text matching process.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1396–1405},
numpages = {10},
keywords = {text matching, long-form text, pagerank algorithm},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3406522.3446020,
author = {Medlar, Alan and Li, Jing and G\l{}owacka, Dorota},
title = {Query Suggestions as Summarization in Exploratory Search},
year = {2021},
isbn = {9781450380553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406522.3446020},
doi = {10.1145/3406522.3446020},
abstract = {Query suggestions have been shown to benefit users performing information retrieval tasks. In exploratory search, however, users may lack the necessary domain knowledge to assess the relevance of query suggestions with respect to their information needs. In this article, we investigate the use of alternative queries in exploratory search. Alternative queries are queries that would retrieve similar search results to those currently visible on-screen. They are independent of the original search query and can, therefore, be updated dynamically as users scroll through search results. In addition to being follow-on queries, alternative queries serve as keyword summaries of the current search results page to help users assess whether results are inline with their search intents. We investigated the use of alternative queries in scientific literature search and their impact on user behavior and perception. In a user study, participants inspected half as many documents per query when alternative queries were present, but were exposed to over 40% more search results overall. Despite using them extensively as follow-on queries, user feedback focused on the summarization properties offered by alternative queries; finding it reassuring that documents were relevant to their search goals.},
booktitle = {Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
pages = {119–128},
numpages = {10},
keywords = {exploratory search, query suggestions, alternative queries, summarization, scientific literature search},
location = {Canberra ACT, Australia},
series = {CHIIR '21}
}

@article{10.1145/3379561,
author = {Yang, Zhong and Zheng, Bolong and Li, Guohui and Hung, Nguyen Quoc Viet and Liu, Guanfeng and Zheng, Kai},
title = {Searching Activity Trajectories by Exemplar},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3379561},
doi = {10.1145/3379561},
abstract = {The rapid explosion of urban cities has modernized the residents’ lives and generated a large amount of data (e.g., human mobility data, traffic data, and geographical data), especially the activity trajectory data that contains spatial and temporal as well as activity information. With these data, urban computing enables to provide better services such as location-based applications for smart cities. Recently, a novel exemplar query paradigm becomes popular that considers a user query as an example of the data of interest, which plays an important role in dealing with the information deluge. In this article, we propose a novel query, called searching activity trajectory by exemplar, where, given an exemplar trajectory τq, the goal is to find the top-k trajectories with the smallest distances to τq. We first introduce an inverted-index-based algorithm (ILA) using threshold ranking strategy. To further improve the efficiency, we propose a gridtree threshold approach (GTA) to quickly locate candidates and prune unnecessary trajectories. In addition, we extend GTA to support parallel processing. Finally, extensive experiments verify the high efficiency and scalability of the proposed algorithms.},
journal = {ACM/IMS Trans. Data Sci.},
month = {sep},
articleno = {19},
numpages = {18},
keywords = {query processing, Spatio-temporal trajectory, exemplar query, trajectorys similarity, activity trajectory}
}

@inproceedings{10.1109/ASE.2019.00042,
author = {Chen, Junjie and He, Xiaoting and Lin, Qingwei and Zhang, Hongyu and Hao, Dan and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei},
title = {Continuous Incident Triage for Large-Scale Online Service Systems},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00042},
doi = {10.1109/ASE.2019.00042},
abstract = {In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {364–375},
numpages = {12},
keywords = {online service systems, deep learning, incident triage},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.5555/3370272.3370278,
author = {Noei, Ehsan and Lyons, Kelly},
title = {A Survey of Utilizing User-Reviews Posted on Google Play Store},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Mobile application (app) markets, such as Google Play Store, provide a rating mechanism for users to rate the hosted apps and leave comments and feedback (i.e., user-reviews). User-reviews contain valuable information, such as bug reports, feature requests, and user experiences. Recent studies have shown the unavoidable impact of studying users' feedback on the success of an app, whereas ignoring users' feedback can endanger the survival of an app in an app market. In this paper, we survey the research papers and solutions that can help developers and researchers to utilize user-reviews and integrate them into the app development process. We provide an overview of each work, briefly explain their applications, and finally mention the limitations. Moreover, derived from the existing body of research, we provide a guideline for researchers and developers, showing them how to collect, preprocess, and analyze user-reviews. Finally, we conclude the survey and provide directions for future research.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {54–63},
numpages = {10},
keywords = {software maintenance, user-review, crowdsourcing, data mining, mobile application},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3025453.3025902,
author = {Towne, W. Ben and Ros\'{e}, Carolyn P. and Herbsleb, James D.},
title = {Conflict in Comments: Learning but Lowering Perceptions, with Limits},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025902},
doi = {10.1145/3025453.3025902},
abstract = {Prior work and perception theory suggests that when exposed to discussion related to a particular piece of crowdsourced text content, readers generally perceive that content to be of lower quality than readers who do not see those comments, and that the effect is stronger if the comments display conflict. This paper presents a controlled experiment with over 1000 participants testing to see if this effect carries over to other documents from the same platform, including those with similar content or by the same author. Although we do generally find that perceived quality of the commented-on document is affected, effects do not carry over to the second item and readers are able to judge the second in isolation from the comment on the first. We confirm a prior finding about the negative effects conflict can have on perceived quality but note that readers report learning more from constructive conflict comments.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {655–666},
numpages = {12},
keywords = {experiment, validation, crowdsourcing, distributed evaluation, social influence, creative work, comments},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/3038912.3052716,
author = {Singer, Philipp and Lemmerich, Florian and West, Robert and Zia, Leila and Wulczyn, Ellery and Strohmaier, Markus and Leskovec, Jure},
title = {Why We Read Wikipedia},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052716},
doi = {10.1145/3038912.3052716},
abstract = {Wikipedia is one of the most popular sites on the Web, with millions of users relying on it to satisfy a broad range of information needs every day. Although it is crucial to understand what exactly these needs are in order to be able to meet them, little is currently known about why users visit Wikipedia. The goal of this paper is to fill this gap by combining a survey of Wikipedia readers with a log-based analysis of user activity. Based on an initial series of user surveys, we build a taxonomy of Wikipedia use cases along several dimensions, capturing users' motivations to visit Wikipedia, the depth of knowledge they are seeking, and their knowledge of the topic of interest prior to visiting Wikipedia. Then, we quantify the prevalence of these use cases via a large-scale user survey conducted on live Wikipedia with almost 30,000 responses. Our analyses highlight the variety of factors driving users to Wikipedia, such as current events, media coverage of a topic, personal curiosity, work or school assignments, or boredom. Finally, we match survey responses to the respondents' digital traces in Wikipedia's server logs, enabling the discovery of behavioral patterns associated with specific use cases. For instance, we observe long and fast-paced page sequences across topics for users who are bored or exploring randomly, whereas those using Wikipedia for work or school spend more time on individual articles focused on topics such as science. Our findings advance our understanding of reader motivations and behavior on Wikipedia and can have implications for developers aiming to improve Wikipedia's user experience, editors striving to cater to their readers' needs, third-party services (such as search engines) providing access to Wikipedia content, and researchers aiming to build tools such as recommendation engines.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1591–1600},
numpages = {10},
keywords = {motivation, log analysis, wikipedia, survey},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3485447.3512084,
author = {Gourru, Antoine and Velcin, Julien and Gravier, Christophe and Jacques, Julien},
title = {Dynamic Gaussian Embedding of Authors},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512084},
doi = {10.1145/3485447.3512084},
abstract = {Authors publish documents in a dynamic manner. Their topic of interest and writing style might shift over time. Tasks such as author classification, author identification or link prediction are difficult to solve in such complex data settings. We propose a new representation learning model, DGEA (for Dynamic Gaussian Embedding of Authors), that is more suited to solve these tasks by capturing this temporal evolution. We formulate a general embedding framework: author representation at time t is a Gaussian distribution that leverages pre-trained document vectors, and that depends on the publications observed until t. The representations should retain some form of multi-topic information and temporal smoothness. We propose two models that fit into this framework. The first one, K-DGEA, uses a first order Markov model optimized with an Expectation Maximization Algorithm with Kalman Equations. The second, R-DGEA, makes use of a Recurrent Neural Network to model the time dependence. We evaluate our method on several quantitative tasks: author identification, classification, and co-authorship prediction, on two datasets written in English. In addition, our model is language agnostic since it only requires pre-trained document embeddings. It outperforms existing baselines by up to 18% on an author classification task on a news articles dataset.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2109–2119},
numpages = {11},
keywords = {Author Embedding, Document Embedding, Representation Learning, Dynamic Gaussian Embedding},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3459637.3482450,
author = {Pang, Liang and Lan, Yanyan and Cheng, Xueqi},
title = {Match-Ignition: Plugging PageRank into Transformer for Long-Form Text Matching},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482450},
doi = {10.1145/3459637.3482450},
abstract = {Neural text matching models have been widely used in community question answering, information retrieval, and dialogue. However, these models designed for short texts cannot well address the long-form text matching problem, because there are many contexts in long-form texts can not be directly aligned with each other, and it is difficult for existing models to capture the key matching signals from such noisy data. Besides, these models are computationally expensive for simply use all textual data indiscriminately. To tackle the effectiveness and efficiency problem, we propose a novel hierarchical noise filtering model, namely Match-Ignition. The main idea is to plug the well-known PageRank algorithm into the Transformer, to identify and filter both sentence and word level noisy information in the matching process. Noisy sentences are usually easy to detect because previous work has shown that their similarity can be explicitly evaluated by the word overlapping, so we directly use PageRank to filter such information based on a sentence similarity graph. Unlike sentences, words rely on their contexts to express concrete meanings, so we propose to jointly learn the filtering and matching process, to well capture the critical word-level matching signals. Specifically, a word graph is first built based on the attention scores in each self-attention block of Transformer, and key words are then selected by applying PageRank on this graph. In this way, noisy words will be filtered out layer by layer in the matching process. Experimental results show that Match-Ignition outperforms both SOTA short text matching models and recent long-form text matching models. We also conduct detailed analysis to show that Match-Ignition efficiently captures important sentences and words, to facilitate the long-form text matching process.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1396–1405},
numpages = {10},
keywords = {text matching, long-form text, pagerank algorithm},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3406522.3446020,
author = {Medlar, Alan and Li, Jing and G\l{}owacka, Dorota},
title = {Query Suggestions as Summarization in Exploratory Search},
year = {2021},
isbn = {9781450380553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406522.3446020},
doi = {10.1145/3406522.3446020},
abstract = {Query suggestions have been shown to benefit users performing information retrieval tasks. In exploratory search, however, users may lack the necessary domain knowledge to assess the relevance of query suggestions with respect to their information needs. In this article, we investigate the use of alternative queries in exploratory search. Alternative queries are queries that would retrieve similar search results to those currently visible on-screen. They are independent of the original search query and can, therefore, be updated dynamically as users scroll through search results. In addition to being follow-on queries, alternative queries serve as keyword summaries of the current search results page to help users assess whether results are inline with their search intents. We investigated the use of alternative queries in scientific literature search and their impact on user behavior and perception. In a user study, participants inspected half as many documents per query when alternative queries were present, but were exposed to over 40% more search results overall. Despite using them extensively as follow-on queries, user feedback focused on the summarization properties offered by alternative queries; finding it reassuring that documents were relevant to their search goals.},
booktitle = {Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
pages = {119–128},
numpages = {10},
keywords = {exploratory search, query suggestions, alternative queries, summarization, scientific literature search},
location = {Canberra ACT, Australia},
series = {CHIIR '21}
}

@article{10.1162/coli_a_00326,
author = {Naim, Iftekhar and Riley, Parker and Gildea, Daniel},
title = {Feature-Based Decipherment for Machine Translation},
year = {2018},
issue_date = {September 2018},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {44},
number = {3},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00326},
doi = {10.1162/coli_a_00326},
abstract = {Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction decipherment for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low-and no-resource contexts.},
journal = {Comput. Linguist.},
month = {sep},
pages = {525–546},
numpages = {22}
}

@inproceedings{10.1145/3132847.3133007,
author = {Dhakad, Lucky and Das, Mrinal and Bhattacharyya, Chiranjib and Datta, Samik and Kale, Mihir and Mehta, Vivek},
title = {SOPER: Discovering the Influence of Fashion and the Many Faces of User from Session Logs Using Stick Breaking Process},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133007},
doi = {10.1145/3132847.3133007},
abstract = {Recommending lifestyle articles is of immediate interest to the e-commerce industry and is beginning to attract research attention. Often followed strategies, such as recommending popular items are inadequate for this vertical because of two reasons. Firstly, users have their own personal preference over items, referred to as personal styles, which lead to the long-tail phenomenon. Secondly, each user displays multiple personas, each persona has a preference over items which could be dictated by a particular occasion, e.g. dressing for a party would be different from dressing to go to office. Recommendation in this vertical is crucially dependent on discovering styles for each of the multiple personas. There is no literature which addresses this problem.We posit a generative model which describes each user by a Simplex Over PERsona, SOPER, where a persona is described as the individuals preferences over prevailing styles modelled as topics over items. The choice of simplex and the long-tail nature necessitates the use of stick-breaking process. The main technical contribution is an efficient collapsed Gibbs sampling based algorithm for solving the attendant inference problem.Trained on large-scale interaction logs spanning more than half-a-million sessions collected from an e-commerce portal, SOPER outperforms previous baselines such as [9] by a large margin of 35% in identifying persona. Consequently it outperforms several competitive baselines comprehensively on the task of recommending from a catalogue of roughly 150 thousand lifestyle articles, by improving the recommendation quality as measured by AUC by a staggering 12.23%, in addition to aiding the interpretability of uncovered personal and fashionable styles thus advancing our precise understanding of the underlying phenomena.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1609–1618},
numpages = {10},
keywords = {topic models, stick-breaking process, lifestyle, fashion, bayesian nonparametrics},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3077136.3080820,
author = {Cao, Cheng and Ge, Hancheng and Lu, Haokai and Hu, Xia and Caverlee, James},
title = {What Are You Known For? Learning User Topical Profiles with Implicit and Explicit Footprints},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080820},
doi = {10.1145/3077136.3080820},
abstract = {User interests and expertise are valuable but often hidden resources on social media. For example, Twitter Lists and LinkedIn's Skill Tags provide a partial perspective on what users are known for (by aggregating crowd tagging knowledge), but the vast majority of users are untagged; their interests and expertise are essentially hidden from important applications such as personalized recommendation, community detection, and expert mining. A natural approach to overcome these limitations is to intelligently learn user topical profiles by exploiting information from multiple, heterogeneous footprints: for instance, Twitter users who post similar hashtags may have similar interests, and YouTube users who upvote the same videos may have similar preferences. And yet identifying "similar" users by exploiting similarity in such a footprint space often provides conflicting evidence, leading to poor-quality user profiles. In this paper, we propose a unified model for learning user topical profiles that simultaneously considers multiple footprints. We show how these footprints can be embedded in a generalized optimization framework that takes into account pairwise relations among all footprints for robustly learning user profiles. Through extensive experiments, we find the proposed model is capable of learning high-quality user topical profiles, and leads to a 10-15% improvement in precision and mean average error versus a cross-triadic factorization state-of-the-art baseline.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {743–752},
numpages = {10},
keywords = {user profile, user behavior, social media},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3038912.3052684,
author = {Hessel, Jack and Lee, Lillian and Mimno, David},
title = {Cats and Captions vs. Creators and the Clock: Comparing Multimodal Content to Context in Predicting Relative Popularity},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052684},
doi = {10.1145/3038912.3052684},
abstract = {The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds; this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to "cats and captions," as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {927–936},
numpages = {10},
keywords = {multimodal, image processing, language modeling, reddit, social media},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3331184.3331216,
author = {Li, Chenliang and Quan, Cong and Peng, Li and Qi, Yunwei and Deng, Yuming and Wu, Libing},
title = {A Capsule Network for Recommendation and Explaining What You Like and Dislike},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331216},
doi = {10.1145/3331184.3331216},
abstract = {User reviews contain rich semantics towards the preference of users to features of items. Recently, many deep learning based solutions have been proposed by exploiting reviews for recommendation. The attention mechanism is mainly adopted in these works to identify words or aspects that are important for rating prediction. However, it is still hard to understand whether a user likes or dislikes an aspect of an item according to what viewpoint the user holds and to what extent, without examining the review details. Here, we consider a pair of a viewpoint held by a user and an aspect of an item as a logic unit. Reasoning a rating behavior by discovering the informative logic units from the reviews and resolving their corresponding sentiments could enable a better rating prediction with explanation.To this end, in this paper, we propose a capsule network based model for rating prediction with user reviews, named CARP. For each user-item pair, CARP is devised to extract the informative logic units from the reviews and infer their corresponding sentiments. The model firstly extracts the viewpoints and aspects from the user and item review documents respectively. Then we derive the representation of each logic unit based on its constituent viewpoint and aspect. A sentiment capsule architecture with a novel Routing by Bi-Agreement mechanism is proposed to identify the informative logic unit and the sentiment based representations in user-item level for rating prediction. Extensive experiments are conducted over seven real-world datasets with diverse characteristics. Our results demonstrate that the proposed CARP obtains substantial performance gain over recently proposed state-of-the-art models in terms of prediction accuracy. Further analysis shows that our model can successfully discover the interpretable reasons at a finer level of granularity.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {275–284},
numpages = {10},
keywords = {user reviews, deep learning, recommender system},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3326285.3329060,
author = {Cai, Hui and Ye, Fan and Yang, Yuanyuan and Zhu, Yanmin and Li, Jie},
title = {Towards Privacy-Preserving Data Trading for Web Browsing History},
year = {2019},
isbn = {9781450367783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326285.3329060},
doi = {10.1145/3326285.3329060},
abstract = {The trading of social media data has attracted wide research interests over years. Especially the trading for web browsing histories probably produces tremendous economic value for data consumers when being applied to targeted advertising. However, the disclosure of entire browsing histories, even in form of anonymous datasets poses a huge threat to user privacy. Although some existing solutions have investigated privacy-preserving outsourcing of social media data, unfortunately, they neglected the impact on the data consumer's utility. In this paper, we propose PEATSE, a new Privacy-prEserving dAta Trading framework for web browSing historiEs. It takes users' diverse privacy preferences and the utility of their web browsing histories into consideration. PEATSE perturbs users' detailed browsing times on released browsing records to protect user privacy, while balancing the privacy-utility tradeoff. Through real-data based experiments, our analysis and evaluation results demonstrate PEATSE indeed achieves user privacy protection, the data consumer's accuracy requirement, and truthfulness, individual rationality as well as budget balance.},
booktitle = {Proceedings of the International Symposium on Quality of Service},
articleno = {25},
numpages = {10},
keywords = {data trading, privacy-preserving, web browsing history},
location = {Phoenix, Arizona},
series = {IWQoS '19}
}

@inproceedings{10.1145/3230833.3230855,
author = {Li, Zongze and Davidson, Matthew and Fu, Song and Blanchard, Sean and Lang, Michael},
title = {Converting Unstructured System Logs into Structured Event List for Anomaly Detection},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3230855},
doi = {10.1145/3230833.3230855},
abstract = {System logs provide invaluable resources for understanding system behavior and detecting anomalies on high performance computing (HPC) systems. As HPC systems continue to grow in both scale and complexity, the sheer volume of system logs and the complex interaction among system components make the traditional manual problem diagnosis and even automated line-by-line log analysis infeasible or ineffective. In this paper, we present a System Log Event Block Detection (SLEBD) framework that identifies groups of log messages that follow certain sequence but with variations, and explore these event blocks for event-based system behavior analysis and anomaly detection. Compared with the existing approaches that analyze system logs line by line, SLEBD is capable of characterizing system behavior and identifying intricate anomalies at a higher (i.e., event) level. We evaluate the performance of SLEBD by using syslogs collected from production supercomputers. Experimental results show that our framework and mechanisms can process streaming log messages, efficiently extract event blocks and effectively detect anomalies, which enables system administrators and monitoring tools to understand and process system events in real time. Additionally, we use the identified event blocks and explore deep learning algorithms to model and classify event sequences.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {15},
numpages = {10},
keywords = {system reliability, anomaly detection, behavior analysis, HPC systems},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@inproceedings{10.1145/3210459.3210464,
author = {Williams, Ashley},
title = {Using Reasoning Markers to Select the More Rigorous Software Practitioners' Online Content When Searching for Grey Literature},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210464},
doi = {10.1145/3210459.3210464},
abstract = {Background: Blog articles have potential value as a source of practitioner generated evidence in grey literature reviews: they could complement already accepted sources (e.g. interviews and focus groups). To be valuable to research, blog articles need to be relevant, rigorous and evidence-based.Objective: This paper focuses on the rigour of blog articles. We develop, evaluate and partially validate a set of reasoning markers that can be used to search for rigorous blog articles. We then demonstrate how these markers can be used in the online search of grey literature for software testing.Method: We identify discourse markers from literature and then select those that are explicit indicators of reasoning. We evaluate the set against a corpus of persuasive essays, and validate false negatives to refine our set further. We then demonstrate the use of the set in a search of grey literature on software testing.Results: The set of markers is reasonably successful at detecting reasoning within the corpus, achieving a precision of 89.6% in the first pass and 91.1% after refining our set. However, recall is low due to specifically focusing only on explicit indicators of reasoning (31.3% in the first pass and 38.7% in the second). Our overall F1-Score is 46.4% in the first pass and 54.3% in the second. This is acceptable for the time being as the current focus is on the quality of results retrieved from a keyword-based search. Improving the recall of results is left for future research.Conclusion: Our work provides a set of discourse markers that can be used to indicate the presence of reasoning in a blog article, and therefore provides an indication of the more rigorous blog article content. We intend to extend the work through considering the presence of evidence, and improving the relevance of blog articles found.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {46–56},
numpages = {11},
keywords = {grey literature reviews, Evidence based software engineering, discourse markers, systematic literature reviews, argumentation, blogs, evidence},
location = {Christchurch, New Zealand},
series = {EASE'18}
}

@inproceedings{10.1145/3196321.3196334,
author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
title = {Deep Code Comment Generation},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196334},
doi = {10.1145/3196321.3196334},
abstract = {During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {200–210},
numpages = {11},
keywords = {comment generation, program comprehension, deep learning},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3132847.3132963,
author = {Phan, Minh C. and Sun, Aixin and Tay, Yi and Han, Jialong and Li, Chenliang},
title = {NeuPL: Attention-Based Semantic Matching and Pair-Linking for Entity Disambiguation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132963},
doi = {10.1145/3132847.3132963},
abstract = {Entity disambiguation, also known as entity linking, is the task of mapping mentions in text to the corresponding entities in a given knowledge base, e.g. Wikipedia. Two key challenges are making use of mention's context to disambiguate (i.e. local objective), and promoting coherence of all the linked entities (i.e. global objective). In this paper, we propose a deep neural network model to effectively measure the semantic matching between mention's context and target entity. We are the first to employ the long short-term memory (LSTM) and attention mechanism for entity disambiguation. We also propose Pair-Linking, a simple but effective and significantly fast linking algorithm. Pair-Linking iteratively identifies and resolves pairs of mentions, starting from the most confident pair. It finishes linking all mentions in a document by scanning the pairs of mentions at most once. Our neural network model combined with Pair-Linking, named NeuPL, outperforms state-of-the-art systems over different types of documents including news, RSS, and tweets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1667–1676},
numpages = {10},
keywords = {semantic matching, pair-linking, entity disambiguation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3025171.3025210,
author = {Hoque, Enamul and Joty, Shafiq and Marquez, Luis and Carenini, Giuseppe},
title = {CQAVis: Visual Text Analytics for Community Question Answering},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025210},
doi = {10.1145/3025171.3025210},
abstract = {Community question answering (CQA) forums can provide effective means for sharing information and addressing a user's information needs about particular topics. However, many such online forums are not moderated, resulting in many low quality and redundant comments, which makes it very challenging for users to find the appropriate answers to their questions. In this paper, we apply a user-centered design approach to develop a system, CQAVis, which supports users in identifying high quality comments and get their questions answered. Informed by the user's requirements, the system combines both text analytics and interactive visualization techniques together in a synergistic way. Given a new question posed by the user, the text analytic module automatically finds relevant answers by exploring existing related questions and the comments within their threads. Then the visualization module presents the search results to the user and supports the exploration of related comments. We have evaluated the system in the wild by deploying it within a CQA forum among thousands of real users. Through the online study, we gained deeper insights about the potential utility of the system, as well as learned generalizable lessons for designing visual text analytics systems for the domain of CQA forums.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {161–172},
numpages = {12},
keywords = {community question answering, text visualization, asynchronous conversation, computer-mediated communication},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@article{10.1145/3519312,
author = {Rai, Sawan and Belwal, Ramesh Chandra and Gupta, Atul},
title = {A Review on Source Code Documentation},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3519312},
doi = {10.1145/3519312},
abstract = {Context: Coding is an incremental activity where a developer may need to understand a code before making suitable changes in the code. Code documentation is considered one of the best practices in software development but requires significant efforts from developers. Recent advances in natural language processing and machine learning have provided enough motivation to devise automated approaches for source code documentation at multiple levels.Objective: The review aims to study current code documentation practices and analyze the existing literature to provide a perspective on their preparedness to address the stated problem and the challenges that lie ahead.Methodology: We provide a detailed account of the literature in the area of automated source code documentation at different levels and critically analyze the effectiveness of the proposed approaches. This also allows us to infer gaps and challenges to address the problem at different levels.Findings: (1) The research community focused on method-level summarization. (2) Deep learning has dominated the past five years of this research field. (3) Researchers are regularly proposing bigger corpora for source code documentation. (4) Java and Python are the widely used programming languages as corpus. (5) Bilingual Evaluation Understudy is the most favored evaluation metric for the research persons.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jun},
articleno = {84},
numpages = {44},
keywords = {Summarization, software documentation, name prediction, deep learning, source code, summary generation}
}

@inproceedings{10.1145/3510003.3510213,
author = {Rong, Guoping and Zhang, Yifan and Yang, Lanxin and Zhang, Fuli and Kuang, Hongyu and Zhang, He},
title = {Modeling Review History for Reviewer Recommendation: A Hypergraph Approach},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510213},
doi = {10.1145/3510003.3510213},
abstract = {Modern code review is a critical and indispensable practice in a pull-request development paradigm that prevails in Open Source Software (OSS) development. Finding a suitable reviewer in projects with massive participants thus becomes an increasingly challenging task. Many reviewer recommendation approaches (recommenders) have been developed to support this task which apply a similar strategy, i.e. modeling the review history first then followed by predicting/recommending a reviewer based on the model. Apparently, the better the model reflects the reality in review history, the higher recommender's performance we may expect. However, one typical scenario in a pull-request development paradigm, i.e. one Pull-Request (PR) (such as a revision or addition submitted by a contributor) may have multiple reviewers and they may impact each other through publicly posted comments, has not been modeled well in existing recommenders. We adopted the hypergraph technique to model this high-order relationship (i.e. one PR with multiple reviewers herein) and developed a new recommender, namely HGRec, which is evaluated by 12 OSS projects with more than 87K PRs, 680K comments in terms of accuracy and recommendation distribution. The results indicate that HGRec outperforms the state-of-the-art recommenders on recommendation accuracy. Besides, among the top three accurate recommenders, HGRec is more likely to recommend a diversity of reviewers, which can help to relieve the core reviewers' workload congestion issue. Moreover, since HGRec is based on hypergraph, which is a natural and interpretable representation to model review history, it is easy to accommodate more types of entities and realistic relationships in modern code review scenarios. As the first attempt, this study reveals the potentials of hypergraph on advancing the pragmatic solutions for code reviewer recommendation.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1381–1392},
numpages = {12},
keywords = {modern code review, hypergraph, reviewer recommendation},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3356473.3365188,
author = {Wei, Hong and Zhou, Hao and Sankaranarayanan, Jagan and Sengupta, Sudipta and Samet, Hanan},
title = {DeLLe: Detecting Latest Local Events from Geotagged Tweets},
year = {2019},
isbn = {9781450369589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356473.3365188},
doi = {10.1145/3356473.3365188},
abstract = {Geotagged tweet streams contain invaluable information about the real-world local events like sports games, protests and traffic accidents. Timely detecting and extracting such events may have various applications but yet unsolved challenges. In this paper, we present DeLLe, a methodology for automatically Detecting Latest Local Events from geotagged tweets. With the help of novel spatio temporal tweet count prediction models, DeLLe first finds unusual locations which have aggregated unexpected number of tweets in the latest time period and thereby imply potential local events. Next, DeLLe calculates, for each such unusual location, a ranking score to identify the ones most likely having ongoing local events by addressing the temporal burstiness, spatial burstiness and topical coherence. Furthermore, DeLLe infers an event candidate's spatio temporal range by tracking its event-focus point, which essentially reflects the most recent representative occurrence site. Finally, DeLLe chooses the most influential tweets to summarize local events and thereby presents succinct but yet representative descriptions. We evaluate DeLLe on the city of Seattle, WA as well as a larger city of New York. The results show that the proposed method generally outperforms competitive baseline approaches.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Local Events and News},
articleno = {4},
numpages = {10},
keywords = {Local Events, Event Detection, Twitter, Geotagged Tweet Stream},
location = {Chicago, IL, USA},
series = {LENS'19}
}

@inproceedings{10.1145/3267809.3267817,
author = {Tian, Huangshi and Yu, Minchen and Wang, Wei},
title = {Continuum: A Platform for Cost-Aware, Low-Latency Continual Learning},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3267817},
doi = {10.1145/3267809.3267817},
abstract = {Many machine learning applications operate in dynamic environments that change over time, in which models must be continually updated to capture the recent trend in data. However, most of today's learning frameworks perform training offline, without a system support for continual model updating.In this paper, we design and implement Continuum, a general-purpose platform that streamlines the implementation and deployment of continual model updating across existing learning frameworks. In pursuit of fast data incorporation, we further propose two update policies, cost-aware and best-effort, that judiciously determine when to perform model updating, with and without accounting for the training cost (machine-time), respectively. Theoretical analysis shows that cost-aware policy is 2-competitive. We implement both polices in Continuum, and evaluate their performance through EC2 deployment and trace-driven simulations. The evaluation shows that Continuum results in reduced data incorporation latency, lower training cost, and improved model quality in a number of popular online learning applications that span multiple application domains, programming languages, and frameworks.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {26–40},
numpages = {15},
keywords = {Competitive Analysis, Continual Learning System, Online Algorithm},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@inproceedings{10.1145/3054977.3054992,
author = {Wang, Shiguang and Giridhar, Prasanna and Wang, Hongwei and Kaplan, Lance and Pham, Tien and Yener, Aylin and Abdelzaher, Tarek},
title = {StoryLine: Unsupervised Geo-Event Demultiplexing in Social Spaces without Location Information},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3054992},
doi = {10.1145/3054977.3054992},
abstract = {Some of the most widely deployed IoT devices in urban areas are smartphones in the possession of urban individuals. Their proliferation has led to the emergence of crowdsensing/crowdsourcing services, where humans collect data about their environment (using phones), and servers aggregate the data for various application purposes of interest. With the emergence of social media, a common alternative form of human data entry has become media posts (e.g., on Twitter). This leads to the prospect of building crowdsensing services on top of social media content, exploiting humans as "sensors". In this paper, we develop one such service, called StoryLine. The service detects and tracks physical urban events of interest to the user, such as car accidents, infrastructure damage (in the aftermath of a natural disaster), or instances of civil unrest. It offers an interface to client-side software that allows browsing such events in real time, as well as an interface for software applications to a structured representation of the events and their related statistics. The service embodies novel algorithms for real-time detection, demultiplexing, and tracking of physical events using social media data. In our evaluation with Twitter feeds, we show that our service outperforms two state-of-the-art baselines in event detection and demultiplexing. We also conduct two case-studies to show the effectiveness of the real-time event detection capability and event tracking performance of our system.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {83–93},
numpages = {11},
keywords = {Social Sensing, Event Tracking},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1145/3018661.3018690,
author = {Ren, Xiang and Lv, Yuanhua and Wang, Kuansan and Han, Jiawei},
title = {Comparative Document Analysis for Large Text Corpora},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018690},
doi = {10.1145/3018661.3018690},
abstract = {This paper presents a novel research problem, Comparative Document Analysis (CDA), that is, joint discovery of commonalities and differences between two individual documents (or two sets of documents) in a large text corpus. Given any pair of documents from a (background) document collection, CDA aims to automatically identify sets of quality phrases to summarize the commonalities of both documents and highlight the distinctions of each with respect to the other informatively and concisely. Our solution uses a general graph-based framework to derive novel measures on phrase semantic commonality and pairwise distinction, where the background corpus is used for computing phrase-document semantic relevance. We use the measures to guide the selection of sets of phrases by solving two joint optimization problems. A scalable iterative algorithm is developed to integrate the maximization of phrase commonality or distinction measure with the learning of phrase-document semantic relevance. Experiments on large text corpora from two different domains---scientific papers and news---demonstrate the effectiveness and robustness of the proposed framework on comparing documents. Analysis on a 10GB+ text corpus demonstrates the scalability of our method, whose computation time grows linearly as the corpus size increases. Our case study on comparing news articles published at different dates shows the power of the proposed method on comparing sets of documents.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {325–334},
numpages = {10},
keywords = {massive text corpus, distinction, large-scale text processing, comparative document analysis, commonalities, automatic summarization},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.5555/3291656.3291668,
author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
title = {Doomsday: Predicting Which Node Will Fail When on Supercomputers},
year = {2018},
publisher = {IEEE Press},
abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {9},
numpages = {14},
keywords = {failure analysis, HPC, machine learning},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/3132847.3132965,
author = {Shi, Yuling and Peng, Zhiyong and Wang, Hongning},
title = {Modeling Student Learning Styles in MOOCs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132965},
doi = {10.1145/3132847.3132965},
abstract = {The recorded student activities in Massive Open Online Course (MOOC) provide us a unique opportunity to model their learning behaviors, identify their particular learning intents, and enable personalized assistance and guidance in online education. In this work, based on a thorough qualitative study of students' behaviors recorded in two MOOC courses with large student enrollments, we develop a non-parametric Bayesian model to capture students' sequential learning activities in a generative manner. Homogeneity of students' learning behaviors is captured by clustering them into latent student groups, where shared model structure characterizes the transitional patterns, intensity and temporal distribution of their learning activities. In the meanwhile, heterogeneity is captured by clustering students into different groups. Both qualitative and quantitative studies on those two MOOC courses confirmed the effectiveness of the proposed model in identifying students' learning behavior patterns and clustering them into related groups for predictive analysis. The identified student groups accurately predict student retention, course satisfaction and demographics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {979–988},
numpages = {10},
keywords = {moocs, sequential data mining, probabilistic modeling, behavior modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1145/3487061,
author = {Nazir, Zulqarnain and Shahzad, Khurram and Malik, Muhammad Kamran and Anwar, Waheed and Bajwa, Imran Sarwar and Mehmood, Khawar},
title = {Authorship Attribution for a Resource Poor Language—Urdu},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3487061},
doi = {10.1145/3487061},
abstract = {Authorship attribution refers to examining the writing style of authors to determine the likelihood of the original author of a document from a given set of potential authors. Due to the wide range of authorship attribution applications, a plethora of studies have been conducted for various Western, as well as Asian, languages. However, authorship attribution research in the Urdu language has just begun, although Urdu is widely acknowledged as a prominent South Asian language. Furthermore, the existing studies on authorship attribution in Urdu have addressed a considerably easier problem of having less than 20 candidate authors, which is far from the real-world settings. Therefore, the findings from these studies may not be applicable to the real-world settings. To that end, we have made three key contributions: First, we have developed a large authorship attribution corpus for Urdu, which is a low-resource language. The corpus is composed of over 2.6 million tokens and 21,938 news articles by 94 authors, which makes it a closer substitute to the real-world settings. Second, we have analyzed hundreds of stylometry features used in the literature to identify 194 features that are applicable to the Urdu language and developed a taxonomy of these features. Finally, we have performed 66 experiments using two heterogeneous datasets to evaluate the effectiveness of four traditional and three deep learning techniques. The experimental results show the following: (a) Our developed corpus is many folds larger than the existing corpora, and it is more challenging than its counterparts for the authorship attribution task, and (b) Convolutional Neutral Networks is the most effective technique, as it achieved a nearly perfect F1 score of 0.989 for an existing corpus and 0.910 for our newly developed corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {44},
numpages = {23},
keywords = {Low-resource languages, urdu, authorship attribution, machine learning techniques, south asian languages, corpus generation}
}

@inproceedings{10.1145/3412569.3412576,
author = {Setia, Simran and Iyengar, S. R.S. and Verma, Amit Arjun},
title = {QWiki: Need for QnA &amp; Wiki to Co-Exist},
year = {2020},
isbn = {9781450387798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412569.3412576},
doi = {10.1145/3412569.3412576},
abstract = {Access to knowledge has never been as easy and quick as it has been in the 21st century. With the advent of the Internet and crowd-sourced knowledge building portals such as Wikipedia, Stack Exchange, Quora, and GitHub, information is just a click away. It is interesting to observe that the crowd builds these information repositories and not the experts. Information accumulation on a wiki-like portal and discussions on the QnA forum function independently as collaborative knowledge building practices. There is a need to understand the best possible practices to acquire and maintain knowledge in such crowdsourced portals. In this paper, we introduce QWiki, a novel approach of integrating a wiki-like portal and a QnA forum, seeking the union of aforementioned independent collaborative practices. The experimental analysis demonstrates that QWiki helps in knowledge acquisition and knowledge building process. The proposed model highlights the importance of interaction between a wiki-like portal and a QnA forum.},
booktitle = {Proceedings of the 16th International Symposium on Open Collaboration},
articleno = {9},
numpages = {12},
keywords = {crowd, QnA Forum, knowledge building, Wiki Portal, triggering},
location = {Virtual conference, Spain},
series = {OpenSym '20}
}

@inproceedings{10.1145/3366423.3380294,
author = {Yao, Jing and Dou, Zhicheng and Xu, Jun and Wen, Ji-Rong},
title = {RLPer: A Reinforcement Learning Model for Personalized Search},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380294},
doi = {10.1145/3366423.3380294},
abstract = {Personalized search improves generic ranking models by taking user interests into consideration and returning more accurate search results to individual users. In recent years, machine learning and deep learning techniques have been successfully applied in personalized search. Most existing personalization models simply regard the search history as a static set of user behaviours and learn fixed ranking strategies based on the recorded data. Though improvements have been observed, it is obvious that these methods ignore the dynamic nature of the search process: search is a sequence of interactions between the search engine and the user. During the search process, the user interests may dynamically change. It would be more helpful if a personalized search model could track the whole interaction process and update its ranking strategy continuously. In this paper, we propose a reinforcement learning based personalization model, referred to as RLPer, to track the sequential interactions between the users and search engine with a hierarchical Markov Decision Process (MDP). In RLPer, the search engine interacts with the user to update the underlying ranking model continuously with real-time feedback. And we design a feedback-aware personalized ranking component to catch the user’s feedback which has impacts on the user interest profile for the next query. Experimental results on the publicly available AOL search log verify that our proposed model can significantly outperform state-of-the-art personalized search models.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2298–2308},
numpages = {11},
keywords = {Reinforcement Learning, MDP, Personalized Search},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3321125,
author = {Yang, Jun and Yang, Runqi and Lu, Hengyang and Wang, Chongjun and Xie, Junyuan},
title = {Multi-Entity Aspect-Based Sentiment Analysis with Context, Entity, Aspect Memory and Dependency Information},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321125},
doi = {10.1145/3321125},
abstract = {Fine-grained sentiment analysis is a useful tool for producers to understand consumers’ needs as well as complaints about products and related aspects from online platforms. In this article, we define a novel task named “Multi-Entity Aspect-Based Sentiment Analysis (ME-ABSA)”. It investigates the sentiment towards entities and their related aspects. It makes the well-studied aspect-based sentiment analysis a special case of this type, where the number of entities is limited to one. We contribute a new dataset for this task, with multi-entity Chinese posts in it. We propose to model context, entity, and aspect memory to address the task and incorporate dependency information for further improvement. Experiments show that our methods perform significantly better than baseline methods on datasets for both ME-ABSA task and ABSA task. The in-depth analysis further validates the effectiveness of our methods and shows that our methods are capable of generalizing to new (entity, aspect) combinations with little loss of accuracy. This observation indicates that data annotation in real applications can be largely simplified.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {47},
numpages = {22},
keywords = {aspect, dependency, entity, Sentiment analysis}
}

@article{10.1145/3523060,
author = {Li, Ke and Guo, Bin and Liu, Jiaqi and Wang, Jiangtao and Ren, Haoyang and Yi, Fei and Yu, Zhiwen},
title = {Dynamic Probabilistic Graphical Model for Progressive Fake News Detection on Social Media Platform},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3523060},
doi = {10.1145/3523060},
abstract = {Recently, fake news has been readily spread by massive amounts of users in social media, and automatic fake news detection has become necessary. The existing works need to prepare the overall data to perform detection, losing important information about the dynamic evolution of crowd opinions, and usually neglect the issue of uneven arrival of data in the real world. To address these issues, in this article, we focus on a kind of approach for fake news detection, namely progressive detection, which can be achieved by the dynamic Probabilistic Graphical Model. Based on the observation on real-world datasets, we adaptively improve the Kalman Filter to the Labeled Variable Dimension Kalman Filter (LVDKF) that learns two universal patterns from true and fake news, respectively, which can capture the temporal information of time-series data that arrive unevenly. It can take sequential data as input, distill the dynamic evolution knowledge regarding a post, and utilize crowd wisdom from users’ responses to achieve progressive detection. Then we derive the formulas using the Forward, Backward, and EM Algorithm, and we design a dynamic detection algorithm using Bayes’ theorem. Finally, we design experimental scenarios simulating progressive detection and evaluate LVDKF on two public datasets. It outperforms the baseline methods in these experimental scenarios, which indicates that it is adequate for progressive detection.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jun},
articleno = {86},
numpages = {24},
keywords = {Progressive fake news detection, Kalman Filter, dynamic evolution, dynamic Probabilistic Graphical Model, uneven arrival}
}

@article{10.1145/3368960,
author = {Sun, Xiao and Li, Jia and Wei, Xing and Li, Changliang and Tao, Jianhua},
title = {Emotional Conversation Generation Based on a Bayesian Deep Neural Network},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3368960},
doi = {10.1145/3368960},
abstract = {The field of conversation generation using neural networks has attracted increasing attention from researchers for several years. However, traditional neural language models tend to generate a generic reply with poor semantic logic and no emotion. This article proposes an emotional conversation generation model based on a Bayesian deep neural network that can generate replies with rich emotions, clear themes, and diverse sentences. The topic and emotional keywords of the replies are pregenerated by introducing commonsense knowledge in the model. The reply is divided into multiple clauses, and then a multidimensional generator based on the transformer mechanism proposed in this article is used to iteratively generate clauses from two dimensions: sentence granularity and sentence structure. Subjective and objective experiments prove that compared with existing models, the proposed model effectively improves the semantic logic and emotional accuracy of replies. This model also significantly enhances the diversity of replies, largely overcoming the shortcomings of traditional models that generate safe replies.},
journal = {ACM Trans. Inf. Syst.},
month = {dec},
articleno = {8},
numpages = {24},
keywords = {natural language processing, deep learning, affective computing, Emotional conversation generation, Bayesian neural network}
}

@article{10.1145/3109480,
author = {Kim, Hyun and Jung, Hun-Young and Kwon, Hongseok and Lee, Jong-Hyeok and Na, Seung-Hoon},
title = {Predictor-Estimator: Neural Quality Estimation Based on Target Word Prediction for Machine Translation},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3109480},
doi = {10.1145/3109480},
abstract = {Recently, quality estimation has been attracting increasing interest from machine translation researchers, aiming at finding a good estimator for the “quality” of machine translation output. The common approach for quality estimation is to treat the problem as a supervised regression/classification task using a quality-annotated noisy parallel corpus, called quality estimation data, as training data. However, the available size of quality estimation data remains small, due to the too-expensive cost of creating such data. In addition, most conventional quality estimation approaches rely on manually designed features to model nonlinear relationships between feature vectors and corresponding quality labels. To overcome these problems, this article proposes a novel neural network architecture for quality estimation task—called the predictor-estimator—that considers word prediction as an additional pre-task. The major component of the proposed neural architecture is a word prediction model based on a modified neural machine translation model—a probabilistic model for predicting a target word conditioned on all the other source and target contexts. The underlying assumption is that the word prediction model is highly related to quality estimation models and is therefore able to transfer useful knowledge to quality estimation tasks. Our proposed quality estimation method sequentially trains the following two types of neural models: (1) Predictor: a neural word prediction model trained from parallel corpora and (2) Estimator: a neural quality estimation model trained from quality estimation data. To transfer word a prediction task to a quality estimation task, we generate quality estimation feature vectors from the word prediction model and feed them into the quality estimation model. The experimental results on WMT15 and 16 quality estimation datasets show that our proposed method has great potential in the various sub-challenges.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
articleno = {3},
numpages = {22},
keywords = {feature extraction, machine translation, neural networks, word prediction, Quality estimation, bidirectional language model}
}

@inproceedings{10.1145/3485447.3512002,
author = {Lee, Dongha and Shen, Jiaming and Kang, Seongku and Yoon, Susik and Han, Jiawei and Yu, Hwanjo},
title = {TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512002},
doi = {10.1145/3485447.3512002},
abstract = {Topic taxonomies, which represent the latent topic (or category) structure of document collections, provide valuable knowledge of contents in many applications such as web search and information filtering. Recently, several unsupervised methods have been developed to automatically construct the topic taxonomy from a text corpus, but it is challenging to generate the desired taxonomy without any prior knowledge. In this paper, we study how to leverage the partial (or incomplete) information about the topic structure as guidance to find out the complete topic taxonomy. We propose a novel framework for topic taxonomy completion, named TaxoCom, which recursively expands the topic taxonomy by discovering novel sub-topic clusters of terms and documents. To effectively identify novel topics within a hierarchical topic structure, TaxoCom devises its embedding and clustering techniques to be closely-linked with each other: (i) locally discriminative embedding optimizes the text embedding space to be discriminative among known (i.e., given) sub-topics, and (ii) novelty adaptive clustering assigns terms into either one of the known sub-topics or novel sub-topics. Our comprehensive experiments on two real-world datasets demonstrate that TaxoCom not only generates the high-quality topic taxonomy in terms of term coherency and topic coverage but also outperforms all other baselines for a downstream task.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2819–2829},
numpages = {11},
keywords = {Hierarchical topic discovery, Text embedding, Topic taxonomy completion, Novelty detection, Text clustering},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3172944.3172964,
author = {Sherkat, Ehsan and Nourashrafeddin, Seyednaser and Milios, Evangelos E. and Minghim, Rosane},
title = {Interactive Document Clustering Revisited: A Visual Analytics Approach},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172964},
doi = {10.1145/3172944.3172964},
abstract = {Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user»s perspectives. To incorporate the user»s perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {281–292},
numpages = {12},
keywords = {interactive document clustering, user study, text, document projection, key-term, visualization},
location = {Tokyo, Japan},
series = {IUI '18}
}

@article{10.14778/3067421.3067424,
author = {Yang, Fan and Shang, Fanhua and Huang, Yuzhen and Cheng, James and Li, Jinfeng and Zhao, Yunjian and Zhao, Ruihao},
title = {LFTF: A Framework for Efficient Tensor Analytics at Scale},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3067421.3067424},
doi = {10.14778/3067421.3067424},
abstract = {Tensors are higher order generalizations of matrices to model multi-aspect data, e.g., a set of purchase records with the schema (user_id, product_id, timestamp, feedback). Tensor factorization is a powerful technique for generating a model from a tensor, just like matrix factorization generates a model from a matrix, but with higher accuracy and richer information as more attributes are available in a higher- order tensor than a matrix. The data model obtained by tensor factorization can be used for classification, recommendation, anomaly detection, and so on. Though having a broad range of applications, tensor factorization has not been popularly applied compared with matrix factorization that has been widely used in recommender systems, mainly due to the high computational cost and poor scalability of existing tensor factorization methods. Efficient and scalable tensor factorization is particularly challenging because real world tensor data are mostly sparse and massive. In this paper, we propose a novel distributed algorithm, called Lock-Free Tensor Factorization (LFTF), which significantly improves the efficiency and scalability of distributed tensor factorization by exploiting asynchronous execution in a re-formulated problem. Our experiments show that LFTF achieves much higher CPU and network throughput than existing methods, converges at least 17 times faster and scales to much larger datasets.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {745–756},
numpages = {12}
}

@inproceedings{10.1145/3485447.3512121,
author = {Iqbal, Hassan and Khan, Usman Mahmood and Khan, Hassan Ali and Shahzad, Muhammad},
title = {Left or Right: A Peek into the Political Biases in Email Spam Filtering Algorithms During US Election 2020},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512121},
doi = {10.1145/3485447.3512121},
abstract = {Email services use spam filtering algorithms (SFAs) to filter emails that are unwanted by the user. However, at times, the emails perceived by an SFA as unwanted may be important to the user. Such incorrect decisions can have significant implications if SFAs treat emails of user interest as spam on a large scale. This is particularly important during national elections. To study whether the SFAs of popular email services have any biases in treating the campaign emails, we conducted a large-scale study of the campaign emails of the US elections 2020 by subscribing to a large number of Presidential, Senate, and House candidates using over a hundred email accounts on Gmail, Outlook, and Yahoo. We analyzed the biases in the SFAs towards the left and the right candidates and further studied the impact of the interactions (such as reading or marking emails as spam) of email recipients on these biases. We observed that the SFAs of different email services indeed exhibit biases towards different political affiliations.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2491–2500},
numpages = {10},
keywords = {Political Bias, Algorithm Bias, US Elections, Emails, Spam, Bias},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3421558.3421584,
author = {Bakana, Sibusiso R. and Zhang, Yongfei and Twala, Bhekisipho},
title = {Mitigating Wild Animals Poaching Through State-of-the-Art Multimedia Data Mining Techniques: A Review},
year = {2020},
isbn = {9781450388412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421558.3421584},
doi = {10.1145/3421558.3421584},
abstract = {Wild animal poaching, particular rhinos, and elephants in Africa, is a serious destruction for biodiversity and eco-tourism. Governments and numerous Non – Government Organizations (NGOs) spent a great amount of human labor and money every year in preventing poaching. Recently, advanced techniques, like intelligent video surveillance and multimedia data mining, have been adopted to help more efficiently mitigate wild animals poaching. In this paper, we provide a detailed review of the state-of-the-art video surveillance and multimedia data mining techniques for mitigating wild animal poaching from four aspects according to processing steps, namely object detection, object classification, object behavior analysis and invader analysis. More specifically, different algorithms in each aspect are further subdivided into sub-categories and compared in terms of pros, cons, efficiency, and complexity. While these techniques have been thoroughly researched separately, such topics have not been superimposed in the paradigm of wild animals poaching.&nbsp;To the best of our knowledge, this is the first such comprehensive review of the recent advances of the intelligent video understanding and multimedia data mining for mitigating wild animals poaching and hopefully it would help the improvement, implementation, and applications of advanced techniques in preventing wild animal poaching and protecting diverse especially endangered species for the one and only one home for us human being.},
booktitle = {2020 2nd International Conference on Image Processing and Machine Vision},
pages = {158–172},
numpages = {15},
keywords = {Wild animals poaching, Objects behavior analysis, Object detection, Object classification, Invader analysis},
location = {Bangkok, Thailand},
series = {IPMV 2020}
}

@inproceedings{10.1145/3342220.3343668,
author = {Poghosyan, Gevorg and Ifrim, Georgiana},
title = {SocialTree: Socially Augmented Structured Summaries of News Stories},
year = {2019},
isbn = {9781450368858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342220.3343668},
doi = {10.1145/3342220.3343668},
abstract = {News story understanding entails having an effective summary of a related group of articles that may span different time ranges, involve different topics and entities, and have connections to other stories. In this work, we present an approach to efficiently extract structured summaries of news stories by augmenting news media with the structure of social discourse as reflected in social media in the form of social tags. Existing event detection, topic-modeling, clustering and summarization methods yield news story summaries based only on noun phrases and named entities. These representations are sensitive to the article wording and the keyword extraction algorithm. Moreover, keyword-based representations are rarely helpful for highlighting the inter-story connections or for reflecting the inner structure of the news story because of high word ambiguity and clutter from the large variety of keywords describing news stories. Our method combines the news and social media domains to create structured summaries of news stories in the form of hierarchies of keywords and social tags, named SocialTree. We show that the properties of social tags can be exploited to augment the construction of hierarchical summaries of news stories and to alleviate the weaknesses of existing keyword-based representations. In our quantitative and qualitative evaluation the proposed method strongly outperforms the state-of-the-art with regard to both coverage and informativeness of the summaries.},
booktitle = {Proceedings of the 30th ACM Conference on Hypertext and Social Media},
pages = {153–162},
numpages = {10},
keywords = {association rules, news summarizarion, social indexing},
location = {Hof, Germany},
series = {HT '19}
}

@article{10.1145/3152114,
author = {Jiang, Shuhui and Wu, Yue and Fu, Yun},
title = {Deep Bidirectional Cross-Triplet Embedding for Online Clothing Shopping},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3152114},
doi = {10.1145/3152114},
abstract = {In this article, we address the cross-domain (i.e., street and shop) clothing retrieval problem and investigate its real-world applications for online clothing shopping. It is a challenging problem due to the large discrepancy between street and shop domain images. We focus on learning an effective feature-embedding model to generate robust and discriminative feature representation across domains. Existing triplet embedding models achieve promising results by finding an embedding metric in which the distance between negative pairs is larger than the distance between positive pairs plus a margin. However, existing methods do not address the challenges in the cross-domain clothing retrieval scenario sufficiently. First, the intradomain and cross-domain data relationships need to be considered simultaneously. Second, the number of matched and nonmatched cross-domain pairs are unbalanced. To address these challenges, we propose a deep cross-triplet embedding algorithm together with a cross-triplet sampling strategy. The extensive experimental evaluations demonstrate the effectiveness of the proposed algorithms well. Furthermore, we investigate two novel online shopping applications, clothing trying on and accessories recommendation, based on a unified cross-domain clothing retrieval framework.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {5},
numpages = {22},
keywords = {triplet embedding, cross domain, deep learning, Clothing retrieval, accessory recommendation}
}

@inproceedings{10.1109/ICSE.2017.71,
author = {Jiang, He and Li, Xiaochen and Yang, Zijiang and Xuan, Jifeng},
title = {What Causes My Test Alarm? Automatic Cause Analysis for Test Alarms in System and Integration Testing},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.71},
doi = {10.1109/ICSE.2017.71},
abstract = {Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {712–723},
numpages = {12},
keywords = {system and integration testing, test alarm analysis, multiclass classification, software testing},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1109/ICSE43902.2021.00089,
author = {Hu, Yangyu and Wang, Haoyu and Ji, Tiantong and Xiao, Xusheng and Luo, Xiapu and Gao, Peng and Guo, Yao},
title = {CHAMP: Characterizing Undesired App Behaviors from User Comments Based on Market Policies},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00089},
doi = {10.1109/ICSE43902.2021.00089},
abstract = {Millions of mobile apps have been available through various app markets. Although most app markets have enforced a number of automated or even manual mechanisms to vet each app before it is released to the market, thousands of low-quality apps still exist in different markets, some of which violate the explicitly specified market policies. In order to identify these violations accurately and timely, we resort to user comments, which can form an immediate feedback for app market maintainers, to identify undesired behaviors that violate market policies, including security-related user concerns. Specifically, we present the first large-scale study to detect and characterize the correlations between user comments and market policies. First, we propose CHAMP, an approach that adopts text mining and natural language processing (NLP) techniques to extract semantic rules through a semi-automated process, and classifies comments into 26 pre-defined types of undesired behaviors that violate market policies. Our evaluation on real-world user comments shows that it achieves both high precision and recall (&gt; 0.9) in classifying comments for undesired behaviors. Then, we curate a large-scale comment dataset (over 3 million user comments) from apps in Google Play and 8 popular alternative Android app markets, and apply CHAMP to understand the characteristics of undesired behavior comments in the wild. The results confirm our speculation that user comments can be used to pinpoint suspicious apps that violate policies declared by app markets. The study also reveals that policy violations are widespread in many app markets despite their extensive vetting efforts. CHAMP can be a whistle blower that assigns policy-violation scores and identifies most informative comments for apps.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {933–945},
numpages = {13},
keywords = {app market, User comment, undesired behavior},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3324884.3416578,
author = {Wei, Bolin and Li, Yongmin and Li, Ge and Xia, Xin and Jin, Zhi},
title = {Retrieve and Refine: Exemplar-Based Neural Comment Generation},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416578},
doi = {10.1145/3324884.3416578},
abstract = {Code comment generation which aims to automatically generate natural language descriptions for source code, is a crucial task in the field of automatic software development. Traditional comment generation methods use manually-crafted templates or information retrieval (IR) techniques to generate summaries for source code. In recent years, neural network-based methods which leveraged acclaimed encoder-decoder deep learning framework to learn comment generation patterns from a large-scale parallel code corpus, have achieved impressive results. However, these emerging methods only take code-related information as input. Software reuse is common in the process of software development, meaning that comments of similar code snippets are helpful for comment generation. Inspired by the IR-based and template-based approaches, in this paper, we propose a neural comment generation approach where we use the existing comments of similar code snippets as exemplars to guide comment generation. Specifically, given a piece of code, we first use an IR technique to retrieve a similar code snippet and treat its comment as an exemplar. Then we design a novel seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to assist in the target comment generation based on the semantic similarity between the source code and the similar code. We evaluate our approach on a large-scale Java corpus, which contains about 2M samples, and experimental results demonstrate that our model outperforms the state-of-the-art methods by a substantial margin.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {349–360},
numpages = {12},
keywords = {deep learning, comment generation},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3377325.3377514,
author = {Arendt, Dustin L. and Nur, Nasheen and Huang, Zhuanyi and Fair, Gabriel and Dou, Wenwen},
title = {Parallel Embeddings: A Visualization Technique for Contrasting Learned Representations},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377514},
doi = {10.1145/3377325.3377514},
abstract = {We introduce "Parallel Embeddings", a new technique that generalizes the classical Parallel Coordinates visualization technique to sequences of learned representations. This visualization technique is designed for concept-oriented "model comparison" tasks, allowing data scientists to understand qualitative differences in how models interpret input data. We compare user performance with our tool against Tensor Board Embedding Projector for understanding model accuracy and qualitative model differences. With our tool, users were more accurate and learned strategies for the tasks more quickly. Furthermore, users' analytical process in the comparison condition was positively influenced by using our tool beforehand.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {259–274},
numpages = {16},
keywords = {machine learning explanations, dimension reduction visualization, user studies, model comparison, image classification},
location = {Cagliari, Italy},
series = {IUI '20}
}

@inproceedings{10.1145/3238147.3238216,
author = {Liu, Xiaoyu and Huang, LiGuo and Ng, Vincent},
title = {Effective API Recommendation without Historical Software Repositories},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238216},
doi = {10.1145/3238147.3238216},
abstract = {It is time-consuming and labor-intensive to learn and locate the correct API for programming tasks. Thus, it is beneficial to perform API recommendation automatically. The graph-based statistical model has been shown to recommend top-10 API candidates effectively. It falls short, however, in accurately recommending an actual top-1 API. To address this weakness, we propose RecRank, an approach and tool that applies a novel ranking-based discriminative approach leveraging API usage path features to improve top-1 API recommendation. Empirical evaluation on a large corpus of (1385+8) open source projects shows that RecRank significantly improves top-1 API recommendation accuracy and mean reciprocal rank when compared to state-of-the-art API recommendation approaches.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {282–292},
numpages = {11},
keywords = {API Recommendation, Machine Learning},
location = {Montpellier, France},
series = {ASE 2018}
}

@article{10.1109/TASLP.2017.2763243,
author = {Liu, Bingquan and Xu, Zhen and Sun, Chengjie and Wang, Baoxun and Wang, Xiaolong and Wong, Derek F. and Zhang, Min and Bingquan Liu and Zhen Xu and Chengjie Sun and Baoxun Wang and Xiaolong Wang and Wong, Derek F. and Min Zhang},
title = {Content-Oriented User Modeling for Personalized Response Ranking in Chatbots},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2763243},
doi = {10.1109/TASLP.2017.2763243},
abstract = {Automatic chatbots also known as chat-agents have attracted much attention from both researching and industrial fields. Generally, the semantic relevance between users' queries and the corresponding responses is considered as the essential element for conversation modeling in both generation and ranking based chat systems. By contrast, it is a nontrivial task to adopt the users' information, such as preference, social role, etc., into conversational models reasonably, while users' profiles play a significant role in the procedure of conversations by providing the implicit contexts. This paper aims to address the personalized response ranking task by incorporating user profiles into the conversation model. In our approach, users' personalized representations are latently learned from the contents posted by them via a two-branch neural network. After that, a deep neural network architecture is further presented to learn the fusion representation of posts, responses, and personal information. In this way, the proposed model could understand conversations from the users' perspective; hence, the more appropriate responses are selected for a specified person. The experimental results on two datasets from social network services demonstrate that our approach is hopeful to represent users' personal information implicitly based on user generated contents, and it is promising to perform as an important component in chatbots to select the personalized responses for each user.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {122–133},
numpages = {12}
}

@article{10.1145/3057283,
author = {Li, Xin and Jiang, Mingming and Hong, Huiting and Liao, Lejian},
title = {A Time-Aware Personalized Point-of-Interest Recommendation via High-Order Tensor Factorization},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057283},
doi = {10.1145/3057283},
abstract = {Recently, location-based services (LBSs) have been increasingly popular for people to experience new possibilities, for example, personalized point-of-interest (POI) recommendations that leverage on the overlapping of user trajectories to recommend POI collaboratively. POI recommendation is yet challenging as it suffers from the problems known for the conventional recommendation tasks such as data sparsity and cold start, and to a much greater extent. In the literature, most of the related works apply collaborate filtering to POI recommendation while overlooking the personalized time-variant human behavioral tendency. In this article, we put forward a fourth-order tensor factorization-based ranking methodology to recommend users their interested locations by considering their time-varying behavioral trends while capturing their long-term preferences and short-term preferences simultaneously. We also propose to categorize the locations to alleviate data sparsity and cold-start issues, and accordingly new POIs that users have not visited can thus be bubbled up during the category ranking process. The tensor factorization is carefully studied to prune the irrelevant factors to the ranking results to achieve efficient POI recommendations. The experimental results validate the efficacy of our proposed mechanism, which outperforms the state-of-the-art approaches significantly.},
journal = {ACM Trans. Inf. Syst.},
month = {jun},
articleno = {31},
numpages = {23},
keywords = {HITS algorithm, Time-aware POI recommendation, tensor factorization}
}

@inproceedings{10.1145/3038912.3052594,
author = {Liu, Yuli and Liu, Yiqun and Zhou, Ke and Zhang, Min and Ma, Shaoping},
title = {Detecting Collusive Spamming Activities in Community Question Answering},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052594},
doi = {10.1145/3038912.3052594},
abstract = {Community Question Answering (CQA) portals provide rich sources of information on a variety of topics. However, the authenticity and quality of questions and answers (Q&amp;As) has proven hard to control. In a troubling direction, the widespread growth of crowdsourcing websites has created a large-scale, potentially difficult-to-detect workforce to manipulate malicious contents in CQA. The crowd workers who join the same crowdsourcing task about promotion campaigns in CQA collusively manipulate deceptive Q&amp;As for promoting a target (product or service). The collusive spamming group can fully control the sentiment of the target. How to utilize the structure and the attributes for detecting manipulated Q&amp;As? How to detect the collusive group and leverage the group information for the detection task?To shed light on these research questions, we propose a unified framework to tackle the challenge of detecting collusive spamming activities of CQA. First, we interpret the questions and answers in CQA as two independent networks. Second, we detect collusive question groups and answer groups from these two networks respectively by measuring the similarity of the contents posted within a short duration. Third, using attributes (individual-level and group-level) and correlations (user-based and content-based), we proposed a combined factor graph model to detect deceptive Q&amp;As simultaneously by combining two independent factor graphs. With a large-scale practical data set, we find that the proposed framework can detect deceptive contents at early stage, and outperforms a number of competitive baselines.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1073–1082},
numpages = {10},
keywords = {crowdsourcing manipulation, factor graph., spam detection, community question answering},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.5555/3546258.3546460,
author = {Liu, Huafeng and Jing, Liping and Wen, Jingxuan and Xu, Pengyu and Wang, Jiaqi and Yu, Jian and Ng, Michael K.},
title = {Interpretable Deep Generative Recommendation Models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {User preference modeling in recommendation system aims to improve customer experience through discovering users' intrinsic preference based on prior user behavior data. This is a challenging issue because user preferences usually have complicated structure, such as inter-user preference similarity and intra-user preference diversity. Among them, inter-user similarity indicates different users may share similar preference, while intra-user diversity indicates one user may have several preferences. In literatures, deep generative models have been successfully applied in recommendation systems due to its exibility on statistical distributions and strong ability for non-linear representation learning. However, they suffer from the simple generative process when handling complex user preferences. Meanwhile, the latent representations learned by deep generative models are usually entangled, and may range from observed-level ones that dominate the complex correlations between users, to latent-level ones that characterize a user's preference, which makes the deep model hard to explain and unfriendly for recommendation. Thus, in this paper, we propose an Interpretable Deep Generative Recommendation Model (InDGRM) to characterize inter-user preference similarity and intra-user preference diversity, which will simultaneously disentangle the learned representation from observed-level and latent-level. In InDGRM, the observed-level disentanglement on users is achieved by modeling the user-cluster structure (i.e., inter-user preference similarity) in a rich multimodal space, so that users with similar preferences are assigned into the same cluster. The observed-level disentanglement on items is achieved by modeling the intra-user preference diversity in a prototype learning strategy, where different user intentions are captured by item groups (one group refers to one intention). To promote disentangled latent representations, InDGRM adopts structure and sparsity-inducing penalty and integrates them into the generative procedure, which has ability to enforce each latent factor focus on a limited subset of items (e.g., one item group) and benefit latent-level disentanglement. Meanwhile, it can be efficiently inferred by minimizing its penalized upper bound with the aid of local variational optimization technique. Theoretically, we analyze the generalization error bound of InDGRM to guarantee its performance. A series of experimental results on four widely-used benchmark datasets demonstrates the superiority of InDGRM on recommendation performance and interpretability.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {202},
numpages = {54},
keywords = {interpretable machine learning, recommendation system, deep generative model, collaborative filtering, latent factor model}
}

@inproceedings{10.1145/3510003.3510129,
author = {Liu, Yalin and Lin, Jinfeng and Anuyah, Oghenemaro and Metoyer, Ronald and Cleland-Huang, Jane},
title = {Generating and Visualizing Trace Link Explanations},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510129},
doi = {10.1145/3510003.3510129},
abstract = {Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic healthcare systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1033–1044},
numpages = {12},
keywords = {software traceability, explanation interface, concept mining},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1109/ASE51524.2021.9678574,
author = {Su, Yanqi and Xing, Zhenchang and Peng, Xin and Xia, Xin and Wang, Chong and Xu, Xiwei and Zhu, Liming},
title = {Reducing Bug Triaging Confusion by Learning from Mistakes with a Bug Tossing Knowledge Graph},
year = {2021},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678574},
doi = {10.1109/ASE51524.2021.9678574},
abstract = {Assigning bugs to the right components is the prerequisite to get the bugs analyzed and fixed. Classification-based techniques have been used in practice for assisting bug component assignments, for example, the BugBug tool developed by Mozilla. However, our study on 124,477 bugs in Mozilla products reveals that erroneous bug component assignments occur frequently and widely. Most errors are repeated errors and some errors are even misled by the BugBug tool. Our study reveals that complex component designs and misleading component names and bug report keywords confuse bug component assignment not only for bug reporters but also developers and even bug triaging tools. In this work, we propose a learning to rank framework that learns to assign components to bugs from correct, erroneous and irrelevant bug-component assignments in the history. To inform the learning, we construct a bug tossing knowledge graph which incorporates not only goal-oriented component tossing relationships but also rich information about component tossing community, component descriptions, and historical closed and tossed bugs, from which three categories and seven types of features for bug, component and bug-component relation can be derived. We evaluate our approach on a dataset of 98,587 closed bugs (including 29,100 tossed bugs) of 186 components in six Mozilla products. Our results show that our approach significantly improves bug component assignments for both tossed and non-tossed bugs over the BugBug tool and the BugBug tool enhanced with component tossing relationships, with &gt;20% Top-k accuracies and &gt;30% NDCG@k (k=1,3,5,10).},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {191–202},
numpages = {12},
keywords = {learning to rank, knowledge graph, bug triaging},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3397271.3401153,
author = {Yao, Jing and Dou, Zhicheng and Wen, Ji-Rong},
title = {Employing Personal Word Embeddings for Personalized Search},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401153},
doi = {10.1145/3397271.3401153},
abstract = {Personalized search is a task to tailor the general document ranking list based on user interests to better satisfy the user's information need. Many personalized search models have been proposed and demonstrated their capability to improve search quality. The general idea of most approaches is to build a user interest profile according to the user's search history, and then re-rank the documents based on the matching scores between the created user profile and candidate documents. In this paper, we propose to solve the problem of personalized search in an alternative way. We know that there are many ambiguous words in natural language such as 'Apple', and people with different knowledge backgrounds and interests have personalized understandings of these words. Therefore, for different users, such a word should own different semantic representations. Motivated by this idea, we design a personalized search model based on personal word embeddings, referred to as PEPS. Specifically, we train personal word embeddings for each user in which the representation of each word is mainly decided by the user's personal data. Then, we obtain the personalized word and contextual representations of the query and documents with an attention function. Finally, we use a matching model to calculate the matching score between the personalized query and document representations. Experiments on two datasets verify that our model can significantly improve state-of-the-art personalization models.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1359–1368},
numpages = {10},
keywords = {personal word embedding, user interest, personalized search},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3313831.3376783,
author = {Smith, C. Estelle and Yu, Bowen and Srivastava, Anjali and Halfaker, Aaron and Terveen, Loren and Zhu, Haiyi},
title = {Keeping Community in the Loop: Understanding Wikipedia Stakeholder Values for Machine Learning-Based Systems},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376783},
doi = {10.1145/3313831.3376783},
abstract = {On Wikipedia, sophisticated algorithmic tools are used to assess the quality of edits and take corrective actions. However, algorithms can fail to solve the problems they were designed for if they conflict with the values of communities who use them. In this study, we take a Value-Sensitive Algorithm Design approach to understanding a community-created and -maintained machine learning-based algorithm called the Objective Revision Evaluation System (ORES)---a quality prediction system used in numerous Wikipedia applications and contexts. Five major values converged across stakeholder groups that ORES (and its dependent applications) should: (1) reduce the effort of community maintenance, (2) maintain human judgement as the final authority, (3) support differing peoples' differing workflows, (4) encourage positive engagement with diverse editor groups, and (5) establish trustworthiness of people and algorithms within the community. We reveal tensions between these values and discuss implications for future research to improve algorithms like ORES.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {peer production, wikipedia, community values, machine learning, ORES, value sensitive algorithm design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3366423.3380164,
author = {Sun, Peijie and Wu, Le and Zhang, Kun and Fu, Yanjie and Hong, Richang and Wang, Meng},
title = {Dual Learning for Explainable Recommendation: Towards Unifying User Preference Prediction and Review Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380164},
doi = {10.1145/3366423.3380164},
abstract = {In many recommender systems, users express item opinions through two kinds of behaviors: giving preferences and writing detailed reviews. As both kinds of behaviors reflect users’ assessment of items, review enhanced recommender systems leverage these two kinds of user behaviors to boost recommendation performance. On the one hand, researchers proposed to better model the user and item embeddings with additional review information for enhancing preference prediction accuracy. On the other hand, some recent works focused on automatically generating item reviews for recommendation explanations with related user and item embeddings. We argue that, while the task of preference prediction with the accuracy goal is well recognized in the community, the task of generating reviews for explainable recommendation is also important to gain user trust and increase conversion rate. Some preliminary attempts have considered jointly modeling these two tasks, with the user and item embeddings are shared. These studies empirically showed that these two tasks are correlated, and jointly modeling them would benefit the performance of both tasks. In this paper, we make a further study of unifying these two tasks for explainable recommendation. Instead of simply correlating these two tasks with shared user and item embeddings, we argue that these two tasks are presented in dual forms. In other words, the input of the primal preference prediction task is exactly the output of the dual review generation task , with and denote the preference value space and review space. Therefore, we could explicitly model the probabilistic correlation between these two dual tasks with . We design a unified dual framework of how to inject the probabilistic duality of the two tasks in the training stage. Furthermore, as the detailed preference and review information are not available for each user-item pair in the test stage, we propose a transfer learning based model for preference prediction and review generation. Finally, extensive experimental results on two real-world datasets clearly show the effectiveness of our proposed model for both user preference prediction and review generation.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {837–847},
numpages = {11},
keywords = {review generation, dual learning, recommender system},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1109/SC.2018.00012,
author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
title = {Doomsday: Predicting Which Node Will Fail When on Supercomputers},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00012},
doi = {10.1109/SC.2018.00012},
abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {9},
numpages = {14},
keywords = {failure analysis, HPC, machine learning},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1145/3308558.3313599,
author = {Yang, Jie and Smirnova, Alisa and Yang, Dingqi and Demartini, Gianluca and Lu, Yuan and Cudre-Mauroux, Philippe},
title = {Scalpel-CD: Leveraging Crowdsourcing and Deep Probabilistic Modeling for Debugging Noisy Training Data},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313599},
doi = {10.1145/3308558.3313599},
abstract = {This paper presents Scalpel-CD, a first-of-its-kind system that leverages both human and machine intelligence to debug noisy labels from the training data of machine learning systems. Our system identifies potentially wrong labels using a deep probabilistic model, which is able to infer the latent class of a high-dimensional data instance by exploiting data distributions in the underlying latent feature space. To minimize crowd efforts, it employs a data sampler which selects data instances that would benefit the most from being inspected by the crowd. The manually verified labels are then propagated to similar data instances in the original training data by exploiting the underlying data structure, thus scaling out the contribution from the crowd. Scalpel-CD is designed with a set of algorithmic solutions to automatically search for the optimal configurations for different types of training data, in terms of the underlying data structure, noise ratio, and noise types (random vs. structural). In a real deployment on multiple machine learning tasks, we demonstrate that Scalpel-CD is able to improve label quality by 12.9% with only 2.8% instances inspected by the crowd.},
booktitle = {The World Wide Web Conference},
pages = {2158–2168},
numpages = {11},
keywords = {crowdsourcing, Debugging training data, deep probabilistic models},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3269206.3271668,
author = {Van Gysel, Christophe and de Rijke, Maarten and Kanoulas, Evangelos},
title = {Mix 'n Match: Integrating Text Matching and Product Substitutability within Product Search},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271668},
doi = {10.1145/3269206.3271668},
abstract = {Two products are substitutes if both can satisfy the same consumer need. Intrinsic incorporation of product substitutability - where substitutability is integrated within latent vector space models - is in contrast to the extrinsic re-ranking of result lists. The fusion of text matching and product substitutability objectives allows latent vector space models to mix and match regularities contained within text descriptions and substitution relations. We introduce a method for intrinsically incorporating product substitutability within latent vector space models for product search that are estimated using gradient descent; it integrates flawlessly with state-of-the-art vector space models. We compare our method to existing methods for incorporating structural entity relations, where product substitutability is incorporated extrinsically by re-ranking. Our method outperforms the best extrinsic method on four benchmarks. We investigate the effect of different levels of text matching and product similarity objectives, and provide an analysis of the effect of incorporating product substitutability on product search ranking diversity. Incorporating product substitutability information improves search relevance at the cost of diversity.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1373–1382},
numpages = {10},
keywords = {latent vector space models, entity similarity, product search},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3465336.3475103,
author = {Karbasian, Habib and Purohit, Hemant and Johri, Aditya},
title = {Improving Diversity in Engineering: A Data-Driven Approach to Support Resource Mobilization and Participation in Hashtag Activism Campaigns},
year = {2021},
isbn = {9781450385510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465336.3475103},
doi = {10.1145/3465336.3475103},
abstract = {A critical barrier facing engineering is inclusiveness of women in the profession. In recent years, engineering diversity advocates have taken to social media platforms to raise awareness of the issue and redress this problem. A recurring challenge for their initiatives though is attracting and mobilizing participants efficiently. For a successful mobilization campaign, organizers need real-time information about their users and also need to understand what messaging works to attract and mobilize them. We hypothesize that participants in any given campaign related to engineering diversity will also be interested in other campaigns related to that issue. Furthermore, since the primary signal for a social media campaign is a hashtag, by using clustering patterns of various co-occurring hashtags along with relevant topics and relatable sentiments, we can better understand participation and also mobilize users for the target campaign.To empirically examine our hypothesis, we study two diversity hashtag activism campaigns on Twitter (#ILookLikeAnEngineer and #WomenInEngineering) using a real-time predictive analytics framework. We design and evaluate the framework with a set of novel features that uses retweetability as an indicator of participation.Our result analysis for topical features found that monetary gain and advertisement-oriented content were less likely to be propagated in the campaigns whereas messaging aligned directly with the issue at hand such as breaking stereotypes in engineering was deemed more retweetable and engaging.In terms of sentiments, informal tone in the messages were considered desirable whereas short-form messaging were not very popular in either movements.These analytical insights can inform activists in effective resource mobilization through message content design, in order to expand the reach of an activism campaign. Our work shows how data-driven techniques can assist in increasing the participation of women in engineering education and the workforce.},
booktitle = {Proceedings of the 32nd ACM Conference on Hypertext and Social Media},
pages = {121–131},
numpages = {11},
keywords = {engineering diversity, hashtag activism},
location = {Virtual Event, USA},
series = {HT '21}
}

@article{10.1145/3423322,
author = {Chen, Xu and Xiong, Kun and Zhang, Yongfeng and Xia, Long and Yin, Dawei and Huang, Jimmy Xiangji},
title = {Neural Feature-Aware Recommendation with Signed Hypergraph Convolutional Network},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3423322},
doi = {10.1145/3423322},
abstract = {Understanding user preference is of key importance for an effective recommender system. For comprehensive user profiling, many efforts have been devoted to extract user feature-level preference from the review information. Despite effectiveness, existing methods mostly assume linear relationships among the users, items, and features, and the collaborative information is usually utilized in an implicit and insufficient manner, which limits the recommender capacity in modeling users’ diverse preferences. For bridging this gap, in this article, we propose to formulate user feature-level preferences by a neural signed hypergraph and carefully design the information propagation paths for diffusing collaborative filtering signals in a more effective manner. By taking the advantages of the neural model’s powerful expressiveness, the complex relationship patterns among users, items, and features are sufficiently discovered and well utilized. By infusing graph structure information into the embedding process, the collaborative information is harnessed in a more explicit and effective way. We conduct comprehensive experiments on real-world datasets to demonstrate the superiorities of our model.},
journal = {ACM Trans. Inf. Syst.},
month = {nov},
articleno = {8},
numpages = {22},
keywords = {feature-based recommendation, graph convolutional network, hypergraph neural network, collaborative filtering, Recommendation system}
}

@article{10.1145/3274421,
author = {Samory, Mattia and Mitra, Tanushree},
title = { 'The Government Spies Using Our Webcams': The Language of Conspiracy Theories in Online Discussions},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274421},
doi = {10.1145/3274421},
abstract = {Conspiracy theories are omnipresent in online discussions---whether to explain a late-breaking event that still lacks official report or to give voice to political dissent. Conspiracy theories evolve, multiply, and interconnect, further complicating efforts to understand them and to limit their propagation. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy---an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. By computationally detecting agent?action?target triplets in conspiratorial statements, and grouping them into semantically coherent clusters, we develop a notion of narrative-motif to detect recurring patterns of triplets. For example, a narrative-motif such as "governmental agency-controls-communications" represents the various ways in which multiple conspiratorial statements denote how governmental agencies control information. Thus, narrative-motifs expose commonalities between multiple conspiracy theories even when they refer to different events or circumstances. In the process, these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media with implications for understanding their adoption and combating their spread.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {152},
numpages = {24},
keywords = {motif, online communities, topic, conspiracy}
}

@article{10.1145/3137597.3137600,
author = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
title = {Fake News Detection on Social Media: A Data Mining Perspective},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3137597.3137600},
doi = {10.1145/3137597.3137600},
abstract = {Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.},
journal = {SIGKDD Explor. Newsl.},
month = {sep},
pages = {22–36},
numpages = {15}
}

@article{10.1145/3122982,
author = {Wang, Di and Al-Rubaie, Ahmad and Clarke, Sandra Stin\v{c}i\'{c} and Davies, John},
title = {Real-Time Traffic Event Detection From Social Media},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3122982},
doi = {10.1145/3122982},
abstract = {Smart communities are composed of groups, organizations, and individuals who share information and make use of that shared information for better decision making. Shared information can come from many sources, particularly, but not exclusively, from sensors and social media. Social media has become an important source of near-instantaneous user-generated information that can be shared and analyzed to support better decision making. One domain where social media data can add value is transportation and traffic management. This article looks at the exploitation of Twitter data in the traffic reporting domain. A key challenge is how to identify relevant information from a huge amount of user-generated data and then analyze the relevant data for automatic geocoded incident detection. The article proposes an instant traffic alert and warning system based on a novel latent Dirichlet allocation (LDA) approach (“tweet-LDA”). The system is evaluated and shown to perform better than related approaches.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {9},
numpages = {23},
keywords = {text mining, tweet mining, incremental learning, latent Dirichlet allocation (LDA), Traffic alert system}
}

@inproceedings{10.1109/ASE.2019.00072,
author = {Nguyen, Son and Nguyen, Tien N. and Li, Yi and Wang, Shaohua},
title = {Combining Program Analysis and Statistical Language Model for Code Statement Completion},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00072},
doi = {10.1109/ASE.2019.00072},
abstract = {Automatic code completion helps improve developers' productivity in their programming tasks. A program contains instructions expressed via code statements, which are considered as the basic units of program execution. In this paper, we introduce AutoSC, which combines program analysis and the principle of software naturalness to fill in a partially completed statement. AutoSC benefits from the strengths of both directions, in which the completed code statement is both frequent and valid. AutoSC is first trained on a large code corpus to derive the templates of candidate statements. Then, it uses program analysis to validate and concretize the templates into syntactically and type-valid candidate statements. Finally, these candidates are ranked by using a language model trained on the lexical form of the source code in the code corpus. Our empirical evaluation on the large datasets of real-world projects shows that AutoSC achieves 38.9--41.3% top-1 accuracy and 48.2--50.1% top-5 accuracy in statement completion. It also outperforms a state-of-the-art approach from 9X--69X in top-1 accuracy.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {710–721},
numpages = {12},
keywords = {statistical language model, program analysis, statement completion, code completion},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/3178048,
author = {Xie, Wei and Zhu, Feida and Xiao, Jing and Wang, Jianzong},
title = {Social Network Monitoring for Bursty Cascade Detection},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3178048},
doi = {10.1145/3178048},
abstract = {Social network services have become important and efficient platforms for users to share all kinds of information. The capability to monitor user-generated information and detect bursts from information diffusions in these social networks brings value to a wide range of real-life applications, such as viral marketing. However, in reality, as a third party, there is always a cost for gathering information from each user or so-called social network sensor. The question then arises how to select a budgeted set of social network sensors to form the data stream for burst detection without compromising the detection performance. In this article, we present a general sensor selection solution for different burst detection approaches. We formulate this problem as a constraint satisfaction problem that has high computational complexity. To reduce the computational cost, we first reduce most of the constraints by making use of the fact that bursty cascades are rare among the whole population. We then transform the problem into an Linear Programming (LP) problem. Furthermore, we use the sub-gradient method instead of the standard simplex method or interior-point method to solve the LP problem, which makes it possible for our solution to scale up to large social networks. Evaluating our solution on millions of real information cascades, we demonstrate both the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {40},
numpages = {24},
keywords = {Social network sensors, sub-gradient method, linear programming}
}

@article{10.1145/3124420,
author = {Joshi, Aditya and Bhattacharyya, Pushpak and Carman, Mark J.},
title = {Automatic Sarcasm Detection: A Survey},
year = {2017},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3124420},
doi = {10.1145/3124420},
abstract = {Automatic sarcasm detection is the task of predicting sarcasm in text. This is a crucial step to sentiment analysis, considering prevalence and challenges of sarcasm in sentiment-bearing text. Beginning with an approach that used speech-based features, automatic sarcasm detection has witnessed great interest from the sentiment analysis community. This article is a compilation of past work in automatic sarcasm detection. We observe three milestones in the research so far: semi-supervised pattern extraction to identify implicit sentiment, use of hashtag-based supervision, and incorporation of context beyond target text. In this article, we describe datasets, approaches, trends, and issues in sarcasm detection. We also discuss representative performance values, describe shared tasks, and provide pointers to future work, as given in prior works. In terms of resources to understand the state-of-the-art, the survey presents several useful illustrations—most prominently, a table that summarizes past papers along different dimensions such as the types of features, annotation techniques, and datasets used.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {73},
numpages = {22},
keywords = {sentiment analysis, opinion, Sarcasm, sentiment, sarcasm detection}
}

@inproceedings{10.1145/3282866.3282868,
author = {Wei, Hong and Sankaranarayanan, Jagan and Samet, Hanan},
title = {Enhancing Local Live Tweet Stream to Detect News},
year = {2018},
isbn = {9781450360357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282866.3282868},
doi = {10.1145/3282866.3282868},
abstract = {Twitter captures invaluable information about real-world news, spanning a wide scale from large national/international stories like a presidential election to small local stories such as a local farmers market. Detecting and extracting small news for a local place is a challenging problem and the focus of this work. The main challenge lies in identifying these small stories that correspond to a local area of interest, which are typically harder to detect compared to national stories in the sense that there may be just a handful of tweets about a local story. A system, called Firefly, is proposed that overcomes the data sparsity and captures thousands of local stories per day from a metropolitan area (e.g., Boston). The key idea lies in combining the enhancement of a local live tweet stream in Twitter, the identification of "locality-aware" keywords, and using these keywords to cluster tweets. Experiments show that the proposed system has a significantly higher recall over a set of representative local news agencies, and at the same time, outperforms the baseline approach TwitterStand. More importantly, the results also demonstrate that our system, by utilizing the enhanced local live tweet stream, discovers much more local news than the methods working only on geotagged tweets, i.e., those with embedded GPS coordinate values.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Analytics for Local Events and News},
articleno = {4},
numpages = {10},
keywords = {News Detection, Geotagging, Apache Spark, Live Tweet Stream, Twitter, Local News},
location = {Seattle, WA, USA},
series = {LENS'18}
}

@article{10.1145/3477127.3477130,
author = {Aslanyan, Tatev Karen and Frasincar, Flavius},
title = {LDA-LFM: A Joint Exploitation of Review Text and Ratings in Recommender Systems},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3477127.3477130},
doi = {10.1145/3477127.3477130},
abstract = {Most of the existing recommender systems are based only on the rating data, and they ignore other sources of information that might increase the quality of recommendations, such as textual reviews, or user and item characteristics. Moreover, the majority of those systems are applicable only on small datasets (with thousands of observations) and are unable to handle large datasets (with millions of observations). We propose a recommender algorithm that combines a rating modeling technique (i.e., Latent Factor Model) with a topic modeling method based on textual reviews (i.e., Latent Dirichlet Allocation), and we extend the algorithm such that it allows adding extra user- and item-specific information to the system. We evaluate the performance of the algorithm using Amazon.com datasets with different sizes, corresponding to 23 product categories. After comparing the built model to four other models, we found that combining textual reviews with ratings leads to better recommendations. Moreover, we found that adding extra user and item features to the model increases its prediction accuracy, which is especially true for medium and large datasets.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {jul},
pages = {33–47},
numpages = {15},
keywords = {e-commerce, recommender systems, textual reviews, latent factor model, latent dirichlet allocation}
}

@article{10.1145/3359253,
author = {Kursuncu, Ugur and Gaur, Manas and Castillo, Carlos and Alambo, Amanuel and Thirunarayan, Krishnaprasad and Shalin, Valerie and Achilov, Dilshod and Arpinar, I. Budak and Sheth, Amit},
title = {Modeling Islamist Extremist Communications on Social Media Using Contextual Dimensions: Religion, Ideology, and Hate},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359253},
doi = {10.1145/3359253},
abstract = {Terror attacks have been linked in part to online extremist content. Online conversations are cloaked in religious ambiguity, with deceptive intentions, often twisted from mainstream meaning to serve a malevolent ideology. Although tens of thousands of Islamist extremism supporters consume such content, they are a small fraction relative to peaceful Muslims. The efforts to contain the ever-evolving extremism on social media platforms have remained inadequate and mostly ineffective. Divergent extremist and mainstream contexts challenge machine interpretation, with a particular threat to the precision of classification algorithms. Radicalization is a subtle long-running persuasive process that occurs over time. Our context-aware computational approach to the analysis of extremist content on Twitter breaks down this persuasion process into building blocks that acknowledge inherent ambiguity and sparsity that likely challenge both manual and automated classification. Based on prior empirical and qualitative research in social sciences, particularly political science, we model this process using a combination of three contextual dimensions -- religion, ideology, and hate -- each elucidating a degree of radicalization and highlighting independent features to render them computationally accessible. We utilize domain-specific knowledge resources for each of these contextual dimensions such as Qur'an for religion, the books of extremist ideologues and preachers for political ideology and a social media hate speech corpus for hate. The significant sensitivity of the Islamist extremist ideology and its local and global security implications require reliable algorithms for modelling such communications on Twitter. Our study makes three contributions to reliable analysis: (i) Development of a computational approach rooted in the contextual dimensions of religion, ideology, and hate, which reflects strategies employed by online Islamist extremist groups, (ii) An in-depth analysis of relevant tweet datasets with respect to these dimensions to exclude likely mislabeled users, and (iii) A framework for understanding online radicalization as a process to assist counter-programming. Given the potentially significant social impact, we evaluate the performance of our algorithms to minimize mislabeling, where our context-aware approach outperforms a competitive baseline by 10.2% in precision, thereby enhancing the potential of such tools for use in human review.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {151},
numpages = {22},
keywords = {multi-dimensional modeling, islamist extremism, contextual dimensions, user modeling, radicalization}
}

@article{10.1109/TASLP.2017.2761546,
author = {Degottex, Gilles and Lanchantin, Pierre and Gales, Mark and Degottex, Gilles and Lanchantin, Pierre and Gales, Mark},
title = {A Log Domain Pulse Model for Parametric Speech Synthesis},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2761546},
doi = {10.1109/TASLP.2017.2761546},
abstract = {Most of the degradation in current Statistical Parametric Speech Synthesis SPSS results from the form of the vocoder. One of the main causes of degradation is the reconstruction of the noise. In this article, a new signal model is proposed that leads to a simple synthesizer, without the need for ad-hoc tuning of model parameters. The model is not based on the traditional additive linear source-filter model, it adopts a combination of speech components that are additive in the log domain. Also, the same representation for voiced and unvoiced segments is used, rather than relying on binary voicing decisions. This avoids voicing error discontinuities that can occur in many current vocoders. A simple binary mask is used to denote the presence of noise in the time-frequency domain, which is less sensitive to classification errors. Four experiments have been carried out to evaluate this new model. The first experiment examines the noise reconstruction issue. Three listening tests have also been carried out that demonstrate the advantages of this model: comparison with the STRAIGHT vocoder; the direct prediction of the binary noise mask by using a mixed output configuration; and partial improvements of creakiness using a mask correction mechanism.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {57–70},
numpages = {14}
}

@article{10.1145/3052930,
author = {Grant, Jason M. and Flynn, Patrick J.},
title = {Crowd Scene Understanding from Video: A Survey},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3052930},
doi = {10.1145/3052930},
abstract = {Crowd video analysis has applications in crowd management, public space design, and visual surveillance. Example tasks potentially aided by automated analysis include anomaly detection (such as a person walking against the grain of traffic or rapid assembly/dispersion of groups of people), population and density measurements, and interactions between groups of people. This survey explores crowd analysis as it relates to two primary research areas: crowd statistics and behavior understanding. First, we survey methods for counting individuals and approximating the density of the crowd. Second, we showcase research efforts on behavior understanding as related to crowds. These works focus on identifying groups, interactions within small groups, and abnormal activity detection such as riots and bottlenecks in large crowds. Works presented in this section also focus on tracking groups of individuals, either as a single entity or a subset of individuals within the frame of reference. Finally, a summary of datasets available for crowd activity video research is provided.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {mar},
articleno = {19},
numpages = {23},
keywords = {datasets, human activity, Crowd analysis}
}

@inproceedings{10.1145/2998181.2998187,
author = {Joseph, Kenneth and Wei, Wei and Carley, Kathleen M.},
title = {Girls Rule, Boys Drool: Extracting Semantic and Affective Stereotypes from Twitter},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998187},
doi = {10.1145/2998181.2998187},
abstract = {Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1362–1374},
numpages = {13},
keywords = {stereotype, computational social science, social psychology, identity, twitter},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/3130920,
author = {Guo, Bin and Ouyang, Yi and Zhang, Cheng and Zhang, Jiafan and Yu, Zhiwen and Wu, Di and Wang, Yu},
title = {CrowdStory: Fine-Grained Event Storyline Generation by Fusion of Multi-Modal Crowdsourced Data},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130920},
doi = {10.1145/3130920},
abstract = {Event summarization based on crowdsourced microblog data is a promising research area, and several researchers have recently focused on this field. However, these previous works fail to characterize the fine-grained evolution of an event and the rich correlations among posts. The semantic associations among the multi-modal data in posts are also not investigated as a means to enhance the summarization performance. To address these issues, this study presents CrowdStory, which aims to characterize an event as a fine-grained, evolutionary, and correlation-rich storyline. A crowd-powered event model and a generic event storyline generation framework are first proposed, based on which a multi-clue--based approach to fine-grained event summarization is presented. The implicit human intelligence (HI) extracted from visual contents and community interactions is then used to identify inter-clue associations. Finally, a cross-media mining approach to selective visual story presentation is proposed. The experiment results indicate that, compared with the state-of-the-art methods, CrowdStory enables fine-grained event summarization (e.g., dynamic evolution) and correctly identifies up to 60% strong correlations (e.g., causality) of clues. The cross-media approach shows diversity and relevancy in visual data selection.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {55},
numpages = {19},
keywords = {Mobile Crowdsourcing, Correlation, Event Sensing, Fine-grained, Storyline}
}

@inproceedings{10.1145/3442381.3450081,
author = {Carmeli, Nofar and Wang, Xiaolan and Suhara, Yoshihiko and Angelidis, Stefanos and Li, Yuliang and Li, Jinfeng and Tan, Wang-Chiew},
title = {Constructing Explainable Opinion Graphs from Reviews},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450081},
doi = {10.1145/3442381.3450081},
abstract = {The Web is a major resource of both factual and subjective information. While there are significant efforts to organize factual information into knowledge bases, there is much less work on organizing opinions, which are abundant in subjective data, into a structured format. We present ExplainIt, a system that extracts and organizes opinions into an opinion graph, which are useful for downstream applications such as generating explainable review summaries and facilitating search over opinion phrases. In such graphs, a node represents a set of semantically similar opinions extracted from reviews and an edge between two nodes signifies that one node explains the other. ExplainIt mines explanations in a supervised method and groups similar opinions together in a weakly supervised way before combining the clusters of opinions together with their explanation relationships into an opinion graph. We experimentally demonstrate that the explanation relationships generated in the opinion graph are of good quality and our labeled datasets for explanation mining and grouping opinions are publicly available at https://github.com/megagonlabs/explainit.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3419–3431},
numpages = {13},
keywords = {explanation, opinion graph construction, Opinion mining},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3449156,
author = {Kang, Laewoo and Jackson, Steven},
title = {Tech-Art-Theory: Improvisational Methods for HCI Learning and Teaching},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449156},
doi = {10.1145/3449156},
abstract = {This paper explores the nature and potential of improvisation as a method for learning and teaching in CSCW and HCI. It starts by reviewing concepts of improvisational learning in classic and more recent work in educational theory, art and music, and HCI that emphasize the reconstructive, materially-driven, error-engaged, transgressive, and collaborative nature of human learning processes. It then describes three pedagogical interventions of our own in which improvisational techniques were deployed as methods of teaching and learning. From this integrated study, we report specific pedagogical conditions (socio-material evaluations, multi-sensory practices, and making safe spaces for error) that can support improvisational learning, and three common challenges of HCI pedagogy relevance, assessment, and inclusion that improvisational methods can help to address.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {82},
numpages = {25},
keywords = {music, improvisation, pedagogy, learning, art, ethnography, performance}
}

@article{10.1145/3446341,
author = {Liu, Hongtao and Wang, Wenjun and Peng, Qiyao and Wu, Nannan and Wu, Fangzhao and Jiao, Pengfei},
title = {Toward Comprehensive User and Item Representations via Three-Tier Attention Network},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3446341},
doi = {10.1145/3446341},
abstract = {Product reviews can provide rich information about the opinions users have of products. However, it is nontrivial to effectively infer user preference and item characteristics from reviews due to the complicated semantic understanding. Existing methods usually learn features for users and items from reviews in single static fashions and cannot fully capture user preference and item features. In this article, we propose a neural review-based recommendation approach that aims to learn comprehensive representations of users/items under a three-tier attention framework. We design a review encoder to learn review features from words via a word-level attention, an aspect encoder to learn aspect features via a review-level attention, and a user/item encoder to learn the final representations of users/items via an aspect-level attention. In word- and review-level attentions, we adopt the context-aware mechanism to indicate importance of words and reviews dynamically instead of static attention weights. In addition, the attentions in the word and review levels are of multiple paradigms to learn multiple features effectively, which could indicate the diversity of user/item features. Furthermore, we propose a personalized aspect-level attention module in user/item encoder to learn the final comprehensive features. Extensive experiments are conducted and the results in rating prediction validate the effectiveness of our method.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {25},
numpages = {22},
keywords = {personalized, Recommender system, multi-aspect, attention, context-aware}
}

@inproceedings{10.1145/3238147.3238191,
author = {Huang, Qiao and Xia, Xin and Xing, Zhenchang and Lo, David and Wang, Xinyu},
title = {API Method Recommendation without Worrying about the Task-API Knowledge Gap},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238191},
doi = {10.1145/3238147.3238191},
abstract = {Developers often need to search for appropriate APIs for their programming tasks. Although most libraries have API reference documentation, it is not easy to find appropriate APIs due to the lexical gap and knowledge gap between the natural language description of the programming task and the API description in API documentation. Here, the lexical gap refers to the fact that the same semantic meaning can be expressed by different words, and the knowledge gap refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes, which are usually the key information in the task description. In this paper, we propose an API recommendation approach named BIKER (Bi-Information source based KnowledgE Recommendation) to tackle these two gaps. To bridge the lexical gap, BIKER uses word embedding technique to calculate the similarity score between two text descriptions. Inspired by our survey findings that developers incorporate Stack Overflow posts and API documentation for bridging the knowledge gap, BIKER leverages Stack Overflow posts to extract candidate APIs for a program task, and ranks candidate APIs by considering the query’s similarity with both Stack Overflow posts and API documentation. It also summarizes supplementary information (e.g., API description, code examples in Stack Overflow posts) for each API to help developers select the APIs that are most relevant to their tasks. Our evaluation with 413 API-related questions confirms the effectiveness of BIKER for both class- and method-level API recommendation, compared with state-of-the-art baselines. Our user study with 28 Java developers further demonstrates the practicality of BIKER for API search.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {293–304},
numpages = {12},
keywords = {API Documentation, Word Embedding, API Recommendation, Stack Overflow},
location = {Montpellier, France},
series = {ASE 2018}
}

@article{10.1145/3384203,
author = {Zhang, Si and Tong, Hanghang and Tang, Jie and Xu, Jiejun and Fan, Wei},
title = {Incomplete Network Alignment: Problem Definitions and Fast Solutions},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3384203},
doi = {10.1145/3384203},
abstract = {Networks are prevalent in many areas and are often collected from multiple sources. However, due to the veracity characteristics, more often than not, networks are incomplete. Network alignment and network completion have become two fundamental cornerstones behind a wealth of high-impact graph mining applications. The state-of-the-art have been addressing these two tasks in parallel. That is, most of the existing network alignment methods have implicitly assumed that the topology of the input networks for alignment are perfectly known a priori, whereas the existing network completion methods admit either a single network (i.e., matrix completion) or multiple aligned networks (e.g., tensor completion). In this article, we argue that network alignment and completion are inherently complementary with each other, and hence propose to jointly address them so that the two tasks can mutually benefit from each other. We formulate the problem from the optimization perspective, and propose an effective algorithm (iNeAt) to solve it. The proposed method offers two distinctive advantages. First (Alignment accuracy), our method benefits from the higher-quality input networks while mitigates the effect of the incorrectly inferred links introduced by the completion task itself. Second (Alignment efficiency), thanks to the low-rank structure of the complete networks and the alignment matrix, the alignment process can be significantly accelerated. We perform extensive experiments which show that (1) the network completion can significantly improve the alignment accuracy, i.e., up to 30% over the baseline methods; (2) the network alignment can in turn help recover more missing edges than the baseline methods; and (3) our method achieves a good balance between the running time and the accuracy, and scales with a provable linear complexity in both time and space.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {may},
articleno = {38},
numpages = {26},
keywords = {Incomplete network alignment, network completion, low rank}
}

@article{10.1007/s00778-019-00564-x,
author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ib\'{a}\~{n}ez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul},
title = {Dataset Search: A Survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00564-x},
doi = {10.1007/s00778-019-00564-x},
abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.},
journal = {The VLDB Journal},
month = {jan},
pages = {251–272},
numpages = {22},
keywords = {Dataset retrieval, Information search and retrieval, Dataset search, Dataset}
}

@article{10.1145/3267442,
author = {Yu, Xianqi and Sun, Yuqing and Bertino, Elisa and Li, Xin},
title = {Modeling User Intrinsic Characteristic on Social Media for Identity Linkage},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2469-7818},
url = {https://doi.org/10.1145/3267442},
doi = {10.1145/3267442},
abstract = {Most users on social media have intrinsic characteristics, such as interests and political views, that can be exploited to identify and track them, thus raising privacy and identity concerns in online communities. In this article, we investigate the problem of user identity linkage on two behavior datasets collected from different experiments. Specifically, we focus on user linkage based on users’ interaction behaviors with respect to content topics. We propose an embedding method to model a topic as a vector in a latent space to interpret its deep semantics. Then a user is modeled as a vector based on his or her interactions with topics. The embedding representations of topics are learned by optimizing the joint-objective: the compatibility between topics with similar semantics, the discriminative abilities of topics to distinguish identities, and the consistency of the same user’s characteristics from two datasets. The effectiveness of our method is verified on real-life datasets and the results show that it outperforms related methods. We also analyze failure cases in the application of our identity linkage method. Our analysis shows that factors such as the visibility and variance of user behaviors and users’ group psychology can result in mis-linkages. We also analyze the details of the behaviors of some representative users to understand the essential reasons for their identity being mis-linked. We find that these users have high variance level in their behaviors. According to the above experimental results, we introduce a confidence score into identity linkage to provide information about the accuracy of the method results.},
journal = {Trans. Soc. Comput.},
month = {dec},
articleno = {11},
numpages = {25},
keywords = {Social media, embedding method, privacy issue, intrinsic characteristic, identity linkage}
}

@article{10.1007/s00778-017-0469-2,
author = {Zhou, Xiangmin and Chen, Lei and Zhang, Yanchun and Qin, Dong and Cao, Longbing and Huang, Guangyan and Wang, Chen},
title = {Enhancing Online Video Recommendation Using Social User Interactions},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0469-2},
doi = {10.1007/s00778-017-0469-2},
abstract = {The creation of media sharing communities has resulted in the astonishing increase of digital videos, and their wide applications in the domains like online news broadcasting, entertainment and advertisement. The improvement of these applications relies on effective solutions for social user access to videos. This fact has driven the research interest in the recommendation in shared communities. Though effort has been put into social video recommendation, the contextual information on social users has not been well exploited for effective recommendation. Motivated by this, in this paper, we propose a novel approach based on the video content and user information for the recommendation in shared communities. A new solution is developed by allowing batch video recommendation to multiple new users and optimizing the subcommunity extraction. We first propose an effective technique that reduces the subgraph partition cost based on graph decomposition and reconstruction for efficient subcommunity extraction. Then, we design a summarization-based algorithm which groups the clicked videos of multiple unregistered users and simultaneously provide recommendation to each of them. Finally, we present a nontrivial social updates maintenance approach for social data based on user connection summarization. We evaluate the performance of our solution over a large dataset considering different strategies for group video recommendation in sharing communities.},
journal = {The VLDB Journal},
month = {oct},
pages = {637–656},
numpages = {20},
keywords = {Online video recommendation, Group video summarization, Social relevance}
}

@article{10.1145/3439816,
author = {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
title = {Conditional Text Generation for Harmonious Human-Machine Interaction},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3439816},
doi = {10.1145/3439816},
abstract = {In recent years, with the development of deep learning, text-generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text-generation technology, that is, the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that much attention has been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summarize several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {14},
numpages = {50},
keywords = {personalization, Human-computer interaction, deep learning, dialog systems, conditional text generation}
}

@article{10.1145/3426974,
author = {Li, Zhixin and Lin, Lan and Zhang, Canlong and Ma, Huifang and Zhao, Weizhong and Shi, Zhiping},
title = {A Semi-Supervised Learning Approach Based on Adaptive Weighted Fusion for Automatic Image Annotation},
year = {2021},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3426974},
doi = {10.1145/3426974},
abstract = {To learn a well-performed image annotation model, a large number of labeled samples are usually required. Although the unlabeled samples are readily available and abundant, it is a difficult task for humans to annotate large numbers of images manually. In this article, we propose a novel semi-supervised approach based on adaptive weighted fusion for automatic image annotation that can simultaneously utilize the labeled data and unlabeled data to improve the annotation performance. At first, two different classifiers, constructed based on support vector machine and covolutional neural network, respectively, are trained by different features extracted from the labeled data. Therefore, these two classifiers are independently represented as different feature views. Then, the corresponding features of unlabeled images are extracted and input into these two classifiers, and the semantic annotation of images can be obtained respectively. At the same time, the confidence of corresponding image annotation can be measured by an adaptive weighted fusion strategy. After that, the images and its semantic annotations with high confidence are submitted to the classifiers for retraining until a certain stop condition is reached. As a result, we can obtain a strong classifier that can make full use of unlabeled data. Finally, we conduct experiments on four datasets, namely, Corel 5K, IAPR TC12, ESP Game, and NUS-WIDE. In addition, we measure the performance of our approach with standard criteria, including precision, recall, F-measure, N+, and mAP. The experimental results show that our approach has superior performance and outperforms many state-of-the-art approaches.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
articleno = {37},
numpages = {23},
keywords = {adaptive weighted fusion, co-training, covolutional neural network, Automatic image annotation, support vector machine, semi-supervised learning}
}

@article{10.1145/3115433,
author = {Tiwari, Akanksha and Weth, Christian Von Der and Kankanhalli, Mohan S.},
title = {Multimodal Multiplatform Social Media Event Summarization},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3115433},
doi = {10.1145/3115433},
abstract = {Social media platforms are turning into important news sources since they provide real-time information from different perspectives. However, high volume, dynamism, noise, and redundancy exhibited by social media data make it difficult to comprehend the entire content. Recent works emphasize on summarizing the content of either a single social media platform or of a single modality (either textual or visual). However, each platform has its own unique characteristics and user base, which brings to light different aspects of real-world events. This makes it critical as well as challenging to combine textual and visual data from different platforms. In this article, we propose summarization of real-world events with data stemming from different platforms and multiple modalities. We present the use of a Markov Random Fields based similarity measure to link content across multiple platforms. This measure also enables the linking of content across time, which is useful for tracking the evolution of long-running events. For the final content selection, summarization is modeled as a subset selection problem. To handle the complexity of the optimal subset selection, we propose the use of submodular objectives. Facets such as coverage, novelty, and significance are modeled as submodular objectives in a multimodal social media setting. We conduct a series of quantitative and qualitative experiments to illustrate the effectiveness of our approach compared to alternative methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {apr},
articleno = {38},
numpages = {23},
keywords = {user study, Markov random fields, evaluation, topic modeling, Social media summarization, multimedia}
}

@article{10.1145/3168361,
author = {Liu, Qi and Wu, Runze and Chen, Enhong and Xu, Guandong and Su, Yu and Chen, Zhigang and Hu, Guoping},
title = {Fuzzy Cognitive Diagnosis for Modelling Examinee Performance},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3168361},
doi = {10.1145/3168361},
abstract = {Recent decades have witnessed the rapid growth of educational data mining (EDM), which aims at automatically extracting valuable information from large repositories of data generated by or related to people’s learning activities in educational settings. One of the key EDM tasks is cognitive modelling with examination data, and cognitive modelling tries to profile examinees by discovering their latent knowledge state and cognitive level (e.g. the proficiency of specific skills). However, to the best of our knowledge, the problem of extracting information from both objective and subjective examination problems to achieve more precise and interpretable cognitive analysis remains underexplored. To this end, we propose a fuzzy cognitive diagnosis framework (FuzzyCDF) for examinees’ cognitive modelling with both objective and subjective problems. Specifically, to handle the partially correct responses on subjective problems, we first fuzzify the skill proficiency of examinees. Then we combine fuzzy set theory and educational hypotheses to model the examinees’ mastery on the problems based on their skill proficiency. Finally, we simulate the generation of examination score on each problem by considering slip and guess factors. In this way, the whole diagnosis framework is built. For further comprehensive verification, we apply our FuzzyCDF to three classical cognitive assessment tasks, i.e., predicting examinee performance, slip and guess detection, and cognitive diagnosis visualization. Extensive experiments on three real-world datasets for these assessment tasks prove that FuzzyCDF can reveal the knowledge states and cognitive level of the examinees effectively and interpretatively.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {48},
numpages = {26},
keywords = {educational data mining, Cognitive, graphic model}
}

@inproceedings{10.1145/3491102.3517474,
author = {Balayn, Agathe and Rikalo, Natasa and Lofi, Christoph and Yang, Jie and Bozzon, Alessandro},
title = {How Can Explainability Methods Be Used to Support Bug Identification in Computer Vision Models?},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517474},
doi = {10.1145/3491102.3517474},
abstract = {Deep learning models for image classification suffer from dangerous issues often discovered after deployment. The process of identifying bugs that cause these issues remains limited and understudied. Especially, explainability methods are often presented as obvious tools for bug identification. Yet, the current practice lacks an understanding of what kind of explanations can best support the different steps of the bug identification process, and how practitioners could interact with those explanations. Through a formative study and an iterative co-creation process, we build an interactive design probe providing various potentially relevant explainability functionalities, integrated into interfaces that allow for flexible workflows. Using the probe, we perform 18 user-studies with a diverse set of machine learning practitioners. Two-thirds of the practitioners engage in successful bug identification. They use multiple types of explanations, e.g. visual and textual ones, through non-standardized sequences of interactions including queries and exploration. Our results highlight the need for interactive, guiding, interfaces with diverse explanations, shedding light on future research directions.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {184},
numpages = {16},
keywords = {machine learning explainability, user interface, machine learning model debugging, computer vision},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3462777,
author = {Aloufi, Samah and Saddik, Abdulmotaleb El},
title = {MMSUM Digital Twins: A Multi-View Multi-Modality Summarization Framework for Sporting Events},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3462777},
doi = {10.1145/3462777},
abstract = {Sporting events generate a massive amount of traffic on social media with live moment-to-moment accounts as any given situation unfolds. The generated data are intensified by fans feelings, reactions, and subjective opinions towards what happens during the event, all of which are based on their individual points of view. Analyzing and summarizing this data will generate a comprehensive overview of the event in terms of how the event evolves and how fans react and view the event based on their perspectives. Previously, most of the summarization works ignore fan reactions and subjective opinions, and focus primarily on generating an objective-view summary. We believe that an effective and useful summary should consider human reactions, sentiment, and point of view, as opposed to simply describing what happens during the event. Accordingly, in this work, we propose MMSUM Digital Twins: a summarization framework that is capable of generating a multi-view multi-modal summary for sporting events in real-time. The proposed digital twins-based framework consists of four main components: sub-event recognition which detects the event’s key moments, tweet categorization, which determines which team the tweets’ writers support and assigns tweets to their teams, sentiment analysis to track fans’ state of mind, and image popularity prediction for selecting representative images. Furthermore, the MMSUM employs a visual-filtering model to address the issue of noisy images that inundate social media, compromising the summarization quality. We leverage the knowledge of sport fans to evaluate the generated multi-view summarization through an online user study. The experiment results confirm the effectiveness of our proposed approach for summarizing sporting events by considering multimedia data, sentiment, and subjective views of the event.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {5},
numpages = {25},
keywords = {sentiment analysis, subjective, social events, social media, multimedia, popularity prediction, multi-view, summarization, Digital twins, sporting events}
}

@article{10.1145/3494998,
author = {Meyer, Johannes and Frank, Adrian and Schlebusch, Thomas and Kasneci, Enkeljeda},
title = {A CNN-Based Human Activity Recognition System Combining a Laser Feedback Interferometry Eye Movement Sensor and an IMU for Context-Aware Smart Glasses},
year = {2022},
issue_date = {Dec 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3494998},
doi = {10.1145/3494998},
abstract = {Smart glasses are considered the next breakthrough in wearables. As the successor of smart watches and smart ear wear, they promise to extend reality by immersive embedding of content in the user's field of view. While advancements in display technology seems to fulfill this promises, interaction concepts are derived from established wearable concepts like touch interaction or voice interaction, preventing full immersion as they require the user to frequently interact with the glasses. To minimize interactions, we propose to add context-awareness to smart glasses through human activity recognition (HAR) by combining head- and eye movement features to recognize a wide range of activities. To measure eye movements in unobtrusive way, we propose laser feedback interferometry (LFI) sensors. These tiny low power sensors are highly robust to ambient light. We combine LFI sensors and an IMU to collect eye and head movement features from 15 participants performing 7 cognitive and physical activities, leading to a unique data set. To recognize activities we propose a 1D-CNN model and apply transfer learning to personalize the classification, leading to an outstanding macro-F1 score of 88.15 % which outperforms state of the art methods. Finally, we discuss the applicability of the proposed system in a smart glasses setup.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {172},
numpages = {24},
keywords = {head and eye movement, Laser Feedback Interferometry, Human activity recognition, context awarness smart glasses}
}

@inproceedings{10.1145/3377811.3380436,
author = {Chen, Lingchao and Hassan, Foyzul and Wang, Xiaoyin and Zhang, Lingming},
title = {Taming Behavioral Backward Incompatibilities via Cross-Project Testing and Analysis},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380436},
doi = {10.1145/3377811.3380436},
abstract = {In modern software development, software libraries play a crucial role in reducing software development effort and improving software quality. However, at the same time, the asynchronous upgrades of software libraries and client software projects often result in incompatibilities between different versions of libraries and client projects. When libraries evolve, it is often very challenging for library developers to maintain the so-called backward compatibility and keep all their external behavior untouched, and behavioral backward incompatibilities (BBIs) may occur. In practice, the regression test suites of library projects often fail to detect all BBIs. Therefore, in this paper, we propose DeBBI to detect BBIs via cross-project testing and analysis, i.e., using the test suites of various client projects to detect library BBIs. Since executing all the possible client projects can be extremely time consuming, DeBBI transforms the problem of cross-project BBI detection into a traditional information retrieval (IR) problem to execute the client projects with higher probability to detect BBIs earlier. Furthermore, DeBBI considers project diversity and test relevance information for even faster BBI detection. The experimental results show that DeBBI can reduce the end-to-end testing time for detecting the first and average unique BBIs by 99.1% and 70.8% for JDK compared to naive cross-project BBI detection. Also, DeBBI has been applied to other popular 3rd-party libraries. To date, DeBBI has detected 97 BBI bugs with 19 already confirmed as previously unknown bugs.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {112–124},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3510003.3510057,
author = {Biswas, Sumon and Wardat, Mohammad and Rajan, Hridesh},
title = {The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines in Theory, in-the-Small, and in-the-Large},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510057},
doi = {10.1145/3510003.3510057},
abstract = {Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2091–2103},
numpages = {13},
keywords = {data science pipelines, data science processes, descriptive, predictive},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3359242,
author = {Santos, Tiago and Lemmerich, Florian and Strohmaier, Markus and Helic, Denis},
title = {What's in a Review: Discrepancies Between Expert and Amateur Reviews of Video Games on Metacritic},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359242},
doi = {10.1145/3359242},
abstract = {As video game press ("experts") and casual gamers ("amateurs") have different motivations when writing video game reviews, discrepancies in their reviews may arise. To study such potential discrepancies, we conduct a large-scale investigation of more than 1 million reviews on the Metacritic review platform. In particular, we assess the existence and nature of discrepancies in video game appraisal by experts and amateurs, and how they manifest in ratings, over time, and in review language. Leveraging these insights, we explore the predictive power of early expert vs. amateur reviews in forecasting video game reputation in the short- and long-term. We find that amateurs, in contrast to experts, give more polarized ratings of video games, rate games surprisingly long after game release, and are positively biased towards older games. On a textual level, we observe that experts write rather complex, less readable texts than amateurs, whose reviews are more emotionally charged. While in the short-term amateur reviews are remarkably predictive of game reputation among other amateurs (achieving 91% ROC AUC in a binary classification), both expert and amateur reviews are equally well suited for long-term predictions. Overall, our work is the first large-scale comparative study of video game reviewing behavior, with practical implications for amateurs when deciding which games to play, and for game developers when planning which games to design, develop, or continuously support. More broadly, our work contributes to the discussion of wisdom of the few vs. wisdom of the crowds, as we uncover the limits of experts in capturing the views of amateurs in the particular context of video game reviews.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {140},
numpages = {22},
keywords = {wisdom of the crowds, video game reviews, review aggregator websites}
}

@article{10.1145/2996183,
author = {Alsaedi, Nasser and Burnap, Pete and Rana, Omer},
title = {Can We Predict a Riot? Disruptive Event Detection Using Twitter},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2996183},
doi = {10.1145/2996183},
abstract = {In recent years, there has been increased interest in real-world event detection using publicly accessible data made available through Internet technology such as Twitter, Facebook, and YouTube. In these highly interactive systems, the general public are able to post real-time reactions to “real world” events, thereby acting as social sensors of terrestrial activity. Automatically detecting and categorizing events, particularly small-scale incidents, using streamed data is a non-trivial task but would be of high value to public safety organisations such as local police, who need to respond accordingly. To address this challenge, we present an end-to-end integrated event detection framework that comprises five main components: data collection, pre-processing, classification, online clustering, and summarization. The integration between classification and clustering enables events to be detected, as well as related smaller-scale “disruptive events,” smaller incidents that threaten social safety and security or could disrupt social order. We present an evaluation of the effectiveness of detecting events using a variety of features derived from Twitter posts, namely temporal, spatial, and textual content. We evaluate our framework on a large-scale, real-world dataset from Twitter. Furthermore, we apply our event detection system to a large corpus of tweets posted during the August 2011 riots in England. We use ground-truth data based on intelligence gathered by the London Metropolitan Police Service, which provides a record of actual terrestrial events and incidents during the riots, and show that our system can perform as well as terrestrial sources, and even better in some cases.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {18},
numpages = {26},
keywords = {classification, clustering, evaluation, event detection, Social media, feature selection}
}

@article{10.1145/3365916,
author = {Kim, Hyun and Na, Seung-Hoon},
title = {Uniformly Interpolated Balancing for Robust Prediction in Translation Quality Estimation: A Case Study of English-Korean Translation},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365916},
doi = {10.1145/3365916},
abstract = {There has been growing interest among researchers in quality estimation (QE), which attempts to automatically predict the quality of machine translation (MT) outputs. Most existing works on QE are based on supervised approaches using quality-annotated training data. However, QE training data quality scores readily become imbalanced or skewed: QE data are mostly composed of high translation quality sentence pairs but the data lack low translation quality sentence pairs. The use of imbalanced data with an induced quality estimator tends to produce biased translation quality scores with “high” translation quality scores assigned even to poorly translated sentences. To address the data imbalance, this article proposes a simple, efficient procedure called uniformly interpolated balancing to construct more balanced QE training data by inserting greater uniformness to training data. The proposed uniformly interpolated balancing procedure is based on the preparation of two different types of manually annotated QE data: (1) default skewed data and (2) near-uniform data. First, we obtain default skewed data in a naive manner without considering the imbalance by manually annotating qualities on MT outputs. Second, we obtain near-uniform data in a selective manner by manually annotating a subset only, which is selected from the automatically quality-estimated sentence pairs. Finally, we create uniformly interpolated balanced data by combining these two types of data, where one half originates from the default skewed data and the other half originates from the near-uniform data. We expect that uniformly interpolated balancing reflects the intrinsic skewness of the true quality distribution and manages the imbalance problem. Experimental results on an English-Korean quality estimation task show that the proposed uniformly interpolated balancing leads to robustness on both skewed and uniformly distributed quality test sets when compared to the test sets of other non-balanced datasets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {37},
numpages = {27},
keywords = {Predictor-Estimator, uniformly interpolated balancing, imbalanced data, Translation quality estimation}
}

@article{10.5555/3122009.3122010,
author = {Ishiguro, Katsuhiko and Sato, Issei and Ueda, Naonori},
title = {Averaged Collapsed Variational Bayes Inference},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This paper presents the Averaged CVB (ACVB) inference and oers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence-guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non-expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1–29},
numpages = {29},
keywords = {collapsed variational Bayes inference, averaged CVB, nonparametric Bayes}
}

@inproceedings{10.1145/3395363.3397383,
author = {Peng, Qianyang and Shi, August and Zhang, Lingming},
title = {Empirically Revisiting and Enhancing IR-Based Test-Case Prioritization},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397383},
doi = {10.1145/3395363.3397383},
abstract = {Test-case prioritization (TCP) aims to detect regression bugs faster via reordering the tests run. While TCP has been studied for over 20 years, it was almost always evaluated using seeded faults/mutants as opposed to using real test failures. In this work, we study the recent change-aware information retrieval (IR) technique for TCP. Prior work has shown it performing better than traditional coverage-based TCP techniques, but it was only evaluated on a small-scale dataset with a cost-unaware metric based on seeded faults/mutants. We extend the prior work by conducting a much larger and more realistic evaluation as well as proposing enhancements that substantially improve the performance. In particular, we evaluate the original technique on a large-scale, real-world software-evolution dataset with real failures using both cost-aware and cost-unaware metrics under various configurations. Also, we design and evaluate hybrid techniques combining the IR features, historical test execution time, and test failure frequencies. Our results show that the change-aware IR technique outperforms stateof-the-art coverage-based techniques in this real-world setting, and our hybrid techniques improve even further upon the original IR technique. Moreover, we show that flaky tests have a substantial impact on evaluating the change-aware TCP techniques based on real test failures.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {324–336},
numpages = {13},
keywords = {Test-case prioritization, continuous integration, information retrieval},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1145/3226029,
author = {Tan, Feng and Liu, Liansheng and Winter, Stefan and Wang, Qixin and Suri, Neeraj and Bu, Lei and Peng, Yu and Liu, Xue and Peng, Xiyuan},
title = {Cross-Domain Noise Impact Evaluation for Black Box Two-Level Control CPS},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3226029},
doi = {10.1145/3226029},
abstract = {Control Cyber-Physical Systems (CPSs) constitute a major category of CPS. In control CPSs, in addition to the well-studied noises within the physical subsystem, we are interested in evaluating the impact of cross-domain noise: the noise that comes from the physical subsystem, propagates through the cyber subsystem, and goes back to the physical subsystem. Impact of cross-domain noise is hard to evaluate when the cyber subsystem is a black box, which cannot be explicitly modeled. To address this challenge, this article focuses on the two-level control CPS, a widely adopted control CPS architecture, and proposes an emulation based evaluation methodology framework. The framework uses hybrid model reachability to quantify the cross-domain noise impact, and exploits Lyapunov stability theories to reduce the evaluation benchmark size. We validated the effectiveness and efficiency of our proposed framework on a representative control CPS testbed. Particularly, 24.1% of evaluation effort is saved using the proposed benchmark shrinking technology.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {2},
numpages = {25},
keywords = {testing, cyber-physical systems, hybrid automata, hybrid model, Lyapunov stable}
}

@article{10.1145/3011019,
author = {Wang, Weiqing and Yin, Hongzhi and Chen, Ling and Sun, Yizhou and Sadiq, Shazia and Zhou, Xiaofang},
title = {ST-SAGE: A Spatial-Temporal Sparse Additive Generative Model for Spatial Item Recommendation},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3011019},
doi = {10.1145/3011019},
abstract = {With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important mobile application, especially when users travel away from home. However, this type of recommendation is very challenging compared to traditional recommender systems. A user may visit only a limited number of spatial items, leading to a very sparse user-item matrix. This matrix becomes even sparser when the user travels to a distant place, as most of the items visited by a user are usually located within a short distance from the user’s home. Moreover, user interests and behavior patterns may vary dramatically across different time and geographical regions. In light of this, we propose ST-SAGE, a spatial-temporal sparse additive generative model for spatial item recommendation in this article. ST-SAGE considers both personal interests of the users and the preferences of the crowd in the target region at the given time by exploiting both the co-occurrence patterns and content of spatial items. To further alleviate the data-sparsity issue, ST-SAGE exploits the geographical correlation by smoothing the crowd’s preferences over a well-designed spatial index structure called the spatial pyramid. To speed up the training process of ST-SAGE, we implement a parallel version of the model inference algorithm on the GraphLab framework. We conduct extensive experiments; the experimental results clearly demonstrate that ST-SAGE outperforms the state-of-the-art recommender systems in terms of recommendation effectiveness, model training efficiency, and online recommendation efficiency.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {48},
numpages = {25},
keywords = {real-time recommendation, efficient retrieval algorithm, Point of interest (POI), online learning, location-based service}
}

@article{10.1145/3380537,
author = {Arora, Udit and Dutta, Hridoy Sankar and Joshi, Brihi and Chetan, Aditya and Chakraborty, Tanmoy},
title = {Analyzing and Detecting Collusive Users Involved in Blackmarket Retweeting Activities},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3380537},
doi = {10.1145/3380537},
abstract = {With the rise in popularity of social media platforms like Twitter, having higher influence on these platforms has a greater value attached to it, since it has the power to influence many decisions in the form of brand promotions and shaping opinions. However, blackmarket services that allow users to inorganically gain influence are a threat to the credibility of these social networking platforms. Twitter users can gain inorganic appraisals in the form of likes, retweets, and follows through these blackmarket services either by paying for them or by joining syndicates wherein they gain such appraisals by providing similar appraisals to other users. These customers tend to exhibit a mix of organic and inorganic retweeting behavior, making it tougher to detect them.In this article, we investigate these blackmarket customers engaged in collusive retweeting activities. We collect and annotate a novel dataset containing various types of information about blackmarket customers and use these sources of information to construct multiple user representations. We adopt Weighted Generalized Canonical Correlation Analysis (WGCCA) to combine these individual representations to derive user embeddings that allow us to effectively classify users as: genuine users, bots, promotional customers, and normal customers. Our method significantly outperforms state-of-the-art approaches (32.95% better macro F1-score than the best baseline).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {35},
numpages = {24},
keywords = {Twitter, blackmarket, collusion, multiview learning, OSNs, Retweeters}
}

@article{10.1145/3265986,
author = {An, J. and Kwak, H. and Jung, S. and Salminen, J. and Admad, M. and Jansen, B.},
title = {Imaginary People Representing Real Numbers: Generating Personas from Online Social Media Data},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/3265986},
doi = {10.1145/3265986},
abstract = {We develop a methodology to automate creating imaginary people, referred to as personas, by processing complex behavioral and demographic data of social media audiences. From a popular social media account containing more than 30 million interactions by viewers from 198 countries engaging with more than 4,200 online videos produced by a global media corporation, we demonstrate that our methodology has several novel accomplishments, including: (a) identifying distinct user behavioral segments based on the user content consumption patterns; (b) identifying impactful demographics groupings; and (c) creating rich persona descriptions by automatically adding pertinent attributes, such as names, photos, and personal characteristics. We validate our approach by implementing the methodology into an actual working system; we then evaluate it via quantitative methods by examining the accuracy of predicting content preference of personas, the stability of the personas over time, and the generalizability of the method via applying to two other datasets. Research findings show the approach can develop rich personas representing the behavior and demographics of real audiences using privacy-preserving aggregated online social media data from major online platforms. Results have implications for media companies and other organizations distributing content via online platforms.},
journal = {ACM Trans. Web},
month = {nov},
articleno = {27},
numpages = {26},
keywords = {Persona, user analytics}
}

@article{10.1145/3196826,
author = {Gysel, Christophe Van and de Rijke, Maarten and Kanoulas, Evangelos},
title = {Neural Vector Spaces for Unsupervised Information Retrieval},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3196826},
doi = {10.1145/3196826},
abstract = {We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed. NVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments.},
journal = {ACM Trans. Inf. Syst.},
month = {jun},
articleno = {38},
numpages = {25},
keywords = {document retrieval, semantic matching, representation learning, Ad-hoc retrieval, latent vector spaces}
}

@article{10.1145/3391250,
author = {Ma, Jing and Gao, Wei and Joty, Shafiq and Wong, Kam-Fai},
title = {An Attention-Based Rumor Detection Model with Tree-Structured Recursive Neural Networks},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391250},
doi = {10.1145/3391250},
abstract = {Rumor spread in social media severely jeopardizes the credibility of online content. Thus, automatic debunking of rumors is of great importance to keep social media a healthy environment. While facing a dubious claim, people often dispute its truthfulness sporadically in their posts containing various cues, which can form useful evidence with long-distance dependencies. In this work, we propose to learn discriminative features from microblog posts by following their non-sequential propagation structure and generate more powerful representations for identifying rumors. For modeling non-sequential structure, we first represent the diffusion of microblog posts with propagation trees, which provide valuable clues on how a claim in the original post is transmitted and developed over time. We then present a bottom-up and a top-down tree-structured models based on Recursive Neural Networks (RvNN) for rumor representation learning and classification, which naturally conform to the message propagation process in microblogs. To enhance the rumor representation learning, we reveal that effective rumor detection is highly related to finding evidential posts, e.g., the posts expressing specific attitude towards the veracity of a claim, as an extension of the previous RvNN-based detection models that treat every post equally. For this reason, we design discriminative attention mechanisms for the RvNN-based models to selectively attend on the subset of evidential posts during the bottom-up/top-down recursive composition. Experimental results on four datasets collected from real-world microblog platforms confirm that (1) our RvNN-based models achieve much better rumor detection and classification performance than state-of-the-art approaches; (2) the attention mechanisms for focusing on evidential posts can further improve the performance of our RvNN-based method; and (3) our approach possesses superior capacity on detecting rumors at a very early stage.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jun},
articleno = {42},
numpages = {28},
keywords = {neural attention, Rumor detection and classification, social media, recursive neural networks, propagation tree}
}

@inproceedings{10.1145/3491102.3502114,
author = {Sun, Jiao and Wu, Tongshuang and Jiang, Yue and Awalegaonkar, Ronil and Lin, Xi Victoria and Yang, Diyi},
title = {Pretty Princess vs. Successful Leader: Gender Roles in Greeting Card Messages},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502114},
doi = {10.1145/3491102.3502114},
abstract = {People write personalized greeting cards on various occasions. While prior work has studied gender roles in greeting card messages, systematic analysis at scale and tools for raising the awareness of gender stereotyping remain under-investigated. To this end, we collect a large greeting card message corpus covering three different occasions (birthday, Valentine’s Day and wedding) from three sources (exemplars from greeting message websites, real-life greetings from social media and language model generated ones). We uncover a wide range of gender stereotypes in this corpus via topic modeling, odds ratio and Word Embedding Association Test (WEAT). We further conduct a survey to understand people’s perception of gender roles in messages from this corpus and if gender stereotyping is a concern. The results show that people want to be aware of gender roles in the messages, but remain unconcerned unless the perceived gender roles conflict with the recipient’s true personality. In response, we developed GreetA, an interactive visualization and writing assistant tool to visualize fine-grained topics in greeting card messages drafted by the users and the associated gender perception scores, but without suggesting text changes as an intervention.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {398},
numpages = {15},
keywords = {greeting card messages, gender role awareness, visualization system},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3449088,
author = {Garg, Radhika and Kapadia, Yash and Sengupta, Subhasree},
title = {Using the Lenses of Emotion and Support to Understand Unemployment Discourse on Reddit},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449088},
doi = {10.1145/3449088},
abstract = {Unemployed individuals experience emotional upheaval and financial constraints, and they struggle to share their challenges and obtain support from their family members and social circles. While prior work has shown that social media platforms enable their users to safely share their turmoil, hardships, and emotional upheavals and seek support, little is known about how the unemployed use such platforms. Based on a mixed-methods analysis of data retrieved from unemployment communities on Reddit, our study reveals that redditors in these communities engage in exchanges that range from queries seeking information about job searching to discussions about sensitive topics such as the health and social implications of unemployment. Also, while redditors use mostly negative emotional tonality in their texts when initiating a new post, the comments in response provide considerable social support in the form of esteem, emotional, informational, network, and instrumental support. Based on these findings, we share the theoretical implications of our work and offer design},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {14},
numpages = {24},
keywords = {reddit, job search, unemployed, social support, emotion, unemployment}
}

@article{10.1145/3297722,
author = {Atanasova, Pepa and Nakov, Preslav and M\`{a}rquez, Llu\'{\i}s and Barr\'{o}n-Cede\~{n}o, Alberto and Karadzhov, Georgi and Mihaylova, Tsvetomila and Mohtarami, Mitra and Glass, James},
title = {Automatic Fact-Checking Using Context and Discourse Information},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297722},
doi = {10.1145/3297722},
abstract = {We study the problem of automatic fact-checking, paying special attention to the impact of contextual and discourse information. We address two related tasks: (i)&nbsp;detecting check-worthy claims and (ii)&nbsp;fact-checking claims. We develop supervised systems based on neural networks, kernel-based support vector machines, and combinations thereof, which make use of rich input representations in terms of discourse cues and contextual features. For the check-worthiness estimation task, we focus on political debates, and we model the target claim in the context of the full intervention of a participant and the previous and following turns in the debate, taking into account contextual meta information. For the fact-checking task, we focus on answer verification in a community forum, and we model the veracity of the answer with respect to the entire question–answer thread in which it occurs as well as with respect to other related posts from the entire forum. We develop annotated datasets for both tasks and we run extensive experimental evaluation, confirming that both types of information—but especially contextual features—play an important role.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {12},
numpages = {27},
keywords = {discourse, Fact-checking, community question-answering}
}

@article{10.1145/3106368,
author = {Costa, Gianni and Ortale, Riccardo},
title = {Mining Overlapping Communities and Inner Role Assignments through Bayesian Mixed-Membership Models of Networks with Context-Dependent Interactions},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3106368},
doi = {10.1145/3106368},
abstract = {Community discovery and role assignment have been recently integrated into an unsupervised approach for the exploratory analysis of overlapping communities and inner roles in networks. However, the formation of ties in these prototypical research efforts is not truly realistic, since it does not account for a fundamental aspect of link establishment in real-world networks, i.e., the explicative reasons that cause interactions among nodes. Such reasons can be interpreted as generic requirements of nodes, that are met by other nodes and essentially pertain both to the nodes themselves and to their interaction contexts (i.e., the respective communities and roles).In this article, we present two new model-based machine-learning approaches, wherein community discovery and role assignment are seamlessly integrated and simultaneously performed through approximate posterior inference in Bayesian mixed-membership models of directed networks. The devised models account for the explicative reasons governing link establishment in terms of node-specific and contextual latent interaction factors. The former are inherently characteristic of nodes, while the latter are characterizations of nodes in the context of the individual communities and roles. The generative process of both models assigns nodes to communities with respective roles and connects them through directed links, which are probabilistically governed by their node-specific and contextual interaction factors. The difference between the proposed models lies in the exploitation of the contextual interaction factors. More precisely, in one model, the contextual interaction factors have the same impact on link generation. In the other model, the contextual interaction factors are weighted by the extent of involvement of the linked nodes in the respective communities and roles.We develop MCMC algorithms implementing approximate posterior inference and parameter estimation within our models.Finally, we conduct an intensive comparative experimentation, which demonstrates their superiority in community compactness and link prediction on various real-world and synthetic networks.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jan},
articleno = {18},
numpages = {32},
keywords = {role assignment, Overlapping community detection, link prediction, Bayesian probabilistic network analysis}
}

@article{10.1145/3133882,
author = {Yuan, Binhang and Murali, Vijayaraghavan and Jermaine, Christopher},
title = {Abridging Source Code},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133882},
doi = {10.1145/3133882},
abstract = {In this paper, we consider the problem of source code abridgment, where the goal is to remove statements from a source code in order to display the source code in a small space, while at the same time leaving the ``important'' parts of the source code intact, so that an engineer can read the code and quickly understand purpose of the code. To this end, we develop an algorithm that looks at a number of examples, human-created source code abridgments, and learns how to remove lines from the code in order to mimic the human abridger. The learning algorithm takes into account syntactic features of the code, as well as semantic features such as control flow and data dependencies. Through a comprehensive user study, we show that the abridgments that our system produces can decrease the time that a user must look at code in order to understand its functionality, as well as increase the accuracy of the assessment, while displaying the code in a greatly reduced area.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {58},
numpages = {26},
keywords = {Source Code Abridgment, Program Comprehension}
}

@article{10.1145/3441451,
author = {Xia, Peike and Jiang, Wenjun and Wu, Jie and Xiao, Surong and Wang, Guojun},
title = {Exploiting Temporal Dynamics in Product Reviews for Dynamic Sentiment Prediction at the Aspect Level},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441451},
doi = {10.1145/3441451},
abstract = {Online reviews and ratings play an important role in shaping the purchase decisions of customers in e-commerce. Many researches have been done to make proper recommendations for users, by exploiting reviews, ratings, user profiles, or behaviors. However, the dynamic evolution of user preferences and item properties haven’t been fully exploited. Moreover, it lacks fine-grained studies at the aspect level. To address the above issues, we define two concepts of user maturity and item popularity, to better explore the dynamic changes for users and items. We strive to exploit fine-grained information at the aspect level and the evolution of users and items, for dynamic sentiment prediction. First, we analyze three real datasets from both the overall level and the aspect level, to discover the dynamic changes (i.e., gradual changes and sudden changes) in user aspect preferences and item aspect properties. Next, we propose a novel model of Aspect-based Sentiment Dynamic Prediction (ASDP), to dynamically capture and exploit the change patterns with uniform time intervals. We further propose the improved model ASDP+ with a bin segmentation algorithm to set the time intervals non-uniformly based on the sudden changes. Experimental results on three real-world datasets show that our work leads to significant improvements.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {68},
numpages = {29},
keywords = {aspect level, sentiment prediction, Review mining, temporal dynamics, opinion evolution}
}

@article{10.14778/3389133.3389136,
author = {Ma, Minghua and Yin, Zheng and Zhang, Shenglin and Wang, Sheng and Zheng, Christopher and Jiang, Xinhao and Hu, Hanwen and Luo, Cheng and Li, Yilin and Qiu, Nengjun and Li, Feifei and Chen, Changcheng and Pei, Dan},
title = {Diagnosing Root Causes of Intermittent Slow Queries in Cloud Databases},
year = {2020},
issue_date = {April 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3389133.3389136},
doi = {10.14778/3389133.3389136},
abstract = {With the growing market of cloud databases, careful detection and elimination of slow queries are of great importance to service stability. Previous studies focus on optimizing the slow queries that result from internal reasons (e.g., poorly-written SQLs). In this work, we discover a different set of slow queries which might be more hazardous to database users than other slow queries. We name such queries Intermittent Slow Queries (iSQs), because they usually result from intermittent performance issues that are external (e.g., at database or machine levels). Diagnosing root causes of iSQs is a tough but very valuable task.This paper presents iSQUAD, Intermittent Slow QUery Anomaly Diagnoser, a framework that can diagnose the root causes of iSQs with a loose requirement for human intervention. Due to the complexity of this issue, a machine learning approach comes to light naturally to draw the interconnection between iSQs and root causes, but it faces challenges in terms of versatility, labeling overhead and interpretability. To tackle these challenges, we design four components, i.e., Anomaly Extraction, Dependency Cleansing, Type-Oriented Pattern Integration Clustering (TOPIC) and Bayesian Case Model. iSQUAD consists of an offline clustering &amp; explanation stage and an online root cause diagnosis &amp; update stage. DBAs need to label each iSQ cluster only once at the offline stage unless a new type of iSQs emerges at the online stage. Our evaluations on real-world datasets from Alibaba OLTP Database show that iSQUAD achieves an iSQ root cause diagnosis average F1-score of 80.4%, and outperforms existing diagnostic tools in terms of accuracy and efficiency.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {1176–1189},
numpages = {14}
}

@article{10.14778/3342263.3342277,
author = {Feng, Kaiyu and Cong, Gao and Jensen, Christian S. and Guo, Tao},
title = {Finding Attribute-Aware Similar Regions for Data Analysis},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342277},
doi = {10.14778/3342263.3342277},
abstract = {With the proliferation of mobile devices and location-based services, increasingly massive volumes of geo-tagged data are becoming available. This data typically also contains non-location information. We study how to use such information to characterize a region and then how to find a region of the same size and with the most similar characteristics. This functionality enables a user to identify regions that share characteristics with a user-supplied region that the user is familiar with and likes. More specifically, we formalize and study a new problem called the attribute-aware similar region search (ASRS) problem. We first define so-called composite aggregators that are able to express aspects of interest in terms of the information associated with a user-supplied region. When applied to a region, an aggregator captures the region's relevant characteristics. Next, given a query region and a composite aggregator, we propose a novel algorithm called DS-Search to find the most similar region of the same size. Unlike any previous work on region search, DS-Search repeatedly discretizes and splits regions until an split region either satisfies a drop condition or it is guaranteed to not contribute to the result. In addition, we extend DS-Search to solve the ASRS problem approximately. Finally, we report on extensive empirical studies that offer insight into the efficiency and effectiveness of the paper's proposals.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1414–1426},
numpages = {13}
}

@article{10.1145/3202662,
author = {Middleton, Stuart E. and Kordopatis-Zilos, Giorgos and Papadopoulos, Symeon and Kompatsiaris, Yiannis},
title = {Location Extraction from Social Media: Geoparsing, Location Disambiguation, and Geotagging},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3202662},
doi = {10.1145/3202662},
abstract = {Location extraction, also called “toponym extraction,” is a field covering geoparsing, extracting spatial representations from location mentions in text, and geotagging, assigning spatial coordinates to content items. This article evaluates five “best-of-class” location extraction algorithms. We develop a geoparsing algorithm using an OpenStreetMap database, and a geotagging algorithm using a language model constructed from social media tags and multiple gazetteers. Third-party work evaluated includes a DBpedia-based entity recognition and disambiguation approach, a named entity recognition and Geonames gazetteer approach, and a Google Geocoder API approach. We perform two quantitative benchmark evaluations, one geoparsing tweets and one geotagging Flickr posts, to compare all approaches. We also perform a qualitative evaluation recalling top N location mentions from tweets during major news events. The OpenStreetMap approach was best (F1 0.90+) for geoparsing English, and the language model approach was best (F1 0.66) for Turkish. The language model was best (F1@1km 0.49) for the geotagging evaluation. The map database was best (R@20 0.60+) in the qualitative evaluation. We report on strengths, weaknesses, and a detailed failure analysis for the approaches and suggest concrete areas for further research.},
journal = {ACM Trans. Inf. Syst.},
month = {jun},
articleno = {40},
numpages = {27},
keywords = {disambiguation, information extraction, geoparsing, social media, location, geotagging, toponym extraction, benchmark, geocoding, toponym, Location extraction}
}

@article{10.1145/3017429,
author = {Cao, Da and He, Xiangnan and Nie, Liqiang and Wei, Xiaochi and Hu, Xia and Wu, Shunxiang and Chua, Tat-Seng},
title = {Cross-Platform App Recommendation by Jointly Modeling Ratings and Texts},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3017429},
doi = {10.1145/3017429},
abstract = {Over the last decade, the renaissance of Web technologies has transformed the online world into an application (App) driven society. While the abundant Apps have provided great convenience, their sheer number also leads to severe information overload, making it difficult for users to identify desired Apps. To alleviate the information overloading issue, recommender systems have been proposed and deployed for the App domain. However, existing work on App recommendation has largely focused on one single platform (e.g., smartphones), while it ignores the rich data of other relevant platforms (e.g., tablets and computers).In this article, we tackle the problem of cross-platform App recommendation, aiming at leveraging users’ and Apps’ data on multiple platforms to enhance the recommendation accuracy. The key advantage of our proposal is that by leveraging multiplatform data, the perpetual issues in personalized recommender systems—data sparsity and cold-start—can be largely alleviated. To this end, we propose a hybrid solution, STAR (short for “croSs-plaTform App Recommendation”) that integrates both numerical ratings and textual content from multiple platforms. In STAR, we innovatively represent an App as an aggregation of common features across platforms (e.g., App’s functionalities) and specific features that are dependent on the resided platform. In light of this, STAR can discriminate a user’s preference on an App by separating the user’s interest into two parts (either in the App’s inherent factors or platform-aware features). To evaluate our proposal, we construct two real-world datasets that are crawled from the App stores of iPhone, iPad, and iMac. Through extensive experiments, we show that our STAR method consistently outperforms highly competitive recommendation methods, justifying the rationality of our cross-platform App recommendation proposal and the effectiveness of our solution.},
journal = {ACM Trans. Inf. Syst.},
month = {jul},
articleno = {37},
numpages = {27},
keywords = {hybrid system, App recommendation, cross-platform, cold-start}
}

@article{10.1145/3342101,
author = {Almoqbel, Mashael and Xu, Songhua},
title = {Computational Mining of Social Media to Curb Terrorism},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3342101},
doi = {10.1145/3342101},
abstract = {In the ever-connected social networking era, terrorists exploit social media platforms via sophisticated approaches. To curb these activities, a rich collection of computational methods was developed. This article surveys the use of social media by terrorists, followed by a temporal classification framework that overviews computational countermeasures at four major stages, including inception of an attack, immediately before an attack, onset of an attack, and after an attack. The literature surveyed was organized around the four temporal stages. The resulting survey is summarized in a table with the main technology used in each stage based on the time of the attack.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {87},
numpages = {25},
keywords = {Terrorism, counterterrorism, computational analytics, social media mining}
}

@article{10.1145/3191778,
author = {Xu, Fengli and Xia, Tong and Cao, Hancheng and Li, Yong and Sun, Funing and Meng, Fanchao},
title = {Detecting Popular Temporal Modes in Population-Scale Unlabelled Trajectory Data},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191778},
doi = {10.1145/3191778},
abstract = {With the rapid process of urbanization, revealing the underlying mechanisms behind urban mobility has become a crucial research problem. The movements of urban dwellers are often constituted by their daily routines, and exhibit distinct and contextual temporal modes, i.e., the patterns of individuals allocating their time across different locations. In this paper, we investigate a novel problem of detecting popular temporal modes in population-scale unlabelled trajectory data. Our key finding is that the detected temporal modes capture the semantic feature of human's living style, and is able to unravel meaningful correlations between urban mobility and human behavior.Specifically, we represent the temporal mode of a trajectory as a partition of the time duration, where the time slices associated with same locations are partitioned into same subsets. Such abstraction decouples the temporal modes from actual physical locations, and allows individuals with similar temporal modes yet completely different physical locations to have similar representations. Based on this insight, we propose a pipeline system composed of three components: 1) noise handler that eliminates the noises in the raw mobility records, 2) representation extractor for temporal modes, and 3) popular temporal modes detector. By applying our system on three real-world mobility datasets, we demonstrate that our system effectively detects the popular temporal modes embedded in population-scale mobility datasets, which is easy to be interpreted and can be justified through the associated PoIs and mobile applications usage. More importantly, our further experiments reveal insightful correlations between the popular temporal modes and individuals' social economic status, i.e. occupation information, which sheds light on the mechanisms behind urban mobility.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {46},
numpages = {25},
keywords = {mobility data, human behaviour modelling, temporal modes}
}

@inproceedings{10.1145/3098822.3098835,
author = {Gupta, Trinabh and Fingler, Henrique and Alvisi, Lorenzo and Walfish, Michael},
title = {Pretzel: Email Encryption and Provider-Supplied Functions Are Compatible},
year = {2017},
isbn = {9781450346535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098822.3098835},
doi = {10.1145/3098822.3098835},
abstract = {Emails today are often encrypted, but only between mail servers---the vast majority of emails are exposed in plaintext to the mail servers that handle them. While better than no encryption, this arrangement leaves open the possibility of attacks, privacy violations, and other disclosures. Publicly, email providers have stated that default end-to-end encryption would conflict with essential functions (spam filtering, etc.), because the latter requires analyzing email text. The goal of this paper is to demonstrate that there is no conflict. We do so by designing, implementing, and evaluating Pretzel. Starting from a cryptographic protocol that enables two parties to jointly perform a classification task without revealing their inputs to each other, Pretzel refines and adapts this protocol to the email context. Our experimental evaluation of a prototype demonstrates that email can be encrypted end-to-end and providers can compute over it, at tolerable cost: clients must devote some storage and processing, and provider overhead is roughly 5x versus the status quo.},
booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
pages = {169–182},
numpages = {14},
keywords = {encrypted email, linear classifiers, secure two-party computation},
location = {Los Angeles, CA, USA},
series = {SIGCOMM '17}
}

@inproceedings{10.1145/3491102.3502076,
author = {Gauthier, Robert P and Costello, Mary Jean and Wallace, James R},
title = {“I Will Not Drink With You Today”: A Topic-Guided Thematic Analysis of Addiction Recovery on Reddit},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502076},
doi = {10.1145/3491102.3502076},
abstract = {Recovery from addiction is a journey that requires a lifetime of support from a strong network of peers. Many people seek out this support through online communities, like those on Reddit. However, as these communities developed outside of existing aid groups and medical practice, it is unclear how they enable recovery. Their scale also limits researchers’ ability to engage through traditional qualitative research methods. To study these groups, we performed a topic-guided thematic analysis that used machine-generated topic models to purposively sample from two recovery subreddits: r/stopdrinking and r/OpiatesRecovery. We show that these communities provide access to an experienced and accessible support group whose discussions include consequences, reflections, and celebrations, but that also play a distinct metacommunicative role in supporting formal treatment. We discuss how these communities can act as knowledge sources to improve in-person recovery support and medical practice, and how computational techniques can enable HCI researchers to study communities at scale.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {20},
numpages = {17},
keywords = {addiction, machine learning, Reddit, thematic analysis, online communities},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3309543,
author = {Zhan, Xueying and Wang, Yaowei and Rao, Yanghui and Li, Qing},
title = {Learning from Multi-Annotator Data: A Noise-Aware Classification Framework},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309543},
doi = {10.1145/3309543},
abstract = {In the field of sentiment analysis and emotion detection in social media, or other tasks such as text classification involving supervised learning, researchers rely more heavily on large and accurate labelled training datasets. However, obtaining large-scale labelled datasets is time-consuming and high-quality labelled datasets are expensive and scarce. To deal with these problems, online crowdsourcing systems provide us an efficient way to accelerate the process of collecting training data via distributing the enormous tasks to various annotators to help create large amounts of labelled data at an affordable cost. Nowadays, these crowdsourcing platforms are heavily needed in dealing with social media text, since the social network platforms (e.g., Twitter) generate huge amounts of data in textual form everyday. However, people from different social and knowledge backgrounds have different views on various texts, which may lead to noisy labels. The existing noisy label aggregation/refinement algorithms mostly focus on aggregating labels from noisy annotations, which would not guarantee their effectiveness on the subsequent classification/ranking tasks. In this article, we propose a noise-aware classification framework that integrates the steps of noisy label aggregation and classification. The aggregated noisy crowd labels are fed into a classifier for training, while the predicted labels are employed as feedback for adjusting the parameters at the label aggregating stage. The classification framework is suitable for directly running on crowdsourcing datasets and applies to various kinds of classification algorithms. The feedback strategy makes it possible for us to find optimal parameters instead of using known data for parameter selection. Simulation experiments demonstrate that our method provide significant label aggregation performance for both binary and multiple classification tasks under various noisy environments. Experimenting on real-world data validates the feasibility of our framework in real noise data and helps us verify the reasonableness of the simulated experiment settings.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {26},
numpages = {28},
keywords = {sentiment analysis, Social media, crowdsourcing, emotion detection}
}

@article{10.1145/3299886,
author = {Wang, Huan and Wu, Jia and Hu, Wenbin and Wu, Xindong},
title = {Detecting and Assessing Anomalous Evolutionary Behaviors of Nodes in Evolving Social Networks},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299886},
doi = {10.1145/3299886},
abstract = {Based on the performance of entire social networks, anomaly analysis for evolving social networks generally ignores the otherness of the evolutionary behaviors of different nodes, such that it is difficult to precisely identify the anomalous evolutionary behaviors of nodes (AEBN). Assuming that a node's evolutionary behavior that generates and removes edges normally follows stable evolutionary mechanisms, this study focuses on detecting and assessing AEBN, whose evolutionary mechanisms deviate from their past mechanisms, and proposes a link prediction detection (LPD) method and a matrix perturbation assessment (MPA) method. LPD describes a node's evolutionary behavior by fitting its evolutionary mechanism, and designs indexes for edge generation and removal to evaluate the extent to which the evolutionary mechanism of a node's evolutionary behavior can be fitted by a link prediction algorithm. Furthermore, it detects AEBN by quantifying the differences among behavior vectors that characterize the node's evolutionary behaviors in different periods. In addition, MPA considers AEBN as a perturbation of the social network structure, and quantifies the effect of AEBN on the social network structure based on matrix perturbation analysis. Extensive experiments on eight disparate real-world networks demonstrate that analyzing AEBN from the perspective of evolutionary mechanisms is important and beneficial.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jan},
articleno = {12},
numpages = {24},
keywords = {Anomalous evolutionary behaviors of nodes, social network, perturbation analysis, assessment, detection}
}

@article{10.1145/3281659,
author = {Chen, Xu and Zhang, Yongfeng and Xu, Hongteng and Qin, Zheng and Zha, Hongyuan},
title = {Adversarial Distillation for Efficient Recommendation with External Knowledge},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3281659},
doi = {10.1145/3281659},
abstract = {Integrating external knowledge into the recommendation system has attracted increasing attention in both industry and academic communities. Recent methods mostly take the power of neural network for effective knowledge representation to improve the recommendation performance. However, the heavy deep architectures in existing models are usually incorporated in an embedded manner, which may greatly increase the model complexity and lower the runtime efficiency.To simultaneously take the power of deep learning for external knowledge modeling as well as maintaining the model efficiency at test time, we reformulate the problem of recommendation with external knowledge into a generalized distillation framework. The general idea is to free the complex deep architecture into a separate model, which is only used in the training phrase, while abandoned at test time. In particular, in the training phrase, the external knowledge is processed by a comprehensive teacher model to produce valuable information to teach a simple and efficient student model. Once the framework is learned, the teacher model is abandoned, and only the succinct yet enhanced student model is used to make fast predictions at test time. In this article, we specify the external knowledge as user review, and to leverage it in an effective manner, we further extend the traditional generalized distillation framework by designing a Selective Distillation Network (SDNet) with adversarial adaption and orthogonality constraint strategies to make it more robust to noise information.Extensive experiments verify that our model can not only improve the performance of rating prediction, but also can significantly reduce time consumption when making predictions as compared with several state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = {dec},
articleno = {12},
numpages = {28},
keywords = {personalization, adversarial training, Recommendation system, external knowledge, distillation network}
}

@article{10.1145/3003725,
author = {Zhang, Dongxiang and Guo, Long and Nie, Liqiang and Shao, Jie and Wu, Sai and Shen, Heng Tao},
title = {Targeted Advertising in Public Transportation Systems with Quantitative Evaluation},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3003725},
doi = {10.1145/3003725},
abstract = {In spite of vast business potential, targeted advertising in public transportation systems is a grossly unexplored research area. For instance, SBS Transit in Singapore can reach 1 billion passengers per year but the annual advertising revenue contributes less than $35 million. To bridge the gap, we propose a probabilistic data model that captures the motion patterns and user interests so as to quantitatively evaluate the impact of an advertisement among the passengers. In particular, we leverage hundreds of millions of bus/train boarding transaction records to quantitatively estimate the probability as well as the extent of a user being influenced by an ad. Based on the influence model, we study a top-k retrieval problem for bus/train ad recommendation, which acts as a primitive operator to support various advanced applications. We solve the retrieval problem efficiently to support real-time decision making. In the experimental study, we use the dataset from SBS Transit as a case study to verify the effectiveness and efficiency of our proposed methodologies.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {20},
numpages = {29},
keywords = {public transportation systems, Targeted advertising, quantitative evaluation, user interest, probabilistic data model, mobility pattern}
}

@article{10.1145/3469887,
author = {Yang, Jun and Ma, Weizhi and Zhang, Min and Zhou, Xin and Liu, Yiqun and Ma, Shaoping},
title = {LegalGNN: Legal Information Enhanced Graph Neural Network for Recommendation},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3469887},
doi = {10.1145/3469887},
abstract = {Recommendation in legal scenario (Legal-Rec) is a specialized recommendation task that aims to provide potential helpful legal documents for users. While there are mainly three differences compared with traditional recommendation: (1) Both the structural connections and textual contents of legal information are important in the Legal-Rec scenario, which means feature fusion is very important here. (2) Legal-Rec users prefer the newest legal cases (the latest legal interpretation and legal practice), which leads to a severe new-item problem. (3) Different from users in other scenarios, most Legal-Rec users are expert and domain-related users. They often concentrate on several topics and have more stable information needs. So it is important to accurately model user interests here. To the best of our knowledge, existing recommendation work cannot handle these challenges simultaneously.To address these challenges, we propose a legal information enhanced graph neural network–based recommendation framework (LegalGNN). First, a unified legal content and structure representation model is designed for feature fusion, where the Heterogeneous Legal Information Network (HLIN) is constructed to connect the structural features (e.g., knowledge graph) and contextual features (e.g., the content of legal documents) for training. Second, to model user interests, we incorporate the queries users issued in legal systems into the HLIN and link them with both retrieved documents and inquired users. This extra information is not only helpful for estimating user preferences, but also valuable for cold users/items (with less interaction history) in this scenario. Third, a graph neural network with relational attention mechanism is applied to make use of high-order connections in HLIN for Legal-Rec. Experimental results on a real-world legal dataset verify that LegalGNN outperforms several state-of-the-art methods significantly. As far as we know, LegalGNN is the first graph neural model for legal recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {33},
numpages = {29},
keywords = {graph neural network, heterogeneous environments, Legal information recommendation, heterogeneous information network}
}

@article{10.1145/3465407,
author = {Kim, Chris and Lin, Xiao and Collins, Christopher and Taylor, Graham W. and Amer, Mohamed R.},
title = {Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3465407},
doi = {10.1145/3465407},
abstract = {While the computer vision problem of searching for activities in videos is usually addressed by using discriminative models, their decisions tend to be opaque and difficult for people to understand. We propose a case study of a novel machine learning approach for generative searching and ranking of motion capture activities with visual explanation. Instead of directly ranking videos in the database given a text query, our approach uses a variant of Generative Adversarial Networks (GANs) to generate exemplars based on the query and uses them to search for the activity of interest in a large database. Our model is able to achieve comparable results to its discriminative counterpart, while being able to dynamically generate visual explanations. In addition to our searching and ranking method, we present an explanation interface that enables the user to successfully explore the model’s explanations and its confidence by revealing query-based, model-generated motion capture clips that contributed to the model’s decision. Finally, we conducted a user study with 44 participants to show that by using our model and interface, participants benefit from a deeper understanding of the model’s conceptualization of the search query. We discovered that the XAI system yielded a comparable level of efficiency, accuracy, and user-machine synchronization as its black-box counterpart, if the user exhibited a high level of trust for AI explanation.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {aug},
articleno = {23},
numpages = {34},
keywords = {model-generated explanation, Explainable artificial intelligence, user study, trust and reliance}
}

@article{10.1145/3298988,
author = {Wu, Libing and Quan, Cong and Li, Chenliang and Wang, Qian and Zheng, Bolong and Luo, Xiangyang},
title = {A Context-Aware User-Item Representation Learning for Item Recommendation},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3298988},
doi = {10.1145/3298988},
abstract = {Both reviews and user-item interactions (i.e., rating scores) have been widely adopted for user rating prediction. However, these existing techniques mainly extract the latent representations for users and items in an independent and static manner. That is, a single static feature vector is derived to encode user preference without considering the particular characteristics of each candidate item. We argue that this static encoding scheme is incapable of fully capturing users’ preferences, because users usually exhibit different preferences when interacting with different items. In this article, we propose a novel context-aware user-item representation learning model for rating prediction, named CARL. CARL derives a joint representation for a given user-item pair based on their individual latent features and latent feature interactions. Then, CARL adopts Factorization Machines to further model higher order feature interactions on the basis of the user-item pair for rating prediction. Specifically, two separate learning components are devised in CARL to exploit review data and interaction data, respectively: review-based feature learning and interaction-based feature learning. In the review-based learning component, with convolution operations and attention mechanism, the pair-based relevant features for the given user-item pair are extracted by jointly considering their corresponding reviews. However, these features are only reivew-driven and may not be comprehensive. Hence, an interaction-based learning component further extracts complementary features from interaction data alone, also on the basis of user-item pairs. The final rating score is then derived with a dynamic linear fusion mechanism. Experiments on seven real-world datasets show that CARL achieves significantly better rating prediction accuracy than existing state-of-the-art alternatives. Also, with the attention mechanism, we show that the pair-based relevant information (i.e., context-aware information) in reviews can be highlighted to interpret the rating prediction for different user-item pairs.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {22},
numpages = {29},
keywords = {neural networks, recommendation systems, Rating prediction}
}

@article{10.1145/3487289,
author = {Jahanbakhsh-Nagadeh, Zoleikha and Feizi-Derakhshi, Mohammad-Reza and Sharifi, Arash},
title = {A Deep Content-Based Model for Persian Rumor Verification},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3487289},
doi = {10.1145/3487289},
abstract = {During the development of social media, there has been a transformation in social communication. Despite their positive applications in social interactions and news spread, it also provides an ideal platform for spreading rumors. Rumors can endanger the security of society in normal or critical situations. Therefore, it is important to detect and verify the rumors in the early stage of their spreading. Many research works have focused on social attributes in the social network to solve the problem of rumor detection and verification, while less attention has been paid to content features. The social and structural features of rumors develop over time and are not available in the early stage of rumor. Therefore, this study presented a content-based model to verify the Persian rumors on Twitter and Telegram early. The proposed model demonstrates the important role of content in spreading rumors and generates a better-integrated representation for each source rumor document by fusing its semantic, pragmatic, and syntactic information. First, contextual word embeddings of the source rumor are generated by a hybrid model based on ParsBERT and parallel CapsNets. Then, pragmatic and syntactic features of the rumor are extracted and concatenated with embeddings to capture the rich information for rumor verification. Experimental results on real-world datasets demonstrated that the proposed model significantly outperforms the state-of-the-art models in the early rumor verification task. Also, it can enhance the performance of the classifier from 2% to 11% on Twitter and from 5% to 23% on Telegram. These results validate the model's effectiveness when limited content information is available.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {12},
numpages = {29},
keywords = {ParsBERT, Rumor verification, speech act, Persian rumor classification, contextual features, neural language model}
}

@article{10.1145/3185045,
author = {Zimbra, David and Abbasi, Ahmed and Zeng, Daniel and Chen, Hsinchun},
title = {The State-of-the-Art in Twitter Sentiment Analysis: A Review and Benchmark Evaluation},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3185045},
doi = {10.1145/3185045},
abstract = {Twitter has emerged as a major social media platform and generated great interest from sentiment analysis researchers. Despite this attention, state-of-the-art Twitter sentiment analysis approaches perform relatively poorly with reported classification accuracies often below 70%, adversely impacting applications of the derived sentiment information. In this research, we investigate the unique challenges presented by Twitter sentiment analysis and review the literature to determine how the devised approaches have addressed these challenges. To assess the state-of-the-art in Twitter sentiment analysis, we conduct a benchmark evaluation of 28 top academic and commercial systems in tweet sentiment classification across five distinctive data sets. We perform an error analysis to uncover the causes of commonly occurring classification errors. To further the evaluation, we apply select systems in an event detection case study. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of the next generation of approaches.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
articleno = {5},
numpages = {29},
keywords = {natural language processing, Sentiment analysis, twitter, social media, opinion mining, benchmark evaluation, text mining}
}

@article{10.1145/3086676,
author = {Farseev, Aleksandr and Chua, Tat-Seng},
title = {Tweet Can Be Fit: Integrating Data from Wearable Sensors and Multiple Social Networks for Wellness Profile Learning},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086676},
doi = {10.1145/3086676},
abstract = {Wellness is a widely popular concept that is commonly applied to fitness and self-help products or services. Inference of personal wellness--related attributes, such as body mass index (BMI) category or disease tendency, as well as understanding of global dependencies between wellness attributes and users’ behavior, is of crucial importance to various applications in personal and public wellness domains. At the same time, the emergence of social media platforms and wearable sensors makes it feasible to perform wellness profiling for users from multiple perspectives. However, research efforts on wellness profiling and integration of social media and sensor data are relatively sparse. This study represents one of the first attempts in this direction. Specifically, we infer personal wellness attributes by utilizing our proposed multisource multitask wellness profile learning framework—WellMTL—which can handle data incompleteness and perform wellness attributes inference from sensor and social media data simultaneously. To gain insights into the data at a global level, we also examine correlations between first-order data representations and personal wellness attributes. Our experimental results show that the integration of sensor data and multiple social media sources can substantially boost the performance of individual wellness profiling.},
journal = {ACM Trans. Inf. Syst.},
month = {aug},
articleno = {42},
numpages = {34},
keywords = {wearable sensors, multitask learning, personal lifestyle assistance, wellness profile learning, Multiple sources integration}
}

@article{10.1145/3492826,
author = {Nova, Fayika Farhat and Coupe, Amanda and Mynatt, Elizabeth D. and Guha, Shion and Pater, Jessica A.},
title = {Cultivating the Community: Inferring Influence within Eating Disorder Networks on Twitter},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492826},
doi = {10.1145/3492826},
abstract = {A growing body of HCI research has sought to understand how online networks are utilized in the adoption and maintenance of disordered activities and behaviors associated with mental illness, including eating habits. However, individual-level influences over discrete online eating disorder (ED) communities are not yet well understood. This study reports results from a comprehensive network and content analysis (combining computational topic modeling and qualitative thematic analysis) of over 32,000 public tweets collected using popular ED-related hashtags during May 2020. Our findings indicate that this ED network in Twitter consists of multiple smaller ED communities where a majority of the nodes are exposed to unhealthy ED contents through retweeting certain influential central nodes. The emergence of novel linguistic indicators and trends (e.g., "#meanspo") also demonstrates the evolving nature of the ED network. This paper contextualizes ED influence in online communities through node-level participation and engagement, as well as relates emerging ED contents with established online behaviors, such as self-harassment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {7},
numpages = {33},
keywords = {self-harassment, eating disorder, influence, meanspo, network analysis, online communities}
}

@article{10.1145/3008662,
author = {Bernaschi, Massimo and Celestini, Alessandro and Guarino, Stefano and Lombardi, Flavio},
title = {Exploring and Analyzing the Tor Hidden Services Graph},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/3008662},
doi = {10.1145/3008662},
abstract = {The exploration and analysis of Web graphs has flourished in the recent past, producing a large number of relevant and interesting research results. However, the unique characteristics of the Tor network limit the applicability of standard techniques and demand for specific algorithms to explore and analyze it. The attention of the research community has focused on assessing the security of the Tor infrastructure (i.e., its ability to actually provide the intended level of anonymity) and on discussing what Tor is currently being used for. Since there are no foolproof techniques for automatically discovering Tor hidden services, little or no information is available about the topology of the Tor Web graph. Even less is known on the relationship between content similarity and topological structure. The present article aims at addressing such lack of information. Among its contributions: a study on automatic Tor Web exploration/data collection approaches; the adoption of novel representative metrics for evaluating Tor data; a novel in-depth analysis of the hidden services graph; a rich correlation analysis of hidden services’ semantics and topology. Finally, a broad interesting set of novel insights/considerations over the Tor Web organization and content are provided.},
journal = {ACM Trans. Web},
month = {jul},
articleno = {24},
numpages = {26},
keywords = {network topology, correlation analysis, Web graphs, automatic web exploration}
}

@article{10.1145/3230665,
author = {Wilson, Shomir and Schaub, Florian and Liu, Frederick and Sathyendra, Kanthashree Mysore and Smullen, Daniel and Zimmeck, Sebastian and Ramanath, Rohan and Story, Peter and Liu, Fei and Sadeh, Norman and Smith, Noah A.},
title = {Analyzing Privacy Policies at Scale: From Crowdsourcing to Automated Annotations},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3230665},
doi = {10.1145/3230665},
abstract = {Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have the time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Some recent efforts have aimed to use a combination of crowdsourcing, machine learning, and natural language processing to interpret privacy policies at scale, thus producing annotations for use in interfaces that inform Internet users of salient policy details. However, little attention has been devoted to studying the accuracy of crowdsourced privacy policy annotations, how crowdworker productivity can be enhanced for such a task, and the levels of granularity that are feasible for automatic analysis of privacy policies. In this article, we present a trajectory of work addressing each of these topics. We include analyses of crowdworker performance, evaluation of a method to make a privacy-policy oriented task easier for crowdworkers, a coarse-grained approach to labeling segments of policy text with descriptive themes, and a fine-grained approach to identifying user choices described in policy text. Together, the results from these efforts show the effectiveness of using automated and semi-automated methods for extracting from privacy policies the data practice details that are salient to Internet users’ interests.},
journal = {ACM Trans. Web},
month = {dec},
articleno = {1},
numpages = {29},
keywords = {Privacy, machine learning, human computer interaction (HCI), crowdsourcing, privacy policies, natural language processing}
}

@inproceedings{10.1145/3487552.3487850,
author = {Zeng, Eric and Wei, Miranda and Gregersen, Theo and Kohno, Tadayoshi and Roesner, Franziska},
title = {Polls, Clickbait, and Commemorative $2 Bills: Problematic Political Advertising on News and Media Websites around the 2020 U.S. Elections},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487850},
doi = {10.1145/3487552.3487850},
abstract = {Online advertising can be used to mislead, deceive, and manipulate Internet users, and political advertising is no exception. In this paper, we present a measurement study of online advertising around the 2020 United States elections, with a focus on identifying dark patterns and other potentially problematic content in political advertising. We scraped ad content on 745 news and media websites from six geographic locations in the U.S. from September 2020 to January 2021, collecting 1.4 million ads. We perform a systematic qualitative analysis of political content in these ads, as well as a quantitative analysis of the distribution of political ads on different types of websites. Our findings reveal the widespread use of problematic tactics in political ads, such as bait-and-switch ads formatted as opinion polls to entice users to click, the use of political controversy by content farms for clickbait, and the more frequent occurrence of political ads on highly partisan news websites. We make policy recommendations for online political advertising, including greater scrutiny of non-official political ads and comprehensive standards across advertising platforms.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {507–525},
numpages = {19},
location = {Virtual Event},
series = {IMC '21}
}

@article{10.1145/3442344,
author = {Lin, Chen and Ouyang, Zhichao and Wang, Xiaoli and Li, Hui and Huang, Zhenhua},
title = {Preserve Integrity in Realtime Event Summarization},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3442344},
doi = {10.1145/3442344},
abstract = {Online text streams such as Twitter are the major information source for users when they are looking for ongoing events. Realtime event summarization aims to generate and update coherent and concise summaries to describe the state of a given event. Due to the enormous volume of continuously coming texts, realtime event summarization has become the de facto tool to facilitate information acquisition. However, there exists a challenging yet unexplored issue in current text summarization techniques: how to preserve the integrity, i.e., the accuracy and consistency of summaries during the update process. The issue is critical since online text stream is dynamic and conflicting information could spread during the event period. For example, conflicting numbers of death and injuries might be reported after an earthquake. Such misleading information should not appear in the earthquake summary at any timestamp. In this article, we present a novel realtime event summarization framework called IAEA (i.e., Integrity-Aware Extractive-Abstractive realtime event summarization). Our key idea is to integrate an inconsistency detection module into a unified extractive–abstractive framework. In each update, important new tweets are first extracted in an extractive module, and the extraction is refined by explicitly detecting inconsistency between new tweets and previous summaries. The extractive module is able to capture the sentence-level attention which is later used by an abstractive module to obtain the word-level attention. Finally, the word-level attention is leveraged to rephrase words. We conduct comprehensive experiments on real-world datasets. To reduce efforts required for building sufficient training data, we also provide automatic labeling steps of which the effectiveness has been empirically verified. Through experiments, we demonstrate that IAEA can generate better summaries with consistent information than state-of-the-art approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {may},
articleno = {49},
numpages = {29},
keywords = {Tweet summarization, real-time event summarization, data integrity, hierarchical deep neural network}
}

@article{10.1145/3038295,
author = {Tholpadi, Goutham and Bhattacharyya, Chiranjib and Shevade, Shirish},
title = {Corpus-Based Translation Induction in Indian Languages Using Auxiliary Language Corpora from Wikipedia},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3038295},
doi = {10.1145/3038295},
abstract = {Identifying translations from comparable corpora is a well-known problem with several applications. Existing methods rely on linguistic tools or high-quality corpora. Absence of such resources, especially in Indian languages, makes this problem hard; for example, state-of-the-art techniques achieve a mean reciprocal rank of 0.66 for English-Italian, and a mere 0.187 for Telugu-Kannada. In this work, we address the problem of comparable corpora-based translation correspondence induction (CC-TCI) when the only resources available are small noisy comparable corpora extracted from Wikipedia. We observe that translations in the source and target languages have many topically related words in common in other “auxiliary” languages. To model this, we define the notion of a translingual theme, a set of topically related words from auxiliary language corpora, and present a probabilistic framework for CC-TCI. Extensive experiments on 35 comparable corpora showed dramatic improvements in performance. We extend these ideas to propose a method for measuring cross-lingual semantic relatedness (CLSR) between words. To stimulate further research in this area, we make publicly available two new high-quality human-annotated datasets for CLSR. Experiments on the CLSR datasets show more than 200% improvement in correlation on the CLSR task. We apply the method to the real-world problem of cross-lingual Wikipedia title suggestion and build the WikiTSu system. A user study on WikiTSu shows a 20% improvement in the quality of titles suggested.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {20},
numpages = {25},
keywords = {translation correspondence induction, auxiliary language, Wikipedia title suggestion, cross-lingual semantic relatedness, bilingual lexicon, Comparable corpora}
}

@article{10.1145/3470564,
author = {Yao, Jing and Dou, Zhicheng and Wen, Ji-Rong},
title = {Clarifying Ambiguous Keywords with Personal Word Embeddings for Personalized Search},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3470564},
doi = {10.1145/3470564},
abstract = {Personalized search tailors document ranking lists for each individual user based on her interests and query intent to better satisfy the user’s information need. Many personalized search models have been proposed. They first build a user interest profile from the user’s search history, and then re-rank the documents based on the personalized matching scores between the created profile and candidate documents. In this article, we attempt to solve the personalized search problem from an alternative perspective of clarifying the user’s intention of the current query. We know that there are many ambiguous words in natural language such as “Apple.” People with different knowledge backgrounds and interests have personalized understandings of these words. Therefore, we propose a personalized search model with personal word embeddings for each individual user that mainly contain the word meanings that the user already knows and can reflect the user interests. To learn great personal word embeddings, we design a pre-training model that captures both the textual information of the query log and the information about user interests contained in the click-through data represented as a graph structure. With personal word embeddings, we obtain the personalized word and context-aware representations of the query and documents. Furthermore, we also employ the current session as the short-term search context to dynamically disambiguate the current query. Finally, we use a matching model to calculate the matching score between the personalized query and document representations for ranking. Experimental results on two large-scale query logs show that our designed model significantly outperforms state-of-the-art personalization models.},
journal = {ACM Trans. Inf. Syst.},
month = {nov},
articleno = {43},
numpages = {29},
keywords = {personal word embedding, user interest, Personalized search, session graph}
}

@article{10.1145/3449152,
author = {Bandy, Jack and Diakopoulos, Nicholas},
title = {More Accounts, Fewer Links: How Algorithmic Curation Impacts Media Exposure in Twitter Timelines},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449152},
doi = {10.1145/3449152},
abstract = {Algorithmic timeline curation is now an integral part of Twitter's platform, affecting information exposure for more than 150 million daily active users. Despite its large-scale and high-stakes impact, especially during a public health emergency such as the COVID-19 pandemic, the exact effects of Twitter's curation algorithm generally remain unknown. In this work, we present a sock-puppet audit that aims to characterize the effects of algorithmic curation on source diversity and topic diversity in Twitter timelines. We created eight sock puppet accounts to emulate representative real-world users, selected through a large-scale network analysis. Then, for one month during early 2020, we collected the puppets' timelines twice per day. Broadly, our results show that algorithmic curation increases source diversity in terms of both Twitter accounts and external domains, even though it drastically decreases the number of external links in the timeline. In terms of topic diversity, algorithmic curation had a mixed effect, slightly amplifying a cluster of politically-focused tweets while squelching clusters of tweets focused on COVID-19 fatalities and health information. Finally, we present some evidence that the timeline algorithm may exacerbate partisan differences in exposure to different sources and topics. The paper concludes by discussing broader implications in the context of algorithmic gatekeeping.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {78},
numpages = {28},
keywords = {content ranking, algorithm auditing, twitter, social media}
}

@article{10.1145/3213898,
author = {Gong, Qingyuan and Chen, Yang and Hu, Jiyao and Cao, Qiang and Hui, Pan and Wang, Xin},
title = {Understanding Cross-Site Linking in Online Social Networks},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/3213898},
doi = {10.1145/3213898},
abstract = {As a result of the blooming of online social networks (OSNs), a user often holds accounts on multiple sites. In this article, we study the emerging “cross-site linking” function available on mainstream OSN services including Foursquare, Quora, and Pinterest. We first conduct a data-driven analysis on crawled profiles and social connections of all 61.39 million Foursquare users to obtain a thorough understanding of this function. Our analysis has shown that the cross-site linking function is adopted by 57.10% of all Foursquare users, and the users who have enabled this function are more active than others. We also find that the enablement of cross-site linking might lead to privacy risks. Based on cross-site links between Foursquare and external OSN sites, we formulate cross-site information aggregation as a problem that uses cross-site links to stitch together site-local information fields for OSN users. Using large datasets collected from Foursquare, Facebook, and Twitter, we demonstrate the usefulness and the challenges of cross-site information aggregation. In addition to the measurements, we carry out a survey collecting detailed user feedback on cross-site linking. This survey studies why people choose to or not to enable cross-site linking, as well as the motivation and concerns of enabling this function.},
journal = {ACM Trans. Web},
month = {sep},
articleno = {25},
numpages = {29},
keywords = {survey, cross-site linking, Online social networks, measurement}
}

@article{10.1145/3182164,
author = {Guo, Long and Zhang, Dongxiang and Wang, Yuan and Wu, Huayu and Cui, Bin and Tan, Kian-Lee},
title = {CO2: Inferring Personal Interests From Raw Footprints by Connecting the Offline World with the Online World},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3182164},
doi = {10.1145/3182164},
abstract = {User-generated trajectories (UGTs), such as travel records from bus companies, capture rich information of human mobility in the offline world. However, some interesting applications of these raw footprints have not been exploited well due to the lack of textual information to infer the subject’s personal interests. Although there is rich semantic information contained in the spatial- and temporal-aware user-generated contents (STUGC) published in the online world, such as Twitter, less effort has been made to utilize this information to facilitate the interest discovery process. In this article, we design an effective probabilistic framework named CO2 to <underline>c</underline>onnect the <underline>o</underline>ffline world with the <underline>o</underline>nline world in order to discover users’ interests directly from their raw footprints in UGT. CO2 first infers trip intentions by utilizing the semantic information in STUGC and then discovers user interests by aggregating the intentions. To evaluate the effectiveness of CO2, we use two large-scale real-world datasets as a case study and further conduct a questionnaire survey to show the superior performance of CO2.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {31},
numpages = {29},
keywords = {user-generated trajectories, trip intention, Personal interests, user-generated contents, raw footprints}
}

@inproceedings{10.1145/3491102.3517446,
author = {Gamage, Dilrukshi and Ghasiya, Piyush and Bonagiri, Vamshi and Whiting, Mark E. and Sasahara, Kazutoshi},
title = {Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517446},
doi = {10.1145/3491102.3517446},
abstract = {Deepfakes are synthetic content generated using advanced deep learning and AI technologies. The advancement of technology has created opportunities for anyone to create and share deepfakes much easier. This may lead to societal concerns based on how communities engage with it. However, there is limited research available to understand how communities perceive deepfakes. We examined deepfake conversations on Reddit from 2018 to 2021—including major topics and their temporal changes as well as implications of these conversations. Using a mixed-method approach—topic modeling and qualitative coding, we found 6,638 posts and 86,425 comments discussing concerns of the believable nature of deepfakes and how platforms moderate them. We also found Reddit conversations to be pro-deepfake and building a community that supports creating and sharing deepfake artifacts and building a marketplace regardless of the consequences. Possible implications derived from qualitative codes indicate that deepfake conversations raise societal concerns. We propose that there are implications for Human Computer Interaction (HCI) to mitigate the harm created from deepfakes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {103},
numpages = {19},
keywords = {deepfake, topic modeling, content analysis, societal implication},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@proceedings{10.5555/3041021,
title = {WWW '17 Companion: Proceedings of the 26th International Conference on World Wide Web Companion},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {We welcome you to the WWW2017 conference, the 26th of the series, and only the second one to be held in Australia.The annual World Wide Web Conference is the premier international forum to present and discuss progress in research, development, standards, and applications related to the Web. This conference is organised under the aegis of the International World Wide Web Conference Committee (IW3C2) in collaboration with local conference organisers in the host city, in this case, the four public universities in Western Australia: Curtin University, Murdoch University, the University of Western Australia and Edith Cowan University. This year, WWW 2017 is offered as the centerpiece of the inaugural Festival of the Web in Perth, a week-long celebration.WWW 2017 provides an opportunity to hear from the leaders of the web including three distinguished keynote speeches by world-class experts: Mark Pesce, Yoelle Maarek, and Melanie Johnston-Hollitt. There is a rich environment of technical activities, including 164 high quality papers in the Research Tracks, 54 papers in the four alternate tracks, over 100 papers in 15 workshops, 13 tutorial sessions, a Ph.D. Symposium track comprising presentations by seven doctoral students, an Industry track consisting of 20 papers focused on applied research, 20 demonstrations, a W3C track examining the latest Web standards and emerging technologies and 64 posters with, for the first time, a number of these offered as e-posters to augment the static poster panels. Overall, WWW2017 provides more than 400 high quality presentations on the key research and development issues of the World Wide Web.Co-located events in the Festival of the Web 2017 include the 4th Big Data Innovators Gathering (BIG 2017), the Web for All conference (W4A2017), and the 5th Serious Games and Applications for Health conference (SeGaH'17). In addition, several new events include Collaboration-Innovation, a one day conference focusing on building smart business innovation through collaboration; the Trust Factory, a curated forum exploring issues of trust and privacy on the web; and Bytes and Rights, a conference focused on issues of web governance, copyright, digital rights, privacy and security on the web. Finally, the Big Day In is a one-day IT careers conference designed by students for students, including tips and advice for secondary school students interested in IT and the web.Given Perth's location in one of the world's richest areas of natural resources, DeepSensor, a world class gathering of industry professionals, is being conducted as part of the Festival as an opportunity for professionals to share their real-world insights into the continuing development of the internet of things in the mining, oil and gas industries.},
location = {Perth, Australia}
}

@article{10.1145/3329168,
author = {Min, Weiqing and Jiang, Shuqiang and Liu, Linhu and Rui, Yong and Jain, Ramesh},
title = {A Survey on Food Computing},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329168},
doi = {10.1145/3329168},
abstract = {Food is essential for human life and it is fundamental to the human experience. Food-related study may support multifarious applications and services, such as guiding human behavior, improving human health, and understanding the culinary culture. With the rapid development of social networks, mobile networks, and Internet of Things (IoT), people commonly upload, share, and record food images, recipes, cooking videos, and food diaries, leading to large-scale food data. Large-scale food data offers rich knowledge about food and can help tackle many central issues of human society. Therefore, it is time to group several disparate issues related to food computing. Food computing acquires and analyzes heterogenous food data from different sources for perception, recognition, retrieval, recommendation, and monitoring of food. In food computing, computational approaches are applied to address food-related issues in medicine, biology, gastronomy, and agronomy. Both large-scale food data and recent breakthroughs in computer science are transforming the way we analyze food data. Therefore, a series of works has been conducted in the food area, targeting different food-oriented tasks and applications. However, there are very few systematic reviews that shape this area well and provide a comprehensive and in-depth summary of current efforts or detail open problems in this area. In this article, we formalize food computing and present such a comprehensive overview of various emerging concepts, methods, and tasks. We summarize key challenges and future directions ahead for food computing. This is the first comprehensive survey that targets the study of computing technology for the food area and also offers a collection of research studies and technologies to benefit researchers and practitioners working in different food-related fields.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {92},
numpages = {36},
keywords = {recipe analysis, monitoring, food retrieval, health, food recognition, food perception, recipe recommendation, Food computing, survey}
}

@article{10.1145/3362695,
author = {Veloso, Br\'{a}ulio M. and Assun\c{c}\~{a}o, Renato M. and Ferreira, Anderson A. and Ziviani, Nivio},
title = {In Search of a Stochastic Model for the E-News Reader},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3362695},
doi = {10.1145/3362695},
abstract = {E-news readers have increasingly at their disposal a broad set of news articles to read. Online newspaper sites use recommender systems to predict and to offer relevant articles to their users. Typically, these recommender systems do not leverage users’ reading behavior. If we know how the topics-reads change in a reading session, we may lead to fine-tuned recommendations, for example, after reading a certain number of sports items, it may be counter-productive to keep recommending other sports news. The motivation for this article is the assumption that understanding user behavior when reading successive online news articles can help in developing better recommender systems. We propose five categories of stochastic models to describe this behavior depending on how the previous reading history affects the future choices of topics. We instantiated these five classes with many different stochastic processes covering short-term memory, revealed-preference, cumulative advantage, and geometric sojourn models. Our empirical study is based on large datasets of E-news from two online newspapers. We collected data from more than 13 million users who generated more than 23 million reading sessions, each one composed by the successive clicks of the users on the posted news. We reduce each user session to the sequence of reading news topics. The models were fitted and compared using the Akaike Information Criterion and the Brier Score. We found that the best models are those in which the user moves through topics influenced only by their most recent readings. Our models were also better to predict the next reading than the recommender systems currently used in these journals showing that our models can improve user satisfaction.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {nov},
articleno = {65},
numpages = {27},
keywords = {Modeling user behavior, online newspapers, stochastic models}
}

@article{10.1145/3061710,
author = {Bianchini, Devis and Antonellis, Valeria De and Melchiori, Michele},
title = {WISeR: A Multi-Dimensional Framework for Searching and Ranking Web APIs},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3061710},
doi = {10.1145/3061710},
abstract = {Mashups are agile applications that aggregate RESTful services, developed by third parties, whose functions are exposed as Web Application Program Interfaces (APIs) within public repositories. From mashups developers’ viewpoint, Web API search may benefit from selection criteria that combine several dimensions used to describe the APIs, such as categories, tags, and technical features (e.g., protocols and data formats). Nevertheless, other dimensions might be fruitfully exploited to support Web API search. Among them, past API usage experiences by other developers may be used to suggest the right APIs for a target application. Past experiences might emerge from the co-occurrence of Web APIs in the same mashups. Ratings assigned by developers after using the Web APIs to create their own mashups or after using mashups developed by others can be considered as well. This article aims to advance the current state of the art for Web API search and ranking from mashups developers’ point of view, by addressing two key issues: multi-dimensional modeling and multi-dimensional framework for selection. The model for Web API characterization embraces multiple descriptive dimensions, by considering several public repositories, that focus on different and only partially overlapping dimensions. The proposed Web API selection framework, called WISeR (Web apI Search and Ranking), is based on functions devoted to developers to exploit the multi-dimensional descriptions, in order to enhance the identification of candidate Web APIs to be proposed, according to the given requirements. Furthermore, WISeR adapts to changes that occur during the Web API selection and mashup development, by revising the dimensional attributes in order to conform to developers’ preferences and constraints. We also present an experimental evaluation of the framework.},
journal = {ACM Trans. Web},
month = {jul},
articleno = {19},
numpages = {32},
keywords = {Multi-dimensional Web API model, mashups, Web API search and ranking, RESTful services}
}

@article{10.1145/3183712,
author = {Liao, Yi and Lam, Wai and Bing, Lidong and Shen, Xin},
title = {Joint Modeling of Participant Influence and Latent Topics for Recommendation in Event-Based Social Networks},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3183712},
doi = {10.1145/3183712},
abstract = {Event-based social networks (EBSNs) are becoming popular in recent years. Users can publish a planned event on an EBSN website, calling for other users to participate in the event. When a user is making a decision on whether to participate in an event in EBSNs, one aspect for consideration is existing participants defined as users who have agreed to join this event. Existing participants of the event may affect the decision of the user, to which we refer as participant influence. However, participant influence is not well studied by previous works. In this article, we propose an event recommendation model that considers participant influence, and exploits the influence of existing participants on the decisions of new participants based on Poisson factorization. The effect of participant influence is associated with the target event, the host group of the event, and the location of the event. Furthermore, our proposed model can extract latent event topics from event text descriptions, and characterize events, groups, and locations by distributions of event topics. Associations between latent event topics and participant influence are exploited for improving event recommendation. Besides making event recommendation, the proposed model is able to reveal the semantic properties of the participant influence between two users semantically. We have conducted extensive experiments on some datasets extracted from a real-world EBSN. Our proposed model achieves superior event recommendation performance over several state-of-the-art models. The results demonstrate that the consideration of participant influence can improve event recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = {mar},
articleno = {29},
numpages = {31},
keywords = {Poisson factorization, Event-based social networks}
}

@article{10.1145/3158226,
author = {Zhang, Amy X. and Chen, Jilin and Chai, Wei and Xu, Jinjun and Hong, Lichan and CHI, Ed},
title = {Evaluation and Refinement of Clustered Search Results with the Crowd},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3158226},
doi = {10.1145/3158226},
abstract = {When searching on the web or in an app, results are often returned as lists of hundreds to thousands of items, making it difficult for users to understand or navigate the space of results. Research has demonstrated that using clustering to partition search results into coherent, topical clusters can aid in both exploration and discovery. Yet clusters generated by an algorithm for this purpose are often of poor quality and do not satisfy users. To achieve acceptable clustered search results, experts must manually evaluate and refine the clustered results for each search query, a process that does not scale to large numbers of search queries. In this article, we investigate using crowd-based human evaluation to inspect, evaluate, and improve clusters to create high-quality clustered search results at scale. We introduce a workflow that begins by using a collection of well-known clustering algorithms to produce a set of clustered search results for a given query. Then, we use crowd workers to holistically assess the quality of each clustered search result to find the best one. Finally, the workflow has the crowd spot and fix problems in the best result to produce a final output. We evaluate this workflow on 120 top search queries from the Google Play Store, some of whom have clustered search results as a result of evaluations and refinements by experts. Our evaluations demonstrate that the workflow is effective at reproducing the evaluation of expert judges and also improves clusters in a way that agrees with experts and crowds alike.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jun},
articleno = {14},
numpages = {28},
keywords = {human computation, clusters, mobile app stores, Clustered search results, crowdsourcing, cluster evaluation, search engines}
}

@article{10.1145/3134678,
author = {Ernala, Sindhu Kiranmai and Rizvi, Asra F. and Birnbaum, Michael L. and Kane, John M. and De Choudhury, Munmun},
title = {Linguistic Markers Indicating Therapeutic Outcomes of Social Media Disclosures of Schizophrenia},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134678},
doi = {10.1145/3134678},
abstract = {Self-disclosure of stigmatized conditions is known to yield therapeutic benefits. Social media sites are emerging as promising platforms enabling disclosure around a variety of stigmatized concerns, including mental illness. What kind of behavioral changes precede and follow such disclosures? Do the therapeutic benefits of "opening up" manifest in these changes? In this paper, we address these questions by focusing on disclosures of schizophrenia diagnoses made on Twitter. We adopt a clinically grounded quantitative approach to first identify temporal phases around disclosure during which symptoms of schizophrenia are likely to be significant. Then, to quantify behaviors before and after disclosures, we define linguistic measures drawing from literature on psycholinguistics and the socio-cognitive model of schizophrenia. Along with significant linguistic differences before and after disclosures, we find indications of therapeutic outcomes following disclosures, including improved readability and coherence in language, future orientation, lower self preoccupation, and reduced discussion of symptoms and stigma perceptions. We discuss the implications of social media as a new therapeutic tool in supporting disclosures of stigmatized conditions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {dec},
articleno = {43},
numpages = {27},
keywords = {twitter, mental health, language, self-disclosure, therapeutic benefits, schizophrenia, social media, psychiatry}
}

@article{10.1145/3441446,
author = {Zhao, Huan and Yao, Quanming and Song, Yangqiu and Kwok, James T. and Lee, Dik Lun},
title = {Side Information Fusion for Recommender Systems over Heterogeneous Information Network},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441446},
doi = {10.1145/3441446},
abstract = {Collaborative filtering (CF) has been one of the most important and popular recommendation methods, which aims at predicting users’ preferences (ratings) based on their past behaviors. Recently, various types of side information beyond the explicit ratings users give to items, such as social connections among users and metadata of items, have been introduced into CF and shown to be useful for improving recommendation performance. However, previous works process different types of information separately, thus failing to capture the correlations that might exist across them. To address this problem, in this work, we study the application of heterogeneous information network (HIN), which offers a unifying and flexible representation of different types of side information, to enhance CF-based recommendation methods. However, we face challenging issues in HIN-based recommendation, i.e., how to capture similarities of complex semantics between users and items in a HIN, and how to effectively fuse these similarities to improve final recommendation performance. To address these issues, we apply metagraph to similarity computation and solve the information fusion problem with a “matrix factorization (MF) + factorization machine (FM)” framework. For the MF part, we obtain the user-item similarity matrix from each metagraph and then apply low-rank matrix approximation to obtain latent features for both users and items. For the FM part, we apply FM with Group lasso (FMG) on the features obtained from the MF part to train the recommending model and, at the same time, identify the useful metagraphs. Besides FMG, a two-stage method, we further propose an end-to-end method, hierarchical attention fusing, to fuse metagraph-based similarities for the final recommendation. Experimental results on four large real-world datasets show that the two proposed frameworks significantly outperform existing state-of-the-art methods in terms of recommendation performance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jun},
articleno = {60},
numpages = {32},
keywords = {heterogeneous information networks, collaborative filtering, Recommender systems, graph attention networks, factorization machine, matrix factorization}
}

@article{10.1145/3406116,
author = {Zhang, Richong and Mensah, Samuel and Kong, Fanshuang and Hu, Zhiyuan and Mao, Yongyi and Liu, Xudong},
title = {Pairwise Link Prediction Model for Out of Vocabulary Knowledge Base Entities},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3406116},
doi = {10.1145/3406116},
abstract = {Real-world knowledge bases such as DBPedia, Yago, and Freebase contain sparse linkage connectivity, which poses a severe challenge to link prediction between entities. To cope with such data scarcity issues, recent models have focused on learning interactions between entity pairs by means of relations that exist between them. However promising, some relations are associated with very few tail entities or head entities, resulting in poor estimation of the relation interaction between entities. In this article, we break the sole dependency of modeling relation interactions between entity pairs by associating a triple with pairwise embeddings, i.e., distributed vector representations for pairs of word-based entities and relation of a triple. We capture the interactions that exist between pairwise embeddings by means of a Pairwise Factorization Model that employs a factorization machine with relation attention. This approach allows parameters for related interactions to be estimated efficiently, ensuring that the pairwise embeddings are discriminative, providing strong supervisory signals for the decoding task of link prediction. The Pairwise Factorization Model we propose exploits a neural bag-of-words model as the encoder, which effectively encodes word-based entities into distributed vector representations for the decoder. The proposed model is simple and enjoys efficiency and capability, showing superior link prediction performance over state-of-the-art complex models on benchmark datasets DBPedia50K and FB15K-237.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {36},
numpages = {28},
keywords = {representation learning, graph convolutional networks, Knowledge bases}
}

@article{10.1145/3501401,
author = {Harrag, Fouzi and Djahli, Mohamed Khalil},
title = {Arabic Fake News Detection: A Fact Checking Based Deep Learning Approach},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3501401},
doi = {10.1145/3501401},
abstract = {Fake news stories can polarize society, particularly during political events. They undermine confidence in the media in general. Current NLP systems are still lacking the ability to properly interpret and classify Arabic fake news. Given the high stakes involved, determining truth in social media has recently become an emerging research that is attracting tremendous attention. Our literature review indicates that applying the state-of-the-art approaches on news content address some challenges in detecting fake news’ characteristics, which needs auxiliary information to make a clear determination. Moreover, the ‘Social-context-based’ and ‘propagation-based’ approaches can be either an alternative or complementary strategy to content-based approaches. The main goal of our research is to develop a model capable of automatically detecting truth given an Arabic news or claim. In particular, we propose a deep neural network approach that can classify fake and real news claims by exploiting ‘Convolutional Neuron Networks’. Our approach attempts to solve the problem from the fact checking perspective, where the fact-checking task involves predicting whether a given news text claim is factually authentic or fake. We opt to use an Arabic balanced corpus to build our model because it unifies stance detection, stance rationale, relevant document retrieval and fact-checking. The model is trained on different well selected attributes. An extensive evaluation has been conducted to demonstrate the ability of the fact-checking task in detecting the Arabic fake news. Our model outperforms the performance of the state-of-the-art approaches when applied to the same Arabic dataset with the highest accuracy of 91%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {75},
numpages = {34},
keywords = {deep learning, social media, Fake news detection, Convolutional Neural Network, Arabic corpus, fact checking}
}

@article{10.1145/3376927,
author = {Qin, Chuan and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Ma, Chao and Chen, Enhong and Xiong, Hui},
title = {An Enhanced Neural Network Approach to Person-Job Fit in Talent Recruitment},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3376927},
doi = {10.1145/3376927},
abstract = {The widespread use of online recruitment services has led to an information explosion in the job market. As a result, recruiters have to seek intelligent ways for Person-Job Fit, which is the bridge for adapting the right candidates to the right positions. Existing studies on Person-Job Fit usually focus on measuring the matching degree between talent qualification and job requirements mainly based on the manual inspection of human resource experts, which could be easily misguided by the subjective, incomplete, and inefficient nature of human judgment. To that end, in this article, we propose a novel end-to-end Topic-based Ability-aware Person-Job Fit Neural Network (TAPJFNN) framework, which has a goal of reducing the dependence on manual labor and can provide better interpretability about the fitting results. The key idea is to exploit the rich information available in abundant historical job application data. Specifically, we propose a word-level semantic representation for both job requirements and job seekers’ experiences based on Recurrent Neural Network (RNN). Along this line, two hierarchical topic-based ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measure the different contribution of each job experience to a specific ability requirement. In addition, we design a refinement strategy for Person-Job Fit prediction based on historical recruitment records. Furthermore, we introduce how to exploit our TAPJFNN framework for enabling two specific applications in talent recruitment: talent sourcing and job recommendation. Particularly, in the application of job recommendation, a novel training mechanism is designed for addressing the challenge of biased negative labels. Finally, extensive experiments on a large-scale real-world dataset clearly validate the effectiveness and interpretability of the TAPJFNN and its variants compared with several baselines.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {15},
numpages = {33},
keywords = {person-job fit, neural network, Recruitment analysis}
}

@inproceedings{10.1145/3491102.3501998,
author = {Cambo, Scott Allen and Gergle, Darren},
title = {Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501998},
doi = {10.1145/3491102.3501998},
abstract = {Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {19},
keywords = {critical data studies, model positionality, Computational reflexivity, position mining, data science, annotator fingerprinting, human-centered machine learning, human-centered data science},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3462327,
author = {Ni, Weijian and Liu, Tong and Zeng, Qingtian and Xie, Nengfu},
title = {Mining Domain Terminologies Using Search Engine's Query Log},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3462327},
doi = {10.1145/3462327},
abstract = {Domain terminologies are a basic resource for various natural language processing tasks. To automatically discover terminologies for a domain of interest, most traditional approaches mostly rely on a domain-specific corpus given in advance; thus, the performance of traditional approaches can only be guaranteed when collecting a high-quality domain-specific corpus, which requires extensive human involvement and domain expertise. In this article, we propose a novel approach that is capable of automatically mining domain terminologies using search engine's query log—a type of domain-independent corpus of higher availability, coverage, and timeliness than a manually collected domain-specific corpus. In particular, we represent query log as a heterogeneous network and formulate the task of mining domain terminology as transductive learning on the heterogeneous network. In the proposed approach, the manifold structure of domain-specificity inherent in query log is captured by using a novel network embedding algorithm and further exploited to reduce the need for the manual annotation efforts for domain terminology classification. We select Agriculture and Healthcare as the target domains and experiment using a real query log from a commercial search engine. Experimental results show that the proposed approach outperforms several state-of-the-art approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {100},
numpages = {32},
keywords = {search engine, Domain terminology, transductive learning, query log, network embedding}
}

@article{10.1145/3359274,
author = {Dash, Abhisek and Shandilya, Anurag and Biswas, Arindam and Ghosh, Kripabandhu and Ghosh, Saptarshi and Chakraborty, Abhijnan},
title = {Summarizing User-Generated Textual Content: Motivation and Methods for Fairness in Algorithmic Summaries},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359274},
doi = {10.1145/3359274},
abstract = {As the amount of user-generated textual content grows rapidly, text summarization algorithms are increasingly being used to provide users a quick overview of the information content. Traditionally, summarization algorithms have been evaluated only based on how well they match human-written summaries (e.g. as measured by ROUGE scores). In this work, we propose to evaluate summarization algorithms from a completely new perspective that is important when the user-generated data to be summarized comes from different socially salient user groups, e.g. men or women, Caucasians or African-Americans, or different political groups (Republicans or Democrats). In such cases, we check whether the generated summaries fairly represent these different social groups. Specifically, considering that an extractive summarization algorithm selects a subset of the textual units (e.g. microblogs) in the original data for inclusion in the summary, we investigate whether this selection is fair or not. Our experiments over real-world microblog datasets show that existing summarization algorithms often represent the socially salient user-groups very differently compared to their distributions in the original data. More importantly, some groups are frequently under-represented in the generated summaries, and hence get far less exposure than what they would have obtained in the original data. To reduce such adverse impacts, we propose novel fairness-preserving summarization algorithms which produce high-quality summaries while ensuring fairness among various groups. To our knowledge, this is the first attempt to produce fair text summarization, and is likely to open up an interesting research direction.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {172},
numpages = {28},
keywords = {fairness in algorithmic decision making, fair summarization, text summarization, group fairness, extractive summarization}
}

@article{10.1145/3369026,
author = {K\"{u}\c{c}\"{u}k, Dilek and Can, Fazli},
title = {Stance Detection: A Survey},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3369026},
doi = {10.1145/3369026},
abstract = {Automatic elicitation of semantic information from natural language texts is an important research problem with many practical application areas. Especially after the recent proliferation of online content through channels such as social media sites, news portals, and forums; solutions to problems such as sentiment analysis, sarcasm/controversy/veracity/rumour/fake news detection, and argument mining gained increasing impact and significance, revealed with large volumes of related scientific publications. In this article, we tackle an important problem from the same family and present a survey of stance detection in social media posts and (online) regular texts. Although stance detection is defined in different ways in different application settings, the most common definition is “automatic classification of the stance of the producer of a piece of text, towards a target, into one of these three classes: {Favor, Against, Neither}.” Our survey includes definitions of related problems and concepts, classifications of the proposed approaches so far, descriptions of the relevant datasets and tools, and related outstanding issues. Stance detection is a recent natural language processing topic with diverse application areas, and our survey article on this newly emerging topic will act as a significant resource for interested researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {12},
numpages = {37},
keywords = {deep learning, social media analysis, Stance detection, Twitter}
}

@article{10.1145/3372390,
author = {Heldens, Stijn and Hijma, Pieter and Werkhoven, Ben Van and Maassen, Jason and Belloum, Adam S. Z. and Van Nieuwpoort, Rob V.},
title = {The Landscape of Exascale Research: A Data-Driven Literature Analysis},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372390},
doi = {10.1145/3372390},
abstract = {The next generation of supercomputers will break the exascale barrier. Soon we will have systems capable of at least one quintillion (billion billion) floating-point operations per second (1018 FLOPS). Tremendous amounts of work have been invested into identifying and overcoming the challenges of the exascale era. In this work, we present an overview of these efforts and provide insight into the important trends, developments, and exciting research opportunities in exascale computing. We use a three-stage approach in which we (1) discuss various exascale landmark studies, (2) use data-driven techniques to analyze the large collection of related literature, and (3) discuss eight research areas in depth based on influential articles. Overall, we observe that great advancements have been made in tackling the two primary exascale challenges: energy efficiency and fault tolerance. However, as we look forward, we still foresee two major concerns: the lack of suitable programming tools and the growing gap between processor performance and data bandwidth (i.e., memory, storage, networks). Although we will certainly reach exascale soon, without additional research, these issues could potentially limit the applicability of exascale computing.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {23},
numpages = {43},
keywords = {data-driven analysis, literature review, high-performance computing, Exascale computing, extreme-scale computing}
}

@article{10.1145/3132039,
author = {Neal, Tempestt and Sundararajan, Kalaivani and Fatima, Aneez and Yan, Yiming and Xiang, Yingfei and Woodard, Damon},
title = {Surveying Stylometry Techniques and Applications},
year = {2017},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3132039},
doi = {10.1145/3132039},
abstract = {The analysis of authorial style, termed stylometry, assumes that style is quantifiably measurable for evaluation of distinctive qualities. Stylometry research has yielded several methods and tools over the past 200 years to handle a variety of challenging cases. This survey reviews several articles within five prominent subtasks: authorship attribution, authorship verification, authorship profiling, stylochronometry, and adversarial stylometry. Discussions on datasets, features, experimental techniques, and recent approaches are provided. Further, a current research challenge lies in the inability of authorship analysis techniques to scale to a large number of authors with few text samples. Here, we perform an extensive performance analysis on a corpus of 1,000 authors to investigate authorship attribution, verification, and clustering using 14 algorithms from the literature. Finally, several remaining research challenges are discussed, along with descriptions of various open-source and commercial software that may be useful for stylometry subtasks.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {86},
numpages = {36},
keywords = {authorship analysis, stylometry, Adversarial stylometry}
}

@article{10.1145/3201578,
author = {Bollegala, Danushka and Atanasov, Vincent and Maehara, Takanori and Kawarabayashi, Ken-Ichi},
title = {ClassiNet -- Predicting Missing Features for Short-Text Classification},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201578},
doi = {10.1145/3201578},
abstract = {Short and sparse texts such as tweets, search engine snippets, product reviews, and chat messages are abundant on the Web. Classifying such short-texts into a pre-defined set of categories is a common problem that arises in various contexts, such as sentiment classification, spam detection, and information recommendation. The fundamental problem in short-text classification is feature sparseness -- the lack of feature overlap between a trained model and a test instance to be classified. We propose ClassiNet -- a network of classifiers trained for predicting missing features in a given instance, to overcome the feature sparseness problem. Using a set of unlabeled training instances, we first learn binary classifiers as feature predictors for predicting whether a particular feature occurs in a given instance. Next, each feature predictor is represented as a vertex vi in the ClassiNet, where a one-to-one correspondence exists between feature predictors and vertices. The weight of the directed edge eij connecting a vertex vi to a vertex vj represents the conditional probability that given vi exists in an instance, vj also exists in the same instance.We show that ClassiNets generalize word co-occurrence graphs by considering implicit co-occurrences between features. We extract numerous features from the trained ClassiNet to overcome feature sparseness. In particular, for a given instance x, we find similar features from ClassiNet that did not appear in x, and append those features in the representation of x. Moreover, we propose a method based on graph propagation to find features that are indirectly related to a given short-text. We evaluate ClassiNets on several benchmark datasets for short-text classification. Our experimental results show that by using ClassiNet, we can statistically significantly improve the accuracy in short-text classification tasks, without having to use any external resources such as thesauri for finding related features.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jun},
articleno = {55},
numpages = {29},
keywords = {short-texts, Classifier networks, text classification, feature sparseness}
}

@article{10.1007/s00778-018-0525-6,
author = {Vald\'{e}s, Fabio and G\"{u}ting, Ralf Hartmut},
title = {A Framework for Efficient Multi-Attribute Movement Data Analysis},
year = {2019},
issue_date = {Aug 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0525-6},
doi = {10.1007/s00778-018-0525-6},
abstract = {In the first two decades of this century, the amount of movement and movement-related data has increased massively, predominantly due to the proliferation of positioning features in ubiquitous devices such as cellphones and automobiles. At the same time, there is a vast number of requirements for managing and analyzing these records for economic, administrative, and private purposes. Since the growth of data quantity outpaces the efficiency development of hardware components, it is necessary to explore innovative methods of extracting information from large sets of movement data. Hence, the management and analysis of such data, also called trajectories, have become a very active research field. In this context, the time-dependent geographic position is only one of arbitrarily many recorded attributes. For several applications processing trajectory (and related) data, it is helpful or even necessary to trace or generate additional time-dependent information, according to the purpose of the evaluation. For example, in the field of aircraft traffic analysis, besides the position of the monitored airplane, also its altitude, the remaining amount of fuel, the temperature, the name of the traversed country and many other parameters that change with time are relevant. Other application domains consider the names of streets, places of interest, or transportation modes which can be recorded during the movement of a person or another entity. In this paper, we present in detail a framework for analyzing large datasets having any number of time-dependent attributes of different types with the help of a pattern language based on regular expression structures. The corresponding matching algorithm uses a collection of different indexes and is divided into a filtering and an exact matching phase. Compared to the previous version of the framework, we have extended the flexibility and expressiveness of the language by changing its semantics. Due to storage adjustments concerning the applied index structures and further optimizations, the efficiency of the matching procedure has been significantly improved. In addition, the user is no longer required to have a deep knowledge of the temporal distribution of the available attributes of the dataset. The expressiveness and efficiency of the novel approach are demonstrated by querying real and synthetic datasets. Our approach has been fully implemented in a DBMS querying environment and is freely available open source software.},
journal = {The VLDB Journal},
month = {aug},
pages = {427–449},
numpages = {23},
keywords = {Indexing, Pattern matching, Multi-attribute data}
}

@article{10.1145/3301303,
author = {Gao, Xiaofeng and Cao, Zhenhao and Li, Sha and Yao, Bin and Chen, Guihai and Tang, Shaojie},
title = {Taxonomy and Evaluation for Microblog Popularity Prediction},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3301303},
doi = {10.1145/3301303},
abstract = {As social networks become a major source of information, predicting the outcome of information diffusion has appeared intriguing to both researchers and practitioners. By organizing and categorizing the joint efforts of numerous studies on popularity prediction, this article presents a hierarchical taxonomy and helps to establish a systematic overview of popularity prediction methods for microblog. Specifically, we uncover three lines of thoughts: the feature-based approach, time-series modelling, and the collaborative filtering approach and analyse them, respectively. Furthermore, we also categorize prediction methods based on their underlying rationale: whether they attempt to model the motivation of users or monitor the early responses. Finally, we put these prediction methods to test by performing experiments on real-life data collected from popular social networks Twitter and Weibo. We compare the methods in terms of accuracy, efficiency, timeliness, robustness, and bias.As far as we are concerned, there is no precedented survey aimed at microblog popularity prediction at the time of submission. By establishing a taxonomy and evaluation for the first time, we hope to provide an in-depth review of state-of-the-art prediction methods and point out directions for further research. Our evaluations show that time-series modelling has the advantage of high accuracy and the ability to improve over time. The feature-based methods using only temporal features performs nearly as well as using all possible features, producing average results. This suggests that temporal features do have strong predictive power and that power is better exploited with time-series models. On the other hand, this implies that we know little about the future popularity of an item before it is posted, which may be the focus of further research.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {15},
numpages = {40},
keywords = {popularity prediction, evaluation, taxonomy, Social network}
}

@article{10.1145/3301284,
author = {Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele},
title = {Urban Computing Leveraging Location-Based Social Network Data: A Survey},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3301284},
doi = {10.1145/3301284},
abstract = {Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {17},
numpages = {39},
keywords = {big data, Urban computing, urban informatics, urban sensing, location-based social networks, urban societies, city dynamics}
}

@article{10.1145/3415149,
author = {Mousset, Paul and Pitarch, Yoann and Tamine, Lynda},
title = {End-to-End Neural Matching for Semantic Location Prediction of Tweets},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3415149},
doi = {10.1145/3415149},
abstract = {The impressive increasing availability of social media posts has given rise to considerable research challenges. This article is concerned with the problem of semantic location prediction of geotagged tweets. The underlying task is to associate to a social media post, the focal spatial object, if any (e.g., Place Of Interest POI), it topically focuses on. Although relevant for a number of applications such as POI recommendation, this problem has not so far received the attention it deserves. In previous work, the problem has mainly been tackled by means of language models that rely on costly probability estimation of word relevance across spatial regions. We propose the Spatially-aware Geotext Matching (SGM) model, which relies on a neural network learning framework. The model combines exact word-word-local interaction matching signals with semantic global tweet-POI interaction matching signals. The local interactions are built over kernel spatial word distributions that allow revealing spatially driven word pair similarity patterns. The global interactions consider the strength of the interaction between the tweet and the POI from both the spatial and semantic perspectives. Experimental results on two real-world datasets demonstrate the effectiveness of our proposed SGM model compared to state-of-the-art baselines including language models and traditional neural interaction-based models.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {3},
numpages = {35},
keywords = {point of interest, Semantic location prediction, tweet, neural text matching}
}

@article{10.1145/3407190,
author = {Deldjoo, Yashar and Schedl, Markus and Cremonesi, Paolo and Pasi, Gabriella},
title = {Recommender Systems Leveraging Multimedia Content},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3407190},
doi = {10.1145/3407190},
abstract = {Recommender systems have become a popular and effective means to manage the ever-increasing amount of multimedia content available today and to help users discover interesting new items. Today’s recommender systems suggest items of various media types, including audio, text, visual (images), and videos. In fact, scientific research related to the analysis of multimedia content has made possible effective content-based recommender systems capable of suggesting items based on an analysis of the features extracted from the item itself. The aim of this survey is to present a thorough review of the state-of-the-art of recommender systems that leverage multimedia content, by classifying the reviewed papers with respect to their media type, the techniques employed to extract and represent their content features, and the recommendation algorithm. Moreover, for each media type, we discuss various domains in which multimedia content plays a key role in human decision-making and is therefore considered in the recommendation process. Examples of the identified domains include fashion, tourism, food, media streaming, and e-commerce.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {106},
numpages = {38},
keywords = {tourism, music, deep learning, e-commerce, signal processing, machine learning, audio, Content-based recommender systems, multimedia, video, food, image, social media, fashion}
}

@article{10.1145/3054132,
author = {Gudmundsson, Joachim and Horton, Michael},
title = {Spatio-Temporal Analysis of Team Sports},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3054132},
doi = {10.1145/3054132},
abstract = {Team-based invasion sports such as football, basketball, and hockey are similar in the sense that the players are able to move freely around the playing area and that player and team performance cannot be fully analysed without considering the movements and interactions of all players as a group. State-of-the-art object tracking systems now produce spatio-temporal traces of player trajectories with high definition and high frequency, and this, in turn, has facilitated a variety of research efforts, across many disciplines, to extract insight from the trajectories. We survey recent research efforts that use spatio-temporal data from team sports as input and involve non-trivial computation. This article categorises the research efforts in a coherent framework and identifies a number of open research questions.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {22},
numpages = {34},
keywords = {hockey, basketball, spatio-temporal data, handball, sports analysis, machine learning, Trajectory, soccer, spatial subdivision, data mining, football, network analysis, performance metrics, american football}
}

@inproceedings{10.1145/3530190.3534801,
author = {Akbar, Syeda Zainab and Sharma, Ankur and Mishra, Dibyendu and Mothilal, Ramaravind Kommiya and Negi, Himani and Nishal, Sachita and Panda, Anmol and Pal, Joyojeet},
title = {Devotees on an Astroturf: Media, Politics, and Outrage in the Suicide of a Popular FilmStar},
year = {2022},
isbn = {9781450393478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530190.3534801},
doi = {10.1145/3530190.3534801},
abstract = {The death of Indian film star Sushant Singh Rajput at the peak of the COVID lockdown triggered chaos on the news cycle in India with a range of conspiracy theories that led to a witch hunt of sorts, and the hounding of several entertainers and public figures in the months that followed. Using data from Twitter, YouTube, and an archive of debunked misinformation stories, we examine the drivers and consequences of social media outrage in this case. We analyse these patterns from the framework of conspiracy and astroturfing and contextualize our findings to the socio-political background currently prevalent in India. Primarily, retweet rates on Twitter suggest that commentators benefited from talking about the case, which got higher engagement than other topics. Moreover, we report evidence of political hands in the way the discourse has shaped online, but more importantly that the story bears warnings for the shape and impact of witch-hunts in the backdrop of a fractured media environment. In conclusion, we consider the effects of Rajput’s outsider status as a small-town implant in the film industry within the broader narrative of systemic injustice, as well as the gendered aspects of mob justice that have taken aim at his former partner in the months since.},
booktitle = {ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies (COMPASS)},
pages = {453–475},
numpages = {23},
keywords = {YouTube, Bollywood, Sushant Singh Rajput, SSR, India, Conspiracy, Twitter, Astroturfing, Misinformation, Social Media Influencers, Rhea Chakraborty, Global South},
location = {Seattle, WA, USA},
series = {COMPASS '22}
}

@article{10.1145/3453476,
author = {Nguyen, Thi Tuyet Hai and Jatowt, Adam and Coustaty, Mickael and Doucet, Antoine},
title = {Survey of Post-OCR Processing Approaches},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453476},
doi = {10.1145/3453476},
abstract = {Optical character recognition (OCR) is one of the most popular techniques used for converting printed documents into machine-readable ones. While OCR engines can do well with modern text, their performance is unfortunately significantly reduced on historical materials. Additionally, many texts have already been processed by various out-of-date digitisation techniques. As a consequence, digitised texts are noisy and need to be post-corrected. This article clarifies the importance of enhancing quality of OCR results by studying their effects on information retrieval and natural language processing applications. We then define the post-OCR processing problem, illustrate its typical pipeline, and review the state-of-the-art post-OCR processing approaches. Evaluation metrics, accessible datasets, language resources, and useful toolkits are also reported. Furthermore, the work identifies the current trend and outlines some research directions of this field.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {124},
numpages = {37},
keywords = {Post-OCR processing, error model, language model, machine learning, statistical and neural machine translation, OCR merging}
}

@article{10.1145/3241380,
author = {Sherkat, Ehsan and Milios, Evangelos E. and Minghim, Rosane},
title = {A Visual Analytics Approach for Interactive Document Clustering},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3241380},
doi = {10.1145/3241380},
abstract = {Document clustering is a necessary step in various analytical and automated activities. When guided by the user, algorithms are tailored to imprint a perspective on the clustering process that reflects the user’s understanding of the dataset. More than just allow for customized adjustment of the clusters, a visual analytics approach will provide tools for the user to draw new insights on the collection. While contributing his or her perspective, the user will also acquire a deeper understanding of the data set. To that effect, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user’s feedback. In the proposed system, initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. A set of coordinated visualizations allow the examination of the dataset and the results of the clustering. The visualization provides the user with the highlights of individual documents and understanding of the evolution of documents over the time period to which they relate. The users then interact with the process by means of changing key-terms that drive the process according to their knowledge of the documents domain. In key-term-based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. We have improved that process with a novel algorithm for choosing proper seeds for the clustering. Results demonstrate that not only the system has improved considerably its precision, but also its effectiveness in the document-based decision making. A set of quantitative experiments and a user study have been conducted to show the advantages of the approach for document analytics based on clustering. We performed and reported on the use of the framework in a real decision-making scenario that relates users discussion by email to decision making in improving patient care. Results show that the framework is useful even for more complex data sets such as email conversations.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {aug},
articleno = {6},
numpages = {33},
keywords = {visualization, Interactive document clustering, document projection, seeding, deterministic, key-term, email list, user study, text}
}

@article{10.1145/3464377,
author = {Ma, Longxuan and Li, Mingda and Zhang, Wei-Nan and Li, Jiapeng and Liu, Ting},
title = {Unstructured Text Enhanced Open-Domain Dialogue System: A Systematic Survey},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3464377},
doi = {10.1145/3464377},
abstract = {Incorporating external knowledge into dialogue generation has been proven to benefit the performance of an open-domain Dialogue System (DS), such as generating informative or stylized responses, controlling conversation topics. In this article, we study the open-domain DS that uses unstructured text as external knowledge sources (Unstructured Text Enhanced Dialogue System (UTEDS)). The existence of unstructured text entails distinctions between UTEDS and traditional data-driven DS and we aim at analyzing these differences. We first give the definition of the UTEDS related concepts, then summarize the recently released datasets and models. We categorize UTEDS into Retrieval and Generative models and introduce them from the perspective of model components. The retrieval models consist of Fusion, Matching, and Ranking modules, while the generative models comprise Dialogue and Knowledge Encoding, Knowledge Selection (KS), and Response Generation modules. We further summarize the evaluation methods utilized in UTEDS and analyze the current models’ performance. At last, we discuss the future development trends of UTEDS, hoping to inspire new research in this field.},
journal = {ACM Trans. Inf. Syst.},
month = {sep},
articleno = {9},
numpages = {44},
keywords = {open-domain dialogue, knowledge selection, Unstructured text, knowledge grounded}
}

@article{10.1145/3291059,
author = {Chong, Wen-Haw and Lim, Ee-Peng},
title = {Fine-Grained Geolocation of Tweets in Temporal Proximity},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3291059},
doi = {10.1145/3291059},
abstract = {In fine-grained tweet geolocation, tweets are linked to the specific venues (e.g., restaurants, shops) from which they were posted. This explicitly recovers the venue context that is essential for applications such as location-based advertising or user profiling. For this geolocation task, we focus on geolocating tweets that are contained in tweet sequences. In a tweet sequence, tweets are posted from some latent venue(s) by the same user and within a short time interval. This scenario arises from two observations: (1) It is quite common that users post multiple tweets in a short time and (2) most tweets are not geocoded. To more accurately geolocate a tweet, we propose a model that performs query expansion on the tweet (query) using two novel approaches. The first approach temporal query expansion considers users’ staying behavior around venues. The second approach visitation query expansion leverages on user revisiting the same or similar venues in the past. We combine both query expansion approaches via a novel fusion framework and overlay them on a Hidden Markov Model to account for sequential information. In our comprehensive experiments across multiple datasets and metrics, we show our proposed model to be more robust and accurate than other baselines.},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {17},
numpages = {33},
keywords = {Tweet geolocation, staying behavior, temporal proximity}
}

@article{10.1145/3309699,
author = {Zannettou, Savvas and Sirivianos, Michael and Blackburn, Jeremy and Kourtellis, Nicolas},
title = {The Web of False Information: Rumors, Fake News, Hoaxes, Clickbait, and Various Other Shenanigans},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3309699},
doi = {10.1145/3309699},
abstract = {A new era of Information Warfare has arrived. Various actors, including state-sponsored ones, are weaponizing information on Online Social Networks to run false-information campaigns with targeted manipulation of public opinion on specific topics. These false-information campaigns can have dire consequences to the public: mutating their opinions and actions, especially with respect to critical world events like major elections. Evidently, the problem of false information on the Web is a crucial one and needs increased public awareness as well as immediate attention from law enforcement agencies, public institutions, and in particular, the research community.In this article, we make a step in this direction by providing a typology of the Web’s false-information ecosystem, composed of various types of false-information, actors, and their motives. We report a comprehensive overview of existing research on the false-information ecosystem by identifying several lines of work: (1) how the public perceives false information; (2) understanding the propagation of false information; (3) detecting and containing false information on the Web; and (4) false information on the political stage. In this work, we pay particular attention to political false information as: (1) it can have dire consequences to the community (e.g., when election results are mutated) and (2) previous work shows that this type of false information propagates faster and further when compared to other types of false information. Finally, for each of these lines of work, we report several future research directions that can help us better understand and mitigate the emerging problem of false-information dissemination on the Web.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {10},
numpages = {37},
keywords = {rumors, false information, fake news, clickbait, social networks, Survey, hoaxes}
}

@article{10.1145/3524025,
author = {Handler, Abram and Mahyar, Narges and O’Connor, Brendan},
title = {ClioQuery: Interactive Query-Oriented Text Analytics for Comprehensive Investigation of Historical News Archives},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3524025},
doi = {10.1145/3524025},
abstract = {Historians and archivists often find and analyze the occurrences of query words in newspaper archives to help answer fundamental questions about society. But much work in text analytics focuses on helping people investigate other textual units, such as events, clusters, ranked documents, entity relationships, or thematic hierarchies. Informed by a study into the needs of historians and archivists, we thus propose ClioQuery, a text analytics system uniquely organized around the analysis of query words in context. ClioQuery&nbsp;applies text simplification techniques from natural language processing to help historians quickly and comprehensively gather and analyze all occurrences of a query word across an archive. It also pairs these new NLP methods with more traditional features like linked views and in-text highlighting to help engender trust in summarization techniques. We evaluate ClioQuery&nbsp;with two separate user studies, in which historians explain how ClioQuery’s novel text simplification features can help facilitate historical research. We also evaluate with a separate quantitative comparison study, which shows that ClioQuery&nbsp;helps crowdworkers find and remember historical information. Such results suggest possible new directions for text analytics in other query-oriented settings.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jul},
articleno = {22},
numpages = {49},
keywords = {user interfaces, interactive text analytics, Digital humanities, history}
}

@article{10.1145/3461459,
author = {Ding, Ming and Wang, Tianyu and Wang, Xudong},
title = {Establishing Smartphone User Behavior Model Based on Energy Consumption Data},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3461459},
doi = {10.1145/3461459},
abstract = {In smartphone data analysis, both energy consumption modeling and user behavior mining have been explored extensively, but the relationship between energy consumption and user behavior has been rarely studied. Such a relationship is explored over large-scale users in this article. Based on energy consumption data, where each users’ feature vector is represented by energy breakdown on hardware components of different apps, User Behavior Models (UBM) are established to capture user behavior patterns (i.e., app preference, usage time). The challenge lies in the high diversity of user behaviors (i.e., massive apps and usage ways), which leads to high dimension and dispersion of data. To overcome the challenge, three mechanisms are designed. First, to reduce the dimension, apps are ranked with the top ones identified as typical apps to represent all. Second, the dispersion is reduced by scaling each users’ feature vector with typical apps to unit ℓ1 norm. The scaled vector becomes Usage Pattern, while the ℓ1 norm of vector before scaling is treated as Usage Intensity. Third, the usage pattern is analyzed with a two-layer clustering approach to further reduce data dispersion. In the upper layer, each typical app is studied across its users with respect to hardware components to identify Typical Hardware Usage Patterns (THUP). In the lower layer, users are studied with respect to these THUPs to identify Typical App Usage Patterns (TAUP). The analytical results of these two layers are consolidated into Usage Pattern Models (UPM), and UBMs are finally established by a union of UPMs and Usage Intensity Distributions (UID). By carrying out experiments on energy consumption data from 18,308 distinct users over 10 days, 33 UBMs are extracted from training data. With the test data, it is proven that these UBMs cover 94% user behaviors and achieve up to 20% improvement in accuracy of energy representation, as compared with the baseline method, PCA. Besides, potential applications and implications of these UBMs are illustrated for smartphone manufacturers, app developers, network providers, and so on.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {25},
numpages = {40},
keywords = {smartphone energy consumption, Data mining, user behavior modeling}
}

@article{10.1145/3092742,
author = {Zhong, Changtao and Sastry, Nishanth},
title = {Systems Applications of Social Networks},
year = {2017},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092742},
doi = {10.1145/3092742},
abstract = {The aim of this article is to provide an understanding of social networks as a useful addition to the standard toolbox of techniques used by system designers. To this end, we give examples of how data about social links have been collected and used in different application contexts. We develop a broad taxonomy-based overview of common properties of social networks, review how they might be used in different applications, and point out potential pitfalls where appropriate. We propose a framework, distinguishing between two main types of social network-based user selection—personalised user selection, which identifies target users who may be relevant for a given source node, using the social network around the source as a context, and generic user selection or group delimitation, which filters for a set of users who satisfy a set of application requirements based on their social properties. Using this framework, we survey applications of social networks in three typical kinds of application scenarios: recommender systems, content-sharing systems (e.g., P2P or video streaming), and systems that defend against users who abuse the system (e.g., spam or sybil attacks). In each case, we discuss potential directions for future research that involve using social network properties.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {63},
numpages = {42},
keywords = {social properties, file sharing, Social networks, recommendation, content delivery, sybil, anti-spam}
}

@article{10.5555/3455716.3455910,
author = {Al-Shedivat, Maruan and Dubey, Avinava and Xing, Eric},
title = {Contextual Explanation Networks},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CENs)-- a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction, valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {194},
numpages = {44}
}

@article{10.5555/3455716.3455787,
author = {Yu, Ming and Gupta, Varun and Kolar, Mladen},
title = {Estimation of a Low-Rank Topic-Based Model for Information Cascades},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or cascades, where the observations for a given cascade consist of only the timestamps of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades, and prove the consistency of the estimator. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {71},
numpages = {47},
keywords = {network science, nonconvex optimization, alternating gradient descent, low-rank models, information diffusion, influence-receptivity model}
}

@article{10.1145/3465401,
author = {Wang, Shoujin and Cao, Longbing and Wang, Yan and Sheng, Quan Z. and Orgun, Mehmet A. and Lian, Defu},
title = {A Survey on Session-Based Recommender Systems},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3465401},
doi = {10.1145/3465401},
abstract = {Recommender systems (RSs) have been playing an increasingly important role for informed consumption, services, and decision-making in the overloaded information era and digitized economy. In recent years, session-based recommender systems (SBRSs) have emerged as a new paradigm of RSs. Different from other RSs such as content-based RSs and collaborative filtering-based RSs that usually model long-term yet static user preferences, SBRSs aim to capture short-term but dynamic user preferences to provide more timely and accurate recommendations sensitive to the evolution of their session contexts. Although SBRSs have been intensively studied, neither unified problem statements for SBRSs nor in-depth elaboration of SBRS characteristics and challenges are available. It is also unclear to what extent SBRS challenges have been addressed and what the overall research landscape of SBRSs is. This comprehensive review of SBRSs addresses the above aspects by exploring in depth the SBRS entities (e.g., sessions), behaviours (e.g., users’ clicks on items), and their properties (e.g., session length). We propose a general problem statement of SBRSs, summarize the diversified data characteristics and challenges of SBRSs, and define a taxonomy to categorize the representative SBRS research. Finally, we discuss new research opportunities in this exciting and vibrant area.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {154},
numpages = {38},
keywords = {session-based recommendations, recommender systems, session-based recommender systems, recommendations}
}

@inproceedings{10.1145/3293881.3295783,
author = {Hellas, Arto and Ihantola, Petri and Petersen, Andrew and Ajanovski, Vangel V. and Gutica, Mirela and Hynninen, Timo and Knutas, Antti and Leinonen, Juho and Messom, Chris and Liao, Soohyun Nam},
title = {Predicting Academic Performance: A Systematic Literature Review},
year = {2018},
isbn = {9781450362238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293881.3295783},
doi = {10.1145/3293881.3295783},
abstract = {The ability to predict student performance in a course or program creates opportunities to improve educational outcomes. With effective performance prediction approaches, instructors can allocate resources and instruction more accurately. Research in this area seeks to identify features that can be used to make predictions, to identify algorithms that can improve predictions, and to quantify aspects of student performance. Moreover, research in predicting student performance seeks to determine interrelated features and to identify the underlying reasons why certain features work better than others. This working group report presents a systematic literature review of work in the area of predicting student performance. Our analysis shows a clearly increasing amount of research in this area, as well as an increasing variety of techniques used. At the same time, the review uncovered a number of issues with research quality that drives a need for the community to provide more detailed reporting of methods and results and to increase efforts to validate and replicate work.},
booktitle = {Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {175–199},
numpages = {25},
keywords = {learning analytics, mapping study, prediction, educational data mining, literature review, performance, analytics},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018 Companion}
}

@article{10.1145/3017678,
author = {Li, Tao and Xie, Ning and Zeng, Chunqiu and Zhou, Wubai and Zheng, Li and Jiang, Yexi and Yang, Yimin and Ha, Hsin-Yu and Xue, Wei and Huang, Yue and Chen, Shu-Ching and Navlakha, Jainendra and Iyengar, S. S.},
title = {Data-Driven Techniques in Disaster Information Management},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3017678},
doi = {10.1145/3017678},
abstract = {Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users’ information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {1},
numpages = {45},
keywords = {data mining, data management, application, Disaster information management}
}

@article{10.1145/3362505,
author = {Han, Jungkyu and Yamana, Hayato},
title = {Geographic Diversification of Recommended POIs in Frequently Visited Areas},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3362505},
doi = {10.1145/3362505},
abstract = {In the personalized Point-Of-Interest (POI) (or venue) recommendation, the diversity of recommended POIs is an important aspect. Diversity is especially important when POIs are recommended in the target users’ frequently visited areas, because users are likely to revisit such areas. In addition to the (POI) category diversity that is a popular diversification objective in recommendation domains, diversification of recommended POI locations is an interesting subject itself. Despite its importance, existing POI recommender studies generally focus on and evaluate prediction accuracy. In this article, geographical diversification&nbsp;(geo-diversification), a novel diversification concept that aims to increase recommendation coverage for a target users’ geographic areas of interest, is introduced, from which a method that improves geo-diversity as an addition to existing state-of-the-art POI recommenders is proposed. In experiments with the datasets from two real Location Based Social Networks&nbsp;(LSBNs), we first analyze the performance of four state-of-the-art POI recommenders from various evaluation perspectives including category diversity and geo-diversity that have not been examined previously. The proposed method consistently improves geo-diversity&nbsp;(CPR(geo)@20) by 5 to 12% when combined with four state-of-the-art POI recommenders with negligible prediction accuracy&nbsp;(Recall@20) loss and provides 6 to 18% geo-diversity improvement with tolerable prediction accuracy loss&nbsp;&nbsp;(up to 2.4%).},
journal = {ACM Trans. Inf. Syst.},
month = {oct},
articleno = {1},
numpages = {39},
keywords = {diversity, recommendation, POI, POI recommendation, geographical diversity, LBSN}
}

@article{10.1145/3338497,
author = {Ros\`{a}, Andrea and Rosales, Eduardo and Binder, Walter},
title = {Analysis and Optimization of Task Granularity on the Java Virtual Machine},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/3338497},
doi = {10.1145/3338497},
abstract = {Task granularity, i.e., the amount of work performed by parallel tasks, is a key performance attribute of parallel applications. On the one hand, fine-grained tasks (i.e., small tasks carrying out few computations) may introduce considerable parallelization overheads. On the other hand, coarse-grained tasks (i.e., large tasks performing substantial computations) may not fully utilize the available CPU cores, leading to missed parallelization opportunities. In this article, we provide a better understanding of task granularity for task-parallel applications running on a single Java Virtual Machine in a shared-memory multicore. We present a new methodology to accurately and efficiently collect the granularity of each executed task, implemented in a novel profiler (available open-source) that collects carefully selected metrics from the whole system stack with low overhead, and helps developers locate performance and scalability problems. We analyze task granularity in the DaCapo, ScalaBench, and Spark Perf benchmark suites, revealing inefficiencies related to fine-grained and coarse-grained tasks in several applications. We demonstrate that the collected task-granularity profiles are actionable by optimizing task granularity in several applications, achieving speedups up to a factor of 5.90\texttimes{}. Our results highlight the importance of analyzing and optimizing task granularity on the Java Virtual Machine.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
articleno = {19},
numpages = {47},
keywords = {performance analysis and optimization, task parallelism, Task granularity, Java virtual machine, vertical profiling, actionable profiles}
}

@article{10.1145/3450963,
author = {Abukmeil, Mohanad and Ferrari, Stefano and Genovese, Angelo and Piuri, Vincenzo and Scotti, Fabio},
title = {A Survey of Unsupervised Generative Models for Exploratory Data Analysis and Representation Learning},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3450963},
doi = {10.1145/3450963},
abstract = {For more than a century, the methods for data representation and the exploration of the intrinsic structures of data have developed remarkably and consist of supervised and unsupervised methods. However, recent years have witnessed the flourishing of big data, where typical dataset dimensions are high and the data can come in messy, incomplete, unlabeled, or corrupted forms. Consequently, discovering the hidden structure buried inside such data becomes highly challenging. From this perspective, exploratory data analysis plays a substantial role in learning the hidden structures that encompass the significant features of the data in an ordered manner by extracting patterns and testing hypotheses to identify anomalies. Unsupervised generative learning models are a class of machine learning models characterized by their potential to reduce the dimensionality, discover the exploratory factors, and learn representations without any predefined labels; moreover, such models can generate the data from the reduced factors’ domain. The beginner researchers can find in this survey the recent unsupervised generative learning models for the purpose of data exploration and learning representations; specifically, this article covers three families of methods based on their usage in the era of big data: blind source separation, manifold learning, and neural networks, from shallow to deep architectures.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {99},
numpages = {40},
keywords = {neural networks, representation learning, unsupervised deep learning, manifold learning, explainable machine learning, Blind source separation, exploratory data analysis}
}

@article{10.5555/3122009.3242050,
author = {Vaughan, Jennifer Wortman},
title = {Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {7026–7071},
numpages = {46},
keywords = {incentives, model evaluation, crowdsourcing, hybrid intelligence, data generation, mechanical turk, behavioral experiments}
}

@article{10.5555/3546258.3546362,
author = {Janizek, Joseph D. and Sturmfels, Pascal and Lee, Su-In},
title = {Explaining Explanations: Axiomatic Feature Interactions for Deep Networks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain the features that are important to a model's prediction on a given input. However, for many tasks, simply identifying significant features may be insufficient for understanding model behavior. The interactions between features within the model may better explain not only the model, but why certain features outrank others in importance. In this work, we present Integrated Hessians, an extension of Integrated Gradients (Sundararajan et al., 2017) that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods, and unlike them, is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {104},
numpages = {54},
keywords = {neural networks, Aumann-Shapley value, feature interaction, interpretability, feature attribution}
}

@article{10.1145/3201408,
author = {Zhang, Ziqi and Gao, Jie and Ciravegna, Fabio},
title = {SemRe-Rank: Improving Automatic Term Extraction by Incorporating Semantic Relatedness with Personalised PageRank},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201408},
doi = {10.1145/3201408},
abstract = {Automatic Term Extraction (ATE) deals with the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a ‘one-size-fit-all’ solution that may never exist, we propose to develop generic methods to ‘enhance’ existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness—an often overlooked venue—into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute ‘semantic importance’ scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K’s), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jun},
articleno = {57},
numpages = {41},
keywords = {text mining, information extraction, information retrieval, Automatic term extraction, word embedding, semantic relatedness, personalised pagerank, termhood, ATE, automatic term recognition, ATR}
}

@article{10.5555/3546258.3546344,
author = {Damianou, Andreas and Lawrence, Neil D. and Ek, Carl Henrik},
title = {Multi-View Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {86},
numpages = {51},
keywords = {Gaussian processes, representation learning, inter-battery factor analysis, factor analysis}
}

@article{10.1007/s00778-019-00569-6,
author = {Magdy, Amr and Abdelhafeez, Laila and Kang, Yunfan and Ong, Eric and Mokbel, Mohamed F.},
title = {Microblogs Data Management: A Survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00569-6},
doi = {10.1007/s00778-019-00569-6},
abstract = {Microblogs data is the microlength user-generated data that is posted on the web, e.g., tweets, online reviews, comments on news and social media. It has gained considerable attention in recent years due to its widespread popularity, rich content, and value in several societal applications. Nowadays, microblogs applications span a wide spectrum of interests including targeted advertising, market reports, news delivery, political campaigns, rescue services, and public health. Consequently, major research efforts have been spent to manage, analyze, and visualize microblogs to support different applications. This paper gives a comprehensive review of major research and system work in microblogs data management. The paper reviews core components that enable large-scale querying and indexing for microblogs data. A dedicated part gives particular focus for discussing system-level issues and on-going effort on supporting microblogs through the rising wave of big data systems. In addition, we review the major research topics that exploit these core data management components to provide innovative and effective analysis and visualization for microblogs, such as event detection, recommendations, automatic geotagging, and user queries. Throughout the different parts, we highlight the challenges, innovations, and future opportunities in microblogs data research.},
journal = {The VLDB Journal},
month = {jan},
pages = {177–216},
numpages = {40},
keywords = {Clustering, Classification, Geotagging, Textual, Systems, Temporal, Event detection, Data analysis, Query processing, Social media, Recommendation, Aggregation, Probabilistic models, Geo, Keyword, Indexing, Top-k, Memory management, Summarization, Twitter, Sampling, Visual analysis, Flushing policy, Statistical, Data management, Microblogs, Spatial, Event, Graph, Event analysis, User, Main-memory, Ranking}
}

@article{10.1145/3344548,
author = {Labatut, Vincent and Bost, Xavier},
title = {Extraction and Analysis of Fictional Character Networks: A Survey},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3344548},
doi = {10.1145/3344548},
abstract = {A character network is a graph extracted from a narrative in which vertices represent characters and edges correspond to interactions between them. A number of narrative-related problems can be addressed automatically through the analysis of character networks, such as summarization, classification, or role detection. Character networks are particularly relevant when considering works of fiction (e.g., novels, plays, movies, TV series), as their exploitation allows developing information retrieval and recommendation systems. However, works of fiction possess specific properties that make these tasks harder.This survey aims at presenting and organizing the scientific literature related to the extraction of character networks from works of fiction, as well as their analysis. We first describe the extraction process in a generic way and explain how its constituting steps are implemented in practice, depending on the medium of the narrative, the goal of the network analysis, and other factors. We then review the descriptive tools used to characterize character networks, with a focus on the way they are interpreted in this context. We illustrate the relevance of character networks by also providing a review of applications derived from their analysis. Finally, we identify the limitations of the existing approaches and the most promising perspectives.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {89},
numpages = {40},
keywords = {work of fiction, Information retrieval, graph extraction, narrative, character network, graph analysis}
}

@article{10.1007/s00778-021-00661-w,
author = {Chen, Zhida and Chen, Lisi and Cong, Gao and Jensen, Christian S.},
title = {Location- and Keyword-Based Querying of Geo-Textual Data: A Survey},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00661-w},
doi = {10.1007/s00778-021-00661-w},
abstract = {With the broad adoption of mobile devices, notably smartphones, keyword-based search for content has seen increasing use by mobile users, who are often interested in content related to their geographical location. We have also witnessed a proliferation of geo-textual content that encompasses both textual and geographical information. Examples include geo-tagged microblog posts, yellow pages, and web pages related to entities with physical locations. Over the past decade, substantial research has been conducted on integrating location into keyword-based querying of geo-textual content in settings where the underlying data is assumed to be either relatively static or is assumed to stream into a system that maintains a set of continuous queries. This paper offers a survey of both the research problems studied and the solutions proposed in these two settings. As such, it aims to offer the reader a first understanding of key concepts and techniques, and it serves as an “index” for researchers who are interested in exploring the concepts and techniques underlying proposed solutions to the querying of geo-textual data.},
journal = {The VLDB Journal},
month = {jul},
pages = {603–640},
numpages = {38},
keywords = {Spatio-textual data, Survey, Geo-textual data, Spatial keyword query}
}

@article{10.5555/3546258.3546421,
author = {Stelmakh, Ivan and Shah, Nihar and Singh, Aarti},
title = {PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We consider the problem of automated assignment of papers to reviewers in conference peer review, with a focus on fairness and statistical accuracy. Our fairness objective is to maximize the review quality of the most disadvantaged paper, in contrast to the commonly used objective of maximizing the total quality over all papers. We design an assignment algorithm based on an incremental max-ow procedure that we prove is near-optimally fair. Our statistical accuracy objective is to ensure correct recovery of the papers that should be accepted. We provide a sharp minimax analysis of the accuracy of the peer-review process for a popular objective-score model as well as for a novel subjective-score model that we propose in the paper. Our analysis proves that our proposed assignment algorithm also leads to a near-optimal statistical accuracy. Finally, we design a novel experiment that allows for an objective comparison of various assignment algorithms, and overcomes the inherent difficulty posed by the absence of a ground truth in experiments on peer-review. The results of this experiment as well as of other experiments on synthetic and real data corroborate the theoretical guarantees of our algorithm.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {163},
numpages = {66},
keywords = {top k recovery, peer review, fairness, accuracy, assignment problem}
}

@proceedings{10.1145/3139295,
title = {SA '17: SIGGRAPH Asia 2017 Symposium on Visualization},
year = {2017},
isbn = {9781450354110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Symposium on Visualization is an ideal platform for attendees to explore the opportunities and challenges of cutting-edge visualization techniques which facilitates human being to understand the data sets. The program aims to cover the development, technology, and demonstration of visualization techniques and their interactive applications.},
location = {Bangkok, Thailand}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to <i>The Web Conference 2019</i>. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

