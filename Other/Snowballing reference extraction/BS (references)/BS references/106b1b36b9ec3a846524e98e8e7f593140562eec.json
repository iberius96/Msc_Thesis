{"paperId": "106b1b36b9ec3a846524e98e8e7f593140562eec", "title": "Model-Free Context-Aware Word Composition", "references": [{"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "80e697e6ed970c48b77caeb3cf6093f568c40100", "title": "Contextualized Word Representations for Reading Comprehension"}, {"paperId": "701730a3e59abc28929cb3bf8c38890e976a5343", "title": "Jointly Learning Word Embeddings and Latent Topics"}, {"paperId": "5e365cf9ae3130451a6f54d9a19d46cd33ebfa0e", "title": "Learning Topic-Sensitive Word Representations"}, {"paperId": "037a4bdc815287a49dfc3a208f8e59f3374dd7e9", "title": "Topic-Aware Deep Compositional Models for Sentence Classification"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "7e2484a08adcf09ea166119b48faa27adf25390a", "title": "Learning Semantically and Additively Compositional Distributional Representations"}, {"paperId": "6067628004373e61b962bd4b470308882e57448b", "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks"}, {"paperId": "395044a2e3f5624b2471fb28826e7dbb1009356e", "title": "Towards Universal Paraphrastic Sentence Embeddings"}, {"paperId": "ecb5336bf7b54a62109f325e7152bb74c4c7f527", "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification"}, {"paperId": "1347bd4f826f72ff561b70e665477edadb2a72be", "title": "Not All Contexts Are Created Equal: Better Word Representations with Variable Attention"}, {"paperId": "e5cbdfe8e90dfa0ddcccc832a0b5b2ae7e47bd53", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning"}, {"paperId": "d691814e162b29725871a75e8390430dd4c6fd7a", "title": "SensEmbed: Learning Sense Embeddings for Word and Relational Similarity"}, {"paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors"}, {"paperId": "6aed419cb7926218d1ffe378c0fc63b1a82f6302", "title": "From Paraphrase Database to Compositional Paraphrase Model and Back"}, {"paperId": "b4ce981138ebb5fdcaa652f135587e35b52a082d", "title": "Short Text Clustering via Convolutional Neural Networks"}, {"paperId": "1492ddfd4f4b152b83f11db8c9ecdfd0d2543294", "title": "Compositional Distributional Semantics with Long Short Term Memory"}, {"paperId": "9a0af9e48aad89512ce3e24b6a1853ed3d5d9142", "title": "Topical Word Embeddings"}, {"paperId": "389ab0215cfd94b73040a5f2d5c84b10162f4567", "title": "Deep Multilingual Correlation for Improved Word Embeddings"}, {"paperId": "f4c018bcc8ea707b83247866bdc8ccb87cd9f5da", "title": "Neural Word Embedding as Implicit Matrix Factorization"}, {"paperId": "964153213e608b65ebd49684fa9dcbfe1c720fb4", "title": "Global Belief Recursive Neural Networks"}, {"paperId": "1a08e135ac11db0249c6afb4540672c5a349495e", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space"}, {"paperId": "aed55e213180b974215e447e5e20906fbe966141", "title": "Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "1a99d1e2e92ee1abb810bee2aa72dda9a1b413e4", "title": "A Probabilistic Model for Learning Multi-Prototype Word Embeddings"}, {"paperId": "7b3e09b3b0ebdc3f390f865a3da4233e6bfb49e8", "title": "Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources"}, {"paperId": "0894b06cff1cd0903574acaa7fcf071b144ae775", "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation"}, {"paperId": "c2669677e2d8647430501800b1040dc940afa06c", "title": "Weak semantic context helps phonetic learning in a model of infant language acquisition"}, {"paperId": "f527bcfb09f32e6a4a8afc0b37504941c1ba2cee", "title": "Distributed Representations of Sentences and Documents"}, {"paperId": null, "title": "Griffiths and Mark Steyvers . 2004 . Finding scientific topics"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "27e38351e48fe4b7da2775bf94341738bc4da07e", "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces"}, {"paperId": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "title": "Improving Word Representations via Global Context and Multiple Word Prototypes"}, {"paperId": "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection"}, {"paperId": "cfa2646776405d50533055ceb1b7f050e9014dcb", "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"}, {"paperId": "745d86adca56ec50761591733e157f84cfb19671", "title": "Composition in Distributional Models of Semantics"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "d470bc09a6beb526e363ab994287a6bb80935520", "title": "Learning Entailment Rules for Unary Templates"}, {"paperId": "57458bc1cffe5caa45a885af986d70f723f406b4", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"}, {"paperId": null, "title": "Gibbslda++: Ac/c++ implementation of latent dirichlet allocation (lda)"}, {"paperId": null, "title": "British national corpus version 3 (bnc xml edition)"}, {"paperId": "7acfdc905f734abf966aed58abb983bc015ff7fe", "title": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources"}, {"paperId": "e99f196cf21e0781ef1e119d14e6db45cd71bf3b", "title": "Finding scientific topics"}, {"paperId": "f198043a866e9187925a8d8db9a55e3bfdd47f2c", "title": "Latent Dirichlet Allocation"}, {"paperId": "f5da4c537bcb54cda94770d267225225e44c6838", "title": "Did Frege Believe Frege's Principle?"}, {"paperId": "44d2abe2175df8153f465f6c39b68b76a0d40ab9", "title": "Long Short-Term Memory"}, {"paperId": "decd9bc0385612bdf936928206d83730718e737e", "title": "Distributional Structure"}]}
