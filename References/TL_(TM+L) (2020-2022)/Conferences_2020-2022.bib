%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-10-27 21:35:05 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{popa-rebedea-2021-bart,
	abstract = {We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pre-trained BART models on a large number of potential labels generated by state of the art non-neural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.},
	address = {Online},
	author = {Popa, Cristian and Rebedea, Traian},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:31 +0200},
	date-modified = {2022-10-24 12:38:31 +0200},
	doi = {10.18653/v1/2021.eacl-main.121},
	month = apr,
	pages = {1418--1425},
	publisher = {Association for Computational Linguistics},
	title = {{BART}-{TL}: Weakly-Supervised Topic Label Generation},
	url = {https://aclanthology.org/2021.eacl-main.121},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.121},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.121}}

@inproceedings{zhao-etal-2021-adversarial,
	abstract = {In this paper, we propose the Brand-Topic Model (BTM) which aims to detect brand-associated polarity-bearing topics from product reviews. Different from existing models for sentiment-topic extraction which assume topics are grouped under discrete sentiment categories such as {`}positive{'}, {`}negative{'} and {`}neural{'}, BTM is able to automatically infer real-valued brand-associated sentiment scores and generate fine-grained sentiment-topics in which we can observe continuous changes of words under a certain topic (e.g., {`}shaver{'} or {`}cream{'}) while its associated sentiment gradually varies from negative to positive. BTM is built on the Poisson factorisation model with the incorporation of adversarial learning. It has been evaluated on a dataset constructed from Amazon reviews. Experimental results show that BTM outperforms a number of competitive baselines in brand ranking, achieving a better balance of topic coherence and unique-ness, and extracting better-separated polarity-bearing topics.},
	address = {Online},
	author = {Zhao, Runcong and Gui, Lin and Pergola, Gabriele and He, Yulan},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:27 +0200},
	date-modified = {2022-10-24 12:38:27 +0200},
	doi = {10.18653/v1/2021.eacl-main.199},
	month = apr,
	pages = {2341--2351},
	publisher = {Association for Computational Linguistics},
	title = {Adversarial Learning of {P}oisson Factorisation Model for Gauging Brand Sentiment in User Reviews},
	url = {https://aclanthology.org/2021.eacl-main.199},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.199}}

@inproceedings{paul-etal-2021-multi,
	abstract = {Universal schema (USchema) assumes that two sentence patterns that share the same entity pairs are similar to each other. This assumption is widely adopted for solving various types of relation extraction (RE) tasks. Nevertheless, each sentence pattern could contain multiple facets, and not every facet is similar to all the facets of another sentence pattern co-occurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they co-occur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings significantly outperform their single-facet embedding counterpart, compositional universal schema (CUSchema) (Verga et al., 2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available.},
	address = {Online},
	author = {Paul, Rohan and Chang, Haw-Shiuan and McCallum, Andrew},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:24 +0200},
	date-modified = {2022-10-24 12:38:24 +0200},
	doi = {10.18653/v1/2021.eacl-main.77},
	month = apr,
	pages = {909--919},
	publisher = {Association for Computational Linguistics},
	title = {Multi-facet Universal Schema},
	url = {https://aclanthology.org/2021.eacl-main.77},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.77},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.77}}

@inproceedings{zehe-etal-2021-detecting,
	abstract = {This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The corpus we describe consists of German-language dime novels (550k tokens) that have been annotated in parallel, achieving an inter-annotator agreement of gamma = 0.7. Baseline experiments using BERT achieve an F1 score of 24{\%}, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like coreference resolution.},
	address = {Online},
	author = {Zehe, Albin and Konle, Leonard and D{\"u}mpelmann, Lea Katharina and Gius, Evelyn and Hotho, Andreas and Jannidis, Fotis and Kaufmann, Lucas and Krug, Markus and Puppe, Frank and Reiter, Nils and Schreiber, Annekea and Wiedmer, Nathalie},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:21 +0200},
	date-modified = {2022-10-24 12:38:21 +0200},
	doi = {10.18653/v1/2021.eacl-main.276},
	month = apr,
	pages = {3167--3177},
	publisher = {Association for Computational Linguistics},
	title = {Detecting Scenes in Fiction: A new Segmentation Task},
	url = {https://aclanthology.org/2021.eacl-main.276},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.276},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.276}}

@inproceedings{saravanakumar-etal-2021-event,
	abstract = {We propose a method for online news stream clustering that is a variant of the non-parametric streaming K-means algorithm. Our model uses a combination of sparse and dense document representations, aggregates document-cluster similarity along these multiple representations and makes the clustering decision using a neural classifier. The weighted document-cluster similarity model is learned using a novel adaptation of the triplet loss into a linear classification objective. We show that the use of a suitable fine-tuning objective and external knowledge in pre-trained transformer models yields significant improvements in the effectiveness of contextual embeddings for clustering. Our model achieves a new state-of-the-art on a standard stream clustering dataset of English documents.},
	address = {Online},
	author = {Saravanakumar, Kailash Karthik and Ballesteros, Miguel and Chandrasekaran, Muthu Kumar and McKeown, Kathleen},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:18 +0200},
	date-modified = {2022-10-24 12:38:18 +0200},
	doi = {10.18653/v1/2021.eacl-main.198},
	month = apr,
	pages = {2330--2340},
	publisher = {Association for Computational Linguistics},
	title = {Event-Driven News Stream Clustering using Entity-Aware Contextual Embeddings},
	url = {https://aclanthology.org/2021.eacl-main.198},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.198},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.198}}

@inproceedings{zhou-etal-2021-challenges,
	abstract = {Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.},
	address = {Online},
	author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:15 +0200},
	date-modified = {2022-10-24 12:38:15 +0200},
	doi = {10.18653/v1/2021.eacl-main.274},
	month = apr,
	pages = {3143--3155},
	publisher = {Association for Computational Linguistics},
	title = {Challenges in Automated Debiasing for Toxic Language Detection},
	url = {https://aclanthology.org/2021.eacl-main.274},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.274},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.274}}

@inproceedings{shen-etal-2021-source,
	abstract = {While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with self-training and by increasing the amount of target side monolingual data.},
	address = {Online},
	author = {Shen, Jiajun and Chen, Peng-Jen and Le, Matthew and He, Junxian and Gu, Jiatao and Ott, Myle and Auli, Michael and Ranzato, Marc{'}Aurelio},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:12 +0200},
	date-modified = {2022-10-24 12:38:12 +0200},
	doi = {10.18653/v1/2021.eacl-main.130},
	month = apr,
	pages = {1519--1533},
	publisher = {Association for Computational Linguistics},
	title = {The Source-Target Domain Mismatch Problem in Machine Translation},
	url = {https://aclanthology.org/2021.eacl-main.130},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.130},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.130}}

@inproceedings{sawhney-etal-2021-phase,
	abstract = {Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the build-up of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users{'} historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user{'}s historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users{'} historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming state-of-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations.},
	address = {Online},
	author = {Sawhney, Ramit and Joshi, Harshit and Flek, Lucie and Shah, Rajiv Ratn},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-24 12:38:09 +0200},
	date-modified = {2022-10-24 12:38:09 +0200},
	doi = {10.18653/v1/2021.eacl-main.205},
	month = apr,
	pages = {2415--2428},
	publisher = {Association for Computational Linguistics},
	title = {{PHASE}: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media},
	url = {https://aclanthology.org/2021.eacl-main.205},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.205},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.205}}

@inproceedings{an-etal-2020-multimodal,
	abstract = {From the perspective of health psychology, human beings with long-term and sustained negativity are highly possible to be diagnosed with depression. Inspired by this, we argue that the global topic information derived from user-generated contents (e.g., texts and images) is crucial to boost the performance of the depression detection task, though this information has been neglected by almost all previous studies on depression detection. To this end, we propose a new Multimodal Topic-enriched Auxiliary Learning (MTAL) approach, aiming at capturing the topic information inside different modalities (i.e., texts and images) for depression detection. Especially, in our approach, a modality-agnostic topic model is proposed to be capable of mining the topical clues from either the discrete textual signals or the continuous visual signals. On this basis, the topic modeling w.r.t. the two modalities are cast as two auxiliary tasks for improving the performance of the primary task (i.e., depression detection). Finally, the detailed evaluation demonstrates the great advantage of our MTAL approach to depression detection over the state-of-the-art baselines. This justifies the importance of the multimodal topic information to depression detection and the effectiveness of our approach in capturing such information.},
	address = {Barcelona, Spain (Online)},
	author = {An, Minghui and Wang, Jingjing and Li, Shoushan and Zhou, Guodong},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:35:00 +0200},
	date-modified = {2022-10-24 12:35:00 +0200},
	doi = {10.18653/v1/2020.coling-main.94},
	month = dec,
	pages = {1078--1089},
	publisher = {International Committee on Computational Linguistics},
	title = {Multimodal Topic-Enriched Auxiliary Learning for Depression Detection},
	url = {https://aclanthology.org/2020.coling-main.94},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.94},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.94}}

@inproceedings{takatsu-etal-2020-sentiment,
	abstract = {As smart speakers and conversational robots become ubiquitous, the demand for expressive speech synthesis has increased. In this paper, to control the emotional parameters of the speech synthesis according to certain dialogue contents, we construct a news dataset with emotion labels ({``}positive,{''} {``}negative,{''} or {``}neutral{''}) annotated for each sentence. We then propose a method to identify emotion labels using a model combining BERT and BiLSTM-CRF, and evaluate its effectiveness using the constructed dataset. The results showed that the classification model performance can be efficiently improved by preferentially annotating news articles with low confidence in the human-in-the-loop machine learning framework.},
	address = {Barcelona, Spain (Online)},
	author = {Takatsu, Hiroaki and Ando, Ryota and Matsuyama, Yoichi and Kobayashi, Tetsunori},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:53 +0200},
	date-modified = {2022-10-24 12:34:53 +0200},
	doi = {10.18653/v1/2020.coling-main.440},
	month = dec,
	pages = {5013--5025},
	publisher = {International Committee on Computational Linguistics},
	title = {Sentiment Analysis for Emotional Speech Synthesis in a News Dialogue System},
	url = {https://aclanthology.org/2020.coling-main.440},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.440},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.440}}

@inproceedings{shang-etal-2020-speaker,
	abstract = {Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.},
	address = {Barcelona, Spain (Online)},
	author = {Shang, Guokan and Tixier, Antoine and Vazirgiannis, Michalis and Lorr{\'e}, Jean-Pierre},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:49 +0200},
	date-modified = {2022-10-24 12:34:49 +0200},
	doi = {10.18653/v1/2020.coling-main.40},
	month = dec,
	pages = {450--464},
	publisher = {International Committee on Computational Linguistics},
	title = {Speaker-change Aware {CRF} for Dialogue Act Classification},
	url = {https://aclanthology.org/2020.coling-main.40},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.40},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.40}}

@inproceedings{zhang-etal-2020-topic,
	abstract = {Conventional neural generative models tend to generate safe and generic responses which have little connection with previous utterances semantically and would disengage users in a dialog system. To generate relevant responses, we propose a method that employs two types of constraints - topical constraint and semantic constraint. Under the hypothesis that a response and its context have higher relevance when they share the same topics, the topical constraint encourages the topics of a response to match its context by conditioning response decoding on topic words{'} embeddings. The semantic constraint, which encourages a response to be semantically related to its context by regularizing the decoding objective function with semantic distance, is proposed. Optimal transport is applied to compute a weighted semantic distance between the representation of a response and the context. Generated responses are evaluated by automatic metrics, as well as human judgment, showing that the proposed method can generate more topic-relevant and content-rich responses than conventional models.},
	address = {Barcelona, Spain (Online)},
	author = {Zhang, Shuying and Zhao, Tianyu and Kawahara, Tatsuya},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:46 +0200},
	date-modified = {2022-10-24 12:34:46 +0200},
	doi = {10.18653/v1/2020.coling-main.359},
	month = dec,
	pages = {4067--4077},
	publisher = {International Committee on Computational Linguistics},
	title = {Topic-relevant Response Generation using Optimal Transport for an Open-domain Dialog System},
	url = {https://aclanthology.org/2020.coling-main.359},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.359},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.359}}

@inproceedings{li-etal-2020-neural,
	abstract = {Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. Collecting sufficient labelled data from expert annotators to train a high-performance coreference resolution system is time-consuming and expensive. Crowdsourcing makes it possible to obtain the required amounts of data rapidly and cost-effectively. However, crowd-sourced labels can be noisy. To ensure high-quality data, it is crucial to infer the correct labels by aggregating the noisy labels. In this paper, we split the aggregation into two subtasks, i.e, mention classification and coreference chain inference. Firstly, we predict the general class of each mention using an autoencoder, which incorporates contextual information about each mention, while at the same time taking into account the mention{'}s annotation complexity and annotators{'} reliability at different levels. Secondly, to determine the coreference chain of each mention, we use weighted voting which takes into account the learned reliability in the first subtask. Experimental results demonstrate the effectiveness of our method in predicting the correct labels. We also illustrate our model{'}s interpretability through a comprehensive analysis of experimental results.},
	address = {Barcelona, Spain (Online)},
	author = {Li, Maolin and Takamura, Hiroya and Ananiadou, Sophia},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:43 +0200},
	date-modified = {2022-10-24 12:34:43 +0200},
	doi = {10.18653/v1/2020.coling-main.507},
	month = dec,
	pages = {5760--5773},
	publisher = {International Committee on Computational Linguistics},
	title = {A Neural Model for Aggregating Coreference Annotation in Crowdsourcing},
	url = {https://aclanthology.org/2020.coling-main.507},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.507},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.507}}

@inproceedings{nevezhin-etal-2020-topic,
	abstract = {Online advertising is one of the most widespread ways to reach and increase a target audience for those selling products. Usually having a form of a banner, advertising engages users into visiting a corresponding webpage. Professional generation of banners requires creative and writing skills and a basic understanding of target products. The great variety of goods presented in the online market enforce professionals to spend more and more time creating new advertisements different from existing ones. In this paper, we propose a neural network-based approach for the automatic generation of online advertising using texts from given webpages as sources. The important part of the approach is training on open data available online, which allows avoiding costly procedures of manual labeling. Collected open data consist of multiple subdomains with high data heterogeneity. The subdomains belong to different topics and vary in used vocabularies, phrases, styles that lead to reduced quality in adverts generation. We try to solve the problem of identifying existed subdomains and proposing a new ensemble approach based on exploiting multiple instances of a seq2seq model. Our experimental study on a dataset in the Russian language shows that our approach can significantly improve the quality of adverts generation.},
	address = {Barcelona, Spain (Online)},
	author = {Nevezhin, Egor and Butakov, Nikolay and Khodorchenko, Maria and Petrov, Maxim and Nasonov, Denis},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:40 +0200},
	date-modified = {2022-10-24 12:34:40 +0200},
	doi = {10.18653/v1/2020.coling-main.206},
	month = dec,
	pages = {2273--2283},
	publisher = {International Committee on Computational Linguistics},
	title = {Topic-driven Ensemble for Online Advertising Generation},
	url = {https://aclanthology.org/2020.coling-main.206},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.206},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.206}}

@inproceedings{bai-etal-2020-pre,
	abstract = {Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria from the pre-trained language model to measure instances and help select more effective instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances.},
	address = {Barcelona, Spain (Online)},
	author = {Bai, Guirong and He, Shizhu and Liu, Kang and Zhao, Jun and Nie, Zaiqing},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:36 +0200},
	date-modified = {2022-10-24 12:34:36 +0200},
	doi = {10.18653/v1/2020.coling-main.130},
	month = dec,
	pages = {1495--1504},
	publisher = {International Committee on Computational Linguistics},
	title = {Pre-trained Language Model Based Active Learning for Sentence Matching},
	url = {https://aclanthology.org/2020.coling-main.130},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.130},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.130}}

@inproceedings{zhao-etal-2020-improving,
	abstract = {Recently, people have been beginning paying more attention to the abstractive dialogue summarization task. Since the information flows are exchanged between at least two interlocutors and key elements about a certain event are often spanned across multiple utterances, it is necessary for researchers to explore the inherent relations and structures of dialogue contents. However, the existing approaches often process the dialogue with sequence-based models, which are hard to capture long-distance inter-sentence relations. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network to model the dialogue as an interaction graph according to the topic word information. A masked graph self-attention mechanism is used to integrate cross-sentence information flows and focus more on the related utterances, which makes it better to understand the dialogue. Moreover, the topic word features are introduced to assist the decoding process. We evaluate our model on the SAMSum Corpus and Automobile Master Corpus. The experimental results show that our method outperforms most of the baselines.},
	address = {Barcelona, Spain (Online)},
	author = {Zhao, Lulu and Xu, Weiran and Guo, Jun},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:33 +0200},
	date-modified = {2022-10-24 12:34:33 +0200},
	doi = {10.18653/v1/2020.coling-main.39},
	month = dec,
	pages = {437--449},
	publisher = {International Committee on Computational Linguistics},
	title = {Improving Abstractive Dialogue Summarization with Graph Structures and Topic Words},
	url = {https://aclanthology.org/2020.coling-main.39},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.39},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.39}}

@inproceedings{du-etal-2020-pointing,
	abstract = {Recurrent neural networks (RNNs) suffer from well-known limitations and complications which include slow inference and vanishing gradients when processing long sequences in text classification. Recent studies have attempted to accelerate RNNs via various ad hoc mechanisms to skip irrelevant words in the input. However, word skipping approaches proposed to date effectively stop at each or a given time step to decide whether or not a given input word should be skipped, breaking the coherence of input processing in RNNs. Furthermore, current methods cannot change skip rates during inference and are consequently unable to support different skip rates in demanding real-world conditions. To overcome these limitations, we propose Pointer- LSTM, a novel LSTM framework which relies on a pointer network to select important words for target prediction. The model maintains a coherent input process for the LSTM modules and makes it possible to change the skip rate during inference. Our evaluation on four public data sets demonstrates that Pointer-LSTM (a) is 1.1xâˆ¼3.5x faster than the standard LSTM architecture; (b) is more accurate than Leap-LSTM (the state-of-the-art LSTM skipping model) at high skip rates; and (c) reaches robust accuracy levels even when the skip rate is changed during inference.},
	address = {Barcelona, Spain (Online)},
	author = {Du, Jinhua and Huang, Yan and Moilanen, Karo},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:29 +0200},
	date-modified = {2022-10-24 12:34:29 +0200},
	doi = {10.18653/v1/2020.coling-main.544},
	month = dec,
	pages = {6184--6193},
	publisher = {International Committee on Computational Linguistics},
	title = {Pointing to Select: A Fast Pointer-{LSTM} for Long Text Classification},
	url = {https://aclanthology.org/2020.coling-main.544},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.544},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.544}}

@inproceedings{pham-le-2020-auto,
	abstract = {Visualization and topic modeling are widely used approaches for text analysis. Traditional visualization methods find low-dimensional representations of documents in the visualization space (typically 2D or 3D) that can be displayed using a scatterplot. In contrast, topic modeling aims to discover topics from text, but for visualization, one needs to perform a post-hoc embedding using dimensionality reduction methods. Recent approaches propose using a generative model to jointly find topics and visualization, allowing the semantics to be infused in the visualization space for a meaningful interpretation. A major challenge that prevents these methods from being used practically is the scalability of their inference algorithms. We present, to the best of our knowledge, the first fast Auto-Encoding Variational Bayes based inference method for jointly inferring topics and visualization. Since our method is black box, it can handle model changes efficiently with little mathematical rederivation effort. We demonstrate the efficiency and effectiveness of our method on real-world large datasets and compare it with existing baselines.},
	address = {Barcelona, Spain (Online)},
	author = {Pham, Dang and Le, Tuan},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:25 +0200},
	date-modified = {2022-10-24 12:34:25 +0200},
	doi = {10.18653/v1/2020.coling-main.458},
	month = dec,
	pages = {5223--5234},
	publisher = {International Committee on Computational Linguistics},
	title = {Auto-Encoding Variational {B}ayes for Inferring Topics and Visualization},
	url = {https://aclanthology.org/2020.coling-main.458},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.458},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.458}}

@inproceedings{srinivasa-desikan-etal-2020-comp,
	abstract = {Popular approaches to natural language processing create word embeddings based on textual co-occurrence patterns, but often ignore embodied, sensory aspects of language. Here, we introduce the Python package comp-syn, which provides grounded word embeddings based on the perceptually uniform color distributions of Google Image search results. We demonstrate that comp-syn significantly enriches models of distributional semantics. In particular, we show that(1) comp-syn predicts human judgments of word concreteness with greater accuracy and in a more interpretable fashion than word2vec using low-dimensional word{--}color embeddings ,and (2) comp-syn performs comparably to word2vec on a metaphorical vs. literal word-pair classification task. comp-syn is open-source on PyPi and is compatible with mainstream machine-learning Python packages. Our package release includes word{--}color embeddings forover 40,000 English words, each associated with crowd-sourced word concreteness judgments.},
	address = {Barcelona, Spain (Online)},
	author = {Srinivasa Desikan, Bhargav and Hull, Tasker and Nadler, Ethan and Guilbeault, Douglas and Abubakar Kar, Aabir and Chu, Mark and Lo Sardo, Donald Ruggiero},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:21 +0200},
	date-modified = {2022-10-24 12:34:21 +0200},
	doi = {10.18653/v1/2020.coling-main.154},
	month = dec,
	pages = {1744--1751},
	publisher = {International Committee on Computational Linguistics},
	title = {comp-syn: Perceptually Grounded Word Embeddings with Color},
	url = {https://aclanthology.org/2020.coling-main.154},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.154},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.154}}

@inproceedings{nouri-etal-2020-mining,
	abstract = {Crowdsourcing is used in academia and industry to solve tasks that are easy for humans but hard for computers, in natural language processing mostly to annotate data. The quality of annotations is affected by problems in the task design, task operation, and task evaluation that workers face with requesters in crowdsourcing processes. To learn about the major problems, we provide a short but comprehensive survey based on two complementary studies: (1) a literature review where we collect and organize problems known from interviews with workers, and (2) an empirical data analysis where we use topic modeling to mine workers{'} complaints from a new English corpus of workers{'} forum discussions. While literature covers all process phases, problems in the task evaluation are prevalent, including unfair rejections, late payments, and unjustified blockings of workers. According to the data, however, poor task design in terms of malfunctioning environments, bad workload estimation, and privacy violations seems to bother the workers most. Our findings form the basis for future research on how to improve crowdsourcing processes.},
	address = {Barcelona, Spain (Online)},
	author = {Nouri, Zahra and Wachsmuth, Henning and Engels, Gregor},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:34:17 +0200},
	date-modified = {2022-10-24 12:34:17 +0200},
	doi = {10.18653/v1/2020.coling-main.551},
	month = dec,
	pages = {6264--6276},
	publisher = {International Committee on Computational Linguistics},
	title = {Mining Crowdsourcing Problems from Discussion Forums of Workers},
	url = {https://aclanthology.org/2020.coling-main.551},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.551},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.551}}

@inproceedings{austin-etal-2022-community,
	abstract = {We present our novel, hyperparameter-free topic modelling algorithm, Community Topic. Our algorithm is based on mining communities from term co-occurrence networks. We empirically evaluate and compare Community Topic with Latent Dirichlet Allocation and the recently developed top2vec algorithm. We find that Community Topic runs faster than the competitors and produces topics that achieve higher coherence scores. Community Topic can discover coherent topics at various scales. The network representation used by Community Topic results in a natural relationship between topics and a topic hierarchy. This allows sub- and super-topics to be found on demand. These features make Community Topic the ideal tool for downstream applications such as applied research and conversational agents.},
	address = {Gyeongju, Republic of Korea},
	author = {Austin, Eric and Za{\"\i}ane, Osmar R. and Largeron, Christine},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:27:19 +0200},
	date-modified = {2022-10-24 12:27:19 +0200},
	month = oct,
	pages = {971--983},
	publisher = {International Committee on Computational Linguistics},
	title = {Community Topic: Topic Model Inference by Consecutive Word Community Discovery},
	url = {https://aclanthology.org/2022.coling-1.81},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.81}}

@inproceedings{antypas-etal-2022-twitter,
	abstract = {Social media platforms host discussions about a wide variety of topics that arise everyday. Making sense of all the content and organising it into categories is an arduous task. A common way to deal with this issue is relying on topic modeling, but topics discovered using this technique are difficult to interpret and can differ from corpus to corpus. In this paper, we present a new task based on tweet topic classification and release two associated datasets. Given a wide range of topics covering the most important discussion points in social media, we provide training and testing data from recent time periods that can be used to evaluate tweet classification models. Moreover, we perform a quantitative evaluation and analysis of current general- and domain-specific language models on the task, which provide more insights on the challenges and nature of the task.},
	address = {Gyeongju, Republic of Korea},
	author = {Antypas, Dimosthenis and Ushio, Asahi and Camacho-Collados, Jose and Silva, Vitor and Neves, Leonardo and Barbieri, Francesco},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:27:14 +0200},
	date-modified = {2022-10-24 12:27:14 +0200},
	month = oct,
	pages = {3386--3400},
	publisher = {International Committee on Computational Linguistics},
	title = {{T}witter Topic Classification},
	url = {https://aclanthology.org/2022.coling-1.299},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.299}}

@inproceedings{liu-etal-2022-bert,
	abstract = {Multi-label Text Classification (MLTC) is the task of categorizing documents into one or more topics. Considering the large volumes of data and varying domains of such tasks, fully supervised learning requires manually fully annotated datasets which is costly and time-consuming. In this paper, we propose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text Classification (WSMLTC) model that reduces the need for full supervision. This new model (1) produces BERT sentence embeddings and calibrates them using a flow model, (2) generates an initial topic-document matrix by averaging results of a seeded sparse topic model and a textual entailment model which only require surface name of topics and 4-6 seed words per topic, and (3) adopts a VAE framework to reconstruct the embeddings under the guidance of the topic-document matrix. Finally, (4) it uses the means produced by the encoder model in the VAE architecture as predictions for MLTC. Experimental results on 6 multi-label datasets show that BFV can substantially outperform other baseline WSMLTC models in key metrics and achieve approximately 84{\%} performance of a fully-supervised model.},
	address = {Gyeongju, Republic of Korea},
	author = {Liu, Ziwen and Grau-Bove, Josep and Orr, Scott Allan Allan},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:27:04 +0200},
	date-modified = {2022-10-24 12:27:04 +0200},
	month = oct,
	pages = {1203--1220},
	publisher = {International Committee on Computational Linguistics},
	title = {{BERT}-Flow-{VAE}: A Weakly-supervised Model for Multi-Label Text Classification},
	url = {https://aclanthology.org/2022.coling-1.104},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.104}}

@inproceedings{xie-etal-2022-gretel,
	abstract = {Recently, neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs), to capture the global semantic information for text summarization. However, in these methods, there remain limitations in the way they capture and integrate the global semantic information. In this paper, we propose a novel model, the graph contrastive topic enhanced language model (GRETEL), that incorporates the graph contrastive topic model with the pre-trained language model, to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs, the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics. Experimental results on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods.},
	address = {Gyeongju, Republic of Korea},
	author = {Xie, Qianqian and Huang, Jimin and Saha, Tulika and Ananiadou, Sophia},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:27:01 +0200},
	date-modified = {2022-10-24 12:27:01 +0200},
	month = oct,
	pages = {6259--6269},
	publisher = {International Committee on Computational Linguistics},
	title = {{GRETEL}: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization},
	url = {https://aclanthology.org/2022.coling-1.546},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.546}}

@inproceedings{wang-etal-2022-imci,
	abstract = {With the rapid development of automatic fake news detection technology, fact extraction and verification (FEVER) has been attracting more attention. The task aims to extract the most related fact evidences from millions of open-domain Wikipedia documents and then verify the credibility of corresponding claims. Although several strong models have been proposed for the task and they have made great process, we argue that they fail to utilize multi-view contextual information and thus cannot obtain better performance. In this paper, we propose to integrate multi-view contextual information (IMCI) for fact extraction and verification. For each evidence sentence, we define two kinds of context, i.e. intra-document context and inter-document context. Intra-document context consists of the document title and all the other sentences from the same document. Inter-document context consists of all other evidences which may come from different documents. Then we integrate the multi-view contextual information to encode the evidence sentences to handle the task. Our experimental results on FEVER 1.0 shared task show that our IMCI framework makes great progress on both fact extraction and verification, and achieves state-of-the-art performance with a winning FEVER score of 73.96{\%} and label accuracy of 77.25{\%} on the online blind test set. We also conduct ablation study to detect the impact of multi-view contextual information.},
	address = {Gyeongju, Republic of Korea},
	author = {Wang, Hao and Li, Yangguang and Huang, Zhen and Dou, Yong},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:26:58 +0200},
	date-modified = {2022-10-24 12:26:58 +0200},
	month = oct,
	pages = {1412--1421},
	publisher = {International Committee on Computational Linguistics},
	title = {{IMCI}: Integrate Multi-view Contextual Information for Fact Extraction and Verification},
	url = {https://aclanthology.org/2022.coling-1.121},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.121}}

@inproceedings{amplayo-etal-2022-attribute,
	abstract = {Metadata attributes (e.g., user and product IDs from reviews) can be incorporated as additional inputs to neural-based NLP models, by expanding the architecture of the models to improve performance. However, recent models rely on pretrained language models (PLMs), in which previously used techniques for attribute injection are either nontrivial or cost-ineffective. In this paper, we introduce a benchmark for evaluating attribute injection models, which comprises eight datasets across a diverse range of tasks and domains and six synthetically sparsified ones. We also propose a lightweight and memory-efficient method to inject attributes into PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules, to include attributes both independently of or jointly with the text. We use approximation techniques to parameterize the model efficiently for domains with large attribute vocabularies, and training mechanisms to handle multi-labeled and sparse attributes. Extensive experiments and analyses show that our method outperforms previous attribute injection methods and achieves state-of-the-art performance on all datasets.},
	address = {Gyeongju, Republic of Korea},
	author = {Amplayo, Reinald Kim and Yoo, Kang Min and Lee, Sang-Woo},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:26:54 +0200},
	date-modified = {2022-10-24 12:26:54 +0200},
	month = oct,
	pages = {1051--1064},
	publisher = {International Committee on Computational Linguistics},
	title = {Attribute Injection for Pretrained Language Models: A New Benchmark and an Efficient Method},
	url = {https://aclanthology.org/2022.coling-1.88},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.88}}

@inproceedings{yin-etal-2022-improving,
	abstract = {Driven by recent advances in neural networks, various Deep Embedding Clustering (DEC) based short text clustering models are being developed. In these works, latent representation learning and text clustering are performed simultaneously. Although these methods are becoming increasingly popular, they use pure cluster-oriented objectives, which can produce meaningless representations. To alleviate this problem, several improvements have been developed to introduce additional learning objectives in the clustering process, such as models based on contrastive learning. However, existing efforts rely heavily on learning meaningful representations at the instance level. They have limited focus on learning global representations, which are necessary to capture the overall data structure at the cluster level. In this paper, we propose a novel DEC model, which we named the deep embedded clustering model with cluster-level representation learning (DECCRL) to jointly learn cluster and instance level representations. Here, we extend the embedded topic modelling approach to introduce reconstruction constraints to help learn cluster-level representations. Experimental results on real-world short text datasets demonstrate that our model produces meaningful clusters.},
	address = {Gyeongju, Republic of Korea},
	author = {Yin, Qing and Wang, Zhihua and Song, Yunya and Xu, Yida and Niu, Shuai and Bai, Liang and Guo, Yike and Yang, Xian},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:26:51 +0200},
	date-modified = {2022-10-24 12:26:51 +0200},
	month = oct,
	pages = {2226--2236},
	publisher = {International Committee on Computational Linguistics},
	title = {Improving Deep Embedded Clustering via Learning Cluster-level Representations},
	url = {https://aclanthology.org/2022.coling-1.195},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.195}}

@inproceedings{huynh-etal-2022-vinli,
	abstract = {Over a decade, the research field of computational linguistics has witnessed the growth of corpora and models for natural language inference (NLI) for rich-resource languages such as English and Chinese. A large-scale and high-quality corpus is necessary for studies on NLI for Vietnamese, which can be considered a low-resource language. In this paper, we introduce ViNLI (Vietnamese Natural Language Inference), an open-domain and high-quality corpus for evaluating Vietnamese NLI models, which is created and evaluated with a strict process of quality control. ViNLI comprises over 30,000 human-annotated premise-hypothesis sentence pairs extracted from more than 800 online news articles on 13 distinct topics. In this paper, we introduce the guidelines for corpus creation which take the specific characteristics of the Vietnamese language in expressing entailment and contradiction into account. To evaluate the challenging level of our corpus, we conduct experiments with state-of-the-art deep neural networks and pre-trained models on our dataset. The best system performance is still far from human performance (a 14.20{\%} gap in accuracy). The ViNLI corpus is a challenging corpus to accelerate progress in Vietnamese computational linguistics. Our corpus is available publicly for research purposes.},
	address = {Gyeongju, Republic of Korea},
	author = {Huynh, Tin Van and Nguyen, Kiet Van and Nguyen, Ngan Luu-Thuy},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:26:47 +0200},
	date-modified = {2022-10-24 12:26:47 +0200},
	month = oct,
	pages = {3858--3872},
	publisher = {International Committee on Computational Linguistics},
	title = {{V}i{NLI}: A {V}ietnamese Corpus for Studies on Open-Domain Natural Language Inference},
	url = {https://aclanthology.org/2022.coling-1.339},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.339}}

@inproceedings{keh-etal-2022-pineapple,
	abstract = {A personification is a figure of speech that endows inanimate entities with properties and actions typically seen as requiring animacy. In this paper, we explore the task of personification generation. To this end, we propose PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification data for Learning Enhanced generation. We curate a corpus of personifications called PersonifCorp, together with automatically generated de-personified literalizations of these personifications. We demonstrate the usefulness of this parallel corpus by training a seq2seq model to personify a given literal input. Both automatic and human evaluations show that fine-tuning with PersonifCorp leads to significant gains in personification-related qualities such as animacy and interestingness. A detailed qualitative analysis also highlights key strengths and imperfections of PINEAPPLE over baselines, demonstrating a strong ability to generate diverse and creative personifications that enhance the overall appeal of a sentence.},
	address = {Gyeongju, Republic of Korea},
	author = {Keh, Sedrick Scott and Lu, Kevin and Gangal, Varun and Feng, Steven Y. and Jhamtani, Harsh and Alikhani, Malihe and Hovy, Eduard},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-24 12:26:43 +0200},
	date-modified = {2022-10-24 12:26:43 +0200},
	month = oct,
	pages = {6270--6284},
	publisher = {International Committee on Computational Linguistics},
	title = {{PINEAPPLE}: Personifying {IN}animate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation},
	url = {https://aclanthology.org/2022.coling-1.547},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.547}}

@inproceedings{doogan-buntine-2021-topic,
	abstract = {When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models{'} generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.},
	address = {Online},
	author = {Doogan, Caitlin and Buntine, Wray},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:49 +0200},
	date-modified = {2022-10-24 12:19:49 +0200},
	doi = {10.18653/v1/2021.naacl-main.300},
	month = jun,
	pages = {3824--3848},
	publisher = {Association for Computational Linguistics},
	title = {Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures},
	url = {https://aclanthology.org/2021.naacl-main.300},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.300},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.300}}

@inproceedings{pergola-etal-2021-disentangled,
	abstract = {The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers{'} subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.},
	address = {Online},
	author = {Pergola, Gabriele and Gui, Lin and He, Yulan},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:45 +0200},
	date-modified = {2022-10-24 12:19:45 +0200},
	doi = {10.18653/v1/2021.naacl-main.228},
	month = jun,
	pages = {2870--2883},
	publisher = {Association for Computational Linguistics},
	title = {A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews},
	url = {https://aclanthology.org/2021.naacl-main.228},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.228},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.228}}

@inproceedings{gupta-etal-2021-multi,
	abstract = {Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.},
	address = {Online},
	author = {Gupta, Pankaj and Chaudhary, Yatin and Sch{\"u}tze, Hinrich},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:41 +0200},
	date-modified = {2022-10-24 12:19:41 +0200},
	doi = {10.18653/v1/2021.naacl-main.332},
	month = jun,
	pages = {4205--4217},
	publisher = {Association for Computational Linguistics},
	title = {Multi-source Neural Topic Modeling in Multi-view Embedding Spaces},
	url = {https://aclanthology.org/2021.naacl-main.332},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.332},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.332}}

@inproceedings{xie-etal-2021-inductive,
	abstract = {Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global word-level graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents.},
	address = {Online},
	author = {Xie, Qianqian and Huang, Jimin and Du, Pan and Peng, Min and Nie, Jian-Yun},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:37 +0200},
	date-modified = {2022-10-24 12:19:37 +0200},
	doi = {10.18653/v1/2021.naacl-main.333},
	month = jun,
	pages = {4218--4227},
	publisher = {Association for Computational Linguistics},
	title = {Inductive Topic Variational Graph Auto-Encoder for Text Classification},
	url = {https://aclanthology.org/2021.naacl-main.333},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.333},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.333}}

@inproceedings{mueller-dredze-2021-fine,
	abstract = {Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer.},
	address = {Online},
	author = {Mueller, Aaron and Dredze, Mark},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:33 +0200},
	date-modified = {2022-10-24 12:19:33 +0200},
	doi = {10.18653/v1/2021.naacl-main.243},
	month = jun,
	pages = {3054--3068},
	publisher = {Association for Computational Linguistics},
	title = {Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling},
	url = {https://aclanthology.org/2021.naacl-main.243},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.243},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.243}}

@inproceedings{sun-etal-2021-tita,
	abstract = {In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topic-aware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at above-mentioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of {``}topic-aware{''}, we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of {``}two-stage interaction{''}, we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system.},
	address = {Online},
	author = {Sun, Xingwu and Cui, Yanling and Tang, Hongyin and Zhu, Qiuyu and Zhang, Fuzheng and Jin, Beihong},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:29 +0200},
	date-modified = {2022-10-24 12:19:29 +0200},
	doi = {10.18653/v1/2021.naacl-main.428},
	month = jun,
	pages = {5431--5440},
	publisher = {Association for Computational Linguistics},
	title = {{TITA}: A Two-stage Interaction and Topic-Aware Text Matching Model},
	url = {https://aclanthology.org/2021.naacl-main.428},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.428},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.428}}

@inproceedings{shen-etal-2021-taxoclass,
	abstract = {Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its {``}core classes{''}, and then check core classes{'} ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document{'}s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25{\%}.},
	address = {Online},
	author = {Shen, Jiaming and Qiu, Wenda and Meng, Yu and Shang, Jingbo and Ren, Xiang and Han, Jiawei},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:25 +0200},
	date-modified = {2022-10-24 12:19:25 +0200},
	doi = {10.18653/v1/2021.naacl-main.335},
	month = jun,
	pages = {4239--4249},
	publisher = {Association for Computational Linguistics},
	title = {{T}axo{C}lass: Hierarchical Multi-Label Text Classification Using Only Class Names},
	url = {https://aclanthology.org/2021.naacl-main.335},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.335},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.335}}

@inproceedings{iida-etal-2021-tabbie,
	abstract = {Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.},
	address = {Online},
	author = {Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:21 +0200},
	date-modified = {2022-10-24 12:19:21 +0200},
	doi = {10.18653/v1/2021.naacl-main.270},
	month = jun,
	pages = {3446--3456},
	publisher = {Association for Computational Linguistics},
	title = {{TABBIE}: Pretrained Representations of Tabular Data},
	url = {https://aclanthology.org/2021.naacl-main.270},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.270},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.270}}

@inproceedings{khanehzar-etal-2021-framing,
	abstract = {Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semi-supervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation.},
	address = {Online},
	author = {Khanehzar, Shima and Cohn, Trevor and Mikolajczak, Gosia and Turpin, Andrew and Frermann, Lea},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:17 +0200},
	date-modified = {2022-10-24 12:19:17 +0200},
	doi = {10.18653/v1/2021.naacl-main.174},
	month = jun,
	pages = {2154--2166},
	publisher = {Association for Computational Linguistics},
	title = {Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames},
	url = {https://aclanthology.org/2021.naacl-main.174},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.174},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.174}}

@inproceedings{zhan-etal-2021-augmenting,
	abstract = {Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pre-training a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pre-trained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledge-grounded dialogue benchmarks indicate that our model achieves better performance over baseline models.},
	address = {Online},
	author = {Zhan, Haolan and Zhang, Hainan and Chen, Hongshen and Ding, Zhuoye and Bao, Yongjun and Lan, Yanyan},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:13 +0200},
	date-modified = {2022-10-24 12:19:13 +0200},
	doi = {10.18653/v1/2021.naacl-main.446},
	month = jun,
	pages = {5621--5630},
	publisher = {Association for Computational Linguistics},
	title = {Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition},
	url = {https://aclanthology.org/2021.naacl-main.446},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.446},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.446}}

@inproceedings{zeng-nie-2021-simple,
	abstract = {Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer {--} conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.},
	address = {Online},
	author = {Zeng, Yan and Nie, Jian-Yun},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:08 +0200},
	date-modified = {2022-10-24 12:19:08 +0200},
	doi = {10.18653/v1/2021.naacl-main.392},
	month = jun,
	pages = {4927--4939},
	publisher = {Association for Computational Linguistics},
	title = {A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation},
	url = {https://aclanthology.org/2021.naacl-main.392},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.392},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.392}}

@inproceedings{he-etal-2021-model,
	abstract = {Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.},
	address = {Online},
	author = {He, Xuanli and Lyu, Lingjuan and Sun, Lichao and Xu, Qiongkai},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:05 +0200},
	date-modified = {2022-10-24 12:19:05 +0200},
	doi = {10.18653/v1/2021.naacl-main.161},
	month = jun,
	pages = {2006--2012},
	publisher = {Association for Computational Linguistics},
	title = {Model Extraction and Adversarial Transferability, Your {BERT} is Vulnerable!},
	url = {https://aclanthology.org/2021.naacl-main.161},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.161},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.161}}

@inproceedings{cui-hu-2021-sliding,
	abstract = {Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from.},
	address = {Online},
	author = {Cui, Peng and Hu, Le},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:19:01 +0200},
	date-modified = {2022-10-24 12:19:01 +0200},
	doi = {10.18653/v1/2021.naacl-main.470},
	month = jun,
	pages = {5881--5891},
	publisher = {Association for Computational Linguistics},
	title = {Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents},
	url = {https://aclanthology.org/2021.naacl-main.470},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.470},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.470}}

@inproceedings{schiller-etal-2021-aspect,
	abstract = {We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.},
	address = {Online},
	author = {Schiller, Benjamin and Daxenberger, Johannes and Gurevych, Iryna},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:18:57 +0200},
	date-modified = {2022-10-24 12:18:57 +0200},
	doi = {10.18653/v1/2021.naacl-main.34},
	month = jun,
	pages = {380--396},
	publisher = {Association for Computational Linguistics},
	title = {Aspect-Controlled Neural Argument Generation},
	url = {https://aclanthology.org/2021.naacl-main.34},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.34},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.34}}

@inproceedings{ji-etal-2021-discrete,
	abstract = {In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations.},
	address = {Online},
	author = {Ji, Lu and Wei, Zhongyu and Li, Jing and Zhang, Qi and Huang, Xuanjing},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:18:54 +0200},
	date-modified = {2022-10-24 12:18:54 +0200},
	doi = {10.18653/v1/2021.naacl-main.431},
	month = jun,
	pages = {5467--5478},
	publisher = {Association for Computational Linguistics},
	title = {Discrete Argument Representation Learning for Interactive Argument Pair Identification},
	url = {https://aclanthology.org/2021.naacl-main.431},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.431},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.431}}

@inproceedings{subramanian-etal-2021-spanpredict,
	abstract = {In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.},
	address = {Online},
	author = {Subramanian, Vivek and Engelhard, Matthew and Berchuck, Sam and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:18:49 +0200},
	date-modified = {2022-10-24 12:18:49 +0200},
	doi = {10.18653/v1/2021.naacl-main.413},
	month = jun,
	pages = {5234--5258},
	publisher = {Association for Computational Linguistics},
	title = {{S}pan{P}redict: Extraction of Predictive Document Spans with Neural Attention},
	url = {https://aclanthology.org/2021.naacl-main.413},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.413},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.413}}

@inproceedings{zhang-etal-2022-kcd,
	abstract = {Political perspective detection has become an increasingly important task that can help combat echo chambers and political polarization. Previous approaches generally focus on leveraging textual content to identify stances, while they fail to reason with background knowledge or leverage the rich semantic and syntactic textual labels in news articles. In light of these limitations, we propose KCD, a political perspective detection approach to enable multi-hop knowledge reasoning and incorporate textual cues as paragraph-level labels. Specifically, we firstly generate random walks on external knowledge graphs and infuse them with news text representations. We then construct a heterogeneous information network to jointly model news content as well as semantic, syntactic and entity cues in news articles. Finally, we adopt relational graph neural networks for graph-level representation learning and conduct political perspective detection. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on two benchmark datasets. We further examine the effect of knowledge walks and textual cues and how they contribute to our approach{'}s data efficiency.},
	address = {Seattle, United States},
	author = {Zhang, Wenqian and Feng, Shangbin and Chen, Zilong and Lei, Zhenyu and Li, Jundong and Luo, Minnan},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:17:43 +0200},
	date-modified = {2022-10-24 12:17:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.304},
	month = jul,
	pages = {4129--4140},
	publisher = {Association for Computational Linguistics},
	title = {{KCD}: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media},
	url = {https://aclanthology.org/2022.naacl-main.304},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.304},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.304}}

@inproceedings{voigt-etal-2022-survey,
	abstract = {Natural language as a modality of interaction is becoming increasingly popular in the field of visualization. In addition to the popular query interfaces, other language-based interactions such as annotations, recommendations, explanations, or documentation experience growing interest. In this survey, we provide an overview of natural language-based interaction in the research area of visualization. We discuss a renowned taxonomy of visualization tasks and classify 119 related works to illustrate the state-of-the-art of how current natural language interfaces support their performance. We examine applied NLP methods and discuss human-machine dialogue structures with a focus on initiative, duration, and communicative functions in recent visualization-oriented dialogue interfaces. Based on this overview, we point out interesting areas for the future application of NLP methods in the field of visualization.},
	address = {Seattle, United States},
	author = {Voigt, Henrik and Alacam, Ozge and Meuschke, Monique and Lawonn, Kai and Zarrie{\ss}, Sina},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:17:39 +0200},
	date-modified = {2022-10-24 12:17:39 +0200},
	doi = {10.18653/v1/2022.naacl-main.27},
	month = jul,
	pages = {348--374},
	publisher = {Association for Computational Linguistics},
	title = {The Why and The How: A Survey on Natural Language Interaction in Visualization},
	url = {https://aclanthology.org/2022.naacl-main.27},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.27},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.27}}

@inproceedings{liu-etal-2022-hiure,
	abstract = {Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.},
	address = {Seattle, United States},
	author = {Liu, Shuliang and Hu, Xuming and Zhang, Chenwei and Li, Shu{'}ang and Wen, Lijie and Yu, Philip},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:17:35 +0200},
	date-modified = {2022-10-24 12:17:35 +0200},
	doi = {10.18653/v1/2022.naacl-main.437},
	month = jul,
	pages = {5970--5980},
	publisher = {Association for Computational Linguistics},
	title = {{H}i{URE}: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction},
	url = {https://aclanthology.org/2022.naacl-main.437},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.437},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.437}}

@inproceedings{sircar-etal-2022-distantly,
	abstract = {Product aspect extraction from reviews is a critical task for e-commerce services to understand customer preferences and pain points. While aspect phrases extraction and sentiment analysis have received a lot of attention, clustering of aspect phrases and assigning human readable names to clusters in e-commerce reviews is an extremely important and challenging problem due to the scale of the reviews that makes human review infeasible. In this paper, we propose fully automated methods for clustering aspect words and generating human readable names for the clusters without any manually labeled data. We train transformer based sentence embeddings that are aware of unique e-commerce language characteristics (eg. incomplete sentences, spelling and grammar errors, vernacular etc.). We also train transformer based sequence to sequence models to generate human readable aspect names from clusters. Both the models are trained using heuristic based distant supervision. Additionally, the models are used to improve each other. Extensive empirical testing showed that the clustering model improves the Silhouette Score by 64{\%} when compared to the state-of-the-art baseline and the aspect naming model achieves a high ROUGE-L score of 0.79.},
	address = {Hybrid: Seattle, Washington + Online},
	author = {Sircar, Prateek and Chakrabarti, Aniket and Gupta, Deepak and Majumdar, Anirban},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
	date-added = {2022-10-24 12:17:31 +0200},
	date-modified = {2022-10-24 12:17:31 +0200},
	doi = {10.18653/v1/2022.naacl-industry.12},
	month = jul,
	pages = {94--102},
	publisher = {Association for Computational Linguistics},
	title = {Distantly Supervised Aspect Clustering And Naming For {E}-Commerce Reviews},
	url = {https://aclanthology.org/2022.naacl-industry.12},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-industry.12},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-industry.12}}

@inproceedings{zhang-etal-2022-seed,
	abstract = {Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users{'} particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.},
	address = {Seattle, United States},
	author = {Zhang, Yu and Meng, Yu and Wang, Xuan and Wang, Sheng and Han, Jiawei},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:17:26 +0200},
	date-modified = {2022-10-24 12:17:26 +0200},
	doi = {10.18653/v1/2022.naacl-main.21},
	month = jul,
	pages = {279--290},
	publisher = {Association for Computational Linguistics},
	title = {Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds},
	url = {https://aclanthology.org/2022.naacl-main.21},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.21},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.21}}

@inproceedings{spangher-etal-2022-newsedits,
	abstract = {News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.},
	address = {Seattle, United States},
	author = {Spangher, Alexander and Ren, Xiang and May, Jonathan and Peng, Nanyun},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:17:19 +0200},
	date-modified = {2022-10-24 12:17:19 +0200},
	doi = {10.18653/v1/2022.naacl-main.10},
	month = jul,
	pages = {127--157},
	publisher = {Association for Computational Linguistics},
	title = {{N}ews{E}dits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge},
	url = {https://aclanthology.org/2022.naacl-main.10},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.10},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.10}}

@inproceedings{bhattacharjee-etal-2022-users,
	abstract = {Users often leave feedback on a myriad of aspects of a product which, if leveraged successfully, can help yield useful insights that can lead to further improvements down the line. Detecting actionable insights can be challenging owing to large amounts of data as well as the absence of labels in real-world scenarios. In this work, we present an aggregation and graph-based ranking strategy for unsupervised detection of these insights from real-world, noisy, user-generated feedback. Our proposed approach significantly outperforms strong baselines on two real-world user feedback datasets and one academic dataset.},
	address = {Hybrid: Seattle, Washington + Online},
	author = {Bhattacharjee, Kasturi and Gangadharaiah, Rashmi and McKeown, Kathleen and Roth, Dan},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
	date-added = {2022-10-24 12:17:15 +0200},
	date-modified = {2022-10-24 12:17:15 +0200},
	doi = {10.18653/v1/2022.naacl-industry.27},
	month = jul,
	pages = {239--246},
	publisher = {Association for Computational Linguistics},
	title = {What Do Users Care About? Detecting Actionable Insights from User Feedback},
	url = {https://aclanthology.org/2022.naacl-industry.27},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-industry.27},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-industry.27}}

@inproceedings{li-etal-2022-corwa,
	abstract = {Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the {``}Related Work{''} section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has focused on the sentence as the basic unit of generation, neglecting the fact that related work sections consist of variable length text fragments derived from different information sources. As a first step toward a linguistically-motivated related work generation framework, we present a Citation Oriented Related Work Annotation (CORWA) dataset that labels different types of citation text fragments from different information sources. We train a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts. We further suggest a novel framework for human-in-the-loop, iterative, abstractive related work generation.},
	address = {Seattle, United States},
	author = {Li, Xiangci and Mandal, Biswadip and Ouyang, Jessica},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:17:10 +0200},
	date-modified = {2022-10-24 12:17:10 +0200},
	doi = {10.18653/v1/2022.naacl-main.397},
	month = jul,
	pages = {5426--5440},
	publisher = {Association for Computational Linguistics},
	title = {{CORWA}: A Citation-Oriented Related Work Annotation Dataset},
	url = {https://aclanthology.org/2022.naacl-main.397},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.397},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.397}}

@inproceedings{kulkarni-etal-2022-ctm,
	abstract = {Automatically associating social media posts with topics is an important prerequisite for effective search and recommendation on many social media platforms. However, topic classification of such posts is quite challenging because of (a) a large topic space (b) short text with weak topical cues, and (c) multiple topic associations per post. In contrast to most prior work which only focuses on post-classification into a small number of topics ($10-20$), we consider the task of large-scale topic classification in the context of Twitter where the topic space is 10 times larger with potentially multiple topic associations per Tweet. We address the challenges above and propose a novel neural model, that (a) supports a large topic space of 300 topics (b) takes a holistic approach to tweet content modeling {--} leveraging multi-modal content, author context, and deeper semantic cues in the Tweet. Our method offers an effective way to classify Tweets into topics at scale by yielding superior performance to other approaches (a relative lift of $\mathbf{20}\%$ in median average precision score) and has been successfully deployed in production at Twitter.},
	address = {Hybrid: Seattle, Washington + Online},
	author = {Kulkarni, Vivek and Leung, Kenny and Haghighi, Aria},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
	date-added = {2022-10-24 12:17:05 +0200},
	date-modified = {2022-10-24 12:17:05 +0200},
	doi = {10.18653/v1/2022.naacl-industry.28},
	month = jul,
	pages = {247--258},
	publisher = {Association for Computational Linguistics},
	title = {{CTM} - A Model for Large-Scale Multi-View Tweet Topic Classification},
	url = {https://aclanthology.org/2022.naacl-industry.28},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-industry.28},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-industry.28}}

@inproceedings{zhu-etal-2022-disentangled,
	abstract = {Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering.},
	address = {Seattle, United States},
	author = {Zhu, Lixing and Fang, Zheng and Pergola, Gabriele and Procter, Robert and He, Yulan},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-24 12:16:57 +0200},
	date-modified = {2022-10-24 12:16:57 +0200},
	doi = {10.18653/v1/2022.naacl-main.112},
	month = jul,
	pages = {1566--1580},
	publisher = {Association for Computational Linguistics},
	title = {Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media},
	url = {https://aclanthology.org/2022.naacl-main.112},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.112},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.112}}

@inproceedings{spell-etal-2020-embedding,
	abstract = {Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators{'} political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators{'} attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.},
	address = {Online},
	author = {Spell, Gregory and Guay, Brian and Hillygus, Sunshine and Carin, Lawrence},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:55 +0200},
	date-modified = {2022-10-24 11:36:55 +0200},
	doi = {10.18653/v1/2020.emnlp-main.46},
	month = nov,
	pages = {627--641},
	publisher = {Association for Computational Linguistics},
	title = {An {E}mbedding {M}odel for {E}stimating {L}egislative {P}references from the {F}requency and {S}entiment of {T}weets},
	url = {https://aclanthology.org/2020.emnlp-main.46},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.46},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.46}}

@inproceedings{tian-etal-2020-learning,
	abstract = {The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.},
	address = {Online},
	author = {Tian, Runzhi and Mao, Yongyi and Zhang, Richong},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:50 +0200},
	date-modified = {2022-10-24 11:36:50 +0200},
	doi = {10.18653/v1/2020.emnlp-main.101},
	month = nov,
	pages = {1315--1325},
	publisher = {Association for Computational Linguistics},
	title = {Learning {VAE}-{LDA} Models with Rounded Reparameterization Trick},
	url = {https://aclanthology.org/2020.emnlp-main.101},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.101},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.101}}

@inproceedings{gao-gormley-2020-training,
	abstract = {Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.},
	address = {Online},
	author = {Gao, Sida and Gormley, Matthew R.},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:45 +0200},
	date-modified = {2022-10-24 11:36:45 +0200},
	doi = {10.18653/v1/2020.emnlp-main.406},
	month = nov,
	pages = {4999--5011},
	publisher = {Association for Computational Linguistics},
	title = {Training for {G}ibbs Sampling on Conditional Random Fields with Neural Scoring Factors},
	url = {https://aclanthology.org/2020.emnlp-main.406},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.406},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.406}}

@inproceedings{liu-etal-2020-cross-lingual-spoken,
	abstract = {Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3{\%} of the target language training data, achieves comparable performance to the supervised training with all the training data.},
	address = {Online},
	author = {Liu, Zihan and Winata, Genta Indra and Xu, Peng and Lin, Zhaojiang and Fung, Pascale},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:41 +0200},
	date-modified = {2022-10-24 11:36:41 +0200},
	doi = {10.18653/v1/2020.emnlp-main.587},
	month = nov,
	pages = {7241--7251},
	publisher = {Association for Computational Linguistics},
	title = {Cross-lingual Spoken Language Understanding with Regularized Representation Alignment},
	url = {https://aclanthology.org/2020.emnlp-main.587},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.587},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.587}}

@inproceedings{ousidhoum-etal-2020-comparative,
	abstract = {Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.},
	address = {Online},
	author = {Ousidhoum, Nedjma and Song, Yangqiu and Yeung, Dit-Yan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:37 +0200},
	date-modified = {2022-10-24 11:36:37 +0200},
	doi = {10.18653/v1/2020.emnlp-main.199},
	month = nov,
	pages = {2532--2542},
	publisher = {Association for Computational Linguistics},
	title = {Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets},
	url = {https://aclanthology.org/2020.emnlp-main.199},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.199}}

@inproceedings{hu-etal-2020-neural,
	abstract = {Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.},
	address = {Online},
	author = {Hu, Xuemeng and Wang, Rui and Zhou, Deyu and Xiong, Yuxuan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:32 +0200},
	date-modified = {2022-10-24 11:36:32 +0200},
	doi = {10.18653/v1/2020.emnlp-main.725},
	month = nov,
	pages = {9018--9030},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Modeling with Cycle-Consistent Adversarial Training},
	url = {https://aclanthology.org/2020.emnlp-main.725},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.725},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.725}}

@inproceedings{meng-etal-2020-text,
	abstract = {Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90{\%} accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.},
	address = {Online},
	author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Xiong, Chenyan and Ji, Heng and Zhang, Chao and Han, Jiawei},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:36:28 +0200},
	date-modified = {2022-10-24 11:36:28 +0200},
	doi = {10.18653/v1/2020.emnlp-main.724},
	month = nov,
	pages = {9006--9017},
	publisher = {Association for Computational Linguistics},
	title = {Text Classification Using Label Names Only: A Language Model Self-Training Approach},
	url = {https://aclanthology.org/2020.emnlp-main.724},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.724},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.724}}

@inproceedings{hoyle-etal-2020-improving,
	abstract = {Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.},
	address = {Online},
	author = {Hoyle, Alexander Miserlis and Goel, Pranav and Resnik, Philip},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:59 +0200},
	date-modified = {2022-10-24 11:35:59 +0200},
	doi = {10.18653/v1/2020.emnlp-main.137},
	month = nov,
	pages = {1752--1771},
	publisher = {Association for Computational Linguistics},
	title = {{I}mproving {N}eural {T}opic {M}odels using {K}nowledge {D}istillation},
	url = {https://aclanthology.org/2020.emnlp-main.137},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.137},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.137}}

@inproceedings{bommasani-cardie-2020-intrinsic,
	abstract = {High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.},
	address = {Online},
	author = {Bommasani, Rishi and Cardie, Claire},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:55 +0200},
	date-modified = {2022-10-24 11:35:55 +0200},
	doi = {10.18653/v1/2020.emnlp-main.649},
	month = nov,
	pages = {8075--8096},
	publisher = {Association for Computational Linguistics},
	title = {Intrinsic Evaluation of Summarization Datasets},
	url = {https://aclanthology.org/2020.emnlp-main.649},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.649},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.649}}

@inproceedings{zhao-chang-2020-logan,
	abstract = {Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.},
	address = {Online},
	author = {Zhao, Jieyu and Chang, Kai-Wei},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:51 +0200},
	date-modified = {2022-10-24 11:35:51 +0200},
	doi = {10.18653/v1/2020.emnlp-main.155},
	month = nov,
	pages = {1968--1977},
	publisher = {Association for Computational Linguistics},
	title = {{LOGAN}: Local Group Bias Detection by Clustering},
	url = {https://aclanthology.org/2020.emnlp-main.155},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.155},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.155}}

@inproceedings{roy-goldwasser-2020-weakly,
	abstract = {In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.},
	address = {Online},
	author = {Roy, Shamik and Goldwasser, Dan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:47 +0200},
	date-modified = {2022-10-24 11:35:47 +0200},
	doi = {10.18653/v1/2020.emnlp-main.620},
	month = nov,
	pages = {7698--7716},
	publisher = {Association for Computational Linguistics},
	title = {Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media},
	url = {https://aclanthology.org/2020.emnlp-main.620},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.620},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.620}}

@inproceedings{wang-etal-2020-continuity,
	abstract = {Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn{'}s existing contents. Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.},
	address = {Online},
	author = {Wang, Lingzhi and Li, Jing and Zeng, Xingshan and Zhang, Haisong and Wong, Kam-Fai},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:34 +0200},
	date-modified = {2022-10-24 11:35:34 +0200},
	doi = {10.18653/v1/2020.emnlp-main.538},
	month = nov,
	pages = {6640--6650},
	publisher = {Association for Computational Linguistics},
	title = {Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations},
	url = {https://aclanthology.org/2020.emnlp-main.538},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.538},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.538}}

@inproceedings{field-tsvetkov-2020-unsupervised,
	abstract = {Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.},
	address = {Online},
	author = {Field, Anjalie and Tsvetkov, Yulia},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:29 +0200},
	date-modified = {2022-10-24 11:35:29 +0200},
	doi = {10.18653/v1/2020.emnlp-main.44},
	month = nov,
	pages = {596--608},
	publisher = {Association for Computational Linguistics},
	title = {Unsupervised Discovery of Implicit Gender Bias},
	url = {https://aclanthology.org/2020.emnlp-main.44},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.44},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.44}}

@inproceedings{pei-jurgens-2020-quantifying,
	abstract = {Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.},
	address = {Online},
	author = {Pei, Jiaxin and Jurgens, David},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:20 +0200},
	date-modified = {2022-10-24 11:35:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.428},
	month = nov,
	pages = {5307--5326},
	publisher = {Association for Computational Linguistics},
	title = {Quantifying Intimacy in Language},
	url = {https://aclanthology.org/2020.emnlp-main.428},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.428},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.428}}

@inproceedings{sawhney-etal-2020-time,
	abstract = {Social media{'}s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user{'}s historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.},
	address = {Online},
	author = {Sawhney, Ramit and Joshi, Harshit and Gandhi, Saumya and Shah, Rajiv Ratn},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:08 +0200},
	date-modified = {2022-10-24 11:35:08 +0200},
	doi = {10.18653/v1/2020.emnlp-main.619},
	month = nov,
	pages = {7685--7697},
	publisher = {Association for Computational Linguistics},
	title = {A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media},
	url = {https://aclanthology.org/2020.emnlp-main.619},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.619},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.619}}

@inproceedings{zhou-jurgens-2020-condolence,
	abstract = {Offering condolence is a natural reaction to hearing someone{'}s distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal{---}trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.},
	address = {Online},
	author = {Zhou, Naitian and Jurgens, David},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-24 11:35:03 +0200},
	date-modified = {2022-10-24 11:35:03 +0200},
	doi = {10.18653/v1/2020.emnlp-main.45},
	month = nov,
	pages = {609--626},
	publisher = {Association for Computational Linguistics},
	title = {Condolence and Empathy in Online Communities},
	url = {https://aclanthology.org/2020.emnlp-main.45},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.45},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.45}}

@inproceedings{dodge-etal-2021-documenting,
	abstract = {Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:23:21 +0200},
	date-modified = {2022-10-24 11:23:21 +0200},
	doi = {10.18653/v1/2021.emnlp-main.98},
	month = nov,
	pages = {1286--1305},
	publisher = {Association for Computational Linguistics},
	title = {Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus},
	url = {https://aclanthology.org/2021.emnlp-main.98},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.98},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.98}}

@inproceedings{li-etal-2021-detecting,
	abstract = {Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based prediction model for identifying health advice in research publications. We annotated a corpus of 6,000 sentences extracted from structured abstracts in PubMed publications as {`}{``}strong advice{''}, {``}weak advice{''}, or {``}no advice{''}, and developed a BERT-based model that can predict, with a macro-averaged F1-score of 0.93, whether a sentence gives strong advice, weak advice, or not. The prediction model generalized well to sentences in both unstructured abstracts and discussion sections, where health advice normally appears. We also conducted a case study that applied this prediction model to retrieve specific health advice on COVID-19 treatments from LitCovid, a large COVID research literature portal, demonstrating the usefulness of retrieving health advice sentences as an advanced research literature navigation function for health researchers and the general public.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Li, Yingya and Wang, Jun and Yu, Bei},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:23:17 +0200},
	date-modified = {2022-10-24 11:23:17 +0200},
	doi = {10.18653/v1/2021.emnlp-main.486},
	month = nov,
	pages = {6018--6029},
	publisher = {Association for Computational Linguistics},
	title = {Detecting Health Advice in Medical Research Literature},
	url = {https://aclanthology.org/2021.emnlp-main.486},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.486},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.486}}

@inproceedings{milbauer-etal-2021-aligning,
	abstract = {The Internet is home to thousands of communities, each with their own unique worldview and associated ideological differences. With new communities constantly emerging and serving as ideological birthplaces, battlegrounds, and bunkers, it is critical to develop a framework for understanding worldviews and ideological distinction. Most existing work, however, takes a predetermined view based on political polarization: the {``}right vs. left{''} dichotomy of U.S. politics. In reality, both political polarization {--} and worldviews more broadly {--} transcend one-dimensional difference, and deserve a more complete analysis. Extending the ability of word embedding models to capture the semantic and cultural characteristics of their training corpora, we propose a novel method for discovering the multifaceted ideological and worldview characteristics of communities. Using over 1B comments collected from the largest communities on Reddit.com representing {\textasciitilde}40{\%} of Reddit activity, we demonstrate the efficacy of this approach to uncover complex ideological differences across multiple axes of polarization.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Milbauer, Jeremiah and Mathew, Adarsh and Evans, James},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:23:13 +0200},
	date-modified = {2022-10-24 11:23:13 +0200},
	doi = {10.18653/v1/2021.emnlp-main.396},
	month = nov,
	pages = {4832--4845},
	publisher = {Association for Computational Linguistics},
	title = {Aligning Multidimensional Worldviews and Discovering Ideological Differences},
	url = {https://aclanthology.org/2021.emnlp-main.396},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.396},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.396}}

@inproceedings{lin-etal-2021-csds,
	abstract = {Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer{'}s issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers{'} viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Lin, Haitao and Ma, Liqun and Zhu, Junnan and Xiang, Lu and Zhou, Yu and Zhang, Jiajun and Zong, Chengqing},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:23:08 +0200},
	date-modified = {2022-10-24 11:23:08 +0200},
	doi = {10.18653/v1/2021.emnlp-main.365},
	month = nov,
	pages = {4436--4451},
	publisher = {Association for Computational Linguistics},
	title = {{CSDS}: A Fine-Grained {C}hinese Dataset for Customer Service Dialogue Summarization},
	url = {https://aclanthology.org/2021.emnlp-main.365},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.365},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.365}}

@inproceedings{yu-etal-2021-exophoric,
	abstract = {Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Yu, Xintong and Zhang, Hongming and Song, Yangqiu and Zhang, Changshui and Xu, Kun and Yu, Dong},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:23:04 +0200},
	date-modified = {2022-10-24 11:23:04 +0200},
	doi = {10.18653/v1/2021.emnlp-main.311},
	month = nov,
	pages = {3832--3845},
	publisher = {Association for Computational Linguistics},
	title = {Exophoric Pronoun Resolution in Dialogues with Topic Regularization},
	url = {https://aclanthology.org/2021.emnlp-main.311},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.311},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.311}}

@inproceedings{manchanda-karypis-2021-evaluating,
	abstract = {Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content of the publications. This limits their ability to provide rigorous quality-related metrics because they cannot account for the reasons that led to a citation. We present approaches to estimate content-aware bibliometrics to quantitatively measure the scholarly impact of a publication. Our approaches assess the impact of a cited publication by the extent to which the cited publication informs the citing publication. We introduce a new metric, called {``}Content Informed Index{''} (CII), that uses the content of the paper as a source of distant-supervision, to quantify how much the cited-node informs the citing-node. We evaluate the weights estimated by our approach on three manually annotated datasets, where the annotations quantify the extent of information in the citation. Particularly, we evaluate how well the ranking imposed by our approach associates with the ranking imposed by the manual annotations. CII achieves up to 103{\%} improvement in performance as compared to the second-best performing approach.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Manchanda, Saurav and Karypis, George},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:58 +0200},
	date-modified = {2022-10-24 11:22:58 +0200},
	doi = {10.18653/v1/2021.emnlp-main.488},
	month = nov,
	pages = {6041--6053},
	publisher = {Association for Computational Linguistics},
	title = {Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics},
	url = {https://aclanthology.org/2021.emnlp-main.488},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.488},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.488}}

@inproceedings{saxon-etal-2021-modeling,
	abstract = {Broader disclosive transparency{---}truth and clarity in communication regarding the function of AI systems{---}is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where {``}too much information{''} clouds a reader{'}s understanding of what a system description means. Disclosive transparency{'}s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between transparency, confusion, and user perceptions in a corpus of real NLP system descriptions.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Saxon, Michael and Levy, Sharon and Wang, Xinyi and Albalak, Alon and Wang, William Yang},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:51 +0200},
	date-modified = {2022-10-24 11:22:51 +0200},
	doi = {10.18653/v1/2021.emnlp-main.153},
	month = nov,
	pages = {2023--2037},
	publisher = {Association for Computational Linguistics},
	title = {Modeling Disclosive Transparency in {NLP} Application Descriptions},
	url = {https://aclanthology.org/2021.emnlp-main.153},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.153},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.153}}

@inproceedings{situ-etal-2021-lifelong,
	abstract = {Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher {--} an arbitrary explanation algorithm {--} on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10{\^{}}2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Situ, Xuelin and Maruf, Sameen and Zukerman, Ingrid and Paris, Cecile and Haffari, Gholamreza},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:47 +0200},
	date-modified = {2022-10-24 11:22:47 +0200},
	doi = {10.18653/v1/2021.emnlp-main.233},
	month = nov,
	pages = {2933--2940},
	publisher = {Association for Computational Linguistics},
	title = {Lifelong Explainer for Lifelong Learners},
	url = {https://aclanthology.org/2021.emnlp-main.233},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.233},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.233}}

@inproceedings{liu-etal-2021-leveraging,
	abstract = {Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with medical literature hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from medical literature and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and dynamic routing, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with medical literature hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with medical literature than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the medical literature, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Liu, Xin and Chen, Qingcai and Chen, Junying and Zhou, Wenxiu and Liu, Tingyu and Yang, Xinlan and Peng, Weihua},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:43 +0200},
	date-modified = {2022-10-24 11:22:43 +0200},
	doi = {10.18653/v1/2021.emnlp-main.285},
	month = nov,
	pages = {3518--3532},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically},
	url = {https://aclanthology.org/2021.emnlp-main.285},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.285},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.285}}

@inproceedings{kang-etal-2021-leveraging,
	abstract = {Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to address the inter-dependency among tags when they are ranked, the autoregressive approach fails to take orderlessness into account because it is designed to utilize sequential relations among tokens. We propose a sequence-oblivious generation method for tag recommendation, in which the next tag to be generated is independent of the order of the generated tags and the order of the ground truth tags occurring in training data. Empirical results on two different domains, Instagram and Stack Overflow, show that our method is significantly superior to the previous approaches.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Kang, Junmo and Kim, Jeonghwan and Shin, Suwon and Myaeng, Sung-Hyon},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:40 +0200},
	date-modified = {2022-10-24 11:22:40 +0200},
	doi = {10.18653/v1/2021.emnlp-main.279},
	month = nov,
	pages = {3464--3476},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Order-Free Tag Relations for Context-Aware Recommendation},
	url = {https://aclanthology.org/2021.emnlp-main.279},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.279},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.279}}

@inproceedings{wang-etal-2021-phrase,
	abstract = {Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Wang, Shufan and Thompson, Laure and Iyyer, Mohit},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:34 +0200},
	date-modified = {2022-10-24 11:22:34 +0200},
	doi = {10.18653/v1/2021.emnlp-main.846},
	month = nov,
	pages = {10837--10851},
	publisher = {Association for Computational Linguistics},
	title = {Phrase-{BERT}: Improved Phrase Embeddings from {BERT} with an Application to Corpus Exploration},
	url = {https://aclanthology.org/2021.emnlp-main.846},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.846},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.846}}

@inproceedings{zhang-etal-2021-howyoutagtweets,
	abstract = {Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user{'}s hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized that one{'}s interests in a hashtag are related with what they said before (user history) and the existing posts present the hashtag (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via multitask learning. In this way, user interests learned from the past can be customized to match future hashtags, which is beyond the capability of existing methods assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our model significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Zhang, Yuji and Zhang, Yubo and Xu, Chunpu and Li, Jing and Jiang, Ziyan and Peng, Baolin},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:30 +0200},
	date-modified = {2022-10-24 11:22:30 +0200},
	doi = {10.18653/v1/2021.emnlp-main.616},
	month = nov,
	pages = {7811--7820},
	publisher = {Association for Computational Linguistics},
	title = {{\#}{H}ow{Y}ou{T}ag{T}weets: Learning User Hashtagging Preferences via Personalized Topic Attention},
	url = {https://aclanthology.org/2021.emnlp-main.616},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.616},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.616}}

@inproceedings{ye-etal-2021-beyond,
	abstract = {Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels{'} hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Ye, Chenchen and Zhang, Linhai and He, Yulan and Zhou, Deyu and Wu, Jie},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:26 +0200},
	date-modified = {2022-10-24 11:22:26 +0200},
	doi = {10.18653/v1/2021.emnlp-main.253},
	month = nov,
	pages = {3162--3171},
	publisher = {Association for Computational Linguistics},
	title = {Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs},
	url = {https://aclanthology.org/2021.emnlp-main.253},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.253},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.253}}

@inproceedings{jin-etal-2021-neural,
	abstract = {Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Jin, Yuan and Zhao, He and Liu, Ming and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-24 11:22:22 +0200},
	date-modified = {2022-10-24 11:22:22 +0200},
	doi = {10.18653/v1/2021.emnlp-main.80},
	month = nov,
	pages = {1042--1052},
	publisher = {Association for Computational Linguistics},
	title = {Neural Attention-Aware Hierarchical Topic Model},
	url = {https://aclanthology.org/2021.emnlp-main.80},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.80},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.80}}

@inproceedings{zang-etal-2020-word,
	abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
	address = {Online},
	author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:57:20 +0200},
	date-modified = {2022-10-24 09:57:20 +0200},
	doi = {10.18653/v1/2020.acl-main.540},
	month = jul,
	pages = {6066--6080},
	publisher = {Association for Computational Linguistics},
	title = {Word-level Textual Adversarial Attacking as Combinatorial Optimization},
	url = {https://aclanthology.org/2020.acl-main.540},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.540},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.540}}

@inproceedings{zhu-etal-2020-batch,
	abstract = {Variational Autoencoder (VAE) is widely used as a generative model to approximate a model{'}s posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as {``}posterior collapse{''}. Previous approaches consider the Kullback{--}Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL{'}s distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior{'}s parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.},
	address = {Online},
	author = {Zhu, Qile and Bi, Wei and Liu, Xiaojiang and Ma, Xiyao and Li, Xiaolin and Wu, Dapeng},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:56:54 +0200},
	date-modified = {2022-10-24 09:56:54 +0200},
	doi = {10.18653/v1/2020.acl-main.235},
	month = jul,
	pages = {2636--2649},
	publisher = {Association for Computational Linguistics},
	title = {A Batch Normalized Inference Network Keeps the {KL} Vanishing Away},
	url = {https://aclanthology.org/2020.acl-main.235},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.235},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.235}}

@inproceedings{jia-etal-2020-mitigating,
	abstract = {Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models{'} top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.},
	address = {Online},
	author = {Jia, Shengyu and Meng, Tao and Zhao, Jieyu and Chang, Kai-Wei},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:56:35 +0200},
	date-modified = {2022-10-24 09:56:35 +0200},
	doi = {10.18653/v1/2020.acl-main.264},
	month = jul,
	pages = {2936--2942},
	publisher = {Association for Computational Linguistics},
	title = {Mitigating Gender Bias Amplification in Distribution by Posterior Regularization},
	url = {https://aclanthology.org/2020.acl-main.264},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.264},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.264}}

@inproceedings{keith-etal-2020-text,
	abstract = {Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual{'}s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.},
	address = {Online},
	author = {Keith, Katherine and Jensen, David and O{'}Connor, Brendan},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:55:45 +0200},
	date-modified = {2022-10-24 09:55:45 +0200},
	doi = {10.18653/v1/2020.acl-main.474},
	month = jul,
	pages = {5332--5344},
	publisher = {Association for Computational Linguistics},
	title = {Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates},
	url = {https://aclanthology.org/2020.acl-main.474},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.474},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.474}}

@inproceedings{vafa-etal-2020-text,
	abstract = {Ideal point models analyze lawmakers{'} votes to quantify their political positions, or ideal points. But votes are not the only way to express a political position. Lawmakers also give speeches, release press statements, and post tweets. In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets. Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points. One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors. To this end, we use it to study tweets from the 2020 Democratic presidential candidates. Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.},
	address = {Online},
	author = {Vafa, Keyon and Naidu, Suresh and Blei, David},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:55:22 +0200},
	date-modified = {2022-10-24 09:55:22 +0200},
	doi = {10.18653/v1/2020.acl-main.475},
	month = jul,
	pages = {5345--5357},
	publisher = {Association for Computational Linguistics},
	title = {Text-Based Ideal Points},
	url = {https://aclanthology.org/2020.acl-main.475},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.475},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.475}}

@misc{https://doi.org/10.48550/arxiv.2006.00998,
	author = {Pavlopoulos, John and Sorensen, Jeffrey and Dixon, Lucas and Thain, Nithum and Androutsopoulos, Ion},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-24 09:55:05 +0200},
	date-modified = {2022-10-24 09:55:05 +0200},
	doi = {10.48550/ARXIV.2006.00998},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Toxicity Detection: Does Context Really Matter?},
	url = {https://arxiv.org/abs/2006.00998},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2006.00998},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1906.04687}}

@inproceedings{wu-etal-2020-neural,
	abstract = {Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.},
	address = {Online},
	author = {Wu, Jiemin and Rao, Yanghui and Zhang, Zusheng and Xie, Haoran and Li, Qing and Wang, Fu Lee and Chen, Ziye},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:54:33 +0200},
	date-modified = {2022-10-24 09:54:33 +0200},
	doi = {10.18653/v1/2020.acl-main.548},
	month = jul,
	pages = {6159--6169},
	publisher = {Association for Computational Linguistics},
	title = {Neural Mixed Counting Models for Dispersed Topic Discovery},
	url = {https://aclanthology.org/2020.acl-main.548},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.548},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.548}}

@inproceedings{viegas-etal-2020-cluhtm,
	abstract = {Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the {``}ideal{''} number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM{'}s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information {--} when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the {``}shape{''} the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500{\%} over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.},
	address = {Online},
	author = {Viegas, Felipe and Cunha, Washington and Gomes, Christian and Pereira, Ant{\^o}nio and Rocha, Leonardo and Goncalves, Marcos},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:54:17 +0200},
	date-modified = {2022-10-24 09:54:17 +0200},
	doi = {10.18653/v1/2020.acl-main.724},
	month = jul,
	pages = {8138--8150},
	publisher = {Association for Computational Linguistics},
	title = {{C}lu{HTM} - Semantic Hierarchical Topic Modeling based on {C}lu{W}ords},
	url = {https://aclanthology.org/2020.acl-main.724},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.724},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.724}}

@inproceedings{wang-etal-2020-neural-topic,
	abstract = {Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6{\%} is observed in accuracy.},
	address = {Online},
	author = {Wang, Rui and Hu, Xuemeng and Zhou, Deyu and He, Yulan and Xiong, Yuxuan and Ye, Chenchen and Xu, Haiyang},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:53:59 +0200},
	date-modified = {2022-10-24 09:53:59 +0200},
	doi = {10.18653/v1/2020.acl-main.32},
	month = jul,
	pages = {340--350},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Modeling with Bidirectional Adversarial Training},
	url = {https://aclanthology.org/2020.acl-main.32},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.32},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.32}}

@inproceedings{peinelt-etal-2020-tbert,
	abstract = {Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.},
	address = {Online},
	author = {Peinelt, Nicole and Nguyen, Dong and Liakata, Maria},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-24 09:53:42 +0200},
	date-modified = {2022-10-24 09:53:42 +0200},
	doi = {10.18653/v1/2020.acl-main.630},
	month = jul,
	pages = {7047--7055},
	publisher = {Association for Computational Linguistics},
	title = {t{BERT}: Topic Models and {BERT} Joining Forces for Semantic Similarity Detection},
	url = {https://aclanthology.org/2020.acl-main.630},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.630},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.630}}

@inproceedings{veselova-vorontsov-2020-topic,
	abstract = {This article proposes a new approach for building topic models on unbalanced collections in topic modelling, based on the existing methods and our experiments with such methods. Real-world data collections contain topics in various proportions, and often documents of the relatively small theme become distributed all over the larger topics instead of being grouped into one topic. To address this issue, we design a new regularizer for Theta and Phi matrices in probabilistic Latent Semantic Analysis (pLSA) model. We make sure this regularizer increases the quality of topic models, trained on unbalanced collections. Besides, we conceptually support this regularizer by our experiments.},
	address = {Online},
	author = {Veselova, Eugeniia and Vorontsov, Konstantin},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	date-added = {2022-10-24 09:53:27 +0200},
	date-modified = {2022-10-24 09:53:27 +0200},
	doi = {10.18653/v1/2020.acl-srw.9},
	month = jul,
	pages = {59--65},
	publisher = {Association for Computational Linguistics},
	title = {Topic Balancing with Additive Regularization of Topic Models},
	url = {https://aclanthology.org/2020.acl-srw.9},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-srw.9},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-srw.9}}

@inproceedings{bilal-etal-2021-evaluation,
	abstract = {Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows.},
	address = {Online},
	author = {Bilal, Iman Munire and Wang, Bo and Liakata, Maria and Procter, Rob and Tsakalidis, Adam},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-24 09:52:32 +0200},
	date-modified = {2022-10-24 09:52:32 +0200},
	doi = {10.18653/v1/2021.acl-long.530},
	month = aug,
	pages = {6800--6814},
	publisher = {Association for Computational Linguistics},
	title = {Evaluation of Thematic Coherence in Microblogs},
	url = {https://aclanthology.org/2021.acl-long.530},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.530},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.530}}

@inproceedings{duan-etal-2021-enslm,
	abstract = {Natural language processing (NLP) often faces the problem of data diversity such as different domains, themes, styles, and so on. Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples. To solve this problem, we firstly propose an autoencoding topic model with a mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity. Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation. Specifically, EnsLM contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters. As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. EnsLM can be trained jointly with mATM with a flexible LM backbone. We evaluate the effectiveness of both mATM and EnsLM on various tasks.},
	address = {Online},
	author = {Duan, Zhibin and Zhang, Hao and Wang, Chaojie and Wang, Zhengjue and Chen, Bo and Zhou, Mingyuan},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-24 09:51:54 +0200},
	date-modified = {2022-10-24 09:51:54 +0200},
	doi = {10.18653/v1/2021.acl-long.230},
	month = aug,
	pages = {2954--2967},
	publisher = {Association for Computational Linguistics},
	title = {{E}ns{LM}: Ensemble Language Model for Data Diversity by Semantic Clustering},
	url = {https://aclanthology.org/2021.acl-long.230},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.230},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.230}}

@misc{https://doi.org/10.48550/arxiv.2106.01071,
	author = {Zhu, Lixing and Pergola, Gabriele and Gui, Lin and Zhou, Deyu and He, Yulan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-24 09:51:42 +0200},
	date-modified = {2022-10-24 09:51:42 +0200},
	doi = {10.48550/ARXIV.2106.01071},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection},
	url = {https://arxiv.org/abs/2106.01071},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2106.01071}}

@misc{https://doi.org/10.48550/arxiv.2106.04408,
	author = {Qi, Tao and Wu, Fangzhao and Wu, Chuhan and Yang, Peiru and Yu, Yang and Xie, Xing and Huang, Yongfeng},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-24 09:51:19 +0200},
	date-modified = {2022-10-24 09:51:19 +0200},
	doi = {10.48550/ARXIV.2106.04408},
	keywords = {Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation},
	url = {https://arxiv.org/abs/2106.04408},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2106.04408}}

@misc{https://doi.org/10.48550/arxiv.2105.14189,
	author = {Wang, Lingzhi and Zeng, Xingshan and Wong, Kam-Fai},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-24 09:50:57 +0200},
	date-modified = {2022-10-24 09:50:57 +0200},
	doi = {10.48550/ARXIV.2105.14189},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations},
	url = {https://arxiv.org/abs/2105.14189},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2105.14189}}

@inproceedings{liu-etal-2021-element,
	abstract = {Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction. Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type. To address this issue, we conduct \textit{Element Intervention}, which intervene on the context and entities respectively to obtain the underlying causal effects of them. We also provide two specific implementations of the interventions based on entity ranking and context contrasting. Experimental results on unsupervised relation extraction datasets show our method to outperform previous state-of-the-art methods and is robust across different datasets.},
	address = {Online},
	author = {Liu, Fangchao and Yan, Lingyong and Lin, Hongyu and Han, Xianpei and Sun, Le},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-24 09:49:49 +0200},
	date-modified = {2022-10-24 09:49:49 +0200},
	doi = {10.18653/v1/2021.acl-long.361},
	month = aug,
	pages = {4683--4693},
	publisher = {Association for Computational Linguistics},
	title = {Element Intervention for Open Relation Extraction},
	url = {https://aclanthology.org/2021.acl-long.361},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.361},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.361}}

@inproceedings{mou-etal-2021-align,
	abstract = {Ideology of legislators is typically estimated by ideal point models from historical records of votes. It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior. However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories. In order to mitigate these two problems, we explore to incorporate both voting behavior and public statements on Twitter to jointly model legislators. In addition, we propose a novel task, namely hashtag usage prediction to model the ideology of legislators on Twitter. In practice, we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage. Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction. Further analysis further demonstrates that legislator representation we learned captures nuances in statements.},
	address = {Online},
	author = {Mou, Xinyi and Wei, Zhongyu and Chen, Lei and Ning, Shangyi and He, Yancheng and Jiang, Changjian and Huang, Xuanjing},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-24 09:49:21 +0200},
	date-modified = {2022-10-24 09:49:21 +0200},
	doi = {10.18653/v1/2021.acl-long.99},
	month = aug,
	pages = {1236--1246},
	publisher = {Association for Computational Linguistics},
	title = {Align Voting Behavior with Public Statements for Legislator Representation Learning},
	url = {https://aclanthology.org/2021.acl-long.99},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.99},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.99}}

@inproceedings{Malkin_2022,
	author = {Nikolay Malkin and Zhen Wang and Nebojsa Jojic},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-23 22:49:20 +0200},
	date-modified = {2022-10-23 22:49:20 +0200},
	doi = {10.18653/v1/2022.acl-long.565},
	publisher = {Association for Computational Linguistics},
	title = {Coherence boosting: When your pretrained language model is not paying enough attention},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.565},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.565}}

@misc{https://doi.org/10.48550/arxiv.2202.13469,
	author = {Li, Jiacheng and Shang, Jingbo and McAuley, Julian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-23 22:48:58 +0200},
	date-modified = {2022-10-23 22:48:58 +0200},
	doi = {10.48550/ARXIV.2202.13469},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining},
	url = {https://arxiv.org/abs/2202.13469},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2202.13469}}

@inproceedings{Ribeiro_2022,
	author = {Marco Tulio Ribeiro and Scott Lundberg},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-23 22:48:13 +0200},
	date-modified = {2022-10-23 22:48:13 +0200},
	doi = {10.18653/v1/2022.acl-long.230},
	publisher = {Association for Computational Linguistics},
	title = {Adaptive Testing and Debugging of {NLP} Models},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.230},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.230}}

@inproceedings{Gabriel_2022,
	author = {Saadia Gabriel and Skyler Hallinan and Maarten Sap and Pemi Nguyen and Franziska Roesner and Eunsol Choi and Yejin Choi},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-23 22:47:38 +0200},
	date-modified = {2022-10-23 22:47:38 +0200},
	doi = {10.18653/v1/2022.acl-long.222},
	publisher = {Association for Computational Linguistics},
	title = {Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.222},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.222}}

@inproceedings{Jin_2022,
	author = {Mali Jin and Daniel Preotiuc-Pietro and A. Seza Do{\u{g}}ru{\"o}z and Nikolaos Aletras},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-23 22:47:00 +0200},
	date-modified = {2022-10-23 22:47:00 +0200},
	doi = {10.18653/v1/2022.acl-long.273},
	publisher = {Association for Computational Linguistics},
	title = {Automatic Identification and Classification of Bragging in Social Media},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.273},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.273}}

@inproceedings{Broscheit_2022,
	author = {Samuel Broscheit and Quynh Do and Judith Gaspers},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-23 22:46:41 +0200},
	date-modified = {2022-10-23 22:46:41 +0200},
	doi = {10.18653/v1/2022.acl-long.139},
	publisher = {Association for Computational Linguistics},
	title = {Distributionally Robust Finetuning {BERT} for Covariate Drift in Spoken Language Understanding},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.139},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.139},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.acl-long.139}}

@inproceedings{2022_Broscheit,
	author = {Samuel Broscheit and Quynh Do and Judith Gaspers},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-23 22:46:39 +0200},
	date-modified = {2022-10-24 09:36:09 +0200},
	doi = {10.18653/v1/2022.acl-long.139},
	publisher = {Association for Computational Linguistics},
	title = {Distributionally Robust Finetuning {BERT} for Covariate Drift in Spoken Language Understanding},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.139},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.139},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.acl-long.139}}

@incollection{Bondarenko_2021,
	author = {Alexander Bondarenko and Lukas Gienapp and Maik Fr{\"o}be and Meriem Beloucif and Yamen Ajjour and Alexander Panchenko and Chris Biemann and Benno Stein and Henning Wachsmuth and Martin Potthast and Matthias Hagen},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:11:33 +0200},
	date-modified = {2022-10-23 18:11:33 +0200},
	doi = {10.1007/978-3-030-72240-1_67},
	pages = {574--582},
	publisher = {Springer International Publishing},
	title = {Overview of Touch{\'{e}} 2021: Argument Retrieval},
	url = {https://doi.org/10.1007%2F978-3-030-72240-1_67},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72240-1_67},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72240-1_67}}

@incollection{Wang_2022,
	author = {Xi Wang and Iadh Ounis and Craig Macdonald},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:11:25 +0200},
	date-modified = {2022-10-23 18:11:25 +0200},
	doi = {10.1007/978-3-030-99736-6_33},
	pages = {487--501},
	publisher = {Springer International Publishing},
	title = {Effective Rating Prediction Using an Attention-Based User Review Sentiment Model},
	url = {https://doi.org/10.1007%2F978-3-030-99736-6_33},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99736-6_33},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99736-6_33}}

@incollection{Ishigaki_2020,
	author = {Tatsuya Ishigaki and Kazuya Machida and Hayato Kobayashi and Hiroya Takamura and Manabu Okumura},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:11:22 +0200},
	date-modified = {2022-10-23 18:11:22 +0200},
	doi = {10.1007/978-3-030-45442-5_23},
	pages = {182--189},
	publisher = {Springer International Publishing},
	title = {Distant Supervision for Extractive Question Summarization},
	url = {https://doi.org/10.1007%2F978-3-030-45442-5_23},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45442-5_23},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45442-5_23}}

@incollection{Esuli_2022,
	author = {Andrea Esuli and Alejandro Moreo and Fabrizio Sebastiani},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:11:17 +0200},
	date-modified = {2022-10-23 18:11:17 +0200},
	doi = {10.1007/978-3-030-99739-7_47},
	pages = {374--381},
	publisher = {Springer International Publishing},
	title = {{LeQua}@{CLEF}2022: Learning to Quantify},
	url = {https://doi.org/10.1007%2F978-3-030-99739-7_47},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99739-7_47},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99739-7_47}}

@incollection{Chelliah_2020,
	author = {Muthusamy Chelliah and Manish Shrivastava and Jaidam Ram Tej},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:11:09 +0200},
	date-modified = {2022-10-23 18:11:09 +0200},
	doi = {10.1007/978-3-030-45442-5_88},
	pages = {663--668},
	publisher = {Springer International Publishing},
	title = {Principle-to-Program: Neural Methods for Similar Question Retrieval in Online Communities},
	url = {https://doi.org/10.1007%2F978-3-030-45442-5_88},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45442-5_88},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45442-5_88}}

@incollection{Meng_2021,
	author = {Rui Meng and Zhen Yue and Alyssa Glass},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:54 +0200},
	date-modified = {2022-10-23 18:10:54 +0200},
	doi = {10.1007/978-3-030-72113-8_29},
	pages = {433--450},
	publisher = {Springer International Publishing},
	title = {Predicting User Engagement Status for~Online Evaluation of Intelligent Assistants},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_29},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_29},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_29}}

@incollection{Batra_2020,
	author = {Vishwash Batra and Aparajita Haldar and Yulan He and Hakan Ferhatosmanoglu and George Vogiatzis and Tanaya Guha},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:51 +0200},
	date-modified = {2022-10-23 18:10:51 +0200},
	doi = {10.1007/978-3-030-45439-5_4},
	pages = {50--64},
	publisher = {Springer International Publishing},
	title = {Variational Recurrent Sequence-to-Sequence Retrieval for Stepwise Illustration},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_4},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_4},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_4}}

@incollection{Abazari_Kia_2021,
	author = {Mahsa Abazari Kia},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:48 +0200},
	date-modified = {2022-10-23 18:10:48 +0200},
	doi = {10.1007/978-3-030-72240-1_78},
	pages = {667--671},
	publisher = {Springer International Publishing},
	title = {Automated Multi-document Text Summarization from Heterogeneous Data Sources},
	url = {https://doi.org/10.1007%2F978-3-030-72240-1_78},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72240-1_78},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72240-1_78}}

@incollection{Montazeralghaem_2020,
	author = {Ali Montazeralghaem and Razieh Rahimi and James Allan},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:44 +0200},
	date-modified = {2022-10-23 18:10:44 +0200},
	doi = {10.1007/978-3-030-45439-5_30},
	pages = {446--460},
	publisher = {Springer International Publishing},
	title = {Relevance Ranking Based on Query-Aware Context Analysis},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_30},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_30},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_30}}

@incollection{Lugo_2021,
	author = {Luis Lugo and Jose G. Moreno and Gilles Hubert},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:36 +0200},
	date-modified = {2022-10-23 18:10:36 +0200},
	doi = {10.1007/978-3-030-72113-8_27},
	pages = {405--418},
	publisher = {Springer International Publishing},
	title = {Modeling User Search Tasks with a Language-Agnostic Unsupervised Approach},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_27},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_27},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_27}}

@incollection{Kuzi_2021,
	author = {Saar Kuzi and ChengXiang Zhai},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:31 +0200},
	date-modified = {2022-10-23 18:10:31 +0200},
	doi = {10.1007/978-3-030-72113-8_19},
	pages = {284--297},
	publisher = {Springer International Publishing},
	title = {A Study of Distributed Representations for Figures of Research Articles},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_19},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_19},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_19}}

@incollection{Kovalchuk_2020,
	author = {Pavlo Kovalchuk and Diogo Proen{\c{c}}a and Jos{\'{e}} Borbinha and Rui Henriques},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:21 +0200},
	date-modified = {2022-10-23 18:10:21 +0200},
	doi = {10.1007/978-3-030-45439-5_19},
	pages = {281--295},
	publisher = {Springer International Publishing},
	title = {Moving from Formal Towards Coherent Concept Analysis: Why, When and How},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_19},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_19},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_19}}

@incollection{Jatowt_2021,
	author = {Adam Jatowt and I-Chen Hung and Michael F{\"a}rber and Ricardo Campos and Masatoshi Yoshikawa},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:16 +0200},
	date-modified = {2022-10-23 18:10:16 +0200},
	doi = {10.1007/978-3-030-72113-8_17},
	pages = {254--269},
	publisher = {Springer International Publishing},
	title = {Exploding {TV} Sets and Disappointing Laptops: Suggesting Interesting Content in News Archives Based on Surprise Estimation},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_17},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_17},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_17}}

@incollection{Brochier_2020,
	author = {Robin Brochier and Adrien Guille and Julien Velcin},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:14 +0200},
	date-modified = {2022-10-23 18:10:14 +0200},
	doi = {10.1007/978-3-030-45439-5_22},
	pages = {326--340},
	publisher = {Springer International Publishing},
	title = {Inductive Document Network Embedding with Topic-Word Attention},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_22},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_22},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_22}}

@incollection{Fard_2020,
	author = {Mazar Moradi Fard and Thibaut Thonet and Eric Gaussier},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:11 +0200},
	date-modified = {2022-10-23 18:10:11 +0200},
	doi = {10.1007/978-3-030-45439-5_1},
	pages = {3--16},
	publisher = {Springer International Publishing},
	title = {Seed-Guided Deep Document Clustering},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_1},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_1}}

@incollection{Palencia_Olivar_2022,
	author = {Miguel Palencia-Olivar},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:10:06 +0200},
	date-modified = {2022-10-23 18:10:06 +0200},
	doi = {10.1007/978-3-030-99739-7_64},
	pages = {520--527},
	publisher = {Springer International Publishing},
	title = {A Topical Approach to Capturing Customer Insight Dynamics in Social Media},
	url = {https://doi.org/10.1007%2F978-3-030-99739-7_64},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99739-7_64},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99739-7_64}}

@incollection{_ahinu__2021,
	author = {Furkan {\c{S}}ahinu{\c{c}} and Cagri Toraman},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:09:49 +0200},
	date-modified = {2022-10-23 18:09:49 +0200},
	doi = {10.1007/978-3-030-72240-1_50},
	pages = {471--478},
	publisher = {Springer International Publishing},
	title = {Tweet Length Matters: A Comparative Analysis on Topic Detection in Microblogs},
	url = {https://doi.org/10.1007%2F978-3-030-72240-1_50},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72240-1_50},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72240-1_50}}

@incollection{Valero_2022,
	author = {Francisco B. Valero and Marion Baranes and Elena V. Epure},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:09:43 +0200},
	date-modified = {2022-10-23 18:09:43 +0200},
	doi = {10.1007/978-3-030-99736-6_32},
	pages = {472--486},
	publisher = {Springer International Publishing},
	title = {Topic Modeling on Podcast Short-Text Metadata},
	url = {https://doi.org/10.1007%2F978-3-030-99736-6_32},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99736-6_32},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99736-6_32}}

@incollection{Zosa_2022,
	author = {Elaine Zosa and Lidia Pivovarova and Michele Boggia and Sardana Ivanova},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-23 18:09:41 +0200},
	date-modified = {2022-10-23 18:09:41 +0200},
	doi = {10.1007/978-3-030-99739-7_29},
	pages = {248--256},
	publisher = {Springer International Publishing},
	title = {Multilingual Topic Labelling of News Topics Using Ontological Mapping},
	url = {https://doi.org/10.1007%2F978-3-030-99739-7_29},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99739-7_29},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99739-7_29}}

@incollection{Yang_2021,
	author = {Haitian Yang and Weiqing Huang and Xuan Zhao and Yan Wang and Yuyan Chen and Bin Lv and Rui Mao and Ning Li},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:06:41 +0200},
	date-modified = {2022-10-23 18:06:41 +0200},
	doi = {10.1007/978-3-030-67664-3_35},
	pages = {584--599},
	publisher = {Springer International Publishing},
	title = {{AMQAN}: Adaptive Multi-Attention Question-Answer Networks for Answer Selection},
	url = {https://doi.org/10.1007%2F978-3-030-67664-3_35},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67664-3_35},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67664-3_35}}

@incollection{Audebert_2020,
	author = {Nicolas Audebert and Catherine Herold and Kuider Slimani and C{\'{e}}dric Vidal},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:06:24 +0200},
	date-modified = {2022-10-23 18:06:24 +0200},
	doi = {10.1007/978-3-030-43823-4_35},
	pages = {427--443},
	publisher = {Springer International Publishing},
	title = {Multimodal Deep Networks for Text and Image-Based Document Classification},
	url = {https://doi.org/10.1007%2F978-3-030-43823-4_35},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-43823-4_35},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-43823-4_35}}

@incollection{Khandelwal_2021,
	author = {Kanishka Khandelwal and Devendra Dhaka and Vivek Barsopia},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:06:19 +0200},
	date-modified = {2022-10-23 18:06:19 +0200},
	doi = {10.1007/978-3-030-67658-2_36},
	pages = {628--643},
	publisher = {Springer International Publishing},
	title = {Predicting Future Classifiers for Evolving Non-linear Decision Boundaries},
	url = {https://doi.org/10.1007%2F978-3-030-67658-2_36},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67658-2_36},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67658-2_36}}

@incollection{Harada_2021,
	author = {Shonosuke Harada and Hisashi Kashima},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:06:09 +0200},
	date-modified = {2022-10-23 18:06:09 +0200},
	doi = {10.1007/978-3-030-67658-2_31},
	pages = {542--558},
	publisher = {Springer International Publishing},
	title = {Counterfactual Propagation for Semi-supervised Individual Treatment Effect Estimation},
	url = {https://doi.org/10.1007%2F978-3-030-67658-2_31},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67658-2_31},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67658-2_31}}

@incollection{Farruque_2020,
	author = {Nawshad Farruque and Osmar Zaiane and Randy Goebel},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:06:04 +0200},
	date-modified = {2022-10-23 18:06:04 +0200},
	doi = {10.1007/978-3-030-46133-1_22},
	pages = {359--375},
	publisher = {Springer International Publishing},
	title = {Augmenting Semantic Representation of Depressive Language: From Forums to Microblogs},
	url = {https://doi.org/10.1007%2F978-3-030-46133-1_22},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-46133-1_22},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-46133-1_22}}

@inproceedings{10.1007/978-3-030-46133-1_33,
	abstract = {Although the majority of news articles are only viewed for days or weeks, there are a small fraction of news articles that are read across years, thus named as evergreen news articles. Because evergreen articles maintain a timeless quality and are consistently of interests to the public, understanding their characteristics better has huge implications for news outlets and platforms yet there are few studies that have explicitly investigated on evergreen articles. Addressing this gap, in this paper, we first propose a flexible parameterized definition of evergreen articles to capture their long-term high traffic patterns. Using a real dataset from the Washington Post, then, we unearth several distinctive characteristics of evergreen articles and build an early prediction model with encouraging results. Although less than {\$}{\$}1{\backslash}{\%}{\$}{\$} of news articles were identified as evergreen, our model achieves 0.961 in ROC AUC and 0.172 in PR AUC in 10-fold cross validation.},
	address = {Cham},
	author = {Liao, Yiming and Wang, Shuguang and Han, Eui-Hong (Sam) and Lee, Jongwuk and Lee, Dongwon},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:05:41 +0200},
	date-modified = {2022-10-23 18:05:41 +0200},
	editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
	isbn = {978-3-030-46133-1},
	pages = {552--568},
	publisher = {Springer International Publishing},
	title = {Characterization and Early Detection of Evergreen News Articles},
	year = {2020}}

@incollection{Bai_2021,
	author = {Zilong Bai and S. S. Ravi and Ian Davidson},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:04:09 +0200},
	date-modified = {2022-10-23 18:04:09 +0200},
	doi = {10.1007/978-3-030-67664-3_3},
	pages = {37--53},
	publisher = {Springer International Publishing},
	title = {Towards Description of Block Model on Graph},
	url = {https://doi.org/10.1007%2F978-3-030-67664-3_3},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67664-3_3},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67664-3_3}}

@incollection{Zhang_2021,
	author = {Jason (Jiasheng) Zhang and Dongwon Lee},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:03:59 +0200},
	date-modified = {2022-10-23 18:03:59 +0200},
	doi = {10.1007/978-3-030-67658-2_15},
	pages = {249--265},
	publisher = {Springer International Publishing},
	title = {{PROMO} for Interpretable Personalized Social Emotion Mining},
	url = {https://doi.org/10.1007%2F978-3-030-67658-2_15},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67658-2_15},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67658-2_15}}

@incollection{Madrid_2020,
	author = {Jorge G. Madrid and Hugo Jair Escalante and Eduardo Morales},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:03:55 +0200},
	date-modified = {2022-10-23 18:03:55 +0200},
	doi = {10.1007/978-3-030-43823-4_6},
	pages = {57--67},
	publisher = {Springer International Publishing},
	title = {Meta-learning of Textual Representations},
	url = {https://doi.org/10.1007%2F978-3-030-43823-4_6},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-43823-4_6},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-43823-4_6}}

@incollection{Ferner_2020,
	author = {Cornelia Ferner and Stefan Wegenkittl},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-23 18:03:28 +0200},
	date-modified = {2022-10-23 18:03:28 +0200},
	doi = {10.1007/978-3-030-46147-8_42},
	pages = {697--710},
	publisher = {Springer International Publishing},
	title = {A Semi-discriminative Approach for Sub-sentence Level Topic Classification on a Small Dataset},
	url = {https://doi.org/10.1007%2F978-3-030-46147-8_42},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-46147-8_42},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-46147-8_42}}

@inproceedings{10.1145/3459637.3482398,
	abstract = {As a well-established probabilistic method, topic models seek to uncover latent semantics from plain text. In addition to having textual content, we observe that documents are usually compared in listwise rankings based on their content. For instance, world-wide countries are compared in an international ranking in terms of electricity production based on their national reports. Such document comparisons constitute additional information that reveal documents' relative similarities. Incorporating them into topic modeling could yield comparative topics that help to differentiate and rank documents. Furthermore, based on different comparison criteria, the observed document comparisons usually cover multiple aspects, each expressing a distinct ranked list. For example, a country may be ranked higher in terms of electricity production, but fall behind others in terms of life expectancy or government budget. Each comparison criterion, or aspect, observes a distinct ranking. Considering such multiple aspects of comparisons based on different ranking criteria allows us to derive one set of topics that inform heterogeneous document similarities. We propose a generative topic model aimed at learning topics that are well aligned to multi-aspect listwise comparisons. Experiments on public datasets demonstrate the advantage of the proposed method in jointly modeling topics and ranked lists against baselines comprehensively.},
	address = {New York, NY, USA},
	author = {Zhang, Delvin Ce and Lauw, Hady W.},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3459637.3482398},
	isbn = {9781450384469},
	keywords = {text mining, generative topic model, comparative documents},
	location = {Virtual Event, Queensland, Australia},
	numpages = {10},
	pages = {2507--2516},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Topic Modeling for Multi-Aspect Listwise Comparisons},
	url = {https://doi.org/10.1145/3459637.3482398},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482398}}

@inproceedings{10.1145/3511808.3557410,
	abstract = {Two general approaches are common for evaluating automatically generated labels in topic modeling: direct human assessment; or performance metrics that can be calculated without, but still correlate with, human assessment. However, both approaches implicitly assume that the quality of a topic label is single-dimensional. In contrast, this paper provides evidence that human assessments about the quality of topic labels consist of multiple latent dimensions. This evidence comes from human assessments of four simple labeling techniques. For each label, study participants responded to several items asking them to assess each label according to a variety of different criteria. Exploratory factor analysis shows that these human assessments of labeling quality have a two-factor latent structure. Subsequent analysis demonstrates that this multi-item, two-factor assessment can reveal nuances that would be missed using either a single-item human assessment of perceived label quality or established performance metrics. The paper concludes by suggesting future directions for the development of human-centered approaches to evaluating NLP and ML systems more broadly.},
	address = {New York, NY, USA},
	author = {Hosseiny Marani, Amin and Levine, Joshua and Baumer, Eric P.S.},
	booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3511808.3557410},
	isbn = {9781450392365},
	keywords = {topic labeling, topic modeling, human assessment, performance metrics, exploratory factor analysis},
	location = {Atlanta, GA, USA},
	numpages = {12},
	pages = {768--779},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {One Rating to Rule Them All? Evidence of Multidimensionality in Human Assessment of Topic Labeling Quality},
	url = {https://doi.org/10.1145/3511808.3557410},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3511808.3557410}}

@inproceedings{10.1145/3340531.3412050,
	abstract = {The task of session search focuses on using interaction data to improve relevance for the user's next query at the session level. In this paper, we formulate session search as a personalization task under the framework of learning to rank. Personalization approaches re-rank results to match a user model. Such user models are usually accumulated over time based on the user's browsing behaviour. We use a pre-computed and transparent set of user models based on concepts from the social science literature. Interaction data are used to map each session to these user models. Novel features are then estimated based on such models as well as sessions' interaction data. Extensive experiments on test collections from the TREC session track show statistically significant improvements over current session search algorithms.},
	address = {New York, NY, USA},
	author = {Aloteibi, Saad and Clark, Stephen},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3340531.3412050},
	isbn = {9781450368599},
	keywords = {personalization, session search, retrieval model, user models},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {15--24},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {Learning to Personalize for Web Search Sessions},
	url = {https://doi.org/10.1145/3340531.3412050},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3412050}}

@inproceedings{10.1145/3511808.3557561,
	abstract = {In this paper, we present a large Chinese news article dataset with 4.4 million articles. These articles are obtained from different news channels and sources. They are labeled with multi-level topic categories, and some of them also have summaries. This is the first Chinese news dataset that has both hierarchical topic labels and article full texts. And it is also the largest Chinese news topic dataset. We describe the data collection, annotation and quality evaluation process. The basic statistics of the dataset, comparison with other datasets and benchmark experiments are also presented.},
	address = {New York, NY, USA},
	author = {Li, Quanzhi and Liu, Yingchi and Chao, Yang},
	booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3511808.3557561},
	isbn = {9781450392365},
	keywords = {hierarchical topic classification, chinese news dataset, news summary, news topic},
	location = {Atlanta, GA, USA},
	numpages = {6},
	pages = {4193--4198},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {CNewsTS - A Large-Scale Chinese News Dataset with Hierarchical Topic Category and Summary},
	url = {https://doi.org/10.1145/3511808.3557561},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3511808.3557561}}

@inproceedings{10.1145/3459637.3482253,
	abstract = {Identifying the dynamic functions of different urban zones enables a variety of smart city applications, such as intelligent urban planning, real-time traffic scheduling, and community precision management. Traditional urban function research using government administrative zoning systems is often conducted in a coarse resolution with fixed split, and ignore the reshaping of zones by city growth. To solve this problem, we propose a two-stage framework in order to represent the high-definition distribution of urban function across the city, by analyzing continuous human traces extracted from the dense, widespread, and full-time cellular data. At the representation stage, we embed the locations of base stations by modeling the user movements with staying and transfer events, along with the consideration of dynamic trip purposes in continuous human traces. At the annotation stage, we first divide the city into the finest unit zones and each covers at least one base station. By clustering the base stations, we further group the unit zones into functional zones. Last, we annotate functional zones based on the local point-of-interest (POI) information. In experiments, we evaluate the proposed high-definition function study in two tasks: (i) in-zone crowd flow prediction, and (ii) zone-enhanced POI recommendation. The results demonstrate the advantage of the proposed method with both the effectiveness of city split and the high-quality function annotation.},
	address = {New York, NY, USA},
	author = {Liu, Chunyu and Yang, Yongjian and Yao, Zijun and Xu, Yuanbo and Chen, Weitong and Yue, Lin and Wu, Haomeng},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3459637.3482253},
	isbn = {9781450384469},
	keywords = {mobile trajectory, zone embedding, signaling data, fine-grained functional zone, urban computing},
	location = {Virtual Event, Queensland, Australia},
	numpages = {10},
	pages = {1048--1057},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Discovering Urban Functions of High-Definition Zoning with Continuous Human Traces},
	url = {https://doi.org/10.1145/3459637.3482253},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482253}}

@inproceedings{10.1145/3340531.3412878,
	abstract = {There are many existing retrieval and question answering datasets. However, most of them either focus on ranked list evaluation or single-candidate question answering. This divide makes it challenging to properly evaluate approaches concerned with ranking documents and providing snippets or answers for a given query. In this work, we present FiRA: a novel dataset of Fine-Grained Relevance Annotations. We extend the ranked retrieval annotations of the Deep Learning track of TREC 2019 with passage and word level graded relevance annotations for all relevant documents. We use our newly created data to study the distribution of relevance in long documents, as well as the attention of annotators to specific positions of the text. As an example, we evaluate the recently introduced TKL document ranking model. We find that although TKL exhibits state-of-the-art retrieval results for long documents, it misses many relevant passages.},
	address = {New York, NY, USA},
	author = {Hofst\"{a}tter, Sebastian and Zlabinger, Markus and Sertkan, Mete and Schr\"{o}der, Michael and Hanbury, Allan},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3340531.3412878},
	isbn = {9781450368599},
	keywords = {relevance distribution, position bias, word-level relevance, fine-grained annotations},
	location = {Virtual Event, Ireland},
	numpages = {8},
	pages = {3031--3038},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering},
	url = {https://doi.org/10.1145/3340531.3412878},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3412878}}

@inproceedings{10.1145/3340531.3411932,
	abstract = {News recommendation systems? purpose is to tackle the immense amount of news and offer personalized recommendations to users. A major issue in news recommendation is to capture the precise news representations for the efficacy of recommended items. Commonly, news contents are filled with well-known entities of different types. However, existing recommendation systems overlook exploiting external knowledge about entities and topical relatedness among the news. To cope with the above problem, in this paper, we propose Topic-Enriched Knowledge Graph Recommendation System(TEKGR). Three encoders in TEKGR handle news titles in two perspectives to obtain news representation embedding: (1) to extract meaning of news words without considering latent knowledge features in the news and (2) to extract semantic knowledge of news through topic information and contextual information from a knowledge graph. After obtaining news representation vectors, an attention network compares clicked news to the candidate news in order to get the user's final embedding. Our TEKGR model is superior to existing news recommendation methods by manipulating topical relations among entities and contextual features of entities. Experimental results on two public datasets show that our approach outperforms state-of-the-art deep recommendation approaches.},
	address = {New York, NY, USA},
	author = {Lee, Dongho and Oh, Byungkook and Seo, Seungmin and Lee, Kyong-Ho},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3340531.3411932},
	isbn = {9781450368599},
	keywords = {recommendation system, knowledge graphs, neural networks, news recommendation},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {695--704},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {News Recommendation with Topic-Enriched Knowledge Graphs},
	url = {https://doi.org/10.1145/3340531.3411932},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411932}}

@inproceedings{10.1145/3511808.3557329,
	abstract = {State-of-the-art Graph Neural Networks (GNNs) have achieved tremendous success in social event detection tasks when restricted to a closed set of events. However, considering the large amount of data needed for training and the limited ability of a neural network in handling previously unknown data, it is hard for existing GNN-based methods to operate in an open set setting. To address this problem, we design a Quality-aware Self-improving Graph Neural Network (QSGNN) which extends the knowledge from known to unknown by leveraging the best of known samples and reliable knowledge transfer. Specifically, to fully exploit the labeled data, we propose a novel supervised pairwise loss with an additional orthogonal inter-class relation constraint to train the backbone GNN encoder. The learnt, already-known events further serve as strong reference bases for the unknown ones, which greatly prompts knowledge acquisition and transfer. When the model is generalized to unknown data, to ensure the effectiveness and reliability, we further leverage the reference similarity distribution vectors for pseudo pairwise label generation, selection and quality assessment. Following the diversity principle of active learning, our method selects diverse pair samples with the generated pseudo labels to fine-tune the GNN encoder. Besides, we propose a novel quality-guided optimization in which the contributions of pseudo labels are weighted based on consistency. Experimental results validate that our model achieves state-of-the-art results and extends well to unknown events.},
	address = {New York, NY, USA},
	author = {Ren, Jiaqian and Jiang, Lei and Peng, Hao and Cao, Yuwei and Wu, Jia and Yu, Philip S. and He, Lifang},
	booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3511808.3557329},
	isbn = {9781450392365},
	keywords = {contrastive learning, social event detection, graph neural network, active learning},
	location = {Atlanta, GA, USA},
	numpages = {10},
	pages = {1696--1705},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {From Known to Unknown: Quality-Aware Self-Improving Graph Neural Network For Open Set Social Event Detection},
	url = {https://doi.org/10.1145/3511808.3557329},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3511808.3557329}}

@inproceedings{10.1145/3459637.3482075,
	abstract = {The economic policy uncertainty (EPU) index is one of the important text-based indexes in finance and economics fields. The EPU indexes of more than 26 countries have been constructed to reflect the policy uncertainty on country-level economic environments and serve as an important economic leading indicator. The EPU indexes are calculated based on the number of news articles with some manually-selected keywords related to economic, uncertainty, and policy. We find that the keyword-based EPU indexes contain noise, which will influence their explainability and predictability. In our experimental dataset, over 40% of news articles with the selected keywords are not related to the EPU. Instead of using keywords only, our proposed models take contextual information into account and get good performance on identifying the articles unrelated to EPU. The noise free EPU index performs better than the keyword-based EPU index in both explainability and predictability.},
	address = {New York, NY, USA},
	author = {Chen, Chung-Chi and Huang, Hen-Hsen and Huang, Yu-Lieh and Chen, Hsin-Hsi},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3459637.3482075},
	isbn = {9781450384469},
	keywords = {denoise, economic index, document filtering, economic policy uncertainty},
	location = {Virtual Event, Queensland, Australia},
	numpages = {5},
	pages = {2915--2919},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Constructing Noise Free Economic Policy Uncertainty Index},
	url = {https://doi.org/10.1145/3459637.3482075},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482075}}

@inproceedings{10.1145/3340531.3411933,
	abstract = {Online health communities (OHCs) provide a popular channel for users to seek information, suggestions and support during their medical treatment and recovery processes. To help users find relevant information easily, we present CLIR, an effective system for recommending relevant discussion threads to users in OHCs. We identify that thread content and user interests can be categorized in two dimensions: topics and concepts. CLIR leverages Latent Dirichlet Allocation model to summarize the topic dimension and uses Convolutional Neural Network to encode the concept dimension. It then builds a thread neural network to capture thread characteristics and builds a user neural network to capture user interests by integrating these two dimensions and their interactions. Finally, it matches the target thread's characteristics with candidate users' interests to make recommendations. Experimental evaluation with multiple OHC datasets demonstrates the performance advantage of CLIR over the state-of-the-art recommender systems on various evaluation metrics.},
	address = {New York, NY, USA},
	author = {Li, Mingda and Gao, Weiting and Chen, Yi},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3340531.3411933},
	isbn = {9781450368599},
	keywords = {online health community, discussion forum, thread recommendation, recommender systems, neural network, latent dirichlet allocation},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {765--774},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {A Topic and Concept Integrated Model for Thread Recommendation in Online Health Communities},
	url = {https://doi.org/10.1145/3340531.3411933},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411933}}

@inproceedings{10.1145/3340531.3411906,
	abstract = {E-Commerce marketplaces support millions of daily transactions, and some disagreements between buyers and sellers are unavoidable. Resolving disputes in an accurate, fast, and fair manner is of great importance for maintaining a trustworthy platform. Simple cases can be automated, but intricate cases are not sufficiently addressed by hard-coded rules, and therefore most disputes are currently resolved by people. In this work we take a first step towards automatically assisting human agents in dispute resolution at scale. We construct a large dataset of disputes from the eBay online marketplace, and identify several interesting behavioral and linguistic patterns. We then train classifiers to predict dispute outcomes with high accuracy. We explore the model and the dataset, reporting interesting correlations, important features, and insights.},
	address = {New York, NY, USA},
	author = {Tsurel, David and Doron, Michael and Nus, Alexander and Dagan, Arnon and Guy, Ido and Shahaf, Dafna},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3340531.3411906},
	isbn = {9781450368599},
	keywords = {online transactions, dispute resolution, e-commerce},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {1465--1474},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {E-Commerce Dispute Resolution Prediction},
	url = {https://doi.org/10.1145/3340531.3411906},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411906}}

@inproceedings{10.1145/3340531.3411890,
	abstract = {False information detection on social media is challenging as it commonly requires tedious evidence-collecting but lacks available comparative information. Clues mined from user comments, as the wisdom of crowds, could be of considerable benefit to this task. However, it is non-trivial to capture the complex semantics from the contents and comments in consideration of their implicit correlations. Although deep neural networks have good expressive power, one major drawback is the lack of explainability. In this paper, we focus on how to learn from the post contents and related comments in social media to understand and detect the false information more effectively, with explainability. We thus propose a Quantum-probability based Signed Attention Network (QSAN) that integrates the quantum-driven text encoding and a novel signed attention mechanism in a unified framework. QSAN is not only able to distinguish important comments from the others, but also can exploit the conflicting social viewpoints in the comments to facilitate the detection. Moreover, QSAN is advantageous with its explainability in terms of transparency due to quantum physics meanings and the attention weights. Extensive experiments on real-world datasets show that our approach outperforms state-of-the-art baselines and can provide different kinds of user comments to explain why a piece of information is detected as false.},
	address = {New York, NY, USA},
	author = {Tian, Tian and Liu, Yudong and Yang, Xiaoyu and Lyu, Yuefei and Zhang, Xi and Fang, Binxing},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3340531.3411890},
	isbn = {9781450368599},
	keywords = {quantum probability, false information detection, explainable AI, stance detection},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {1445--1454},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {QSAN: A Quantum-Probability Based Signed Attention Network for Explainable False Information Detection},
	url = {https://doi.org/10.1145/3340531.3411890},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411890}}

@inproceedings{10.1145/3459637.3482450,
	abstract = {Neural text matching models have been widely used in community question answering, information retrieval, and dialogue. However, these models designed for short texts cannot well address the long-form text matching problem, because there are many contexts in long-form texts can not be directly aligned with each other, and it is difficult for existing models to capture the key matching signals from such noisy data. Besides, these models are computationally expensive for simply use all textual data indiscriminately. To tackle the effectiveness and efficiency problem, we propose a novel hierarchical noise filtering model, namely Match-Ignition. The main idea is to plug the well-known PageRank algorithm into the Transformer, to identify and filter both sentence and word level noisy information in the matching process. Noisy sentences are usually easy to detect because previous work has shown that their similarity can be explicitly evaluated by the word overlapping, so we directly use PageRank to filter such information based on a sentence similarity graph. Unlike sentences, words rely on their contexts to express concrete meanings, so we propose to jointly learn the filtering and matching process, to well capture the critical word-level matching signals. Specifically, a word graph is first built based on the attention scores in each self-attention block of Transformer, and key words are then selected by applying PageRank on this graph. In this way, noisy words will be filtered out layer by layer in the matching process. Experimental results show that Match-Ignition outperforms both SOTA short text matching models and recent long-form text matching models. We also conduct detailed analysis to show that Match-Ignition efficiently captures important sentences and words, to facilitate the long-form text matching process.},
	address = {New York, NY, USA},
	author = {Pang, Liang and Lan, Yanyan and Cheng, Xueqi},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-23 17:57:02 +0200},
	date-modified = {2022-10-23 17:57:02 +0200},
	doi = {10.1145/3459637.3482450},
	isbn = {9781450384469},
	keywords = {text matching, pagerank algorithm, long-form text},
	location = {Virtual Event, Queensland, Australia},
	numpages = {10},
	pages = {1396--1405},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Match-Ignition: Plugging PageRank into Transformer for Long-Form Text Matching},
	url = {https://doi.org/10.1145/3459637.3482450},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482450}}

@inproceedings{10.1145/3447548.3467410,
	abstract = {Long story generation (LSG) is one of the coveted goals in natural language processing. Different from most text generation tasks, LSG requires to output a long story of rich content based on a much shorter text input, and often suffers from information sparsity. In this paper, we propose TopNet to alleviate this problem, by leveraging the recent advances in neural topic modeling to obtain high-quality skeleton words to complement the short input. In particular, instead of directly generating a story, we first learn to map the short text input to a low-dimensional topic distribution (which is pre-assigned by a topic model). Based on this latent topic distribution, we can use the reconstruction decoder of the topic model to sample a sequence of inter-related words as a skeleton for the story. Experiments on two benchmark datasets show that our proposed framework is highly effective in skeleton word selection and significantly outperforms the state-of-the-art models in both automatic evaluation and human evaluation.},
	address = {New York, NY, USA},
	author = {Yang, Yazheng and Pan, Boyuan and Cai, Deng and Sun, Huan},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3447548.3467410},
	isbn = {9781450383325},
	keywords = {deep learning, long story generation, topic model, natural language processing, story telling},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {1997--2005},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {TopNet: Learning from Neural Topic Model to Generate Long Stories},
	url = {https://doi.org/10.1145/3447548.3467410},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467410}}

@inproceedings{10.1145/3534678.3542675,
	abstract = {Electronic health records (EHRs) provide rich clinical information and the opportunities to extract epidemiological patterns to understand and predict patient disease risks with suitable machine learning methods such as topic models. However, existing topic models do not generate identifiable topics each predicting a unique phenotype. One promising direction is to use known phenotype concepts to guide topic inference. We present a seed-guided Bayesian topic model called MixEHR-Seed with 3 contributions: (1) for each phenotype, we infer a dual-form of topic distribution: a seed-topic distribution over a small set of key EHR codes and a regular topic distribution over the entire EHR vocabulary; (2) we model age-dependent disease progression as Markovian dynamic topic priors; (3) we infer seed-guided multi-modal topics over distinct EHR data types. For inference, we developed a variational inference algorithm. Using MixEHR-Seed, we inferred 1569 PheCode-guided phenotype topics from an EHR database in Quebec, Canada covering 1.3 million patients for up to 20-year follow-up with 122 million records for 8539 and 1126 unique diagnostic and drug codes, respectively. We observed (1) accurate phenotype prediction by the guided topics, (2) clinically relevant PheCode-guided disease topics, (3) meaningful age-dependent disease prevalence. Source code is available at GitHub: https://github.com/li-lab-mcgill/MixEHR-Seed.},
	address = {New York, NY, USA},
	author = {Song, Ziyang and Hu, Yuanyi and Verma, Aman and Buckeridge, David L. and Li, Yue},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3534678.3542675},
	isbn = {9781450393850},
	keywords = {variational autoencoder, predictive healthcare, topic modeling, electronic health records},
	location = {Washington DC, USA},
	numpages = {11},
	pages = {4713--4723},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Automatic Phenotyping by a Seed-Guided Topic Model},
	url = {https://doi.org/10.1145/3534678.3542675},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3534678.3542675}}

@inproceedings{10.1145/3447548.3467426,
	abstract = {Extreme Multi-label Learning (XML) involves assigning the subset of most relevant labels to a data point from millions of label choices. A hitherto unaddressed challenge in XML is that of predicting unseen labels with no training points. These form a significant fraction of total labels and contain fresh and personalized information desired by end users. Most existing extreme classifiers are not equipped for zero-shot label prediction and hence fail to leverage unseen labels. As a remedy, this paper proposes a novel approach called ZestXML for the task of Generalized Zero-shot XML (GZXML) where relevant labels have to be chosen from all available seen and unseen labels. ZestXML learns to project a data point's features close to the features of its relevant labels through a highly sparsified linear transform. This L0-constrained linear map between the two high-dimensional feature vectors is tractably recovered through a novel optimizer based on Hard Thresholding. By effectively leveraging the sparsities in features, labels and the learnt model, ZestXML achieves higher accuracy and smaller model size than existing XML approaches while also promoting efficient training &amp; prediction, real-time label update as well as explainable prediction.Experiments on large-scale GZXML datasets demonstrated that ZestXML can be up to 14% and 10% more accurate than state-of-the-art extreme classifiers and leading BERT-based dense retrievers respectively, while having 10x smaller model size. ZestXML trains on largest dataset with 31M labels in just 30 hours on a single core of a commodity desktop. When added to an large ensemble of existing models in Bing Sponsored Search Advertising, ZestXML significantly improved click yield of IR based system by 17% and unseen query coverage by 3.4% respectively. ZestXML's source code and benchmark datasets for GZXML will be publically released for research purposes here.},
	address = {New York, NY, USA},
	author = {Gupta, Nilesh and Bohra, Sakina and Prabhu, Yashoteja and Purohit, Saurabh and Varma, Manik},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3447548.3467426},
	isbn = {9781450383325},
	keywords = {label metadata, sponsored search advertising, extreme multi-label classification, zero-shot learning},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {527--535},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {Generalized Zero-Shot Extreme Multi-Label Learning},
	url = {https://doi.org/10.1145/3447548.3467426},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467426}}

@inproceedings{10.1145/3534678.3539310,
	abstract = {While Variational Graph Auto-Encoder (VGAE) has presented promising ability to learn representations for documents, most existing VGAE methods do not model a latent topic structure and therefore lack semantic interpretability. Exploring hidden topics within documents and discovering key words associated with each topic allow us to develop a semantic interpretation of the corpus. Moreover, documents are usually associated with authors. For example, news reports have journalists specializing in writing certain type of events, academic papers have authors with expertise in certain research topics, etc. Modeling authorship information could benefit topic modeling, since documents by the same authors tend to reveal similar semantics. This observation also holds for documents published on the same venues. However, most topic models ignore the auxiliary authorship and publication venues. Given above two challenges, we propose a Variational Graph Author Topic Model for documents to integrate both semantic interpretability and authorship and venue modeling into a unified VGAE framework. For authorship and venue modeling, we construct a hierarchical multi-layered document graph with both intra- and cross-layer topic propagation. For semantic interpretability, three word relations (contextual, syntactic, semantic) are modeled and constitute three word sub-layers in the document graph. We further propose three alternatives for variational divergence. Experiments verify the effectiveness of our model on supervised and unsupervised tasks.},
	address = {New York, NY, USA},
	author = {Zhang, Delvin Ce and Lauw, Hady W.},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3534678.3539310},
	isbn = {9781450393850},
	keywords = {author topic modeling, graph neural networks, text mining, variational graph auto-encoder},
	location = {Washington DC, USA},
	numpages = {10},
	pages = {2429--2438},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Variational Graph Author Topic Modeling},
	url = {https://doi.org/10.1145/3534678.3539310},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3534678.3539310}}

@inproceedings{10.1145/3394486.3403242,
	abstract = {Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.},
	address = {New York, NY, USA},
	author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Zhang, Yu and Zhang, Chao and Han, Jiawei},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3394486.3403242},
	isbn = {9781450379984},
	keywords = {topic hierarchy, tree embedding, text embedding, topic mining},
	location = {Virtual Event, CA, USA},
	numpages = {10},
	pages = {1908--1917},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding},
	url = {https://doi.org/10.1145/3394486.3403242},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3403242}}

@inproceedings{10.1145/3394486.3403244,
	abstract = {Taxonomy is not only a fundamental form of knowledge representation, but also crucial to vast knowledge-rich applications, such as question answering and web search. Most existing taxonomy construction methods extract hypernym-hyponym entity pairs to organize a "universal" taxonomy. However, these generic taxonomies cannot satisfy user's specific interest in certain areas and relations. Moreover, the nature of instance taxonomy treats each node as a single word, which has low semantic coverage for people to fully understand. In this paper, we propose a method for seed-guided topical taxonomy construction, which takes a corpus and a seed taxonomy described by concept names as input, and constructs a more complete taxonomy based on user's interest, wherein each node is represented by a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill this goal. A relation transferring module learns and transfers the user's interested relation along multiple paths to expand the seed taxonomy structure in width and depth. A concept learning module enriches the semantics of each concept node by jointly embedding the taxonomy and text. Comprehensive experiments conducted on real-world datasets show that CoRel generates high-quality topical taxonomies and outperforms all the baselines significantly.},
	address = {New York, NY, USA},
	author = {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Zhang, Yunyi and Han, Jiawei},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3394486.3403244},
	isbn = {9781450379984},
	keywords = {semantic computing, relation extraction, taxonomy construction, topic discovery},
	location = {Virtual Event, CA, USA},
	numpages = {9},
	pages = {1928--1936},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring},
	url = {https://doi.org/10.1145/3394486.3403244},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3403244}}

@inproceedings{10.1145/3534678.3539107,
	abstract = {Online education, which educates students that cannot be present at school, has become an important supplement to traditional education. Without the direct supervision and instruction of teachers, online education is always concerned with potential distractions and misunderstandings. Learning Style Classification (LSC) is proposed to analyze the learning behavior patterns of online learning users, based on which personalized learning paths are generated to help them learn and maintain their interests.Existing LSC studies rely on expert-labored labeling, which is infeasible in large-scale applications, so we resort to unsupervised classification techniques. However, current unsupervised classification methods are not applicable due to two important challenges: C1) the unawareness of the LSC problem formulation and pedagogy domain knowledge; C2) the absence of any supervision signals. In this paper, we give a formal definition of the unsupervised LSC problem and summarize the domain knowledge into problem-solving heuristics (which addresses C1). A rule-based approach is first designed to provide a tentative solution in a principled manner (which addresses C2). On top of that, a novel Deep Unsupervised Classifier with domain Knowledge (DUCK) is proposed to convert the discovered conclusions and domain knowledge into learnable model components (which addresses both C1 and C2), which significantly improves the effectiveness, efficiency, and robustness. Extensive offline experiments on both public and industrial datasets demonstrate the superiority of our proposed methods. Moreover, the proposed methods are now deployed in the Huawei Education Center, and the ongoing A/B testing results verify the effectiveness of the methods.},
	address = {New York, NY, USA},
	author = {He, Zhicheng and Xia, Wei and Dong, Kai and Guo, Huifeng and Tang, Ruiming and Xia, Dingyin and Zhang, Rui},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3534678.3539107},
	isbn = {9781450393850},
	keywords = {user behavior analysis, unsupervised classification, educational data mining, deep clustering, learning style classification},
	location = {Washington DC, USA},
	numpages = {10},
	pages = {2997--3006},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Unsupervised Learning Style Classification for Learning Path Generation in Online Education Platforms},
	url = {https://doi.org/10.1145/3534678.3539107},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3534678.3539107}}

@inproceedings{10.1145/3447548.3467390,
	abstract = {What should a malicious user write next to fool a detection model? Identifying malicious users is critical to ensure the safety and integrity of internet platforms. Several deep learning based detection models have been created. However, malicious users can evade deep detection models by manipulating their behavior, rendering these models of little use. The vulnerability of such deep detection models against adversarial attacks is unknown. Here we create a novel adversarial attack model against deep user sequence embedding-based classification models, which use the sequence of user posts to generate user embeddings and detect malicious users. In the attack, the adversary generates a new post to fool the classifier. We propose a novel end-to-end Personalized Text Generation Attack model, called PETGEN, that simultaneously reduces the efficacy of the detection model and generates posts that have several key desirable properties. Specifically, PETGEN generates posts that are personalized to the user's writing style, have knowledge about a given target context, are aware of the user's historical posts on the target context, and encapsulate the user's recent topical interests. We conduct extensive experiments on two real-world datasets (Yelp and Wikipedia, both with ground-truth of malicious users) to show that PETGEN significantly reduces the performance of popular deep user sequence embedding-based classification models. PETGEN outperforms five attack baselines in terms of text quality and attack efficacy in both white-box and black-box classifier settings. Overall, this work paves the path towards the next generation of adversary-aware sequence classification models.},
	address = {New York, NY, USA},
	author = {He, Bing and Ahamad, Mustaque and Kumar, Srijan},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3447548.3467390},
	isbn = {9781450383325},
	keywords = {deep learning, attack, user classification, sequence classification, adversarial text generation},
	location = {Virtual Event, Singapore},
	numpages = {10},
	pages = {575--584},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-Based Classification Models},
	url = {https://doi.org/10.1145/3447548.3467390},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467390}}

@inproceedings{10.1145/3447548.3467302,
	abstract = {Treatment effect estimation from observational data is a critical research topic across many domains. The foremost challenge in treatment effect estimation is how to capture hidden confounders. Recently, the growing availability of networked observational data offers a new opportunity to deal with the issue of hidden confounders. Unlike networked data in traditional graph learning tasks, such as node classification and link detection, the networked data under the causal inference problem has its particularity, i.e., imbalanced network structure. In this paper, we propose a Graph Infomax Adversarial Learning (GIAL) model for treatment effect estimation, which makes full use of the network structure to capture more information by recognizing the imbalance in network structure. We evaluate the performance of our GIAL model on two benchmark datasets, and the results demonstrate superiority over the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Chu, Zhixuan and Rathbun, Stephen L. and Li, Sheng},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3447548.3467302},
	isbn = {9781450383325},
	keywords = {graph mining, social network analysis, causal inference},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {176--184},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data},
	url = {https://doi.org/10.1145/3447548.3467302},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467302}}

@inproceedings{10.1145/3394486.3403179,
	abstract = {The competitive relationship of Points of Interest (POIs) refers to the degree of competition between two POIs for business opportunities from third parties in an urban area. Existing studies for competitive analysis usually focus on mining competitive relationships of entities, such as companies or products, from textual data. However, there are few studies which have a focus on competitive analysis for POIs. Indeed, the growing availability of user behavior data about POIs, such as POI reviews and human mobility data, enables a new paradigm for understanding the competitive relationships among POIs. To this end, in this paper, we study how to predict the POI competitive relationship. Along this line, a very first challenge is how to integrate heterogeneous user behavior data with the spatial features of POIs. As a solution, we first build a heterogeneous POI information network (HPIN) from POI reviews and map search data. Then, we develop a graph neural network-based deep learning framework, named DeepR, for POI competitive relationship prediction based on HPIN. Specifically, DeepR contains two components: a spatial adaptive graph neural network (SA-GNN) and a POI pairwise knowledge extraction learning (PKE) model. The SA-GNN is a novel GNN architecture with incorporating POI's spatial information and location distribution by a specially designed spatial oriented aggregation layer and spatial-dependency attentive propagation mechanism. In addition, PKE is devised to distill the POI pairwise knowledge in HPIN being useful for relationship prediction into condensate vectors with relational graph convolution and cross attention. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of our method.},
	address = {New York, NY, USA},
	author = {Li, Shuangli and Zhou, Jingbo and Xu, Tong and Liu, Hao and Lu, Xinjiang and Xiong, Hui},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-23 17:41:11 +0200},
	date-modified = {2022-10-23 17:41:11 +0200},
	doi = {10.1145/3394486.3403179},
	isbn = {9781450379984},
	keywords = {graph neural networks, competitive analysis, point of interest, heterogeneous information network},
	location = {Virtual Event, CA, USA},
	numpages = {10},
	pages = {1265--1274},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {Competitive Analysis for Points of Interest},
	url = {https://doi.org/10.1145/3394486.3403179},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3403179}}

@inproceedings{10.1145/3477495.3531877,
	abstract = {We present a novel semantic context prior-based venue recommendation system that uses only the title and the abstract of a paper. Based on the intuition that the text in the title and abstract have both semantic and syntactic components, we demonstrate that a joint training of a semantic feature extractor and syntactic feature extractor collaboratively leverages meaningful information that helps to provide venues for papers. The proposed methodology that we call DeSCoVeR at first elicits these semantic and syntactic features using a Neural Topic Model and text classifier respectively. The model then executes a transfer learning optimization procedure to perform a contextual transfer between the feature distributions of the Neural Topic Model and the text classifier during the training phase. DeSCoVeR also mitigates the document-level label bias using a Causal back-door path criterion and a sentence-level keyword bias removal technique. Experiments on the DBLP dataset show that DeSCoVeR outperforms the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Rajanala, Sailaja and Pal, Arghya and Singh, Manish and Phan, Rapha\"{e}l C.-W. and Wong, KokSheik},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:58 +0200},
	date-modified = {2022-10-23 17:37:58 +0200},
	doi = {10.1145/3477495.3531877},
	isbn = {9781450387323},
	keywords = {document classification, joint learning, mutual transfer, causal debiasing, topic modeling},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {2456--2461},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {DeSCoVeR: Debiased Semantic Context Prior for Venue Recommendation},
	url = {https://doi.org/10.1145/3477495.3531877},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531877}}

@inproceedings{10.1145/3477495.3531812,
	abstract = {When searching the web for answers to health questions, people can make incorrect decisions that have a negative effect on their lives if the search results contain misinformation. To reduce health misinformation in search results, we need to be able to detect documents with correct answers and promote them over documents containing misinformation. Determining the correct answer has been a difficult hurdle to overcome for participants in the TREC Health Misinformation Track. In the 2021 track, automatic runs were not allowed to use the known answer to a topic's health question, and as a result, the top automatic run had a compatibility-difference score of 0.043 while the top manual run, which used the known answer, had a score of 0.259. The compatibility-difference measures the ability of methods to rank correct and credible documents before incorrect and non-credible documents. By using an existing set of health questions and their known answers, we show it is possible to learn which web hosts are trustworthy, from which we can predict the correct answers to the 2021 health questions with an accuracy of 76%. Using our predicted answers, we can promote documents that we predict contain this answer and achieve a compatibility-difference score of 0.129, which is a three-fold increase in performance over the best previous automatic method.},
	address = {New York, NY, USA},
	author = {Zhang, Dake and Vakili Tahami, Amir and Abualsaud, Mustafa and Smucker, Mark D.},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:58 +0200},
	date-modified = {2022-10-23 17:37:58 +0200},
	doi = {10.1145/3477495.3531812},
	isbn = {9781450387323},
	keywords = {stance detection, web search, health misinformation},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {2099--2104},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search},
	url = {https://doi.org/10.1145/3477495.3531812},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531812}}

@inproceedings{10.1145/3477495.3531817,
	abstract = {Dialogue topic segmentation is a challenging task in which dialogues are split into segments with pre-defined topics. Existing works on topic segmentation adopt a two-stage paradigm, including text segmentation and segment labeling. However, such methods tend to focus on the local context in segmentation, and the inter-segment dependency is not well captured. Besides, the ambiguity and labeling noise in dialogue segment bounds bring further challenges to existing models. In this work, we propose the Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to address the above issues. Specifically, we propose the parallel extraction network to perform segment extractions, optimizing the bipartite matching cost of segments to capture inter-segment dependency. Furthermore, we propose neighbor smoothing to handle the segment-bound noise and ambiguity. Experiments on a dialogue-based and a document-based topic segmentation dataset show that PEN-NS outperforms state-the-of-art models significantly.},
	address = {New York, NY, USA},
	author = {Xia, Jinxiong and Liu, Cao and Chen, Jiansong and Li, Yuchen and Yang, Fan and Cai, Xunliang and Wan, Guanglu and Wang, Houfeng},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:58 +0200},
	date-modified = {2022-10-23 17:37:58 +0200},
	doi = {10.1145/3477495.3531817},
	isbn = {9781450387323},
	keywords = {dialogue topic segmentation, neighbor smoothing., boundary ambiguity, parallel extraction, data noise},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {2126--2131},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing},
	url = {https://doi.org/10.1145/3477495.3531817},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531817}}

@inproceedings{10.1145/3477495.3531784,
	abstract = {Previous studies about event-level sentiment analysis (SA) usually model the event as a topic, a category or target terms, while the structured arguments (e.g., subject, object, time and location) that have potential effects on the sentiment are not well studied. In this paper, we redefine the task as structured event-level SA and propose an End-to-End Event-level Sentiment Analysis (E3SA) approach to solve this issue. Specifically, we explicitly extract and model the event structure information for enhancing event-level SA. Extensive experiments demonstrate the great advantages of our proposed approach over the state-of-the-art methods. Noting the lack of the dataset, we also release a large-scale real-world dataset with event arguments and sentiment labelling for promoting more researches.},
	address = {New York, NY, USA},
	author = {Zhang, Qi and Zhou, Jie and Chen, Qin and Bai, Qingchun and He, Liang},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:58 +0200},
	date-modified = {2022-10-23 17:37:58 +0200},
	doi = {10.1145/3477495.3531784},
	isbn = {9781450387323},
	keywords = {datasets, structured, event-level sentiment analysis},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {1944--1949},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {Enhancing Event-Level Sentiment Analysis with Structured Arguments},
	url = {https://doi.org/10.1145/3477495.3531784},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531784}}

@inproceedings{10.1145/3477495.3532084,
	abstract = {Community question answering (CQA) becomes increasingly prevalent in recent years, providing platforms for users with various backgrounds to obtain information and share knowledge. However, the redundancy and lengthiness issues of crowd-sourced answers limit the performance of answer selection, thus leading to difficulties in reading or even misunderstandings for community users. To solve these problems, we propose the dual graph question-answer attention networks (DGQAN) for answer selection task. Aims to fully understand the internal structure of the question and the corresponding answer, firstly, we construct a dual-CQA concept graph with graph convolution networks using the original question and answer text. Specifically, our CQA concept graph exploits the correlation information between question-answer pairs to construct two sub-graphs (QSubject-Answer and QBody-Answer), respectively. Further, a novel dual attention mechanism is incorporated to model both the internal and external semantic relations among questions and answers. More importantly, we conduct experiment to investigate the impact of each layer in the BERT model. The experimental results show that DGQAN model achieves state-of-the-art performance on three datasets (SemEval-2015, 2016, and 2017), outperforming all the baseline models.},
	address = {New York, NY, USA},
	author = {Yang, Haitian and Zhao, Xuan and Wang, Yan and Li, Min and Chen, Wei and Huang, Weiqing},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:58 +0200},
	date-modified = {2022-10-23 17:37:58 +0200},
	doi = {10.1145/3477495.3532084},
	isbn = {9781450387323},
	keywords = {dual graph attention, community question answering, answer selection},
	location = {Madrid, Spain},
	numpages = {10},
	pages = {1230--1239},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {DGQAN: Dual Graph Question-Answer Attention Networks for Answer Selection},
	url = {https://doi.org/10.1145/3477495.3532084},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3532084}}

@inproceedings{10.1145/3404835.3463080,
	abstract = {We propose VADEC, a multi-task framework that exploits the correlation between the categorical and dimensional models of emotion representation for better subjectivity analysis. Focusing primarily on the effective detection of emotions from tweets, we jointly train multi-label emotion classification and multi-dimensional emotion regression, thereby utilizing the inter-relatedness between the tasks. Co-training especially helps in improving the performance of the classification task as we outperform the strongest baselines with 3.4%, 11%, and 3.9% gains in Jaccard Accuracy, Macro-F1, and Micro-F1 scores respectively on the AIT dataset [17]. We also achieve state-of-the-art results with 11.3% gains averaged over six different metrics on the SenWave dataset [27]. For the regression task, VADEC, when trained with SenWave, achieves 7.6% and 16.5% gains in Pearson Correlation scores over the current state-of-the-art on the EMOBANK dataset [5] for the Valence (V) and Dominance (D) affect dimensions respectively. We conclude our work with a case study on COVID-19 tweets posted by Indians that further helps in establishing the efficacy of our proposed solution.},
	address = {New York, NY, USA},
	author = {Mukherjee, Rajdeep and Naik, Atharva and Poddar, Sriyash and Dasgupta, Soham and Ganguly, Niloy},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:32 +0200},
	date-modified = {2022-10-23 17:37:32 +0200},
	doi = {10.1145/3404835.3463080},
	isbn = {9781450380379},
	keywords = {multi-task learning, COVID, coarse-grained emotion analysis, twitter, fine-grained emotion analysis, valence-arousal-dominance},
	location = {Virtual Event, Canada},
	numpages = {5},
	pages = {2303--2307},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {Understanding the Role of Affect Dimensions in Detecting Emotions from Tweets: A Multi-Task Approach},
	url = {https://doi.org/10.1145/3404835.3463080},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3463080}}

@inproceedings{10.1145/3404835.3462975,
	abstract = {Understanding how knowledge is technically transferred across academic disciplines is very relevant for understanding and facilitating innovation. There are two challenges for this purpose, namely the semantic ambiguity and the asymmetric influence across disciplines. In this paper we investigate knowledge propagation and characterize semantic correlations for cross discipline paper recommendation. We adopt a generative model to represent a paper content as the probabilistic association with an existing hierarchically classified discipline to reduce the ambiguity of word semantics. The semantic correlation across disciplines is represented by an influence function, a correlation metric and a ranking mechanism. Then a user interest is represented as a probabilistic distribution over the target domain semantics and the correlated papers are recommended. Experimental results on real datasets show the effectiveness of our methods. We also discuss the intrinsic factors of results in an interpretable way. Compared with traditional word embedding based methods, our approach supports the evolution of domain semantics that accordingly lead to the update of semantic correlation. Another advantage of our approach is its flexibility and uniformity in supporting user interest specifications by either a list of papers or a query of key words, which is suited for practical scenarios.},
	address = {New York, NY, USA},
	author = {Xie, Yi and Sun, Yuqing and Bertino, Elisa},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:32 +0200},
	date-modified = {2022-10-23 17:37:32 +0200},
	doi = {10.1145/3404835.3462975},
	isbn = {9781450380379},
	keywords = {academic paper, recommendation, semantic correlation},
	location = {Virtual Event, Canada},
	numpages = {10},
	pages = {706--715},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {Learning Domain Semantics and Cross-Domain Correlations for Paper Recommendation},
	url = {https://doi.org/10.1145/3404835.3462975},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3462975}}

@inproceedings{10.1145/3404835.3462938,
	abstract = {Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines.},
	address = {New York, NY, USA},
	author = {Zogan, Hamad and Razzak, Imran and Jameel, Shoaib and Xu, Guandong},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:32 +0200},
	date-modified = {2022-10-23 17:37:32 +0200},
	doi = {10.1145/3404835.3462938},
	isbn = {9781450380379},
	keywords = {machine learning, deep learning, text summarization, social network, depression detection},
	location = {Virtual Event, Canada},
	numpages = {10},
	pages = {133--142},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {DepressionNet: Learning Multi-Modalities with User Post Summarization for Depression Detection on Social Media},
	url = {https://doi.org/10.1145/3404835.3462938},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3462938}}

@inproceedings{10.1145/3404835.3463100,
	abstract = {We propose AutoName, an unsupervised framework that extracts a name for a set of query entities from a large-scale text corpus. Entity-set naming is useful in many tasks related to natural language processing and information retrieval such as session-based and conversational information seeking. Previous studies mainly extract set names from knowledge bases which provide highly reliable entity relations, but suffer from limited coverage of entities and set names that represent broad semantic classes. To address these problems, AutoName generates hypernym-anchored candidate phrases via probing a pre-trained language model and the entities' context in documents. Phrases are then clustered to identify ones that describe common concepts among query entities. Finally, AutoName ranks refined phrases based on the co-occurrences of their words with query entities and the conceptual integrity of their respective clusters. We built a new benchmark dataset for this task, consisting of 130 entity sets with name labels. Experimental results show that AutoName generates coherent and meaningful set names and significantly outperforms all baselines.},
	address = {New York, NY, USA},
	author = {Huang, Zhiqi and Rahimi, Razieh and Yu, Puxuan and Shang, Jingbo and Allan, James},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:32 +0200},
	date-modified = {2022-10-23 17:37:32 +0200},
	doi = {10.1145/3404835.3463100},
	isbn = {9781450380379},
	keywords = {conceptual clustering, entity set naming, language model probing},
	location = {Virtual Event, Canada},
	numpages = {5},
	pages = {2101--2105},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {AutoName: A Corpus-Based Set Naming Framework},
	url = {https://doi.org/10.1145/3404835.3463100},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3463100}}

@inproceedings{10.1145/3404835.3462798,
	abstract = {We present the IR Anthology, a corpus of information retrieval publications accessible via a metadata browser and a full-text search engine. Following the example of the well-known ACL Anthology, the IR Anthology serves as a hub for researchers interested in information retrieval. Our search engine ChatNoir indexes the publications' full texts, enabling a focused search and linking users to the respective publisher's site for personal access. Listing more than 40,000 publications at the time of writing, the IR Anthology can be freely accessed at https://IR.webis.de.},
	address = {New York, NY, USA},
	author = {Potthast, Martin and G\"{u}nther, Sebastian and Bevendorff, Janek and Bittner, Jan Philipp and Bondarenko, Alexander and Fr\"{o}be, Maik and Kahmann, Christian and Niekler, Andreas and V\"{o}lske, Michael and Stein, Benno and Hagen, Matthias},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:32 +0200},
	date-modified = {2022-10-23 17:37:32 +0200},
	doi = {10.1145/3404835.3462798},
	isbn = {9781450380379},
	keywords = {bibliography, scientific literature analysis, scholarly search},
	location = {Virtual Event, Canada},
	numpages = {6},
	pages = {2550--2555},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {The Information Retrieval Anthology},
	url = {https://doi.org/10.1145/3404835.3462798},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3462798}}

@inproceedings{10.1145/3397271.3401168,
	abstract = {Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1)the presence of metadataâ€¯: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2)label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into the same semantic space to encode heterogeneous signals. Then, based on the same generative process, we synthesize training samples to address the bottleneck of label scarcity. We conduct a thorough evaluation on a wide range of datasets. Experimental results prove the effectiveness of MetaCat over many competitive baselines.},
	address = {New York, NY, USA},
	author = {Zhang, Yu and Meng, Yu and Huang, Jiaxin and Xu, Frank F. and Wang, Xuan and Han, Jiawei},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401168},
	isbn = {9781450380164},
	keywords = {weak supervision, text classification, metadata},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {1231--1240},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Minimally Supervised Categorization of Text with Metadata},
	url = {https://doi.org/10.1145/3397271.3401168},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401168}}

@inproceedings{10.1145/3397271.3401179,
	abstract = {Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models.},
	address = {New York, NY, USA},
	author = {Zhuang, Honglei and Guo, Fang and Zhang, Chao and Liu, Liyuan and Han, Jiawei},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401179},
	isbn = {9781450380164},
	keywords = {aspect-based sentiment analysis, autoencoder, weakly-supervised},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {1241--1250},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Joint Aspect-Sentiment Analysis with Minimal User Guidance},
	url = {https://doi.org/10.1145/3397271.3401179},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401179}}

@inproceedings{10.1145/3397271.3401185,
	abstract = {Topic modelling is a popular unsupervised method for identifying the underlying themes in document collections that has many applications in information retrieval. A topic is usually represented by a list of terms ranked by their probability but, since these can be difficult to interpret, various approaches have been developed to assign descriptive labels to topics. Previous work on the automatic assignment of labels to topics has relied on a two-stage approach: (1) candidate labels are retrieved from a large pool (e.g. Wikipedia article titles); and then (2) re-ranked based on their semantic similarity to the topic terms. However, these extractive approaches can only assign candidate labels from a restricted set that may not include any suitable ones. This paper proposes using a sequence-to-sequence neural-based approach to generate labels that does not suffer from this limitation. The model is trained over a new large synthetic dataset created using distant supervision. The method is evaluated by comparing the labels it generates to ones rated by humans.},
	address = {New York, NY, USA},
	author = {Alokaili, Areej and Aletras, Nikolaos and Stevenson, Mark},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401185},
	isbn = {9781450380164},
	keywords = {topic representation, neural network, topic modeling},
	location = {Virtual Event, China},
	numpages = {4},
	pages = {1965--1968},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Automatic Generation of Topic Labels},
	url = {https://doi.org/10.1145/3397271.3401185},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401185}}

@inproceedings{10.1145/3397271.3401269,
	abstract = {Manually extracting relevant aspects and opinions from large volumes of user-generated text is a time-consuming process. Summaries, on the other hand, help readers with limited time budgets to quickly consume the key ideas from the data. State-of-the-art approaches for multi-document summarization, however, do not consider user preferences while generating summaries. In this work, we argue the need and propose a solution for generating personalized aspect-based opinion summaries from large collections of online tourist reviews. We let our readers decide and control several attributes of the summary such as the length and specific aspects of interest among others. Specifically, we take an unsupervised approach to extract coherent aspects from tourist reviews posted onTripAdvisor. We then propose an Integer Linear Programming (ILP) based extractive technique to select an informative subset of opinions around the identified aspects while respecting the user-specified values for various control parameters. Finally, we evaluate and compare our summaries using crowdsourcing and ROUGE-based metrics and obtain competitive results.},
	address = {New York, NY, USA},
	author = {Mukherjee, Rajdeep and Peruri, Hari Chandana and Vishnu, Uppada and Goyal, Pawan and Bhattacharya, Sourangshu and Ganguly, Niloy},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401269},
	isbn = {9781450380164},
	keywords = {personalization, controllable summarization, aspect-based opinion mining, tourism, unsupervised extractive opinion summarization},
	location = {Virtual Event, China},
	numpages = {4},
	pages = {1825--1828},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Read What You Need: Controllable Aspect-Based Opinion Summarization of Tourist Reviews},
	url = {https://doi.org/10.1145/3397271.3401269},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401269}}

@inproceedings{10.1145/3397271.3401128,
	abstract = {Many sellers on e-commerce platforms offer buyers product bundles, which package together two or more different items. The identification of such bundles is a necessary step to support a variety of related services, from recommendation to dynamic pricing. In this work, we present a comprehensive study of bundle identification on a large e-commerce website. Our analysis of bundle compared to non-bundle listed items reveals several key differentiating characteristics, spanning the listing's title, image, and attributes. Following, we experiment with a multi-modal classifier, which takes advantage of these characteristics as features. Our analysis also shows that a bundle indicator input by sellers tends to be highly noisy and carries only a weak signal. The bundle identification task therefore faces the challenge of having a small set of manually-labeled clean examples and a larger set of noisy-labeled examples, in conjunction with class imbalance due to the relative scarcity of bundles.Our experiments with basic supervised classifiers, using the manually-labeled and/or the noisy-labeled data for training, demonstrates only moderate performance. We therefore turn to a semisupervised approach and propose GREED, a self-training ensemblebased algorithm with a greedy model selection. Our evaluation over two different meta-categories shows a superior performance of semi-supervised approaches for the bundle identification task, with GREED outperforming several semi-supervised alternatives. The combination of textual, image, and some metadata features is shown to yield the best performance, reaching an AUC of 0.89 and 0.92 for the two meta-categories, respectively},
	address = {New York, NY, USA},
	author = {Tzaban, Hen and Guy, Ido and Greenstein-Messica, Asnat and Dagan, Arnon and Rokach, Lior and Shapira, Bracha},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401128},
	isbn = {9781450380164},
	keywords = {product bundling, semi-supervised learning, self-training, electronic commerce, ensemble learning},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {791--800},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Product Bundle Identification Using Semi-Supervised Learning},
	url = {https://doi.org/10.1145/3397271.3401128},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401128}}

@inproceedings{10.1145/3397271.3401150,
	abstract = {In this work, we aim to investigate the practical task of flexible fashion search with attribute manipulation, where users can retrieve the target fashion items by replacing the unwanted attributes of an available query image with the desired ones (e.g., changing the collar attribute from v-neck to round). Although several pioneer efforts have been dedicated to fulfilling the task, they mainly ignore the potential of generative models in enhancing the visual understanding of target fashion items. To this end, we propose an end-to-end generative attribute manipulation scheme, which consists of a generator and a discriminator. The generator works on producing the prototype image that meets the user's requirement of attribute manipulation over the query image with the regularization of visual-semantic consistency and pixel-wise consistency. Besides, the discriminator aims to jointly fulfill the semantic learning towards correct attribute manipulation and adversarial metric learning for fashion search. Pertaining to the adversarial metric learning, we provide two general paradigms: the pair-based scheme and the triplet-based scheme, where the fake generated prototype images that closely resemble the ground truth images of target items are incorporated as hard negative samples to boost the model performance. Extensive experiments on two real-world datasets verify the effectiveness of our scheme.},
	address = {New York, NY, USA},
	author = {Yang, Xin and Song, Xuemeng and Han, Xianjing and Wen, Haokun and Nie, Jie and Nie, Liqiang},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401150},
	isbn = {9781450380164},
	keywords = {attribute manipulation, fashion search, deep metric learning, generative adversarial networks},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {941--950},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Generative Attribute Manipulation Scheme for Flexible Fashion Search},
	url = {https://doi.org/10.1145/3397271.3401150},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401150}}

@inproceedings{10.1145/3397271.3401047,
	abstract = {Recent years have witnessed a growing trend of fashion compatibility modeling, which scores the matching degree of the given outfit and then provides people with some dressing advice. Existing methods have primarily solved this problem by analyzing the discrete interaction among multiple complementary items. However, the fashion items would present certain occlusion and deformation when they are worn on the body. Therefore, the discrete item interaction cannot capture the fashion compatibility in a combined manner due to the neglect of a crucial factor: the overall try-on appearance. In light of this, we propose a multi-modal try-on-guided compatibility modeling scheme to jointly characterize the discrete interaction and try-on appearance of the outfit. In particular, we first propose a multi-modal try-on template generator to automatically generate a try-on template from the visual and textual information of the outfit, depicting the overall look of its composing fashion items. Then, we introduce a new compatibility modeling scheme which integrates the outfit try-on appearance into the traditional discrete item interaction modeling. To fulfill the proposal, we construct a large-scale real-world dataset from SSENSE, named FOTOS, consisting of 11,000 well-matched outfits and their corresponding realistic try-on images. Extensive experiments have demonstrated its superiority to state-of-the-arts.},
	address = {New York, NY, USA},
	author = {Dong, Xue and Wu, Jianlong and Song, Xuemeng and Dai, Hongjun and Nie, Liqiang},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-23 17:37:04 +0200},
	date-modified = {2022-10-23 17:37:04 +0200},
	doi = {10.1145/3397271.3401047},
	isbn = {9781450380164},
	keywords = {fashion analysis, compatibility modeling, try-on-guided scheme},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {771--780},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Fashion Compatibility Modeling through a Multi-Modal Try-on-Guided Scheme},
	url = {https://doi.org/10.1145/3397271.3401047},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401047}}

@article{2022_Chauhan,
	abstract = {We are not able to deal with a mammoth text corpus without summarizing them into a relatively small subset. A computational tool is extremely needed to understand such a gigantic pool of text. Probabilistic Topic Modeling discovers and explains the enormous collection of documents by reducing them in a topical subspace. In this work, we study the background and advancement of topic modeling techniques. We first introduce the preliminaries of the topic modeling techniques and review its extensions and variations, such as topic modeling over various domains, hierarchical topic modeling, word embedded topic models, and topic models in multilingual perspectives. Besides, the research work for topic modeling in a distributed environment, topic visualization approaches also have been explored. We also covered the implementation and evaluation techniques for topic models in brief. Comparison matrices have been shown over the experimental results of the various categories of topic modeling. Diverse technical challenges and future directions have been discussed.},
	address = {New York, NY, USA},
	articleno = {145},
	author = {Chauhan, Uttam and Shah, Apurva},
	date-added = {2022-10-22 10:47:42 +0200},
	date-modified = {2022-10-22 10:48:03 +0200},
	doi = {10.1145/3462478},
	issn = {0360-0300},
	issue_date = {September 2022},
	journal = {ACM Comput. Surv.},
	keywords = {probabilistic model, statistical inference, Topic modeling, gibbs sampling, latent dirichlet allocation},
	month = {sep},
	number = {7},
	numpages = {35},
	publisher = {Association for Computing Machinery},
	title = {Topic Modeling Using Latent Dirichlet Allocation: A Survey},
	url = {https://doi.org/10.1145/3462478},
	volume = {54},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3462478}}

@article{2021_Churchill,
	abstract = {Topic models have been applied to everything from books to newspapers to social media posts in an effort to identify the most prevalent themes of a text corpus. We provide an in-depth analysis of unsupervised topic models from their inception to today. We trace the origins of different types of contemporary topic models, beginning in the 1990s, and we compare their proposed algorithms, as well as their different evaluation approaches. Throughout, we also describe settings in which topic models have worked well and areas where new research is needed, setting the stage for the next generation of topic models.},
	address = {New York, NY, USA},
	author = {Churchill, Rob and Singh, Lisa},
	date-added = {2022-10-22 10:46:24 +0200},
	date-modified = {2022-10-22 10:46:39 +0200},
	doi = {10.1145/3507900},
	issn = {0360-0300},
	journal = {ACM Comput. Surv.},
	keywords = {temporal topic modeling, topic modeling, social media, online topic modeling},
	month = {dec},
	note = {Just Accepted},
	publisher = {Association for Computing Machinery},
	title = {The Evolution of Topic Modeling},
	url = {https://doi.org/10.1145/3507900},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3507900}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>ACL</string>
		<key>keys</key>
		<string>2022_Broscheit,Jin_2022,Gabriel_2022,Ribeiro_2022,https://doi.org/10.48550/arxiv.2202.13469,Malkin_2022,mou-etal-2021-align,liu-etal-2021-element,https://doi.org/10.48550/arxiv.2105.14189,https://doi.org/10.48550/arxiv.2106.04408,https://doi.org/10.48550/arxiv.2106.01071,duan-etal-2021-enslm,bilal-etal-2021-evaluation,veselova-vorontsov-2020-topic,peinelt-etal-2020-tbert,wang-etal-2020-neural-topic,viegas-etal-2020-cluhtm,wu-etal-2020-neural,https://doi.org/10.48550/arxiv.2006.00998,vafa-etal-2020-text,keith-etal-2020-text,jia-etal-2020-mitigating,zhu-etal-2020-batch,zang-etal-2020-word</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>CIKM</string>
		<key>keys</key>
		<string>10.1145/3459637.3482398,10.1145/3511808.3557410,10.1145/3340531.3412050,10.1145/3511808.3557561,10.1145/3459637.3482253,10.1145/3340531.3412878,10.1145/3340531.3411932,10.1145/3511808.3557329,10.1145/3459637.3482075,10.1145/3340531.3411933,10.1145/3340531.3411906,10.1145/3340531.3411890,10.1145/3459637.3482450</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>COLING</string>
		<key>keys</key>
		<string>keh-etal-2022-pineapple,huynh-etal-2022-vinli,yin-etal-2022-improving,amplayo-etal-2022-attribute,wang-etal-2022-imci,xie-etal-2022-gretel,liu-etal-2022-bert,antypas-etal-2022-twitter,austin-etal-2022-community,nouri-etal-2020-mining,srinivasa-desikan-etal-2020-comp,pham-le-2020-auto,du-etal-2020-pointing,zhao-etal-2020-improving,bai-etal-2020-pre,nevezhin-etal-2020-topic,li-etal-2020-neural,zhang-etal-2020-topic,shang-etal-2020-speaker,takatsu-etal-2020-sentiment,an-etal-2020-multimodal</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>EACL</string>
		<key>keys</key>
		<string>sawhney-etal-2021-phase,shen-etal-2021-source,zhou-etal-2021-challenges,saravanakumar-etal-2021-event,zehe-etal-2021-detecting,paul-etal-2021-multi,zhao-etal-2021-adversarial,popa-rebedea-2021-bart</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>ECIR</string>
		<key>keys</key>
		<string>Zosa_2022,Valero_2022,_ahinu__2021,Palencia_Olivar_2022,Fard_2020,Brochier_2020,Jatowt_2021,Kovalchuk_2020,Kuzi_2021,Lugo_2021,Montazeralghaem_2020,Abazari_Kia_2021,Batra_2020,Meng_2021,Chelliah_2020,Esuli_2022,Ishigaki_2020,Wang_2022,Bondarenko_2021</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>ECML PKDD</string>
		<key>keys</key>
		<string>Ferner_2020,Madrid_2020,Zhang_2021,Bai_2021,10.1007/978-3-030-46133-1_33,Farruque_2020,Harada_2021,Khandelwal_2021,Audebert_2020,Yang_2021</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>EMNLP</string>
		<key>keys</key>
		<string>jin-etal-2021-neural,ye-etal-2021-beyond,zhang-etal-2021-howyoutagtweets,wang-etal-2021-phrase,kang-etal-2021-leveraging,liu-etal-2021-leveraging,situ-etal-2021-lifelong,saxon-etal-2021-modeling,manchanda-karypis-2021-evaluating,yu-etal-2021-exophoric,lin-etal-2021-csds,milbauer-etal-2021-aligning,li-etal-2021-detecting,dodge-etal-2021-documenting,zhou-jurgens-2020-condolence,sawhney-etal-2020-time,pei-jurgens-2020-quantifying,field-tsvetkov-2020-unsupervised,wang-etal-2020-continuity,roy-goldwasser-2020-weakly,zhao-chang-2020-logan,bommasani-cardie-2020-intrinsic,hoyle-etal-2020-improving,meng-etal-2020-text,hu-etal-2020-neural,ousidhoum-etal-2020-comparative,liu-etal-2020-cross-lingual-spoken,gao-gormley-2020-training,tian-etal-2020-learning,spell-etal-2020-embedding</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>KDD</string>
		<key>keys</key>
		<string>10.1145/3447548.3467410,10.1145/3534678.3542675,10.1145/3447548.3467426,10.1145/3534678.3539310,10.1145/3394486.3403242,10.1145/3394486.3403244,10.1145/3534678.3539107,10.1145/3447548.3467390,10.1145/3447548.3467302,10.1145/3394486.3403179</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>NAACL</string>
		<key>keys</key>
		<string>zhu-etal-2022-disentangled,kulkarni-etal-2022-ctm,li-etal-2022-corwa,bhattacharjee-etal-2022-users,spangher-etal-2022-newsedits,zhang-etal-2022-seed,sircar-etal-2022-distantly,liu-etal-2022-hiure,voigt-etal-2022-survey,zhang-etal-2022-kcd,subramanian-etal-2021-spanpredict,ji-etal-2021-discrete,schiller-etal-2021-aspect,cui-hu-2021-sliding,he-etal-2021-model,zeng-nie-2021-simple,zhan-etal-2021-augmenting,khanehzar-etal-2021-framing,iida-etal-2021-tabbie,shen-etal-2021-taxoclass,sun-etal-2021-tita,mueller-dredze-2021-fine,xie-etal-2021-inductive,gupta-etal-2021-multi,pergola-etal-2021-disentangled,doogan-buntine-2021-topic</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>SIGIR</string>
		<key>keys</key>
		<string>10.1145/3397271.3401168,10.1145/3397271.3401179,10.1145/3397271.3401185,10.1145/3397271.3401269,10.1145/3397271.3401128,10.1145/3397271.3401150,10.1145/3397271.3401047,10.1145/3404835.3463080,10.1145/3404835.3462975,10.1145/3404835.3462938,10.1145/3404835.3463100,10.1145/3404835.3462798,10.1145/3477495.3531877,10.1145/3477495.3531812,10.1145/3477495.3531817,10.1145/3477495.3531784,10.1145/3477495.3532084</string>
	</dict>
</array>
</plist>
}}
