%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-10-24 13:27:31 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{ALI2020113790,
	abstract = {The huge amount of research papers on the web makes finding a relevant manuscript a difficult task. In recent years many models were introduced to support researchers by providing personalized citation recommendations. Moreover, deep learning methods have been employed in this domain to improve the quality of the final recommendations. However, a thorough study that classifies citation recommendation models and examines their (a) strengths and weaknesses, (b) evaluation metrics used, (c) popular datasets, and challenges faced is missing. Therefore, with this survey, we present a new classification approach for deep learning models that provide citation recommendation. Our approach uses the following six criteria: data factors, data representation methods, methodologies, types of recommendations used, problems addressed, and personalization. Additionally, we present a comparative analysis of those models that use the same set of evaluation metrics and datasets. Moreover, we examine hot upcoming issues and solutions in light of explored literature. Also, the survey discusses and analyzes the evaluation metrics and datasets adopted by the explored models. Finally, we conclude our survey with trends and future directions to further assist research on that domain.},
	author = {Zafar Ali and Pavlos Kefalas and Khan Muhammad and Bahadar Ali and Muhammad Imran},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113790},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, Citation recommendation, Neural networks, Paper recommendation, Machine learning, Deep learning},
	pages = {113790},
	title = {Deep learning in citation recommendation models survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420306126},
	volume = {162},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420306126},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113790}}

@article{ULIAN2021115341,
	abstract = {News recommendations distinguishes from general content recommendations as it takes in consideration news freshness, sparsity, monotony and time. Recent works approach these features using hybrid Collaborative-Content-based Filtering methods, adapting clustering techniques to handle sparsity and monotony without considering the effects that different clustering methods may have over recommendation results. Such studies often evaluate the results of varying different parameters individually, ignoring possible interaction effects between them. They also base their results on metrics such as accuracy and recall that are sensitive to bias. To investigate the importance of clustering method selection to News Recommender System results we evaluated the effects of different traditional techniques in recommending news articles. We implemented an algorithm that used a hybrid Collaborative-Content-based Filtering method to incorporate user behavior, user interest, article popularity and time effect. The system uses an article selection method that built the recommendation set based on content features. With this algorithm, we examined the existence of interaction effects between the input parameters. We used a Gaussian regression process to explore the response surface while sequentially optimizing parameters. To avoid being misled by underlying biases we used Informedness, an accuracy metric that captures both positive and negative information from prediction results. Our results demonstrated that different clustering methods had a significant influence on the recommendation results. It was also found that a traditional hierarchical method outperformed optimization methods with important performance improvement. In addition, we demonstrated that parameters may interact with each other and that analyzing them separately may mislead interpretation.},
	author = {Douglas Zanatta Ulian and Jo{\~a}o Luiz Becker and Carla Bonato Marcolin and Eusebio Scornavacca},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115341},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, News recommender systems, Clustering, Collaborative filtering, Content filtering, Data mining},
	pages = {115341},
	title = {Exploring the effects of different Clustering Methods on a News Recommender System},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421007697},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421007697},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115341}}

@article{YOO2020112965,
	abstract = {In order to make computers understand human languages and to reason, human knowledge needs to be represented and stored in a form that can be processed by computers. Knowledge graphs have been developed for use as a form of the knowledge base for words and general relationships among words. However, they have two limitations. One is that the knowledge graph is limited in size and scope for most of the human languages. Another is that they are not able to deal with neologisms that form a part of the human common sense. Addressing these problems, we have developed and validated PolarisX which can automatically expand a knowledge graph, by crawling and analyzing the news sites and social media in real-time. We utilize and fine-tune the pre-trained multilingual BERT model for the construction of knowledge graphs without language dependencies. We extract new relationships using the BERT-based relation extraction model and integrate them into the knowledge graph. We verify the novelty and accuracy of PolarisX. It deals with neologisms and does not have language dependencies.},
	author = {SoYeop Yoo and OkRan Jeong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112965},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Auto expansion, Knowledge graph, Neologisms, Semantic analysis, Multilingual BERT model},
	pages = {112965},
	title = {Automating the expansion of a knowledge graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419306839},
	volume = {141},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419306839},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112965}}

@article{DU2021114791,
	abstract = {Predicting hospital readmission with effective machine learning techniques has attracted a great attention in recent years. The fundamental challenge of this task stems from characteristics of the data extracted from electronic health records (EHR), which are imbalanced class distributions. This challenge further leads to the failure of most existing models that only provide a partial understanding for the learning problem and result in a biased and inaccurate prediction. To address this challenge, we propose a new graph-based class-imbalance learning method by fully making use of the data from different classes. First, we conduct graph construction for learning the pattern discrimination from between-class and within-class data samples. Then we design an optimization framework to incorporate the constructed graphs to obtain a class-imbalance aware graph embedding and further alleviate performance degeneration. Finally, we design a neural network model as the classifier to conduct imbalanced classification, i.e., hospital readmission prediction. Comprehensive experiments on six real-world readmission datasets show that the proposed method outperforms state-of-the-art approaches in readmission prediction task.},
	author = {Guodong Du and Jia Zhang and Fenglong Ma and Min Zhao and Yaojin Lin and Shaozi Li},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114791},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hospital readmission, Graph embedding, Class-imbalance learning, Neural network model},
	pages = {114791},
	title = {Towards graph-based class-imbalance learning for hospital readmission},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002323},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002323},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114791}}

@article{REUBEN2022117027,
	abstract = {Retrieving information from an online search engine is the first and most important step in many data mining tasks, such as fake news detection. Most of the search engines currently available on the web, including all social media platforms, are black-boxes (i.e., opaque) supporting short keyword queries. In these settings, it is challenging to retrieve all posts and comments discussing a particular news item automatically and on a large scale. In this paper, we propose a method for generating short keyword queries given a prototype document. The proposed iterative query selection (IQS) algorithm interacts with the opaque search engine to iteratively improve the query, by maximizing the number of relevant results retrieved. Our evaluation of IQS was performed on the Twitter TREC Microblog 2012 and TREC-COVID 2019 datasets and demonstrated the algorithm's superior performance compared to state-of-the-art. In addition, we implemented IQS algorithm to automatically collect a large-scale dataset for fake news detection task of about 70K true and fake news items. The dataset, which we have made publicly available to the research community, includes over 22M accounts and 61M tweets. We demonstrate the usefulness of the dataset for fake news detection task achieving state-of-the-art performance.},
	author = {Maor Reuben and Aviad Elyashar and Rami Puzis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117027},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Query selection, Opaque search engine, Pseudo relevance feedback, Fake news},
	pages = {117027},
	title = {Iterative query selection for opaque search engines with pseudo relevance feedback},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422004432},
	volume = {201},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422004432},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117027}}

@article{BHOPALE2020113441,
	abstract = {This work explores the integrated power of swarm intelligence and advances in data mining techniques to solve the information retrieval (IR) problem of rapidly growing digital content on the World Wide Web. We propose a swarm optimized cluster based framework with frequent pattern mining techniques to retrieve user-specific knowledge from extensive document collections. In the pre-processing phase, we split the task into two sub-tasks. The first is to decompose the document collection into groups using a bio-inspired K-Flock clustering algorithm, while the second extracts frequent patterns from each cluster using a memory-efficient Recursive Elimination (RElim) algorithm. In the next phase, we implement a cosine similarity based probabilistic model to retrieve query-specific documents from clusters based on the matching scores between the closed frequent patterns of queries and clusters. The performance of a system is evaluated by conducting several experiments which are carried out on five well-known, diverse and variable size datasets viz- TREC 2014-15 CDS (Clinical Decision Support) datasets containing 733,138 records, OHSUMED dataset with 348,566 records from Medline database, NPL dataset with 11,429 records, LISA document collection of 6004 records, CACM (Collection of ACM) dataset of 3204 records. The results show that the proposed IR framework significantly outperforms the traditional sequential IR approach and other state-of-the-art IR approaches, both in terms of the quality of the returned documents and the time of execution.},
	author = {Amol P. Bhopale and Ashish Tiwari},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113441},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information retrieval, Swarm intelligence, Big data clustering, Frequent pattern mining, Unsupervised learning},
	pages = {113441},
	title = {Swarm optimized cluster based framework for information retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420302657},
	volume = {154},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420302657},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113441}}

@article{LIU2021115752,
	abstract = {In various recommender systems, ratings and reviews are the main information to show user preferences. However, recommendation models that only use ratings, such as collaborative filtering, are vulnerable to data sparsity. And models only using review information will also suffer from the sparsity of reviews. On one hand, most ratings and reviews are interrelated and complementary, reviews may explain why a user gives a high or low rating to an item. On the other hand, ratings and reviews are numerical and textual information, respectively, and they reflect the preference of the user from a coarse-grained level and a fine-grained level A user may comment positively about some aspects of an item, even he gives a very low score to this item. There are specific information among each of them because of their heterogeneity. Therefore, it is possible to learn more accurate representation of users and items by effectively integrating ratings and text reviews from different views, that is, shared-view and specific-view. In this paper, we propose a Shared-view and Specific-view Information extraction model for Recommendation (SSIR), which integrates the information from reviews and interaction matrix to predict ratings Our model has two key components, including shared-view information extraction and specific-view exploitation. From the perspective of shared-view, SSIR jointly minimizes the loss of confusion adversarial and rating prediction loss to extract the shared information from reviews and user--item interaction matrix. For the specific-view part, SSIR applies orthogonal constraints on shared-view and specific-view modules to extract the discriminative features from reviews and interaction data. We fuse the features extracted from these two views to predict the final ratings. In addition, we use auxiliary reviews to deal with the sparsity problem of reviews. Experimental results on eight datasets show the effectiveness and robustness of our method, which could adapt to the recommendation scenarios with fewer reviews and ratings.},
	author = {Huiting Liu and Jindou Zhao and Peipei Li and Peng Zhao and Xindong Wu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115752},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender system, Dual-view learning, Review analysis},
	pages = {115752},
	title = {Shared-view and specific-view information extraction for recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421011283},
	volume = {186},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421011283},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115752}}

@article{XUE2022116057,
	abstract = {Subway passenger flow prediction is of great significance in transportation planning and operation. Special events, as for vocal concerts and sports games, lead large-scaled passenger flow with few periodic trends. Therefore, predicting subway outbound flow during events is a challenging task. In recent years, social media has been used for socio-economic forecasting. Correlation analysis shows that the trend of social media volume can be used for passenger flow prediction under events occurrences. In this paper, besides traditional smart card data, we incorporate social media data into passenger flow prediction. The multivariate disturbance-based hybrid deep neural network (MDB-HDNN), which models the disturbances of the inbound flow from the nearby stations and the social media post trends, is proposed for subway passenger flow prediction during events. Experimental results on three real-world datasets demonstrate that the MDB-HDNN performs well under various settings and has better robustness. Our findings and results can provide decision support for schedule formulation and passenger flow guidance.},
	author = {Gang Xue and Shifeng Liu and Long Ren and Yicao Ma and Daqing Gong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116057},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social media, Subway passenger flow prediction, Attention, Spatiotemporal disturbances},
	pages = {116057},
	title = {Forecasting the subway passenger flow under event occurrences with multivariate disturbances},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421013981},
	volume = {188},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421013981},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116057}}

@article{SEONG2021113988,
	abstract = {With the development of machine learning technologies, predicting stock movements by analyzing news articles has been studied actively. Most of the existing studies utilize only the datasets of target companies, and some studies use datasets of the relevant companies in the Global Industry Classification Standard (GICS) sectors. However, we show that GICS has a limitation in finding relevance regarding stock prediction because heterogeneity exists in the GICS sectors. To solve this limitation, we suggest a methodology that reflects heterogeneity and searches for homogeneous groups of companies which have high relevance. Stock price movements are predicted using the K-means clustering and multiple kernel learning technique which integrates information from the target company and its homogeneous cluster. We experiment using three-year data from the Republic of Korea and compare the results of the proposed method with those of existing methods. The results show that the proposed method shows higher predictability than existing methods in the majority of cases. The results also imply that the necessity of cluster analysis depends on the heterogeneity in the sector, and it is essential to perform cluster analysis with a larger number of clusters as the heterogeneity increases.},
	author = {Nohyoon Seong and Kihwan Nam},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113988},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stock prediction, Data mining, Machine learning, Heterogeneity, Cluster analysis},
	pages = {113988},
	title = {Predicting stock movements based on financial news with segmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030765X},
	volume = {164},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030765X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113988}}

@article{LOPEZMONROY2020112909,
	abstract = {The Author Profiling (AP) task aims to predict demographic characteristics about the authors from documents (e.g., age, gender, native language). The research so far has focused only on forensic scenarios by performing post-analysis using all the available text evidence. This paper introduces the task of Early Author Profiling (EAP) in Twitter. The goal is to effectively recognize profiles using as few tweets as possible from the user history. The task is highly relevant to support social media analysis and different problems related to security and marketing, where prevention and anticipation is crucial. This work proposes a novel strategy that combines a state of the art representation for early text classification and specialized word-vectors for author profiling tasks. In this strategy we build prototypical features called Profile based Meta-Words, which allow us to model AP information at different levels of granularity. Our evaluation shows that the proposed methodology is well suited for profiling little text evidence (e.g., a handful of tweets) in early stages, but as more tweets become available other granularities better encode larger amounts of text in late stages. We evaluated the proposed ideas on gender and language variety identification for English and Spanish, and showed that the proposal outperforms state of the art methodologies.},
	author = {A. Pastor L{\'o}pez-Monroy and Fabio A. Gonz{\'a}lez and Thamar Solorio},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112909},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Early text classification, Author profiling, Social media analysis, Text mining},
	pages = {112909},
	title = {Early author profiling on Twitter using profile features with multi-resolution},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930627X},
	volume = {140},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930627X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112909}}

@article{KUMARASWAMY2022118433,
	abstract = {Insurance fraud is ranked second in the list of expensive crimes in the United States, with healthcare fraud being the second highest amongst all insurance fraud. Contrary to the popular belief, insurance fraud is not a victimless crime. The cost of crime is passed onto law-abiding citizens in the form of increased premiums or serious harm or danger to beneficiaries. To combat this kind of societal threat, there is an intense need for healthcare fraud detection systems to evolve. Some common roadblocks in implementing digital advancements (as seen in other domains) to healthcare are the complexity, heterogeneity of the data systems, and varied health program models across the United States. In other words, data are not stored in a centralized manner due to the sensitive domain nature, thus making it difficult to implement a robust real-world fraud-detection system. At the same time, in addition to the complexity of the varied systems involved, there is also the need to meet certain standards before a fraud actor can be prosecuted in a litigation setting. Thus, there is a human aspect to the fraud detection process flow in the real-world. In this article, a novel framework was outlined that converts diverse prescription claims (both fee-for-service and managed care) into a set of input variables/features suitable for implementation of an advanced statistical modeling fraud framework. This article thus aims to contribute to the existing literature by describing a process to transform prescription claims data to secondary features specific to provider fraud detection. The core idea was to focus on three main aspects of fraud (business heuristics on claims, provider-to-prescriber relations, and provider's client populations) to design the input features. A systematic method was proposed to extract features that have the potential to detect billing or behavioral outliers among pharmacy providers using information extracted from a secondary database (outpatient prescriptions). The application of a commonly used dimensionality reduction method, the Principal Component Analysis (PCA), was evaluated. PCA evaluates and reduces the extensive feature subspace to only those that captures the most variance in the data. To evaluate the features extracted from this framework, the application of the engineered features and the principal components to out-of-the-box logistic regression and Random Forest algorithms were considered to identify potential fraud. The engineered features when tested in different experimental settings with a logistic regression model had the highest area under the Receiver Operating Characteristic (ROC) curve of 0.76 and a weighted F score of 0.85 while a random forest model had the highest area under curve of 0.74 and a weighted F score of 0.88.},
	author = {Nishamathi Kumaraswamy and Mia K. Markey and Jamie C. Barner and Karen Rascati},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118433},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Medicaid, Fraud detection, Class imbalance, Machine learning, Statistical models},
	pages = {118433},
	title = {Feature engineering to detect fraud using healthcare claims data},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015330},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015330},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118433}}

@article{FIOK2021115771,
	abstract = {Many institutions and companies find it valuable to know how people feel about their ventures; hence, scientific research in sentiment analysis has been intensely developed over time. Automated sentiment analysis can be considered as a machine learning (ML) prediction task, with classes representing human affective states. Due to the rapid development of ML and deep learning (DL), improvements in automatic sentiment analysis performance are achieved almost every year. Since 2013, Semantic Evaluation (SemEval) has hosted a worldwide community-acknowledged competition that allows for comparisons of recent innovations. The sentiment analysis tasks focus on assessing sentiment in Twitter posts authored by various publishers and addressing multiple subjects. Our study aimed to compare selected popular and recent natural language processing methods using a new data set of Twitter posts sent to a single Twitter account. For improved comparability of our experiments with SemEval, we adopted their metrics and also deployed our models on data published for SemEval-2017. In addition, we investigated if an unsupervised ML technique applied for the detection of topics in tweets can be leveraged to improve the predictive performance of a selected transformer model. We also demonstrated how a recent explainable artificial intelligence technique can be used in Twitter sentiment analysis to gain a deeper understanding of the models' predictions. Our results show that the most recent DL language modeling approach provides the highest quality; however, this quality comes at reduced model transparency.},
	author = {Krzysztof Fiok and Waldemar Karwowski and Edgar Gutierrez and Maciej Wilamowski},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115771},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Deep learning, Sentiment analysis, Machine learning, Explainability, Twitter},
	pages = {115771},
	title = {Analysis of sentiment in tweets addressed to a single domain-specific Twitter account: Comparison of model performance and explainability of predictions},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421011428},
	volume = {186},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421011428},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115771}}

@article{ZHOU2022116560,
	abstract = {As a variant problem of aspect-based sentiment analysis (ABSA), aspect category sentiment analysis (ACSA) aims to identify the aspect categories discussed in sentences and predict their sentiment polarities. However, most aspect-based sentiment analysis (ABSA) research focuses on predicting the sentiment polarities of given aspect categories or aspect terms explicitly discussed in sentences. In contrast, aspect categories are often discussed implicitly. Additionally, most of the research does not consider the relations between contextual words and aspect categories. This paper proposes a novel Semantic Relatedness-enhanced Graph Network (SRGN) model which integrates the semantic relatedness information through an Edge-gated Graph Convolutional Network (EGCN). We introduce an ontology-based approach and a distributional approach to calculate the semantic relatedness values between contextual words and aspect categories. EGCN with the capability to aggregate multi-channel edge features, is then applied to model the semantic relatedness values in a graphical structure. We also employ an aspect--context attention module to generate aspect-specific representations. The proposed SRGN is evaluated on five datasets constructed based on SemEval 2015, SemEval 2016 and MAMC-ACSA datasets. Experimental results indicate that our proposed model outperforms the baseline models in both accuracy and F1 score.},
	author = {Tao Zhou and Kris M.Y. Law},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116560},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect category sentiment analysis, Semantic relatedness, Edge-Gated Graph Convolutional Network, Aspect--context attention},
	pages = {116560},
	title = {Semantic Relatedness Enhanced Graph Network for aspect category sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000586},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000586},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116560}}

@article{HOOSHYAR2022116670,
	abstract = {Despite the multiple deep knowledge tracing (DKT) methods developed for intelligent tutoring systems and online learning environments, there exists only a few applications of such methods in educational computer games. One key challenge is that a player may deploy several interweaved and overlapped skills during gameplay, making the assessment task nontrivial. In this research, we present a generalizable DKT approach called GameDKT that integrates state-of-the-art machine learning with domain knowledge to model the learners' knowledge state during gameplay, in an attempt to monitor and trace their proficiency level for the different skills required for educational games. Our findings reveal that GameDKT approach could successfully predict the performance of players in the coming game task using the cross-validated CNN model with accuracy and AUC of roughly 85% and 0.913, respectively, thus outperforming the MLP baseline model by up to 14%. When the performance of players is forecasted for up to four game tasks in advance, results show that the CNN model can achieve more than 70% accuracy. Interestingly, this model seems to be better and faster at identifying local patterns and it could achieve a higher performance compared to RNN and LSTM in both one-step and multi-step prediction of learners' performance in game tasks.},
	author = {Danial Hooshyar and Yueh-Min Huang and Yeongwook Yang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116670},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Learner model, Deep knowledge tracing, Educational game, Prediction of player performance, Deep learning},
	pages = {116670},
	title = {GameDKT: Deep knowledge tracing in educational games},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422001555},
	volume = {196},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422001555},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116670}}

@article{QORIB2023118715,
	abstract = {In 2019 there was an outbreak of coronavirus pandemic also known as COVID-19. Many scientists believe that the pandemic originated from Wuhan, China, before spreading to other parts of the globe. To reduce the spread of the disease, decision makers encouraged measures such as hand washing, face masking, and social distancing. In early 2021, some countries including the United States began administering COVID-19 vaccines. Vaccination brought a relief to the public; it also generated a lot of debates from anti-vaccine and pro-vaccine groups. The controversy and debate surrounding COVID-19 vaccine influenced the decision of several people in either to accept or reject vaccination. Because of data limitations, social media data, collected through live streaming public tweets using an Application Programming Interface (API) search, is considered a viable and reliable resource to study the opinion of the public on Covid-19 vaccine hesitancy. Thus, this study examines 3 sentiment computation methods (Azure Machine Learning, VADER, and TextBlob) to analyze COVID-19 vaccine hesitancy. Five learning algorithms (Random Forest, Logistics Regression, Decision Tree, LinearSVC, and Na{\"\i}ve Bayes) with different combination of three vectorization methods (Doc2Vec, CountVectorizer, and TF-IDF) were deployed. Vocabulary normalization was threefold; potter stemming, lemmatization, and potter stemming with lemmatization. For each vocabulary normalization strategy, we designed, developed, and evaluated 42 models. The study shows that Covid-19 vaccine hesitancy slowly decreases over time; suggesting that the public gradually feels warm and optimistic about COVID-19 vaccination. Moreover, combining potter stemming and lemmatization increased model performances. Finally, the result of our experiment shows that TextBlob + TF-IDF + LinearSVC has the best performance in classifying public sentiment into positive, neutral, or negative with an accuracy, precision, recall and F1 score of 0.96752, 0.96921, 0.92807 and 0.94702 respectively. It means that the best performance was achieved when using TextBlob sentiment score, with TF-IDF vectorization and LinearSVC classification model. We also found out that combining two vectorizations (CountVectorizer and TF-IDF) decreases model accuracy.},
	author = {Miftahul Qorib and Timothy Oladunni and Max Denis and Esther Ososanya and Paul Cotae},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118715},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Covid-19, Vaccine Hesitancy, Sentiment Analysis, Machine Learning, Twitter},
	pages = {118715},
	title = {Covid-19 vaccine hesitancy: Text mining, sentiment analysis and machine learning on COVID-19 vaccination Twitter dataset},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017407},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422017407},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118715}}

@article{KWON2021114488,
	abstract = {In digital forensics, user profiling aims to predict characteristics of the user from digital evidence extracted from digital devices (e.g. smartphone, laptop, tablet). Previous researches showed promising results, but there are limitations to apply practical investigations. The researches so far have focused only on specific applications, devices, or operating systems by analyzing the order of execution or volatile data such as network traffic and online content. This paper introduces a user profiling method, named Entity Profiling with Binary Predicates (EPBP) model, which analyzes non-volatile data remained on digital devices. The proposed model defines that a user has two properties: tendency and impact, which indicate patterns of application usage. Based on the attributes, the EPBP model generates users' profiles and performs similarity analysis to differentiate between the users. We also present methods for clustering and anomaly detection through real case studies.},
	author = {Hongkyun Kwon and Sangjin Lee and Doowon Jeong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114488},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {User profiling, Digital forensics, Application usage, User similarity, Anomaly detection},
	pages = {114488},
	title = {User profiling via application usage pattern on digital devices for digital forensics},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420311349},
	volume = {168},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420311349},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114488}}

@article{SHAO2022118221,
	abstract = {The past decade has witnessed the rapid development of Artificial Intelligence (AI), especially the explosion of deep learning-related connectionist approaches. This study combines traditional literature review, bibliometric methods, and the Science of Science (SciSci) theory to scrutinize the development context of AI in the last decade on AMiner.44www.aminer.cn, AMiner is an academic mining system, which will be introduced in the following section. With the assistance of AMiner tools and datasets, this paper aims to describe a further explicit context and evolution of AI in the past decade from the development of connectionist approaches. Five aspects of the past decade are highlighted: self-learning and self-coding algorithms, Recurrent Neural Networks (RNN) algorithms, reinforcement learning, pre-trained models, and other typical deep learning algorithms, which represent the significant progress of this field. By combining these critical parts, we then summarize the current limitations and corresponding future of AI trends in the next decade and discuss some topics about the next generation of AI. Discoveries in this paper will benefit AI research in promoting understanding of the current critical stage and future trends of AI development and the AI industry in the dramatic ascendant for the academic research results transformation and its industrial layout.},
	author = {Zhou Shao and Ruoyan Zhao and Sha Yuan and Ming Ding and Yongli Wang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118221},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Artificial Intelligence, Frontier research, Future trend, Data analytics, Science of Science},
	pages = {118221},
	title = {Tracing the evolution of AI in the past decade and forecasting the emerging trends},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013732},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422013732},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118221}}

@article{WANG2022117317,
	abstract = {A large amount of information exists in many e-commerce and review websites as a valuable source for recommender systems. Recent solutions focus on exploring the correlation between sentiment and textual reviews in the review-based recommendation. However, these studies usually pay less attention to the differences of different users in sentimental expression styles or language usage habits when a user writes reviews. In this work, we argue that the individual reviewing behavior is closely related to personality, and sentimental expression is a manifestation of personality. Therefore, we propose a novel Persona-driven Sentimental Attentive Recommendation model (named PSAR) via personalized sentimental interactive representation learning for the review-based recommendation. The proposed model is devised to learn fragment-level and sequence-level personalized sentimental representation simultaneously from reviews. Besides, an attentive persona-driven interaction module is designed to capture word-level usage habits and sentence-level analogous tones. Comprehensive experimental results on four real-world datasets demonstrate that our model outperforms the state-of-the-art methods.},
	author = {Peipei Wang and Lin Li and Ru Wang and Xinhao Zheng and Jiaxi He and Guandong Xu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117317},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Review mining, Sentimental representation learning, Attention neural networks, Review-based recommendation},
	pages = {117317},
	title = {Learning persona-driven personalized sentimental representation for review-based recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422006789},
	volume = {203},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422006789},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117317}}

@article{JANG2021114042,
	abstract = {The role of text mining based on technological documents such as patents is important in the research field of technology intelligence for technology R&D planning. In addition, WordNet, an English-based lexical database, is widely used for pre-processing text data such as word lemmatization and synonym search. However, technological vocabulary information is complex and specific, and WordNet's ability to analyze technological information is limited in its reflecting technological features. Thus, to improve the text mining performance of technological information, this study proposes a methodology for designing a TechWord-based lexical database that is based on the lexical characteristics of technological words that are differentiated from general words. To do this, we define TechWord, a technology lexical information, and construct a TechSynset, a synonym set between TechWords. First, through dependency parsing between words, TechWord, a unit word that describes a technology, is structured and identifies nouns and verbs. The importance of connectivity is investigated by a network centrality index analysis based on the dependency relations of words. Subsequently, to search for synonyms suitable for the target technology domain, a TechSynset is constructed through synset information, with an additional analysis that calculates cosine similarity based on a word embedding vector. Applying the proposed methodology to the actual technology-related information analysis, we collect patent data on the technological fields of the automotive field, and present the results of the TechWord and TechSynset. This study improves technological information-based text mining by structuring the word-to-word link information in technological documents based on an automated process.},
	author = {Hyejin Jang and Yujin Jeong and Byungun Yoon},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114042},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Patent mining, Natural language processing, Text mining, Lexical analysis, WordNet},
	pages = {114042},
	title = {TechWord: Development of a technology lexical database for structuring textual technology information based on natural language processing},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420308101},
	volume = {164},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420308101},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114042}}

@article{RINALDI2021114320,
	abstract = {The amount of available multimedia data in different formats and from different sources increases everyday. From an information retrieval point of view, this high volume and heterogeneity of data involves several issues to be addressed related to information overload and lacks of well structured information. Even if modern information retrieval systems offer to the user manifold search options, it is still hard to find systems with optimal performances in the document seeking process starting from a given topic. In recent years, several frameworks have been proposed and developed to support this task based on different models and techniques. In this paper we propose a semantic approach to document classification using both textual and visual topic detection techniques based on deep neural networks and multimedia knowledge graph. A semantic multimedia knowledge base has been exploited and several experimental results show the effectiveness of our proposed approach.},
	author = {Antonio M. Rinaldi and Cristiano Russo and Cristian Tommasino},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114320},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multimedia topic detection, Document classification, Semantic analysis, Ontologies, Big data, Deep neural networks, Knowledge graph},
	pages = {114320},
	title = {A semantic approach for document classification using deep neural networks and multimedia knowledge graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310149},
	volume = {169},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420310149},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114320}}

@article{DAI2023118841,
	abstract = {The outbreak of COVID-19 brings almost the biggest explosions of scientific literature ever. Facing such volume literature, it is hard for researches to find desired citation when carrying out COVID-19 related research, especially for junior researchers. This paper presents a novel neural network based method, called citation relational BERT with heterogeneous deep graph convolutional network (CRB-HDGCN), for COVID-19 inline citation recommendation task. The CRB-HDGCN contains two main stages. The first stage is to enhance the representation learning of BERT model for COVID-19 inline citation recommendation task through CRB. To achieve the above goal, an augmented citation sentence corpus, which replaces the citation placeholder with the title of the cited papers, is used to lightly retrain BERT model. In addition, we extract three types of sentence pair according citation relation, and establish sentence prediction tasks to further fine-tune the BERT model. The second stage is to learn effective dense vector of nodes among COVID-19 bibliographic graph through HDGCN. The HDGCN contains four layers which are essentially all sub neural networks. The first layer is initial embedding layer which generates initial input vectors with fixed size through CRB and a multilayer perceptron. The second layer is a heterogeneous graph convolutional layer. In this layer, we expand traditional homogeneous graph convolutional network into heterogeneous by subtly adding heterogeneous nodes and relations. The third layer is a deep attention layer. This layer uses trainable project vectors to reweight the node importance simultaneously according to both node types and convolution layers, which further promotes the performance of learnt node vectors. The last decoder layer recovers the graph structure and let the whole network trainable. The recommendation is finally achieved by integrating the high performance heterogeneous vectors learnt from CRB-HDGCN with the query vectors. We conduct experiments on the CORD-19 and LitCovid datasets. The results show that compared with the second best method CO-Search, CRB-HDGCN improves MAP, MRR, P@100 and R@100 with 21.8%, 22.7%, 37.6% and 21.2% on CORD-19, and 29.1%, 25.9%, 15.3% and 11.3% on LitCovid, respectively.},
	author = {Tao Dai and Jie Zhao and Dehong Li and Shun Tian and Xiangmo Zhao and Shirui Pan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118841},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {COVID-19 citation recommendation, Deep graph convolutional network, Heterogeneous graph, Citation enhanced BERT, Text representation learning},
	pages = {118841},
	title = {Heterogeneous deep graph convolutional network with citation relational BERT for COVID-19 inline citation recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422018590},
	volume = {213},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422018590},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118841}}

@article{KHALID2022115926,
	abstract = {Massive Open Online Courses (MOOCs) are receiving attention from learners because MOOCs enable them to satisfy their learning needs through an open, participatory, and distributed way. With the increased interest from learners, the number of MOOCs available is increasing which has increased options for learners. This as a result has created the need for recommendation systems that help learners select suitable MOOCs. This literature review covers analysis of recommender systems (RSs) that have been implemented in MOOCs with the goal of providing insights on the trends reported in the academic literature on recommender systems in MOOCs. The review discusses different recommendation techniques, recommendation types and evaluation techniques that have been used and reported on. This review includes research work over eight years, i.e. from 1st January 2012 to 17th November 2020. After the filtering process, 67 papers were selected from journals and conferences from four academic databases (i.e., IEEE, ACM, Science Direct, and Springer). A framework is designed that classifies literature on the basis of both design and evaluation aspects of RS in MOOCs. This review concludes by highlighting gaps found in the literature.},
	author = {Asra Khalid and Karsten Lundqvist and Anne Yates},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115926},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Massive Open Online Courses (MOOCs), Recommender systems (RSs), Recommendation techniques, Evaluation techniques and classification framework},
	pages = {115926},
	title = {A literature review of implemented recommendation techniques used in Massive Open online Courses},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742101280X},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742101280X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115926}}

@article{AKKASI2021115162,
	abstract = {Word Sense Induction (WSI) concerns the automatic identification of the various senses of polysemous words. Any improvement in this process can directly affect the quality of the applications in which knowing the word's senses is important. For example, word sense disambiguation, information retrieval, and clustering of web search result in lexically ambiguous queries. In this paper, we propose a novel WSI model that makes use of automatically generated lexical substitutes for a target word to construct a graph and data preparation for the next steps. Following the data preparation step, we make use of Leader--Follower graph clustering to find the basic senses of the target word. The senses of the target word inside the remaining or new upcoming instances will be decided according to their contextual embedding's similarities with the basic sense. Besides, to make the number of found sense groups of a target word much closer to the reality, we apply post-processing at the end. The results of experiments on SemEval2010 dataset confirm that the proposed method outperforms all the state-of-the-art solutions in terms of both harmonic and geometric v-measure and f-score with a lower average number of sense groups.},
	author = {Abbas Akkasi and Jan Snajder},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115162},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Word sense induction, Natural language processing, Graph clustering, Clustering refinement, Lexical substitution},
	pages = {115162},
	title = {Word sense induction using leader-follower clustering of automatically generated lexical substitutes},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421006035},
	volume = {181},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421006035},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115162}}

@article{KAUR2023118997,
	abstract = {The assignment of codes to free-text clinical narratives have long been recognised to be beneficial for secondary uses such as funding, insurance claim processing and research. The current scenario of assigning clinical codes is a manual process which is very expensive, time-consuming and error prone. In recent years, many researchers have studied the use of Natural Language Processing (NLP), related machine learning and deep learning methods and techniques to resolve the problem of manual coding of clinical narratives and to assist human coders to assign clinical codes more accurately and efficiently. The main objective of this systematic literature review is to provide a comprehensive overview of automated clinical coding systems that utilise appropriate NLP, machine learning and deep learning methods and techniques to assign the International Classification of Diseases (ICD) codes to discharge summaries. We have followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and conducted a comprehensive search of publications from January, 2010 to December 2021 in four high quality academic databases: PubMed, ScienceDirect, Association for Computing Machinery (ACM) Digital Library, and the Association for Computational Linguistics (ACL) Anthology. We reviewed 6128 publications; 42 met the inclusion criteria. This review identified: 6 datasets having discharge summaries (2 publicly available, 4 acquired from hospitals); 14 NLP techniques along with some other data extraction processes, different feature extraction and embedding techniques. The review also shows that there is a significant increase in the use of deep learning models compared to machine learning. To measure the performance of classification methods, different evaluation metrics are used. Efforts are still required to improve ICD code prediction accuracy, availability of large-scale de-identified clinical corpora with the latest version of the classification system. This can be a platform to guide and share knowledge with the less experienced coders and researchers.},
	author = {Rajvir Kaur and Jeewani Anupama Ginige and Oliver Obst},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118997},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Computer assisted clinical coding, Clinical classification and coding, Discharge summaries, Natural Language Processing, Machine learning, Deep learning},
	pages = {118997},
	title = {AI-based ICD coding and classification approaches using discharge summaries: A systematic literature review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422020152},
	volume = {213},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422020152},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118997}}

@article{THIRUMOORTHY2021115040,
	abstract = {In this digital era, millions of Internet users are contributing vast amounts of data in the form of unstructured text documents. Organizing this material is a tedious task. The clustering of text document plays a vital role for organizing these unstructured text documents. In our paper, we make use of Hybrid Jaya Optimization algorithm (HJO) for text Document Clustering (DC), referred to as HJO-DC. We have used the Silhouette index as a metric to measure the quality of a solution. The proposed work is compared with partitioning techniques such as K-Means and K-Medoids and metaheuristic techniques such as Genetic algorithm, Cuckoo Search, Particle Swarm Optimizer, Firefly and Grey Wolf Optimizer. Remarkably, the proposed algorithm achieves the highest quality clustering in all benchmark examples.},
	author = {Karpagalingam Thirumoorthy and Karuppaiah Muneeswaran},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115040},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Document clustering, Jaya optimization, Cosine similarity, Crossover, Mutation},
	pages = {115040},
	title = {A hybrid approach for text document clustering using Jaya optimization algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421004814},
	volume = {178},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421004814},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115040}}

@article{MOJRIAN2021114555,
	abstract = {The explosive growth of textual data on the web and the problem of obtaining desired information through this enormous volume of data has led to a dramatic increase in demand for developing automatic text summarization systems. For this reason, this paper presents a novel multi-document text summarization approach, called MTSQIGA, which extracts salient sentences from source document collection to generate the summary. The proposed generic summarizer models extractive summarization as a binary optimization problem that applies a modified quantum-inspired genetic algorithm (QIGA) in its processing stage to find the best solution. Objective function of our approach plays an important role in optimizing linear combination of coverage, relevance, and redundancy factors which consists of six sentence scoring measures. To ensures the generation of a summary with predefined length limit, the presented QIGA employs a modified quantum measurement and a self-adaptive quantum rotation gate based on the quality and length of the summary. Evaluation of the proposed system was performed on DUC 2005 and 2007 benchmark datasets in terms of ROUGE standard measures. Comparison of MTSQIGA with existing state-of-the-art approaches for multi-document summarization shows superior performance of the proposed systems over other methods on both existing benchmark datasets. It also indicates promising efficiency of our proposed algorithm on applying quantum-inspired genetic algorithm to the text summarization tasks.},
	author = {Mohammad Mojrian and Seyed Abolghasem Mirroshandel},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114555},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Extractive summarization, Multi-document summarization, Quantum-inspired genetic algorithm, Objective function, Self-adaptive rotation gate, Quantum measurement},
	pages = {114555},
	title = {A novel extractive multi-document text summarization system using quantum-inspired genetic algorithm: MTSQIGA},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420311994},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420311994},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114555}}

@article{BI2022118352,
	abstract = {With the prosperous development of unconventional oil and gas (UOG) began in the mid-1990 s, the proliferation of digital textual compliance reports from the UOG production life-cycle makes it imperative for experts to develop efficient ways of supporting emergency responses based on the textual based data sources. In this respect, we utilized the UOG compliance reports from the Pennsylvania Department of Environmental Protection from 2000 to 2019, then established an attentive neural-network framework to support on-site emergency responses. The advantages of attentive-based neural networks over the other mechanisms are that it not only generates powerful contextual vectors for follow-up tasks but also it allows us to observe the importance of violation factors with respect to different scenarios. The experimental results show that our model can extract valid representation from narrative texts in UOG violation compliance reports and achieve high performance in emergency response. At the same time, we obtained two intriguing practical implications: first, geographical and time characteristics are powerful indicators for supporting decision making in UOG on-site emergency responses; second, there is an urgent need for governments to implement different inspection strategies according to unique UOG sites rather than counties concerning specific geological features, which benefits from saving human labor and financial expenditures.},
	author = {Dan Bi and Ju-e Guo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118352},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Unconventional oil and gas, Violation analysis, Attentive neural network, Emergency response, Decision support system},
	pages = {118352},
	title = {Introducing attentive neural networks into unconventional oil and gas violation analysis and emergency response system},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014713},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014713},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118352}}

@article{ZHU2023118364,
	abstract = {Financial markets are based on the daily movements of thousands of tradable assets, such as stocks, resulting in billion-dollar trade volumes and affecting investors and companies around the globe. In this volatile and high-stakes environment, financial-service firms employ analysts to create compact market commentaries that serve as insightful summaries with key pieces of information. In this work, we attempt to automate this process by formally defining and algorithmically solving the Market Commentary Generation (MCG) problem. In addition to saving time and cost via automation, our approach makes a number of contributions that differentiate it from previous related work. These include the consideration of thousands of underlying time series, the ability to capture and encode significant market events that involve multiple financial entities, and the ability to deliver high quality commentary even in the presence of small and unlabeled historical datasets. Finally, our approach takes into account the strict compliance requirements of the finance domain, which prevent the use of black-box methods that can produce language that violates key rules and regulations. We compare our work against competitive baselines via an evaluation that includes both qualitative and quantitative experiments.},
	author = {Di Zhu and Theodoros Lappas and Thami Rachidi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118364},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {NLP, NLG, Text mining, Summarization, Financial markets},
	pages = {118364},
	title = {Commentary generation for financial markets},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014798},
	volume = {211},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014798},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118364}}

@article{IWATSUKI2022115840,
	abstract = {Formulaic expressions, such as `in this paper we propose', are helpful for authors of scholarly papers because they convey communicative functions; in the above, it is `showing the aim of this paper'. Thus, resources of formulaic expressions, such as a dictionary, that could be looked up easily would be useful. However, forms of formulaic expressions can often vary to a great extent. For example, `in this paper we propose', `in this study we propose' and `in this paper we propose a new method to' are all regarded as formulaic expressions. Such a diversity of spans and forms causes problems in both extraction and evaluation of formulaic expressions. In this paper, we propose a new approach that is robust to variation of spans and forms of formulaic expressions. Our approach regards a sentence as consisting of a formulaic part and non-formulaic part. Then, instead of trying to extract formulaic expressions from a whole corpus, by extracting them from each sentence, different forms can be dealt with at once. Based on this formulation, to avoid the diversity problem, we propose evaluating extraction methods by how much they convey specific communicative functions rather than by comparing extracted expressions to an existing lexicon. We also propose a new extraction method that utilises named entities and dependency structures to remove the non-formulaic part from a sentence. Experimental results show that the proposed extraction method achieved the best performance compared to other existing methods.},
	author = {Kenichi Iwatsuki and Florian Boudin and Akiko Aizawa},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115840},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Formulaic expressions, Multi-word expressions, Writing assistance, English for academic purposes},
	pages = {115840},
	title = {Extraction and evaluation of formulaic expressions used in scholarly papers},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742101201X},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742101201X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115840}}

@article{LI2022116600,
	abstract = {The explosive growth of text data has attracted many researchers to explore the efficient method to extract valuable hidden information. Many technologies, especially deep learning methods, have achieved great success in text analysis. However, the most powerful methods always require a considerable quantity of data for training, which may suffer from imbalanced data in some cases. In this paper, we propose a network-based Convolution Neural Network (NCNN) to mitigate the effect of imbalanced data. The proposed model first generates new synthetic samples for the imbalanced data based on the random walking of the network. Then an extra layer called Polar Layer is introduced to connect the output from the network model of the text to the classical CNN. Two electing strategies (n-NCNN and x-NCNN) are proposed to improve the performance of NCNN further. In the experimental section, the proposed model is applied to Reuters 21578 and WebKb. By comparing with six approaches, we prove the effectiveness of the proposed NCNN model on the imbalanced text data.},
	author = {Keping Li and Dongyang Yan and Yanyan Liu and Qiaozhen Zhu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116600},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Complex Network, CNN, Text Analysis, Imbalanced Data, Random Walk},
	pages = {116600},
	title = {A network-based feature extraction model for imbalanced text data},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742200094X},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742200094X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116600}}

@article{GAO2021114191,
	abstract = {With the development of mobile Internet, microblog has become one of the most popular social platforms. The enormous user-generated microblogs have caused the problem of information overload, which makes users difficult to find the microblogs they actually need. Hence, how to provide users with accurate microblogs has become a hot and urgent issue. In this paper, we propose an approach of hybrid microblog recommendation, which is developed on a framework of deep neural network with a group of heterogeneous features as its input. Specifically, two new recommendation strategies are first constructed in terms of the extended user-interest tags and user interest topics, respectively. These two strategies additionally with the collaborative filtering are employed together to obtain the candidate microblogs for final recommendation. Then, we propose the heterogeneous features related to personal interests of users, interest in authors and microblog quality to describe the candidate microblogs. Finally, a deep neural network with multiple hidden layers is designed to predict and rank the microblogs. Extensive experiments conducted on the datasets of Sina Weibo and Twitter indicate that our proposed approach significantly outperforms the state-of-the-art methods. The code and the two datasets of this paper are publicly available at GitHub.},
	author = {Jiameng Gao and Chunxia Zhang and Yanyan Xu and Meiqiu Luo and Zhendong Niu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114191},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hybrid microblog recommendation, Deep neural network, Heterogeneous features, Extended user interest tags, Topic links},
	pages = {114191},
	title = {Hybrid microblog recommendation with heterogeneous features using deep neural network},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420309246},
	volume = {167},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420309246},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114191}}

@article{LI2020112839,
	abstract = {Circle structure of online brand communities allows companies to conduct cross-marketing activities by the influence of friends in different circles and build strong and lasting relationships with customers. However, existing works on the friend recommendation in social network do not consider establishing friendships between users in different circles, which has the problems of network sparsity, neither do they study the adaptive generation of appropriate link prediction algorithms for different circle features. In order to fill the gaps in previous works, the intelligent attention allocation link prediction algorithm is proposed to adaptively build attention allocation index (AAI) according to the sparseness of the network and predict the possible friendships between users in different circles. The AAI reflects the amount of attention allocated to the user pair by their common friend in the triadic closure structure, which is decided by the friend count of the common friend. Specifically, for the purpose of overcoming the problem of network sparsity, the AAIs of both the direct common friends and indirect ones are developed. Next, the decision tree (DT) method is constructed to adaptively select the suitable AAIs for the circle structure based on the density of common friends and the dispersion level of common friends' attention. In addition, for the sake of further improving the accuracy of the selected AAI, its complementary AAIs are identified with support vector machine model according to their similarity in value, direction, and ranking. Finally, the mutually complementary indices are combined into a composite one to comprehensively portray the attention distribution of common friends of users in different circles and predict their possible friendships for cross-marketing activities. Experimental results on Twitter and Google+ show that the model has highly reliable prediction performance.},
	author = {Shugang Li and Xuewei Song and Hanyu Lu and Linyi Zeng and Miaojing Shi and Fang Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112839},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Friend recommendation, Link prediction, AAI, Mutually complementary indices, Cross marketing},
	pages = {112839},
	title = {Friend recommendation for cross marketing in online brand community based on intelligent attention allocation link prediction algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930541X},
	volume = {139},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930541X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112839}}

@article{ZAMIRI2021114657,
	abstract = {Image tag recommendation, aiming at assigning a set of relevant tags for images, is a useful way to help users organize images' content. Early methods in image tagging mainly demonstrated using low-level visual features. However, two visually similar photos may have different concepts (semantic gap). Although different multi-view tagging methods are proposed to learn the discriminative features, they usually do not consider the geographical correlation among images. Moreover, geographical-based image tagging models generally focused on the relevance criterion, i.e., how well the suggested tags describe image content. Diversity and redundancy should be controlled to guarantee the recommendation models' effectiveness and promote complementary information among tags. This paper proposes a robust multi-view image tagging method, termed MVDF-RSC, which considers the relevance, diversity, and redundancy criteria. Precisely, the proposed method consists of two phases: training and prediction. We propose a new robust optimization problem in the training phase to determine the similarity between data via the early fusion of multiple views of images and obtain clusters. In the prediction phase, relevant tags are recommended to each test data using a search-based method and a late fusion strategy. Comprehensive experiments on two geo-tagged image datasets demonstrate the proposed method's effectiveness over state-of-the-art alternatives.},
	author = {Mona Zamiri and Tahereh Bahraini and Hadi Sadoghi Yazdi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114657},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-view spectral clustering, Image annotation, Geo-tagged photos, Image tagging, Recommender systems, Geographical information},
	pages = {114657},
	title = {MVDF-RSC: Multi-view data fusion via robust spectral clustering for geo-tagged image tagging},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000981},
	volume = {173},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000981},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114657}}

@article{CALI2022118440,
	abstract = {E-commerce websites include large volume of online customer data regarding customer preferences. This study puts forward a novel Bayesian methodology to estimate the impact of product attributes on customer satisfaction by analyzing online data, so that product designers and market researchers are facilitated in their decision making processes. This method proves that valuable information on customer insights can be provided even if we have only data of overall customer satisfaction score and product attribute characteristics. The unknown data are acquired via statistical methods such that non-parametric density estimation is utilized to estimate distributions of satisfaction scores of product attributes. The impacts of product attributes on customer satisfaction are considered as weights of mixture kernel distributions and posterior distributions of weights are simulated with Markov Chain Monte Carlo method. The applicability of the proposed methodology is demonstrated by a case study, in which online data of mobile phone market are analyzed.},
	author = {Sedef {\c C}alı and Adil Baykaso{\u g}lu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118440},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Attribute weighting, Customer satisfaction, Bayesian inference, Markov Chain Monte Carlo, Online ratings},
	pages = {118440},
	title = {A Bayesian based approach for analyzing customer's online sales data to identify weights of product attributes},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015378},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015378},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118440}}

@article{WANG2021114557,
	abstract = {Phrases are widely used in many text-based expert and intelligent systems. Phrase mining is a critical and preprocessing operation for these systems. With the increase of text data, errors in text corpus widely exist. Existing approaches focus on mining phrases on clean text corpus. However, neglecting to handle these errors may generate inaccurate results, which further leads to quality decline. To address the problem, we propose an error-tolerant phrase mining method, which not only conducts phrase mining in text corpus but also correct those phrases from errors. It could help to improve the performance of text-based expert and intelligent systems. To improve the performance and scalability, we propose several efficient and effective techniques to optimize the mining process. Experimental results show that our method achieves higher performance compared with state-of-the-art methods.},
	author = {Jiaying Wang and Jing Shan and Odafen Ehiaribho Santos and Jinling Bao},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114557},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Phrase mining, Error tolerant, Scalability},
	pages = {114557},
	title = {High quality error-tolerant phrase mining on text corpus},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742031201X},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742031201X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114557}}

@article{ELBOUSHAKI2020112829,
	abstract = {Human gesture recognition has become a pillar of today's intelligent Human-Computer Interfaces as it typically provides more comfortable and ubiquitous interaction. Such expert system has a promising prospect in various applications, including smart houses, gaming, healthcare, and robotics. However, recognizing human gestures in videos is one of the most challenging topics in computer vision, because of some irrelevant environmental factors like complex background, occlusion, lighting conditions, and so on. With the recent development of deep learning, many researchers have addressed this problem by building single deep networks to learn spatiotemporal features from video data. However, the performance is still unsatisfactory due to the limitation that the single deep networks are incapable of handling these challenges simultaneously. Hence, the extracted features cannot efficiently capture both relevant shape information and detailed spatiotemporal variation of the gestures. One solution to overcome the aforementioned drawbacks is to fuse multiple features from different models learned on multiple vision cues. Aiming at this objective, we present in this paper an effective multi-dimensional feature learning approach, termed as MultiD-CNN, for human gesture recognition in RGB-D videos. The key to our design is to learn high-level gesture representations by taking advantages from Convolutional Residual Networks (ResNets) for training extremely deep models and Convolutional Long Short-Term Memory Networks (ConvLSTM) for dealing with time-series connections. More specifically, we first construct an architecture to simultaneously learn the spatiotemporal features from RGB and depth sequences through 3D ResNets which are then linked to a ConvLSTM to capture the temporal dependencies between them, and we show that they better combine appearance and motion information effectively. Second, to alleviate distractions from background and other variations, we propose a method that encodes the temporal information into a motion representation, while a two-stream architecture based on 2D-ResNets is then employed to extract deep features from this representation. Third, we investigate different fusion strategies at different levels for blending the classification results, and we show that integrating multiple ways of encoding the spatial and temporal information leads to a robust and stable spatiotemporal feature learning with better generalization capability. Finally, we perform different experiments to evaluate the performance of the investigated architectures on four kinds of challenging datasets, demonstrating that our approach is particularly impressive where it outperforms prior arts in both accuracy and efficiency. The obtained results affirm also the importance of embedding the proposed approach in other intelligent systems application areas.},
	author = {Abdessamad Elboushaki and Rachida Hannane and Karim Afdel and Lahcen Koutti},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112829},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Gesture recognition, Deep learning, Convolutional neural networks, Multimodal learning, Feature fusion, RGB-D video processing},
	pages = {112829},
	title = {MultiD-CNN: A multi-dimensional feature learning approach based on deep convolutional networks for gesture recognition in RGB-D image sequences},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419305317},
	volume = {139},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419305317},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112829}}

@article{AGUADO2022118103,
	abstract = {This work presents a Case-Based Reasoning (CBR) module that integrates sentiment and stress analysis on text and keystroke dynamics data with context information of users interacting on Social Network Sites (SNSs). The context information used in this work is the history of positive or negative messages of the user, and the topics being discussed on the SNSs. The CBR module uses this data to generate useful feedback for users, providing them with warnings if it detects potential future negative repercussions caused by the interaction of the users in the system. We aim to help create a safer and more satisfactory experience for users on SNSs or in other social environments. In a set of experiments, we compare the effectiveness of the CBR module to the effectiveness of different affective state detection methods. We compare the capacity to detect cases of messages that would generate future problems or negative repercussions on the SNS. For this purpose, we use messages generated in a private SNS, called Pesedia. In the experiments in the laboratory, the CBR module managed to outperform the other proposed analyzers in almost every case. The CBR module was fine-tuned to explore its performance when populating the case base with different configurations.},
	author = {G. Aguado and V. Julian and A. Garcia-Fornes and A. Espinosa},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118103},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-agent system, Social networks, Sentiment analysis, Stress analysis, Case-based reasoning},
	pages = {118103},
	title = {A CBR for integrating sentiment and stress analysis for guiding users on social network sites},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012945},
	volume = {208},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422012945},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118103}}

@article{MARTINEZHUERTAS2021115621,
	abstract = {Usually, computerized assessments of constructed responses use a predictive-centered approach instead of a validity-centered one. Here, we compared the convergent and discriminant validity of two computerized assessment methods designed to detect semantic topics in constructed responses: Inbuilt Rubric (IR) and Partial Contents Similarity (PCS). While both methods are distributional models of language and use the same Latent Semantic Analysis (LSA) prior knowledge, they produce different semantic representations. PCS evaluates constructed responses using non-meaningful semantic dimensions (this method is the standard LSA assessment of constructed responses), but IR endows original LSA semantic space coordinates with meaning. In the present study, 255 undergraduate and high school students were allocated one of three texts and were tasked to make a summary. A topic-detection task was conducted comparing IR and PCS methods. Evidence from convergent and discriminant validity was found in favor of the IR method for topic-detection in computerized constructed response assessments. In this line, the multicollinearity of PCS method was larger than the one of IR method, which means that the former is less capable of discriminating between related concepts or meanings. Moreover, the semantic representations of both methods were qualitatively different, that is, they evaluated different concepts or meanings. The implications of these automated assessment methods are also discussed. First, the meaningful coordinates of the Inbuilt Rubric method can accommodate expert rubrics for computerized assessments of constructed responses improving computer-assisted language learning. Second, they can provide high-quality computerized feedback accurately detecting topics in other educational constructed response assessments. Thus, it is concluded that: (1) IR method can represent different concepts and contents of a text, simultaneously mapping a considerable variability of contents in constructed responses; (2) IR method semantic representations have a qualitatively different meaning than the LSA ones and present a desirable multicollinearity that promotes the discriminant validity of the scores of distributional models of language; and (3) IR method can extend the performance and the applications of current LSA semantic representations by endowing the dimensions of the semantic space with semantic meanings.},
	author = {Jos{\'e} {\'A}. Mart{\'\i}nez-Huertas and Ricardo Olmos and Jos{\'e} A. Le{\'o}n},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115621},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Inbuilt rubric, Constructed responses, Summaries, Topic detection, Latent semantic analysis, Automated summary evaluation},
	pages = {115621},
	title = {Enhancing topic-detection in computerized assessments of constructed responses with distributional models of language},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421010150},
	volume = {185},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421010150},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115621}}

@article{SALAHIAN2022119051,
	abstract = {Nonnegative Matrix Factorization is a data analysis method to discover parts-based, linear representations of data. It has been successfully used in a great variety of applications. Deep Nonnegative Matrix Factorization (deep NMF) was recently established to cope with the extraction of hierarchical latent feature representation, and it has been demonstrated to achieve outstanding results in unsupervised representation learning. However, defining a suitable regularization for the deep models is a key challenge, and the existing Deep NMF approaches lack a well-suited regularization. In this paper, we propose the Deep Autoencoder-like NMF with Contrastive Regularization and Feature Relationship preservation (DANMF-CRFR) to address the above problem. Inspired by contrastive learning, this deep model is able to learn discriminative and instructive deep features while adequately enforcing the local and global structures of the data to its decoder and encoder components. Meanwhile, DANMF-CRFR also imposes feature correlations on the basis matrices during feature learning to improve part-based learning capabilities. Multiplicative updating rules and convergence guarantees are also provided. Extensive experimental results demonstrate the advantages of the proposed model. The source code for reproducing our results can be found at https://github.com/NavidSalahian/DANMF_CRFR.},
	author = {Navid Salahian and Fardin Akhlaghian Tab and Seyed Amjad Seyedi and Jovan Chavoshinejad},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.119051},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deep learning, Autoencoder structure, Nonnegative matrix factorization, Contrastive regularization, Data representation},
	pages = {119051},
	title = {Deep Autoencoder-like NMF with Contrastive Regularization and Feature Relationship Preservation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422020693},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422020693},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.119051}}

@article{ALTINEL2022118606,
	abstract = {Due to the huge size of the data accumulated on microblogging sites, recently, two fundamental questions have become very popular: 1) What percentage of this accumulated data has positive or negative sentiment polarity? 2) How is the distribution of this accumulated data on different topics? Inspired by these motivated necessities, this paper presents several different algorithms which are based on the Label Propagation Algorithm (LPA) in order to handle previously mentioned two fundamentals tasks: sentiment polarity detection task and topic-based text classification task. These algorithms are the Label Propagated- Relevance Frequency Classifier (LP-RFC) and LP-Abstract Frequency Classifier (LP-AFC). These algorithms can be defined as new semantic smoothing classifiers, which take advantage of the semantic connections among terms in the label propagation phase of the LPA. Additionally, another classifier, namely LP-ComRFC+AFC, was built. LP-ComRFC+AFC is actually a weighted summation classifier of the individual LP-RFC and LP-AFC. Furthermore, considering the shortage of labeled data in real-world scenarios, a semi-supervised version of LP-RFC and LP-AFC, namely ``Merging Unlabeled and Labeled Instances with Semantic Values of Terms'' (MULIS), was designed and implemented. For the experiments of the sentiment polarity detection task, three different datasets were use and for the experiments of topic-based text classification task, a self-collected tweet dataset was use. According to the experimental results, the suggested algorithms, and their composite form, LP-ComRFC+AFC, generated higher F1 scores than all of the baseline algorithms at nearly all of the training splits on the datasets.},
	author = {Ay{\c s}e Berna Altınel},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118606},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Label propagation algorithm, Social media analysis, Topic-based tweet classification, Sentiment polarity detection},
	pages = {118606},
	title = {Social media analysis by innovative hybrid algorithms with label propagation},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742201658X},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742201658X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118606}}

@article{MOIRANGTHEM2021113898,
	abstract = {Text classification, using deep learning techniques, has become a research challenge in natural language processing. Most of the existing deep learning models for text classification face difficulties when the length of the input text increases. Most models work well on shorter text inputs, however, their performance degrades with the increase in the input length. In this work, we introduce a model for text classification that can alleviate this problem. We present the hierarchical and lateral multiple timescales gated recurrent units (HL-MTGRU), in combination with pre-trained encoders to address the long text classification problem. HL-MTGRU can represent multiple temporal scale dependencies for the discrimination task. By combining the slow and fast units of the HL-MTGRU, our model effectively classifies long multi-sentence texts into the desired classes. We also show that the HL-MTGRU structure helps the model to prevent degradation of performance on longer text inputs. We demonstrate that the proposed network with the help of the latest pre-trained encoders for feature extraction outperforms the conventional models on various long text classification benchmark datasets.},
	author = {Dennis Singh Moirangthem and Minho Lee},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113898},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text classification, Multiple timescale, Temporal hierarchy, BERT, Pre-trained encoder},
	pages = {113898},
	title = {Hierarchical and lateral multiple timescales gated recurrent units with pre-trained encoder for long text classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030693X},
	volume = {165},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030693X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113898}}

@article{JIANG2021115537,
	abstract = {Stock market prediction has been a classical yet challenging problem, with the attention from both economists and computer scientists. With the purpose of building an effective prediction model, both linear and machine learning tools have been explored for the past couple of decades. Lately, deep learning models have been introduced as new frontiers for this topic and the rapid development is too fast to catch up. Hence, our motivation for this survey is to give a latest review of recent works on deep learning models for stock market prediction. We not only category the different data sources, various neural network structures, and common used evaluation metrics, but also the implementation and reproducibility. Our goal is to help the interested researchers to synchronize with the latest progress and also help them to easily reproduce the previous studies as baselines. Based on the summary, we also highlight some future research directions in this topic.},
	author = {Weiwei Jiang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115537},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stock market prediction, Deep learning, Machine learning, Feedforward neural network, Convolutional neural network, Recurrent neural network},
	pages = {115537},
	title = {Applications of deep learning in stock market prediction: Recent progress},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421009441},
	volume = {184},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421009441},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115537}}

@article{MOHSIN2021113808,
	abstract = {One of the areas most in need of improvement in the field of automated bug fixing, localization and triaging systems is that of an effective categorization, as this would bugs to reduce the time, cost and effort required to locate, assign and fix the bug. The existing approaches depend upon the textual similarity of the bug description and category in a given reported bug; accordingly, the challenges of unstructured bugs, technical terms, versatile ways of reporting the same bug, the diverse nature and sizes of datasets etc. are often overlooked. Consequently, this limits the classifier performance to a specific type of dataset, resulting in classification inefficiency. To this end, we propose a novel Self-Paced Bug Classifier (SPBC) that is capable of locating the target categories from the bug description of the historical data, maintained by multiple open-source software packages (Bugzilla, Mentis, Redmine). The proposed model introduces a self-paced back-traceable algorithm, controlled by a self-paced regularizer, which classifies textually independent bug descriptions with weighted data-independent tokens (the easy samples). Later on, the regularizer sets comparatively hard samples for textually dependent classification by capturing intra-class and inter-class discrimination features from bug descriptions, based on the weighted similarities of words; this is done with the help of a Key Feature Identification Matrix (KFIM), a Non-Independent and Identically Distributed (NIID) matrix. Easy-to-hard self-pace learning, integrated with textually dependent and independent classification, makes SPBC capable of simultaneously enhancing the effectiveness and robustness of intelligent systems through a substantial increase in precision (5--15% on average). The main advantage of SPBC is that it targets the spatial relationship between the data and the system, which makes it an apt learner of data and allows it to maintains sample insertion into the classifier at a controlled pace. Additionally, it maintains stability, which is not affected by the dataset's dimensionality and traits. As is evidenced by the experimental results on four different datasets from open-source projects, our model outperforms the baseline and state-of-the-art methods through a single-stroke solution with improved accuracy and stable performance (average 95% precision and 4% decrease in kappa); hence, it is significant for improving intelligent bug fixing and triaging systems.},
	author = {Hufsa Mohsin and Chongyang Shi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113808},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bug triaging, Defect localization, Self-paced learning, Bug report analysis, Bug classification},
	pages = {113808},
	title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420306230},
	volume = {167},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420306230},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113808}}

@article{SARKAR2021115026,
	abstract = {The significance of tourism in the globe today is enormous since it is a major source of income and jobs for a nation. Tourists are facing a range of difficulties as they select suitable tours, consisting of several itineraries in terms of their interests and distinct constraints. An itinerary consists of many Points of Interest (POIs) and a POI can further be splitted into several attractions which are named as POI within POI. For selecting the itinerary, the existing techniques use the characteristics of POIs. However, a POI consists of many attractions. Out of these, one dominating attraction's type is considered as POI type. This ignores the other type of attraction's present in that POI. It may cause improper selection of itineraries. Therefore, selection of itineraries by considering POI within POI is of great benefit. But, it is very challenging. For this task, we suggest an algorithm called PWP. It recommends multiple itineraries that are based on the interest of visitors, popularity of itineraries and the cost of itineraries. If a tourist wants to visit unknown areas, the PWP algorithm can be expanded further. We have taken the similar user's features to advise multiple itineraries using the Flickr dataset. The findings show that the proposed PWP algorithm out-performs the baseline algorithms in terms of real-life matrices and heuristic based metrics.},
	author = {Joy Lal Sarkar and Abhishek Majumder},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115026},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommendation system, Tourist, Cost, Itineraries, POIs},
	pages = {115026},
	title = {A new point-of-interest approach based on multi-itinerary recommendation engine},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100467X},
	volume = {181},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100467X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115026}}

@article{NGUYEN2022117096,
	abstract = {Nowadays, there has been a rapidly increasing number of scientific submissions in multiple research domains. A large number of journals have various acceptance rates, impact factors, and rankings in different publishers. It becomes time-consuming for many researchers to select the most suitable journal to submit their work with the highest acceptance rate. A paper submission recommendation system is more critical for the research community and publishers as it gives scientists another support to complete their submission conveniently. This paper investigates the submission recommendation system for two main research topics: computer science and applied mathematics. Unlike the previous works (Wang et al., 2018; Son et al., 2020) that extract TF--IDF and statistical features as well as utilize numerous machine learning algorithms (logistics regression and multiple perceptrons) for building the recommendation engine, we present an efficient paper submission recommendation algorithm by using different bidirectional transformer encoders and the Mixture of Transformer Encoders technique. We compare the performance between our methodology and other approaches by one dataset from Wang et al. (2018) with 14012 papers in computer science and another dataset collected by us with 223,782 articles in 178 Springer applied mathematics journals in terms of top K accuracy (K=1,3,5,10). The experimental results show that our proposed method extensively outperforms other state-of-the-art techniques with a significant margin in all top K accuracy for both two datasets. We publish all datasets collected and our implementation codes for further references.11https://github.com/BinhMisfit/PSRMTE.},
	author = {Dac Huu Nguyen and Son Thanh Huynh and Cuong Viet Dinh and Phong Tan Huynh and Binh Thanh Nguyen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117096},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommendation system, Deep learning, Transformer encoders},
	pages = {117096},
	title = {PSRMTE: Paper submission recommendation using mixtures of transformer},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422005024},
	volume = {202},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422005024},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117096}}

@article{LATHABAI2022118317,
	abstract = {The shift from `trust-based funding' to `performance-based funding' is one of the factors that has forced institutions to strive for continuous improvement of performance. Several studies have established the importance of collaboration in enhancing the performance of paired institutions. However, identification of suitable institutions for collaboration is sometimes difficult and therefore institutional collaboration recommendation systems can be vital. Currently, there are no well-developed institutional collaboration recommendation systems. In order to bridge this gap, we design a framework that recognizes the thematic strengths and core competencies of institutions, which can in turn be used for collaboration recommendations. The framework, based on NLP and network analysis techniques, is capable of determining the strengths of an institution in different thematic areas within a field and thereby determining the core competency and potential core competency areas of that institution. It makes use of recently proposed expertise indices such as x and x(g) indices for determination of core and potential core competency areas and can toss two kinds of recommendations: (i) for enhancement of strength of strong areas or core competency areas of an institution and (ii) for complementing the potentially strong areas or potential core competency areas of an institution. A major advantage of the system is that it can help to determine and improve the research portfolio of an institution within a field through suitable collaboration, which may lead to the overall improvement of the performance of the institution in that field. The framework is demonstrated by analyzing the performance of 195 Indian institutions in the field of `Computer Science'. Upon validation using standard metrics for novelty, coverage and diversity of recommendation systems, the framework is found to be of sufficient coverage and capable of tossing novel and diverse recommendations. The article thus presents an institutional collaboration recommendation system which can be used by institutions to identify potential collaborators.},
	author = {Hiran H. Lathabai and Abhirup Nandy and Vivek Kumar Singh},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118317},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Institutional collaboration, Recommendation system, NLP, Network analysis, Research expertise, Expertise indices},
	pages = {118317},
	title = {Institutional collaboration recommendation: An expertise-based framework using NLP and network analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014464},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014464},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118317}}

@article{HAN2022116472,
	abstract = {Online communities, where lead users openly share their experiences and knowledge on product and technology in the form of a post, have become a fruitful source of innovation. While efforts have been made to provide a way of data-driven case-based reasoning (CBR), existing studies have limitations in reflecting lead users' characteristics into the CBR process. Current research has emphasized the retrieval and adaptation phase only, which retrieves, reuses, and revises cases. However, what is at the core of lead user characteristics is to find out the problem, and solve the problem by themselves before mass customers. This means CBR needs to focus on uncovering and defining problems and finding relevant solutions for designated problems. In response, this research suggests a novel approach for problem-oriented CBR approach to reflect lead user characteristics. First, this study defines problems, by extracting problem--solution sets related to the specific function using sentiment analysis. Second, this study improves case representation and case retrieval using subject-action-object (SAO) analysis and technology tree respectively. This study demonstrates the approach through a case of drone technology using lead user communities (diydrones.com), and our findings suggest that the approach can help firms broaden the knowledge of existing products to make an improvement.},
	author = {Mintak Han and Youngjung Geum},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116472},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Technology-intensive product innovation, Lead user, CBR, SAO analysis, Sentiment analysis},
	pages = {116472},
	title = {Problem-oriented CBR: Finding potential problems from lead user communities},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421017528},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421017528},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116472}}

@article{GUVEN2022116592,
	abstract = {In recent years, deep learning models have been used in the implementation of question answering systems. In this study, the performance of the question answering system was evaluated from the perspective of natural language processing using SQuAD, which was developed to measure the performance of deep learning language models. In line with the evaluations, in order to increase the performance, 3 natural language based methods, namely RNP, that can be used with pre-trained BERT language models have been proposed and they have increased the performance of the question answering system in which the pre-trained BERT models are used by 1.1% to 2.4%. As a result of the application of RNP methods with sentence selection, an increase in accuracy between 6.6% and 8.76% was achieved in answer detection. Since these methods don't require any training process, it has been shown that they can be used in question answering systems to increase the performance of any deep learning model.},
	author = {Zekeriya Anil Guven and Murat Osman Unalir},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116592},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, BERT, Text analysis, Question answering, SQuAD},
	pages = {116592},
	title = {Natural language based analysis of SQuAD: An analytical approach for BERT},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000884},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000884},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116592}}

@article{ZHANG2022115826,
	abstract = {Citation recommendation can help researchers quickly find supplementary or alternative references in massive academic resources. Current research on citation recommendation mainly focuses on the citing papers, resulting in the enormous cited papers are ignored, including the relations among cited papers and their citation context cited in citing papers. Moreover, cited paper's content is often denoted with its original title and abstract, which is hard to acquire and rarely considers different citation motivations. Furthermore, the most appropriate method for semantic representation of cited papers' relations and content is uncertain. Therefore, this paper studies citation recommendation from the perspective of semantic representation of cited papers' relations and content. Firstly, four forms of citation context are designed and extracted as cited papers' content considering citation motivations, as well as co-citation relationships are extracted as cited papers' relations. Secondly, 132 methods are designed for generating semantic vector of cited paper, including four network embedding methods, 16 methods by combining four text representation algorithms with four forms of citation content, and 112 fusion methods. Finally, similarity among cited papers is calculated for citation recommendation and a quantitative evaluation method based on link prediction is designed, to find the most appropriate form of citation content and the optimal method. The result shows that doc2vecC (Document to Vector through Corruption) with the form of CS&SS (Current Sentences and Surrounding Sentences) performs best, in which the AUC (Area Under Curve) and MAP (Macro Average Precision) reach 0.877 and 0.889 and have increased by 0.462 and 0.370 compared with the worst-performing method. This performance is slightly improved by parameters adjustment, and a case study is performed whose results have further proved the effectiveness of this method. In addition, among four forms of cited papers' content, CS&SS performs best in almost all methods. Furthermore, the fusion methods not always perform better than the single methods, where doc2vecC (CS&SS) performs better than the best fusion method GCN (Graph Convolutional Network). These results not only prove the effectiveness of citation recommendation from the perspective of cited paper, but also provide helpful and useful suggestions for method selection and citation content selection. The data and conclusions can be extended to other text mining-related tasks. Simultaneously, it is a preliminary research which needs to be further studied in other domains using emerging semantic representation methods.},
	author = {Jinzhu Zhang and Lipeng Zhu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115826},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Citation recommendation, Cited paper, Co-citation, Citation content, Semantic representation},
	pages = {115826},
	title = {Citation recommendation using semantic representation of cited papers' relations and content},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742101191X},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742101191X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115826}}

@article{ELKASSAS2021113679,
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	pages = {113679},
	title = {Automatic text summarization: A comprehensive survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	volume = {165},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113679}}

@article{ZHANG2021115439,
	abstract = {Recently, deep learning has dominated the recommender system, as it is able to effectively capture nonlinear and nontrivial user--item relationships, and perform complex nonlinear transformations. However, there are still some issues with respects to the existing methods. Firstly, they always treat user--item interactions independently, and may fail to cover more complex and hidden information that is inherently implicit in the local neighborhood surrounding an interaction sample. Secondly, by quantifying the dependence degree of user--item sequences, it demonstrates that both short-term and long-term dependent behavioral patterns co-exist. Unfortunately, typical deep learning methods might be problematic when coping with very long-term sequential dependencies. To address these issues, we propose a novel unified neural collaborative recommendation algorithm that capitalizes on memory networks for learning attention embedding from implicit interaction (NCRAE). Particularly, the attention is capable of learning the relative importance of different users and items from user--item interaction sequences, which provides a better solution for concentrating on inputs and helps to better memorize long-term sequential dependencies. Extensive experiments on three real-world datasets show significant improvements of our proposed NCRAE algorithm over the competitive methods. Empirical evidence shows that using memory networks for learning attention embeddings of users' implicit interaction yields better recommendation performance.},
	author = {Yihao Zhang and Xiaoyang Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115439},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Memory networks, Collaborative filtering, Attention embeddings, Behavioral patterns, Recommender systems},
	pages = {115439},
	title = {Learning attention embeddings based on memory networks for neural collaborative recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008538},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421008538},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115439}}

@article{LI2022118336,
	abstract = {Session-based recommendation (SBR) is a practical task that predicts the next item based on an anonymous behavior sequence. Most of current methods employ graph neural network to model neighboring item transition information from global and local contexts (i.e, other and current sessions). However, they treat neighbors from other sessions equally without considering its items, which may have different contributions with the target item on varied aspects. In other words, they have not explored finer-granularity transition information in the global context, leading to sub-optimal performance. This paper fills this gap by proposing a novel method called Transition Information Enhanced Disentangled Graph Neural Network (TIE-DGNN) to capture finer-granular transition information between items and try to interpret the transition reason by modeling various factors of items. Specifically, we first propose a position-aware global graph to model neighboring item transition in the global context. Then, we slice item embeddings into blocks, each of which represents a factor, and use global-level disentangling layers to separately learn factor embeddings. Meanwhile, we train local-level item embeddings by using attention mechanisms to capture transition information from the current session. Further, inter-session and intra-session embeddings are generated by two types of item embeddings, respectively. Finally, we use contrastive learning techniques to enhance the robustness of two session embeddings. To this end, our model considers two levels of transition information. Especially in global context, it not only consider finer-granularity transition information between items but also take user intents at factor-level into account to interpret the key reason for the transition. Extensive experiments on three benchmark datasets demonstrate its superiority over state-of-the-art methods.},
	author = {Ansong Li and Jihua Zhu and Zhongyu Li and Haozhe Cheng},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118336},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Session-based recommendation, Graph neural networks, Disentangled representation learning, Contrastive learning.},
	pages = {118336},
	title = {Transition Information Enhanced Disentangled Graph Neural Networks for session-based recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014592},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014592},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118336}}

@article{ZHOU2020113361,
	abstract = {Heart failure (HF) is among the most costly diseases to our society, and the prevalence keeps on increasing these days. Early detection of HF plays a vital role in saving lives through adjusting lifestyles and drug interventions that can slow down disease progression or prevent HF. There are many cardiovascular risk factors associated with HF, and they often coexist. In this paper, we assess the predictive value of pathological factors for early HF detection through a social network based approach. We use electronic health records (collected from the project HeartCarer) and compute the similarity of risk factors. The similarity values are used to construct an unweighted and a weighted medical social network. The constructed medical social network is further divided into a HF high-risk group and HF low-risk group using a group division algorithm. Patients in the high-risk group will be suggested for early screening. To evaluate the prediction value of our method, we perform four experiments based on real world data. The results demonstrate the high effectiveness of our method on heart failure risk assessment, with the best accuracy close to 90%.},
	author = {Chunjie Zhou and Ali Li and Aihua Hou and Zhiwang Zhang and Zhenxing Zhang and Pengfei Dai and Fusheng Wang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113361},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Heart failure, Early warning, Social network, Risk factors, Medical big data},
	pages = {113361},
	title = {Modeling methodology for early warning of chronic heart failure based on real medical big data},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030186X},
	volume = {151},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030186X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113361}}

@article{FAHFOUH2020113517,
	abstract = {Opinion review is of great importance for both customers and organizations. Indeed, it helps customers in buying decisions and represents a valuable feedback for the companies, allowing them to improve their productions. However, numerous greedy companies resort to fake reviews in order to influence the customer and brighten the brand image, or to defame the one of their competitors. Various models are proposed in order to detect deceptive opinion reviews. Most of these models adopt traditional methods focusing on feature extraction and traditional classifiers. Unfortunately, these models do not capture the semantic aspect while ignoring the opinion's context. In order to tackle this issue, we propose a new approach based on Paragraph Vector Distributed Bag of Words (PV-DBOW) and the Denoising Autoencoder (DAE). The proposed customized model provides a strong representation which is based on a global representation of the opinions while preserving their semantics. Indeed, the embedding vectors capture the semantic meaning of all words in the context of each opinion. The generated review representations are fed into a fully connected neural network in order to detect deceptive opinion spam. The obtained results concerning the deception dataset show that our model is effective and outperforms the existing state-of-the-art methodologies.},
	author = {Anass Fahfouh and Jamal Riffi and Mohamed {Adnane Mahraz} and Ali Yahyaouy and Hamid Tairi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113517},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive opinion spam, Neural networks, Machine learning, Deep learning, Paragraph vector model, Denoising autoencoder model},
	pages = {113517},
	title = {PV-DAE: A hybrid model for deceptive opinion spam based on neural network architectures},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420303419},
	volume = {157},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420303419},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113517}}

@article{JEONG2022118375,
	abstract = {Social network services have become widely used, and hashtags, which are implicitly involved in delivering specific information, have shown to greatly improve user engagement. A number of prior studies have attempted to recommend appropriate hashtags for each social media user considering his/her posts by consequently extracting the important features from text and images. To develop this multi-dimensionality with hashtag recommendation, user demographic information also plays a significant role in the manner of personalized hashtag recommendation. Thus, this paper proposes the demographic hashtag recommendation (DemoHash) model to utilize users' demographic information extracted from their selfie images, in addition to textual and visual information. The experimental results with the datasets from Instagram show that our proposed model achieves a greater performance with F1-score, Precision, and Recall than the existing hashtag recommendation methods by average of 4.19%, 18.45%, and 3.91%, respectively. Our approach effectively combined the content-based as well as user-oriented modeling for personalized hashtag recommendation.},
	author = {Dahye Jeong and Soyoung Oh and Eunil Park},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118375},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hashtag recommendation, Multi-modal model, Demographic information},
	pages = {118375},
	title = {DemoHash: Hashtag recommendation based on user demographic information},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014877},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014877},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118375}}

@article{BENSASSI2021115375,
	abstract = {Context-aware recommender systems have received considerable attention from industry and academic areas. In this paper, we pay heed to the growing interest in integrating context-awareness and multi-criteria decision making in recommender systems, to deal with the most pressing challenges in music recommender systems, namely the diversity of the recommended playlist, the scalability of the system, and the cold start problem. This paper introduces a new multi-criteria recommendation approach, named MORec, which generates Top-N music recommendations by bootstrapping the system using beforehand collected data. We usher by gauging the relevance of contextual information from the relation between three elements: user, music genre, and the user's context. Then, we apply an aggregation technique to uncover the relationship between the context and the overall rating. Besides, we apply the K-means algorithm to generate a predictive model that comprises clusters of similar contexts defining the association between contextual dimensions and music genres. Carried out experiments emphasize very promising results of our approach in terms of clustering quality, compared to the Partitioning Around Medoids algorithm in terms of connectivity and stability. The comparison versus pioneering recommendation baselines underscored the effectiveness of MORec in terms of recommendation quality and usefulness.},
	author = {Imen {Ben Sassi} and Sadok {Ben Yahia} and Innar Liiv},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115375},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, User-based Study, Multi-criteria recommendation, Context aware recommender system (CARS), Clustering, Music online recommendation (MORec)},
	pages = {115375},
	title = {MORec: At the crossroads of context-aware and multi-criteria decision making for online music recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008010},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421008010},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115375}}

@article{RAHIMI2022116518,
	abstract = {A limitation of many probabilistic topic models such as Latent Dirichlet Allocation (LDA) is their inflexibility to use local contexts. As a result, these models cannot directly benefit from short-distance co-occurrences, which are more likely to be indicators of meaningful word relationships. Some models such as the Bigram Topic Model (BTM) consider local context by integrating language and topic models. However, due to taking the exact word order into account, such models suffer severely from sparseness. Some other models like Latent Dirichlet Co-Clustering (LDCC) try to solve the problem by adding another level of granularity assuming a document as a bag of segments, while ignoring the word order. In this paper, we introduce a new topic model which uses overlapping windows to encode local word relationships. In the proposed model, we assume a document is comprised of fixed-size overlapping windows, and formulate a new generative process accordingly. In the inference procedure, each word is sampled once in only a single window, while influencing the sampling of its other fellow co-occurring words in other windows. Word relationships are discovered in the document level, but the topic of each word is derived considering only its neighbor words in a window, to emphasize local word relationships. By using overlapping windows, without assuming an explicit dependency between adjacent words, we avoid ignoring the word order completely. The proposed model is straightforward, not severely prone to sparseness and as the experimental results show, produces more meaningful and more coherent topics compared to the three mentioned established models.},
	author = {Marziea Rahimi and Morteza Zahedi and Hoda Mashayekhi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116518},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Probabilistic topic model, Latent Dirichlet Allocation, Document clustering, Context window, Local co-occurrence, Word order},
	pages = {116518},
	title = {A probabilistic topic model based on short distance Co-occurrences},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000197},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000197},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116518}}

@article{ALMUZAINI2022117384,
	abstract = {Every day the world produces an enormous amount of textual data. This unstructured text is of little use unless it is labeled using a combination of categories, keywords, tags. Humans can never annotate such massive data, and with a growing divide between the daily produced data and those annotated, the only alternative is to mechanize it. Automatic annotation process helps in saving resources in terms of time and cost. The process of multi-label annotation involves associating a document with multiple relevant labels. This paper proposes an unsupervised model to annotate corpus using multi-labels automatically. The model is based on multi-label topic modeling and genetic algorithm (GA). Topic modeling is a technique to extract the hidden topics from text, and the GA is used to find the optimal number of topics. We hyper-tuned the parameters of the topic modeling using two different training methods: variational Bayes and Gibbs sampling. The class imbalance in a corpus can affect the result of topic modeling, where the majority class dominates the minority class. We overcome this problem using the partitioning method. Though the proposed model was developed for the Arabic dataset, it is language neutral. We tested our model on three large Arabic corpora and three large English social media datasets. For the Arabic language, our work being the first work that tackles multi-label annotation, we needed a reference to compare our model. For the Arabic corpus, we compared the result of automatic annotation against humans using crowdsourcing (whose labeling was checked for quality). The analysis of the annotation shows an agreement among models (machine vs. human) of 79.30%. Moreover, for the English dataset, the results are quite competitive.},
	author = {Huda A. Almuzaini and Aqil M. Azmi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117384},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Arabic corpus, Topic modeling, Multi-label annotation, Genetic algorithm, Latent Dirichlet allocation, Crowdsourcing},
	pages = {117384},
	title = {An unsupervised annotation of Arabic texts using multi-label topic modeling and genetic algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422007266},
	volume = {203},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422007266},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117384}}

@article{CHRISTOPHE2021114831,
	abstract = {Identifying changes in the dynamics of a classification scheme is an important task to solve using textual data streams. Changes in the volume of documents classified into one category could be a sign of a new emerging structure, which therefore gives clues on the need to update the classification scheme. In this paper, we present a method based on forecasting techniques, change detection and time series monitoring in order to raise alerts as soon as a change occurs in the volume of a given category. We build features only based on the textual content that enable us to accurately predict the expected temporal evolution of such category. Then, we use statistical process control to determine if the current volume is too far away from the one we might expect. We test our method on the New York Times Annotated Corpus and on an industrial data set from Electricit{\'e} de France (EDF) and we observe that it raises alerts at the right time compared to other techniques from the literature.},
	author = {Cl{\'e}ment Christophe and Julien Velcin and Jairo Cugliari and Philippe Suignard and Manel Boumghar},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114831},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Change detection, Text streams, Forecasting},
	pages = {114831},
	title = {Change detection in textual classification with unexpected dynamics},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002724},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002724},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114831}}

@article{LI2021114585,
	abstract = {Currently, it is common to see untruthful opinions (also known as review spam, fraud or shilling attack) that resemble each other explicitly or implicitly across multiple business-to-customer websites or opinion sharing communities. Unfortunately, these fake recommendations can be fabricated by individual spammers or results of a manipulation campaign. Considering its severe harmfulness in influencing a product's reputation, grouped spam is more urgent to detect than individual fraud. Most state-of-the-art techniques of labeling grouped spam, e.g., Frequent Itemset Mining (FIM) or Latent Dirichlet Allocation (LDA), are completely unsupervised and incapable of making good use of officially recommended topics, such as appearance, speed and standby are three suggested aspects along a cell phone product in JD.com. In this paper, we introduce a novel approach based on aspect-oriented sentiment mining that can identify spam groups supported by nominated topics. Experiments show that our method is effective and outperforms several state-of-the-art solutions with statistical significance on two metrics, content duplication and burstiness of time.},
	author = {Jiandun Li and Pin Lv and Wei Xiao and Liu Yang and Pengpeng Zhang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114585},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Review spam, Opinion spam, Spam group, Nominated topic, Aspect-oriented sentiment},
	pages = {114585},
	title = {Exploring groups of opinion spam using sentiment analysis guided by nominated topics},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000269},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000269},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114585}}

@article{ARBANE2023118710,
	abstract = {Internet public social media and forums provide a convenient channel for people concerned about public health issues, such as COVID-19, to share and discuss information/misinformation with each other. In this paper, we propose a natural language processing (NLP) method based on Bidirectional Long Short-Term Memory (Bi-LSTM) technique to perform sentiment classification and uncover various issues related to COVID-19 public opinions. Bi-LSTM is an improved version of conventional LSTMs for generating the output from both left and right contexts at each time step. We experimented with real datasets extracted from Twitter and Reddit social media platforms, and our experimental results showed improved metrics compared with the conventional LSTM model as well as recent studies available in the literature. The proposed model can be used by official institutions to mitigate the effects of negative messages and to understand peoples' concerns during the pandemic. Furthermore, our findings shed light on the importance of using NLP techniques to analyze public opinion and to combat the spreading of misinformation and to guide health decision-making.},
	author = {Mohamed Arbane and Rachid Benlamri and Youcef Brik and Ayman Diyab Alahmar},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118710},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bi-LSTM, COVID-19, Deep learning, Natural language processing, Sentiment classification, Social media},
	pages = {118710},
	title = {Social media-based COVID-19 sentiment classification model using Bi-LSTM},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017353},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422017353},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118710}}

@article{KIM2020113401,
	abstract = {Blockchain has become one of the core technologies in Industry 4.0. To help decision-makers establish action plans based on blockchain, it is an urgent task to analyze trends in blockchain technology. However, most of existing studies on blockchain trend analysis are based on effort demanding full-text investigation or traditional bibliometric methods whose study scope is limited to a frequency-based statistical analysis. Therefore, in this paper, we propose a new topic modeling method called Word2vec-based Latent Semantic Analysis (W2V-LSA), which is based on Word2vec and Spherical k-means clustering to better capture and represent the context of a corpus. We then used W2V-LSA to perform an annual trend analysis of blockchain research by country and time for 231 abstracts of blockchain-related papers published over the past five years. The performance of the proposed algorithm was compared to Probabilistic LSA, one of the common topic modeling techniques. The experimental results confirmed the usefulness of W2V-LSA in terms of the accuracy and diversity of topics by quantitative and qualitative evaluation. The proposed method can be a competitive alternative for better topic modeling to provide direction for future research in technology trend analysis and it is applicable to various expert systems related to text mining.},
	author = {Suhyeon Kim and Haecheong Park and Junghye Lee},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113401},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Trend analysis, Topic modeling, Word2vec, Probabilistic latent semantic analysis, Blockchain},
	pages = {113401},
	title = {Word2vec-based latent semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology trend analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420302256},
	volume = {152},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420302256},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113401}}

@article{EFFROSYNIDIS2022117541,
	abstract = {This work creates and makes publicly available the most comprehensive dataset to date regarding climate change and human opinions via Twitter. It has the heftiest temporal coverage, spanning over 13 years, includes over 15 million tweets spatially distributed across the world, and provides the geolocation of most tweets. Seven dimensions of information are tied to each tweet, namely geolocation, user gender, climate change stance and sentiment, aggressiveness, deviations from historic temperature, and topic modeling, while accompanied by environmental disaster events information. These dimensions were produced by testing and evaluating a plethora of state-of-the-art machine learning algorithms and methods, both supervised and unsupervised, including BERT, RNN, LSTM, CNN, SVM, Naive Bayes, VADER, Textblob, Flair, and LDA.},
	author = {Dimitrios Effrosynidis and Alexandros I. Karasakalidis and Georgios Sylaios and Avi Arampatzis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117541},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Climate change, Machine learning, Sentiment analysis, Topic modeling, Twitter},
	pages = {117541},
	title = {The climate change Twitter dataset},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422008624},
	volume = {204},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422008624},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117541}}

@article{FANG2021114306,
	abstract = {The purpose of this paper is to develop a technology-based model for identifying various criteria in a decision-making situation. We used topic modeling to discover critical criteria and their corresponding weights in the Analytic Hierarchy Process (AHP). Approximately 100,000 hotel reviews and 100,000 restaurant reviews were scraped from TripAdvisor.com for criteria determination. Next, an AHP model with criteria and 12 hotels/restaurants as alternatives were compared and ranked. The results compared favorably with more than 1000 reviews of these hotels/restaurants in TripAdvisor.com, thus validating the methodology.},
	author = {Jin Fang and Fariborz Y. Partovi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114306},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Analytic hierarchy process, Topic model, Latent Dirichlet allocation, Hotel selection, Restaurant selection, Group decision},
	pages = {114306},
	title = {Criteria determination of analytic hierarchy process using a topic model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310046},
	volume = {169},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420310046},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114306}}

@article{HUSSAIN2022118119,
	abstract = {Aspect-based sentiment analysis (ABSA) has gained a rising concentration recently. It aims to provide a set of aspect terms and sentiments from a piece of text. Educational Data Mining (EDM) is now an essential tool for analysing pedagogical data. In academic institutions, student feedback is an influential gauge to measure the quality of the teaching--learning process. It helps higher education institutions to reconsider and improve their policies for student recruitment and retention. This paper proposed a situation awareness multi-layer topic modelling and enhanced hybrid machine learning approach for evaluating students' textual feedback data in academic institutions. The proposed Aspect2Labels (A2L) approach is divided our system into three layers. To preserve semantic information, we extracted general aspects terms in the first layer known as high-level aspects. We pulled low-level aspects terms associated with high-level aspect terms in the second layer and the third layer used for sentiment orientation. We used zero-shot learning, LDA, and different variants of LDA for the aspect extraction process. We performed annotation on unlabelled students' comments using our proposed A2L approach, and we obtained 91.3% accuracy in this process. We developed and tested novel algorithms for aspect terms mapping to label each aspect term to corresponding feedback. Different machine learning algorithms have been used to classify sentiments according to extracted aspects. We have also proposed and used Variable Global Feature Selection Scheme (VGFSS) and Variable Stopwords Filtering (VSF) to improve the performance of classifiers. We have managed to get 97% and 93% accuracy on the test dataset using Support Vector Machine (SVM) and Artificial Neural Networks (ANN), respectively. We highly suggest that our novel approach of aspect-oriented sentiment analysis could provide adequate understanding to analyse students' feedback.},
	author = {Shabir Hussain and Muhammad Ayoub and Ghulam Jilani and Yang Yu and Akmal Khan and Junaid Abdul Wahid and Muhammad Farhan Ali Butt and Guangqin Yang and Dietmar P.F. Moller and Hou Weiyan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118119},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect-based sentiment analysis, Machine learning applications in education, Aspect extraction, Topic modelling, Opinion mining, Situational awareness},
	pages = {118119},
	title = {Aspect2Labels: A novelistic decision support system for higher educational institutions by using multi-layer topic modelling approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013045},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422013045},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118119}}

@article{CAO2020113465,
	abstract = {Electronic commerce has become a popular shopping mode. To enhance their reputations, attract more customers, and finally obtain more benefits, dishonest sellers often recruit buyers or robots to post a large number of deceptive reviews to mislead users. According to the interpretability of learning results, existing methods for detecting deceptive reviews can be mainly divided into explicit feature-based mining ones and neural network-based implicit feature mining ones. The nature of these works is accurate text classification based on coarse-grained features (e.g., topic, sentence, and document) or fine-grained features (e.g., word). To take full merits of existing approaches, this paper proposes a new framework that explores a method to combine the coarse-grained features and the fine-grained features. In this framework, the coarse-grained implicit semantic features of the topic distribution are learned by the concatenation of a Latent Dirichlet Allocation (LDA) topic model and a 2-layered neural network. The fine-grained implicit semantic features from the word vectors representation of the reviews are parallelly learned by a deep learning framework. Finally, these two granular features are combined and adopted to train a Support Vector Machine (SVM) classifier for detecting whether a review is deceptive or not. To verify the effectiveness and performance of this framework, we derive three models by specifying three popular deep learning models, such as TextCNN, long short-term memory (LSTM), and Bi-directional LSTM (BiLSTM) to learn the fine-grained features. Experimental results on a mixed-domain dataset and balanced/unbalanced in-domain datasets show that all the combination models are superior to the corresponding baseline models considering single features.},
	author = {Ning Cao and Shujuan Ji and Dickson K.W. Chiu and Mingxiang He and Xiaohong Sun},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113465},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive reviews detection, LDA topic model, Deep learning, Coarse-grained features, Fine-grained features},
	pages = {113465},
	title = {A deceptive review detection framework: Combination of coarse and fine-grained features},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030289X},
	volume = {156},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030289X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113465}}

@article{KIM2022117983,
	abstract = {Given the exponential growth of patent documents, automatic patent summarization methods to facilitate the patent analysis process are in strong demand. Recently, the development of natural language processing (NLP), text-mining, and deep learning has greatly improved the performance of text summarization models for general documents. However, existing models cannot be successfully applied to patent documents, because patent documents describing an inventive technology and using domain-specific words have many differences from general documents. To address this challenge, we propose in this study a multi-patent summarization approach based on deep learning to generate an abstractive summarization considering the characteristics of a patent. Single patent summarization and multi-patent summarization were performed through a patent-specific feature extraction process, a summarization model based on generative adversarial network (GAN), and an inference process using topic modeling. The proposed model was verified by applying it to a patent in the drone technology field. In consequence, the proposed model performed better than existing deep learning summarization models. The proposed approach enables high-quality information summary for a large number of patent documents, which can be used by R&D researchers and decision-makers. In addition, it can provide a guideline for deep learning research using patent data.},
	author = {Sunhye Kim and Byungun Yoon},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117983},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Patent summarization, Generative adversarial network (GAN), Patent analysis, Natural language processing (NLP), Text mining},
	pages = {117983},
	title = {Multi-document summarization for patent documents based on generative adversarial network},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012118},
	volume = {207},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422012118},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117983}}

@article{ZHOU2022116194,
	abstract = {Projection-based methods for generating high-quality Cross-Lingual Embeddings (CLEs) have shown state-of-the-art performance in many multilingual applications. Supervised methods that rely on character-level information or unsupervised methods that need only monolingual information are both popular and have their pros and cons. However, there are still problems in terms of the quality of monolingual word embedding spaces and the generation of the seed dictionaries. In this work, we aim to generate effective CLEs with auxiliary Topic Models. We utilize both monolingual and bilingual topic models in the procedure of generating monolingual embedding spaces and seed dictionaries for projection. We present a comprehensive evaluation of our proposed model through the means of bilingual lexicon extraction, cross-lingual semantic word similarity and cross-lingual document classification tasks. We show that our proposed model outperforms existing supervised and unsupervised CLE models built on basic monolingual embedding spaces and seed dictionaries. It also exceeds CLE models generated from representative monolingual topical word embeddings.},
	author = {Dong Zhou and Xiaoya Peng and Lin Li and Jun-mei Han},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116194},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Cross-lingual embeddings, Topical models, Word embedding models, Projection-based methods, Seed dictionaries},
	pages = {116194},
	title = {Cross-lingual embeddings with auxiliary topic models},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421015116},
	volume = {190},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421015116},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116194}}

@article{ZOTOVA2021114547,
	abstract = {Popular social media networks provide the perfect environment to study the opinions and attitudes expressed by users. While interactions in social media such as Twitter occur in many natural languages, research on stance detection (the position or attitude expressed with respect to a specific topic) within the Natural Language Processing field has largely been done for English. Although some efforts have recently been made to develop annotated data in other languages, there is a telling lack of resources to facilitate multilingual and crosslingual research on stance detection. This is partially due to the fact that manually annotating a corpus of social media texts is a difficult, slow and costly process. Furthermore, as stance is a highly domain- and topic-specific phenomenon, the need for annotated data is specially demanding. As a result, most of the manually labeled resources are hindered by their relatively small size and skewed class distribution. This paper presents a method to obtain multilingual datasets for stance detection in Twitter. Instead of manually annotating on a per tweet basis, we leverage user-based information to semi-automatically label large amounts of tweets. Empirical monolingual and cross-lingual experimentation and qualitative analysis show that our method helps to overcome the aforementioned difficulties to build large, balanced and multilingual labeled corpora. We believe that our method can be easily adapted to easily generate labeled social media data for other Natural Language Processing tasks and domains.},
	author = {Elena Zotova and Rodrigo Agerri and German Rigau},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114547},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stance detection, Multilingualism, Text categorization, Fake news, Deep learning},
	pages = {114547},
	title = {Semi-automatic generation of multilingual datasets for stance detection in Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742031191X},
	volume = {170},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742031191X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114547}}

@article{JEYARAJ2022115896,
	abstract = {With overwhelming volumes of official emails being exchanged in enterprises every day, emails have become vital information storehouses. Automatic generation of FAQs from email systems helps in identifying important information and could serve potential applications such as chatbots and intelligent email answering. While there exist studies in the literature focusing on automatic FAQ generation and automated email answering, there are few studies that apply recently developed deep learning techniques to fetch FAQs from emails. This paper proposes a novel framework named F-Gen, which is an expert system that generates potential FAQs from emails utilizing state-of-the-art methodologies. The key characteristics of this study are as follows: 1. Designing F-Gen with various subsystems that interoperate together for the FAQ generation 2. Identifying the parameters that determine a valid FAQ. The three subsystems of F-Gen are: (a) query classifier subsystem (QC subsystem) for email texts, (b) FAQ group generator subsystem (FGG subsystem) for generating FAQ groups from email queries. And (c) FAQ generator subsystem (FG subsystem) for conversion of email query clusters into FAQs. Experiments on the email dataset that practically reflect the above-mentioned problem resulted in FAQs with a ROUGE-1 F-Score of 74.10% when compared with the ground truth.},
	author = {Shiney Jeyaraj and Raghuveera T.},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115896},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Expert system, Deep learning applications, Email text mining, Information retrieval, FAQ generation},
	pages = {115896},
	title = {A deep learning based end-to-end system (F-Gen) for automated email FAQ generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421012525},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421012525},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115896}}

@article{JOSHI2022116846,
	abstract = {In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we formulate a novel strategy to arrange them in their order of importance. A graph-based strategy is applied to find the significant keywords and related sentence rankings in the document. We also formulate a sentence novelty measure based on bigrams, trigrams, and sentence embeddings to eliminate redundant sentences from the summary. The ranks of all the sentences -- computed for each feature -- are finally fused to get the final score for each sentence in the document. We evaluate our approach on publicly available summarization datasets --- CNN/DailyMail and DUC 2002. Experimental results show that our approach outperforms other existing state-of-the-art summarization methods.},
	author = {Akanksha Joshi and Eduardo Fidalgo and Enrique Alegre and Rocio Alaiz-Rodriguez},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116846},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text summarization, Extractive, Topic, Embeddings, Keywords},
	pages = {116846},
	title = {RankSum---An unsupervised extractive text summarization based on rank fusion},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422002998},
	volume = {200},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422002998},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116846}}

@article{MA2023118695,
	abstract = {Mental models play a crucial role in explaining and driving human innovation activities. To help researchers clarify the changes of mental models in various innovation situations, an exploration of its topic dynamic evolution changes is urgently needed. However, most existing works have discussed the topic-semantic distributions of collected documents along the overall timeline, which ignores the semantic details of fusion and evolution between topics in continuous time. This paper discovers and reveals the multi-level information evolving of topics, by integrating latent Dirichlet allocation (LDA) and Word2vec harmoniously to generate the topic evolution maps of the corpus from global to local perspectives. Which include topic distribution trends and their dynamic evolution under the overall time series, as well as the merging and splitting of semantic information between topics in the adjacent time span. These reveal the correlation between topics and the full life cycle of a topic emerging, developing, maturing, and fading. Then, the integrated method was used to perform an analysis of topic evolution with 3984 abstracts of mental model-related papers published between 1980 and 2020. Finally, the performance of the proposed method was compared to that of three traditional topic evolution generated methods based on the standard evaluation metrics. The experimental results demonstrated that our method outperforms other methods both in terms of the content and strength of topic evolution. The proposed method could mine the latent evolution information more clearly and comprehensively from a vast number of papers and is also suited to the various applications of expert systems related to information mining works.},
	author = {Jian Ma and Lei Wang and Yuan-Rong Zhang and Wei Yuan and Wei Guo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118695},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Mental models, Topic evolution, Word2vec, Semantic correlation},
	pages = {118695},
	title = {An integrated latent Dirichlet allocation and Word2vec method for generating the topic evolution of mental models from global to local},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017250},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422017250},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118695}}

@article{GREGORIADES2021115546,
	abstract = {This paper presents a machine learning approach involving tourists' electronic word of mouth (eWOM) to support destination marketing campaigns. This approach enhances optimisation of a critical aspect of marketing campaigns, that is, the communication of the right content to the right consumers. The proposed method further considers aggregate cultural and economic-related information of the tourists' country of origin with topic modelling and Decision Tree (DT) models. Each DT addresses different dimensions of culture and purchasing power and the way these dimensions are associated with the topics discussed in eWOM, thus revealing patterns relating tourists' experiences with potential explanations for their dissatisfaction/satisfaction. The method is implemented in a case study in the context of tourism in Cyprus focusing on two hotel groups (2/3 and 4/5 stars) to account for their differences. Patterns emerged from the extraction of rules from DTs illuminate combinations of variables associated with tourist experience (negative or positive) for each of the two hotel categories and verify the asymmetric relationship between service performance and satisfaction. The approach can be used by management during marketing campaigns to design messages to better address the desires and needs of tourists from different cultural and economic backgrounds, as these emerge from the data analysis.},
	author = {Andreas Gregoriades and Maria Pampaka and Herodotos Herodotou and Evripides Christodoulou},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115546},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic modelling, Cultural and economic distance, Decision trees, Shapley additive explanation, Tourists' reviews},
	pages = {115546},
	title = {Supporting digital content marketing and messaging through topic modelling and decision trees},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421009532},
	volume = {184},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421009532},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115546}}

@article{LIU2022116741,
	abstract = {The focused crawler based on semantic analysis is a research hotspot in the field of information retrieval. The domain ontology is generally applied to construct the topic model of the focused crawler. In order to overcome the limitations of builders' knowledge reserve and subjective consciousness in the process of constructing artificially ontology, a semi-automatic construction method of domain ontology based on ontology learning technology combining the latent Dirichlet allocation and the Apriori algorithm is proposed in this article. When evaluating the relevance between a hyperlink and a specific topic, the joint evaluation method considering both the web text and the link structure is usually used. However, the traditional weighted sum method is difficult to reasonably determine the optimal weights of these evaluating indicators. To solve this problem, a multi-objective optimization model for link evaluation and a subsequent multi-objective ant colony optimization algorithm (MOACO) are proposed. In the MOACO, a method of the nearest farthest candidate solution (NFCS) is combined with the fast non-dominated sorting to select a set of Pareto-optimal hyperlinks and guide the crawlers' search directions. The experimental results of the focused crawling on the domain knowledge of typhoon disasters and rainstorm disasters prove that the ability of the proposed focused crawlers to retrieve topic-relevant webpages.},
	author = {Jingfa Liu and Yi Dong and Zhaoxia Liu and Duanbing Chen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116741},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Focused crawler, Multi-objective ant colony optimization, Ontology, Ontology learning},
	pages = {116741},
	title = {Applying ontology learning and multi-objective ant colony optimization method for focused crawling to meteorological disasters domain knowledge},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742200210X},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742200210X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116741}}

@article{HARALABOPOULOS2021114769,
	abstract = {Text has traditionally been used to train automated classifiers for a multitude of purposes, such as: classification, topic modelling and sentiment analysis. State-of-the-art LSTM classifier require a large number of training examples to avoid biases and successfully generalise. Labelled data greatly improves classification results, but not all modern datasets include large numbers of labelled examples. Labelling is a complex task that can be expensive, time-consuming, and potentially introduces biases. Data augmentation methods create synthetic data based on existing labelled examples, with the goal of improving classification results. These methods have been successfully used in image classification tasks and recent research has extended them to text classification. We propose a method that uses sentence permutations to augment an initial dataset, while retaining key statistical properties of the dataset. We evaluate our method with eight different datasets and a baseline Deep Learning process. This permutation method significantly improves classification accuracy by an average of 4.1%. We also propose two more text augmentations that reverse the classification of each augmented example, antonym and negation. We test these two augmentations in three eligible datasets, and the results suggest an -averaged, across all datasets-improvement in classification accuracy of 0.35% for antonym and 0.4% for negation, when compared to our proposed permutation augmentation.},
	author = {Giannis Haralabopoulos and Mercedes Torres Torres and Ioannis Anagnostopoulos and Derek McAuley},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114769},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text, Augmentation, Multilabel, Multiclass, LSTM},
	pages = {114769},
	title = {Text data augmentations: Permutation, antonyms and negation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002104},
	volume = {177},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002104},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114769}}

@article{DARGAHINOBARI2021114303,
	abstract = {Telegram is a new Instant Messaging application providing key features for both public and private messaging. Telegram is similar to group broadcast or micro-blogging platforms, while on the other hand, it has features of ordinary Instant Messaging applications such as WhatsApp. In this paper, investigating a real dataset crawled from Telegram, we provide several observations which can explain the information flow, business model of content providers, and social sensing aspects of Telegram. The crawled dataset which is manually labeled by six persons contains two months of public messages of selected Telegram channels. Moreover, we introduce the viral messages in instant messaging services and propose formal definition of these messages as well as deeply analyzing their characteristics and features. Detection of virality characteristics of messages in Telegram can be beneficial for both end-users and digital marketers. Consequently, we propose statistical and word embedding approaches to detect viral messages and their sentiment and message category.Our experiments indicate that the word embedding approach can significantly outperform other baseline models.},
	author = {Arash {Dargahi Nobari} and Malikeh Haj Khan Mirzaye Sarraf and Mahmood Neshati and Farnaz {Erfanian Daneshvar}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114303},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Telegram, Instant messaging, Sentiment analysis, Social sensing, Viral message},
	pages = {114303},
	title = {Characteristics of viral messages on Telegram; The world's largest hybrid public and private messenger},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310010},
	volume = {168},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420310010},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114303}}

@article{VANDINTER2021115261,
	abstract = {The systematic literature review (SLR) process includes several steps to collect secondary data and analyze it to answer research questions. In this context, the document retrieval and primary study selection steps are heavily intertwined and known for their repetitiveness, high human workload, and difficulty identifying all relevant literature. This study aims to reduce human workload and error of the document retrieval and primary study selection processes using a decision support system (DSS). An open-source DSS is proposed that supports the document retrieval step, dataset preprocessing, and citation classification. The DSS is domain-independent, as it has proven to carefully select an article's relevance based solely on the title and abstract. These features can be consistently retrieved from scientific database APIs. Additionally, the DSS is designed to run in the cloud without any required programming knowledge for reviewers. A Multi-Channel CNN architecture is implemented to support the citation screening process. With the provided DSS, reviewers can fill in their search strategy and manually label only a subset of the citations. The remaining unlabeled citations are automatically classified and sorted based on probability. It was shown that for four out of five review datasets, the DSS's use achieved significant workload savings of at least 10%. The cross-validation results show that the system provides consistent results up to 88.3% of work saved during citation screening. In two cases, our model yielded a better performance over the benchmark review datasets. As such, the proposed approach can assist the development of systematic literature reviews independent of the domain. The proposed DSS is effective and can substantially decrease the document retrieval and citation screening steps' workload and error rate.},
	author = {Raymon {van Dinter} and Cagatay Catal and Bedir Tekinerdogan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115261},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Systematic literature review (SLR), Citation screening, Document retrieval, Decision support, Automation, Deep learning, Convolutional neural network, Natural language processing},
	pages = {115261},
	title = {A decision support system for automating document retrieval and citation screening},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100693X},
	volume = {182},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100693X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115261}}

@article{MEILIAN2020113427,
	abstract = {Learning the low-dimensional vector representation of networks can effectively reduce the complexity of various network analysis tasks, such as link prediction, clustering and classification. However, most of the existing network representation learning (NRL) methods are aimed at homogeneous or static networks, while the real-world networks are usually heterogeneous and tend to change dynamically over time, therefore providing an intelligent insight into the evolution of heterogeneous networks is more practical and significant. Based on this consideration, we focus on the dynamic representation learning problem for heterogeneous information networks, and propose a random walk based Dynamic Representation Learning method for Heterogeneous Information Networks (HIN_DRL), which can learn the representation of network nodes at different timestamps. Specifically, we improve the first step of the existing random walk based NRL methods, which generally include two steps: constructing node sequences through random walk process, and then learning node representations by throwing the node sequences into a homogeneous or heterogeneous Skip-Gram model. In order to construct optimized node sequences for evolving heterogeneous networks, we propose a method for automatically extracting and extending meta-paths, and propose a new method for generating node sequences via dynamic random walk based on meta-path and timestamp information of networks. We also propose two strategies for adjusting the quantity and length of node sequences during each random walk process, which makes it more effective to construct the node sequences for heterogeneous information networks at a specific timestamp, thus improving the effect of dynamic representation learning. Extensive experimental results show that compared with the state-of-art algorithms, HIN_DRL achieves better results in Macro-F1, Micro-F1 and NMI for multi-label node classification, multi-class node classification and node clustering on several real-world network datasets. Furthermore, case studies of visualization and dynamic on Microsoft Academic dataset demonstrate that HIN_DRL can learn network representation dynamically and more effectively.},
	author = {LU Meilian and YE Danna},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113427},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Dynamic representation learning, Heterogeneous information networks, Meta path, Dynamic random walk},
	pages = {113427},
	title = {HIN_DRL: A random walk based dynamic network representation learning method for heterogeneous information networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420302517},
	volume = {158},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420302517},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113427}}

@article{WAHID2022116562,
	abstract = {The abundant use of social media impacts every aspect of life, including crisis management. Disaster management needs real-time data to be used in machine learning and deep learning models to aid their decision making. Mostly the data that is newly generated from social media is unstructured and unlabeled. Current text classification models based on supervised deep learning models heavily rely on human-labeled data that very small size and imbalanced in the context of disasters, ultimately affecting the generalization of models. In this study, we propose Topic2labels (T2L) framework which provides an automated way of labeling the data through LDA (latent dirichlet allocation) topic modeling approach and utilize Bert (the bidirectional encoder representation from transformer) embeddings for construction of feature vector to be employed to classify the data contextually. Our framework consists of three layers. In the first layer, we adopt LDA to generate the topics from the data, and develop a new algorithm to rank the topics, and map the highest ranked dominant topic into label to annotate the data. In the second layer, we transform the labeled text into feature representation through Bert embeddings and in the third layer we leveraged deep learning models as classifiers to classify the textual data into multiple categories. Experimental results on crisis-related datasets show that our framework performs better in terms of classification performance and yields improvement as compared to other baseline approaches.},
	author = {Junaid Abdul Wahid and Lei Shi and Yufei Gao and Bei Yang and Lin Wei and Yongcai Tao and Shabir Hussain and Muhammad Ayoub and Imam Yagoub},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116562},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social media, Natural language processing, Neural network, Topic modeling, Annotation, Classification, Transformer, Crisis response},
	pages = {116562},
	title = {Topic2Labels: A framework to annotate and classify the social media data through LDA topics and deep learning models for crisis response},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000604},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000604},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116562}}

@article{FALLAHNEJAD2022116433,
	abstract = {The growing popularity of community question answering websites can be seen by the growing number of users. Many methods are proposed to identify talented users in these communities, but many of them suffer from vocabulary mismatches. The solution to this problem can be found in translation approaches. The present paper proposes two translation methods for extracting more relevant translations. The proposed methods rely on the attention mechanism. The methods use multi-label classifiers that take each question as input and predict the skills related to the question. Using the attention mechanism, the model is able to focus on specific parts of the given input and predict the correct labels. The ultimate goal of these networks is to predict skills related to questions. Using word attention scores, we can find out how relevant a single word is to a particular skill. As a result of these attention scores, we obtain more relevant translations for each skill. We then use these translations to bridge the lexical gap and improve expert retrieval results. Extensive experiments on two large sub-collections of the StackOverflow dataset demonstrate that the proposed methods outperform the best baseline method by up to 14.11/% MAP improvement.},
	author = {Zohreh Fallahnejad and Hamid Beigy},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116433},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Expert finding, Semantic matching, Translation models, StackOverflow},
	pages = {116433},
	title = {Attention-based skill translation models for expert finding},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421017206},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421017206},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116433}}

@article{DAU2020112871,
	abstract = {The main goal of Aspect-Based Opinion Mining is to extract product's aspects and the associated user opinions from the user text review. Although this serves as vital source information for enhancing rating prediction performance, few studies have attempted to fully utilize it for better accuracy of recommendation systems. Most of these studies typically assign equal weights to all aspects in the opinion mining process, however, in practices; users tend to give different priority on different aspects of the product when reaching overall ratings. In addition, most of the existing methods typically rely on handcrafted, rule-based or double propagation methods in the opinion mining process which are known to be time-consuming and often inclined to errors. This could affect the reliability and performance of the recommender systems (RS). Therefore, in this paper, we propose a weighted Aspect-based Opinion mining using Deep learning method for Recommender system (AODR) that can extract product's aspects and the underlying weighted user opinions from the review text using a deep learning method and then fuse them into extended collaborative filtering (CF) technique for improving the RS. The proposed method is basically comprised of two components: (1) Aspect-based opinion mining module which aims to extract the product aspects from the review text to generate aspect rating matrix. (2) Recommendation generation component that uses tensor factorization (TF) technique to compute weighted aspect ratings and finally infer the overall rating prediction. We evaluate the proposed model in terms of both aspect extraction and recommendation performance. Experiment results on different datasets show that our AODR model achieves better results compared to the baselines.},
	author = {Aminu Da'u and Naomie Salim and Idris Rabiu and Akram Osman},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112871},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect-based opinion mining, Convolutional neural network, Deep learning, Collaborative filtering, Recommender system, Rating prediction},
	pages = {112871},
	title = {Weighted aspect-based opinion mining using deep learning for recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419305810},
	volume = {140},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419305810},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112871}}

@article{CATELLI2022118290,
	abstract = {Today, reviews are the advertising medium par excellence through which companies are able to influence customers' spending decisions. Although the initial purpose of reviews was to provide companies with a feedback tool to improve products and services based on customer needs, they soon became a way to climb the sales rankings, often illegally. In fact, deceptive and fake reviews have managed to evade the often non-existent means of validation of online platforms, proliferating a new business. To combat this phenomenon, several classification methods have been developed to train automated tools in the arduous task of distinguishing between genuine and misleading reviews, the most recent based on machine and deep learning techniques. This paper proposes a multi-label classification methodology based on the Google BERT neural language model to build a deceptive review detector aided by its sentiment awareness: improved modeling of the link between sentiment polarity and deceptiveness during the fine-tuning phase by exploiting the Binary Cross Entropy with Logits loss function adds to the advantages provided by pre-trained contextual models, which are able to capture word polysemy through word embeddings and benefit from pre-training on huge corpora. Tests were performed on the Deceptive Opinion Spam Corpus and Yelp New York City datasets, providing a quantitative and qualitative analysis of the results which, when compared with the state of the art available in the literature, showed an encouraging increase in performance.},
	author = {Rosario Catelli and Hamido Fujita and Giuseppe {De Pietro} and Massimo Esposito},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118290},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive reviews, Sentiment, BERT, Multi-label, Deep learning, Neural language model},
	pages = {118290},
	title = {Deceptive reviews and sentiment polarity: Effective link by exploiting BERT},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014269},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014269},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118290}}

@article{GOMEZ2022118400,
	abstract = {The recent pandemic has changed the way we see education. During recent years, Massive Open Online Course (MOOC) providers, such as Coursera or edX, are reporting millions of new users signing up on their platforms. Though online review systems are standard among many verticals, no standardized or fully decentralized review systems exist in the MOOC ecosystem. In this vein, we believe that there is an opportunity to leverage available open MOOC reviews in order to build simpler and more transparent reviewing systems, allowing users to really identify the best courses out there. Specifically, in our research we analyze 2.4 million reviews (which is the largest MOOC reviews dataset used until now) from five different platforms in order to determine the following: (1) if the numeric ratings provide discriminant information to learners, (2) if NLP-driven sentiment analysis on textual reviews could provide valuable information to learners, (3) if we can leverage NLP-driven topic finding techniques to infer themes that could be important for learners, and (4) if we can use these models to effectively characterize MOOCs based on the open reviews. Results show that numeric ratings are clearly biased (63% of them are 5-star ratings), and the topic modeling reveals some interesting topics related with course advertisements, the real applicability, or the difficulty of the different courses.},
	author = {Manuel J. Gomez and Mario Calder{\'o}n and Victor S{\'a}nchez and F{\'e}lix J. Garc{\'\i}a Clemente and Jos{\'e} A. Ruip{\'e}rez-Valiente},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118400},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Massive Open Online Courses, Natural language processing, Sentiment analysis, Recommendation systems, Online education},
	pages = {118400},
	title = {Large scale analysis of open MOOC reviews to support learners' course selection},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015081},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015081},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118400}}

@article{LEBENA2022117303,
	abstract = {In this work, we cope with the classification of Electronic Health Records (EHR) in Spanish according to the International Classification of Diseases (ICD). We employ Topic Models representing each document as a probabilistic distribution over topics, offering a low-dimensional representation of documents. The trend is to turn to an embedding text representation, but these approaches require large amounts of textual data. We found Topic Models as a suitable alternative approach to deal with the few resources available for Spanish clinical text mining. Besides, they are interpretable and aid the explainability in artificial intelligence (XAI). We explored two different methods, known as Latent Dirichlet Allocation (LDA) and Partially Labelled Latent Dirichlet Allocation (PLDA), the supervised approach of the former. We assessed the results attained in Spanish with an analogous task in English as a reference. Evaluation methods were applied directly to the representation, with metrics to determine topic coherence and the relationship between topics and ICD labels. We learned that PLDA was able to discover topics associated with the ICD. This finding means that this representation itself can reveal ICD codes previous to classification. Also, this representation was used as predictive features to feed a conventional classifier to show their competence in a downstream task. We conclude that in a context with a lack of big data availability, PLDA emerges as a versatile candidate, able to offer a competitive representation of EHRs. While other works are primarily concerned with supervised categorization and do not pay attention to the representation, LDA and PLDA offer an interpretable approach that can be associated with ICDs. Moreover, compared with those that employ LDA, we demonstrate how its' supervised version, PLDA, can be more intuitive as it shows a closer relation with the ICDs.},
	author = {Nuria Lebe{\~n}a and Alberto Blanco and Alicia P{\'e}rez and Arantza Casillas},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117303},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-label classification, Document classification, Electronic Health Records, ICD classification, Topic models, Partially labelled dirichlet allocation},
	pages = {117303},
	title = {Preliminary exploration of topic modelling representations for Electronic Health Records coding according to the International Classification of Diseases in Spanish},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422006662},
	volume = {204},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422006662},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117303}}

@article{ALDUNATE2022118309,
	abstract = {It is of utmost importance for marketing academics and service industry practitioners to understand the factors that influence customer satisfaction. This study proposes a novel framework to analyze open-ended survey data and extract drivers of customer satisfaction. This is done automatically via deep learning models for natural language processing. According to 11 drivers acknowledged by the marketing literature to determine customer experience, the data is cast into a multi-label classification problem. This expert system not only supports the automatic analysis of new data but also ranks the drivers according to their importance to various service industries and provides important insights into their applications. Experiments carried out using 25,943 customer survey responses related to 39 service companies in 13 different economic sectors show that the drivers can be identified accurately.},
	author = {{\'A}ngeles Aldunate and Sebasti{\'a}n Maldonado and Carla Vairetti and Guillermo Armelini},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118309},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Analytics, Customer satisfaction, Customer feedback, Natural language processing, Deep learning, BERT},
	pages = {118309},
	title = {Understanding customer satisfaction via deep learning and natural language processing},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014397},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014397},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118309}}

@article{SRINIVASARAO2022116475,
	abstract = {Email messaging is the most common way of providing effective communication between internauts. Consequently, the total sent and received emails count will be increased. But, the internaut can't remember all such emails. Even though email thread identification approaches give satisfactory benefits to the internauts, but they may fail to alert them for a cause to identify the sentiments behind an email thread. To address, this issue Probabilistic Latent Semantic Analysis clustering algorithm has been used in this paper to identify the email sentiment thread sequence. The sentiment and the thread sequence within the emails have been discovered as clustering sentiment polarity and temporal categories with the help of PLSA clusters. At the initial stage, we used three feature extraction methods, latent semantic analysis (LDA), bag of words (BoW), TF-IDF and SentiWordNet (SWN) lexicon for generating sentiment features of email. Next, Probabilistic Latent Semantic Analysis algorithm is used to form email clusters based on sentiment features. Thus, it helps to identify thread sentiment and sequence of sentiment threads. Email threads give a mechanism by which any user will be able to find out the sequence in the thread on the basis of sentiment analysis of email related to a specific set of communication during a specific time period. Various parameters evaluation measures have been considered in this work to evaluate the proposed model such as accuracy, precision, recall and F-measure, and the proposed algorithm is compared with other standard algorithms. Furthermore, a statistical test has also been performed.},
	author = {Ulligaddala Srinivasarao and Aakanksha Sharaff},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116475},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Sentiment email clusters, Probabilistic latent semantic analysis, Topic modeling, SWN lexicon, Sentiment sequence of threads},
	pages = {116475},
	title = {Email thread sentiment sequence identification using PLSA clustering algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421017553},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421017553},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116475}}

@article{SKRJANC2022117881,
	abstract = {In this paper, we present an evolving data-based approach to automatically cluster Twitter users according to their behavior. The clustering method is based on the Gaussian probability density distribution combined with a Takagi--Sugeno fuzzy consequent part of order zero (eGauss0). This means that this method can be used as a classifier that is actually a mapping from the feature space to the class label space. The eGauss method is very flexible, is computed recursively, and the most important thing is that it starts learning ``from scratch''. The structure adapts to the new data using adding and merging mechanisms. The most important feature of the evolving method is that it can process data from thousands of Twitter profiles in real time, which can be characterized as a Big Data problem. The final clusters yield classes of Twitter profiles, which are represented as different activity levels of each profile. In this way, we could classify each member as ordinary, very active, influential and unusual user. The proposed method was also tested on the Iris and Breast Cancer Wisconsin datasets and compared with other methods. In both cases, the proposed method achieves high classification rates and shows competitive results.},
	author = {Igor {\v S}krjanc and Goran Andonovski and Jos{\'e} Antonio Iglesias and Mar{\'\i}a Paz Sesmero and Araceli Sanchis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117881},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Evolving clustering, Twitter data analysis, Online method, Gaussian probability},
	pages = {117881},
	title = {Evolving Gaussian on-line clustering in social network analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422011320},
	volume = {207},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422011320},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117881}}

@article{CHEN2022116574,
	abstract = {Recommending the appropriate APIs from a large volume of Open APIs to application developers both accurately and efficiently has become a challenging problem. Established work usually takes only one feature of Open APIs into account, which decreases the accuracy of recommendations. In order to overcome this problem, we propose an ensemble-based approach to Open APIs recommendation with a multiple feature model, which integrates both machine learning and deep learning and synthesizes a set of multiple features to accurately make the recommendation. This approach employs One-hot, Word2vec similarity and Matrix Factorization techniques to obtain the multiple features information respectively, then concatenates the features to obtain a Multivariate Information Feature (MIF) matrix, and leverages an optimized Gradient Boosting Decision Tree (GBDT) and Gated Recurrent Unit (GRU) for feature selection. GBDT is good at processing dense numerical features, while GRU is good at processing sparse categorical features, Finally, the results are synthesized to obtain a recommendation result. We have compared our approach with other four Open APIs recommendation approaches on the Programmable Web, and verified the effectiveness of our approach in precision, recall, F1-measure.},
	author = {Junwu Chen and Ye Wang and Qiao Huang and Bo Jiang and Pengxiang Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116574},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Open APIs, APIs recommendation, Neural networks, Machine learning, Ensemble model},
	pages = {116574},
	title = {Open APIs recommendation with an ensemble-based multi-feature model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000719},
	volume = {196},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000719},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116574}}

@article{ZHANG2022116882,
	abstract = {The World Health Organization (WHO) declared on 11th March 2020 the spread of the coronavirus disease 2019 (COVID-19) a pandemic. The traditional infectious disease surveillance had failed to alert public health authorities to intervene in time and mitigate and control the COVID-19 before it became a pandemic. Compared with traditional public health surveillance, harnessing the rich data from social media, including Twitter, has been considered a useful tool and can overcome the limitations of the traditional surveillance system. This paper proposes an intelligent COVID-19 early warning system using Twitter data with novel machine learning methods. We use the natural language processing (NLP) pre-training technique, i.e., fine-tuning BERT as a Twitter classification method. Moreover, we implement a COVID-19 forecasting model through a Twitter-based linear regression model to detect early signs of the COVID-19 outbreak. Furthermore, we develop an expert system, an early warning web application based on the proposed methods. The experimental results suggest that it is feasible to use Twitter data to provide COVID-19 surveillance and prediction in the US to support health departments' decision-making.},
	author = {Yiming Zhang and Ke Chen and Ying Weng and Zhuo Chen and Juntao Zhang and Richard Hubbard},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116882},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {COVID-19 surveillance, Early warning system, Text classification, BERT, Epidemic intelligence},
	pages = {116882},
	title = {An intelligent early warning system of analyzing Twitter data using machine learning on COVID-19 surveillance in the US},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422003268},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422003268},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116882}}

@article{ZHANG2022116717,
	abstract = {Explainable rating predication becomes challenging with the largely growing number of information and items. Of particular interest is to capture users' preferences for various items by using textual reviews to achieve accurate and interpretable recommendations. In this paper, we report an aspect-aware explainable neural attentional recommender model for rating predication (AENAR) and this model enables intelligent predication and recommendation by capturing the varying aspect attentions that users pay to different items. The experimental results based on six public datasets reveals that the designed model consistently outperforms five existing state-of-the-art alternatives. Furthermore, the designed attention network allows to highlight the context-aware information in textual reviews that unambiguously suggest users' aspect-level preference for their desired items, improving the interpretability of the rating prediction.},
	author = {Tianwei Zhang and Chuanhou Sun and Zhiyong Cheng and Xiangjun Dong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116717},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, Neural networks, Attention mechanism, Deep learning},
	pages = {116717},
	title = {AENAR: An aspect-aware explainable neural attentional recommender model for rating predication},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422001920},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422001920},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116717}}

@article{TERROSOSAENZ2020112892,
	abstract = {Nowadays, cities are dynamic ecosystems where urban changes occur at a very fast pace. Hence, social sensing has become a powerful tool to uncover the actual land-use of a metropolis. However, current solutions for land-use discovery based on user-generated data usually rely on an information retrieval mechanism applied on a textual corpus. This causes ad-hoc place labelling with limited semantic meaning. In this line, the present work introduces a novel data-driven methodology that extends existing solutions by means of a classifier based on a pre-defined hierarchy of land categories. Two types of social networks --text-based and venue-based platforms-- are utilized to train the classifier, which is then applied to infer the use of the land based on text data in areas where venue data are not available. The approach has been evaluated by using large datasets comprising two large cities, showing an accuracy above 90% in predicting the land-use categories.},
	author = {Fernando Terroso-Saenz and Andr{\'e}s Mu{\~n}oz},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112892},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Urban computing, Volunteer Geographic Information (VGI), Land usage, Supervised classification},
	pages = {112892},
	title = {Land use discovery based on Volunteer Geographic Information classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419306086},
	volume = {140},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419306086},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112892}}

@article{WANG2022115887,
	abstract = {Sentiment mining has been a helpful mechanism that targets to understand the market feedback on certain commodities by utilizing user comments. In general, the process of yielding each comment is essentially associated with his/her criteria for rating (i.e., the degree of harshness) , which makes users provide biased comments. For instance, for a tolerant user, although the user is extremely dissatisfied with the product, harshness still makes her yield a neutral comment which cannot indicate the product quality. Existing work straightforwardly removes the comments of harsh users and those of tolerant ones, which is not the best strategy. To this end, we propose a harshness-aware sentiment analysis framework for product review. First, we depict the process of providing comments from users as a probabilistic graphical model in which the harshness is incorporated. Second, we employ a Bayesian-based inference for sentiment mining. Extensive experimental evaluations have shown that the results of the proposed method are more consistent with the expert evaluations than those of the state-of-the-art methods, and even outperform the method which infers the final evaluations with the ground truth of comments without considering users' harshness.},
	author = {Xun Wang and Ting Zhou and Xiaoyang Wang and Yili Fang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115887},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Sentiment mining, Bayesian inference, Probabilistic graphical model},
	pages = {115887},
	title = {Harshness-aware sentiment mining framework for product review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421012458},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421012458},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115887}}

@article{ZHENG2021115030,
	abstract = {Recently, the explosive increase in social media data enables manufacturers to collect product defect information promptly. Extant literature gathers defect information like defective components or defect symptoms without distinguishing defect-related (DR) texts from defect-unrelated (DUR) texts and thus makes defects discussed by few texts buried in enormous DUR texts. Moreover, existing studies do not consider the defect severity which is valuable and important for manufacturers to make remedial decisions. To bridge these research gaps, we propose a novel approach that integrates the probabilistic graphic model named Product Defect Identification and Analysis Model (PDIAM) with Failure Mode and Effect Analysis (FMEA) to derive product defect information from social media data. Comparing to extant studies, PDIAM identifies DR texts and then extracts defect information from these texts. And PDIAM provides more defect information than previous researches. Besides, we further analyze defect severity with the combination of FMEA and PDIAM which alleviates the inherent subjectivity brought by expert evaluation in the traditional FMEA. A case study in the automobile industry proves the predominant performance of our approach and great potential in defect management.},
	author = {Lu Zheng and Zhen He and Shuguang He},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115030},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Product defect discovery, Social media data, Probabilistic graphic model, FMEA, Text analysis},
	pages = {115030},
	title = {An integrated probabilistic graphic model and FMEA approach to identify product defects from social media data},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421004711},
	volume = {178},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421004711},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115030}}

@article{TANG2021115070,
	abstract = {The rapid adoption of services-related technologies, such as cloud computing, has lead to the explosive growth of web services. Automated service classification that groups web services by similar functionality is a widely used technique to facilitate the management and discovery of web services within a large-scale repository. The existing service classification approaches primarily focus on learning the isolated representations of service features but ignored their internal semantic correlations. To address the aforementioned issue, we propose a novel deep neural network with the Co-Attentive Representation Learning (CARL-Net) mechanism for effectively classifying services by learning interdependent characteristics of service without feature engineering. Specifically, we propose a service data augmentation mechanism by extracting informative words from the service description using information gain theory. Such a mechanism can learn a correlation matrix among embedded augmented data and description, thereby obtaining their interdependent semantic correlation representations for service classification. We evaluate the effectiveness of CARL-Net by comprehensive experiments based on a real-world dataset collected from ProgrammableWeb, which includes 10,943 web services. Compared with seven web service classification baselines based on CNN, LSTM, Recurrent-CNN, C-LSTM, BLSTM, ServeNet and ServeNet-BERT, the CARL-Net can achieve an improvement of 5.66%--172.21% in the F-measure of web service classification.},
	author = {Bin Tang and Meng Yan and Neng Zhang and Ling Xu and Xiaohong Zhang and Haijun Ren},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115070},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Service computing, Web service classification, Co-attentive representation},
	pages = {115070},
	title = {Co-attentive representation learning for web services classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100511X},
	volume = {180},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100511X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115070}}

@article{LYKOUSAS2021114808,
	abstract = {Social networks are evolving to engage their users more by providing them with more functionalities. One of the most attracting ones is streaming. Users may broadcast part of their daily lives to thousands of others world-wide and interact with them in real-time. Unfortunately, this feature is reportedly exploited for grooming. In this work, we provide the first in-depth analysis of this problem for social live streaming services. More precisely, using a dataset that we collected, we identify predatory behaviours and grooming on chats that bypassed the moderation mechanisms of the LiveMe, the service under investigation. Beyond the traditional text approaches, we also investigate the relevance of emojis in this context, as well as the user interactions through the gift mechanisms of LiveMe. Finally, our analysis indicates the possibility of grooming towards minors, showing the extent of the problem in such platforms.},
	author = {Nikolaos Lykousas and Constantinos Patsakis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114808},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Online grooming, Social networks, LDA, Text analysis, Emoji},
	pages = {114808},
	title = {Large-scale analysis of grooming in modern social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002499},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002499},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114808}}

@article{DHEERAJ2021115265,
	abstract = {Mining the emotions in the text related to mental health-care oriented is a challenging aspect, especially dealing with a long-text sequence of data. The extraction of emotions depends upon the various psychological depression factors like negative and ambiguity. Identifying these factors is the most perplexing task for every psychiatrist to treat their patients. Our study includes the deep learning (DL) models with global vector representations (GloVe) embeddings to capture the text sequence of data. We proposed a model multi-head attention with bidirectional long short-term memory and convolutional neural network (MHA-BCNN) is a pre-eminent mechanism that outperforms better than past research works for capturing the negative text-based emotions. In this paper, by using DL extracted the various negative mental-health emotions like addiction, anxiety, depression, insomnia, stress, and obsessive cleaning disorder (OCD). By using the GloVe embeddings and handled the ambiguity factors like multiple emotion words in a certain sequence. As we proposed a vigorous appliance in our research to capture and hoard the long-term dependencies. We extracted the questions related to mental health issues were posted by the patients in an online mental healthcare-oriented platform. We efficaciously handled both negative and ambiguity factors at the document level. Our suggested exemplary MHA-BCNN surmounts various aspects from preceding research works and ensued preeminent performance. Experimental results show that our proposed framework MHA-BCNN outperformed than the erstwhile research works.},
	author = {Kodati Dheeraj and Tene Ramakrishnudu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115265},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Emotion analysis, Mental-health-care, GloVe-embeddings, Deep learning, MHA-BCNN},
	pages = {115265},
	title = {Negative emotions detection on online mental-health related patients texts using the deep learning with MHA-BCNN model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421006977},
	volume = {182},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421006977},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115265}}

@article{RANI2022118461,
	abstract = {Disaster event detection aims to identify events like terrorist attacks, fire incidents, stampede incidents, building collapse, etc., reported in the online news articles or social media. Place of occurrence of disaster event is a significant feature associated with events for location-sensitive disaster event detection. Efficient feature selection and their augmentation with location information can contribute towards the evolution of traditional approaches and their adoption for location-sensitive disaster event detection leading to improvement in the overall process as a whole. Since the evaluation of event detection techniques deliberates various intrinsic and extrinsic performance metrics, the decision-making for the selection of feature sets is treated as a Multiple-Criteria Decision Making (MCDM) problem. This paper proposes a framework, GeoClust, that is based on feature engineering of traditional textual features in order to enhance their capability for improved location-sensitive disaster event detection. The framework augments context-free and context-based textual feature sets with feature sets of place of occurrence of the events and evaluates their performance using unsupervised machine learning algorithms for various performance metrics. Finally, the best feature set is selected using AHP-TOPSIS technique of MCDM in order to tune the system for automatic and efficient location-sensitive disaster event detection in real-time. Extensive set of experiments have been performed in order to evaluate the framework on a dataset of online news articles reporting disaster events about terrorist attacks, fire incidents, stampede incidents, building collapse and maoist attacks happened at different locations in India. The results show that the location-augmented feature sets significantly improve performance of location-sensitive disaster event detection as compared with traditional feature sets. The results also demonstrate that the context-based feature sets with location-augmentation are ranked higher than the context-free feature sets in MCDM analysis.},
	author = {Monika Rani and Sakshi Kaushal},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118461},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Location-sensitive disaster event detection, Feature engineering, Multiple-criteria decision making (MCDM), AHP-TOPSIS, Context-free and context-based feature sets},
	pages = {118461},
	title = {GeoClust: Feature engineering based framework for location-sensitive disaster event detection using AHP-TOPSIS},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015548},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015548},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118461}}

@article{BELFORD2020113709,
	abstract = {Topic modeling is a popular unsupervised technique that is used to discover the latent thematic structure in text corpora. The evaluation of topic models typically involves measuring the semantic coherence of the terms describing each topic, where a single value is used to summarize the quality of an overall model. However, this can create difficulties when one seeks to interpret the strengths and weaknesses of a given topic model. With this in mind, we propose a new ensemble topic modeling approach that incorporates both stability information, in the form of term co-associations, and semantic similarity information, as derived from a word embedding constructed on a background corpus. Our evaluations show that this approach can simultaneously yield higher quality models when considering the produced topic descriptors and document-topic assignments, while also facilitating the comparison and evaluation of solutions through the visualization of the discovered topical structure, the ordering of the topic descriptors, and the ranking of term pairs which appear in topic descriptors.},
	author = {Mark Belford and Derek Greene},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113709},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic modeling, Ensemble learning, Evaluation, Word embeddings, Interpretation},
	pages = {113709},
	title = {Ensemble topic modeling using weighted term co-associations},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305339},
	volume = {161},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420305339},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113709}}

@article{ZHANG2020113073,
	abstract = {Openness to experience, one of the essential individual characteristics, is of great theoretical and practical value in psychological and behavioral domains. Although typical machine learning methods can be utilized to extract individuals' openness to experience from the large-scale textual data like the unprecedented massive user generated contents (UGCs), they are often regarded as ``black boxes'' because they are unable to provide knowledge about the influential factors of openness to experience. This is of no help for us to investigate why a particular level of openness to experience is predicted for an individual. In addition, high dimensionality and sparseness of textual data impairs the performance of the typical machine learning method in extracting individuals' characteristics. In this study, we propose an interpretable data-driven mixture method for qualified modeling and predicting individuals' openness to experience. The proposed method extends the latent Dirichlet allocation (LDA) to overcome the problem of high dimensionality and sparseness in modeling the textual data, and can effectively extract two influential variables, namely, the topic preference and the expressed emotional intensity, to make an accurate prediction and to help us fully understand individuals' openness to experience lurking in the textual data. Experimental results indicate the effectiveness of the proposed method in drawing individuals' openness to experience, and also validate the predictive ability of topic preference and expressed emotional intensity which are indicated in psychological literature to be influential factors of openness to experience.},
	author = {Yishi Zhang and Haiying Wei and Yaxuan Ran and Yang Deng and Dan Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.113073},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Openness to experience, Interpretability, Topic modeling, Maximum-A-Posteriori estimation, Data-driven},
	pages = {113073},
	title = {Drawing openness to experience from user generated contents: An interpretable data-driven topic modeling approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419307900},
	volume = {144},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419307900},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.113073}}

@article{ERFANIAN2022116086,
	abstract = {With the huge expansion of user generated content on social networks, event detection has emerged as a major challenge and source of knowledge discovery. This knowledge is employed in different applications such as recommender systems, crisis management systems, and decision support systems. Dynamicity, overlapping, and evolutionary behavior are the most important issues in event detection. This paper proposes a novel evolutionary model for event detection to capture the dynamism and evolving behavior of events. The proposed method uses a matrix decomposition technique and a Dirichlet Process to detect events and handle their dynamicity. This model consists of two components, namely preliminary event detection and event evolvement tracking. The former component extracts preliminary events from the available data using the matrix decomposition method. Then, subsequent data is employed into a Non-Parametric Bayesian Network, namely Dirichlet Process Mixture Model to evolve the preliminary events. During the evolvement process, data may migrate between extracted events or new events may be discovered. The experimental results and comparisons with several recently developed approaches show the superiority of the proposed approach, and its ability to capture the evolutionary behavior of events over time.},
	author = {P.M.A. Yashar Erfanian and Bagher Rahimpour Cami and Hamid Hassanpour},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116086},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Event detection, Event evolution, Topic modeling, Social network analysis, Incremental clustering},
	pages = {116086},
	title = {An evolutionary event detection model using the Matrix Decomposition Oriented Dirichlet Process},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421014226},
	volume = {189},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421014226},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116086}}

@article{KIM2020113288,
	abstract = {Due to its simplicity and intuitive interpretability, spherical k-means is often used for clustering a large number of documents. However, there exist a number of drawbacks that need to be addressed for much effective document clustering. Without well-dispersed initial points, spherical k-means fails to converge quickly, which is critical for clustering a large number of documents. Furthermore, its dense centroid vectors needlessly incorporate the impact of infrequent and less-informative words, thereby distorting the distance calculation between the document vectors. In this paper, we propose practical improvements on spherical k-means to overcome these issues during document clustering. Our proposed initialization method not only guarantees dispersed initial points, but is also up to 1000 times faster than previously well-known initialization method such as k-means++. Furthermore, we enforce sparsity on the centroid vectors by using a data-driven threshold that is capable of dynamically adjusting its value depending on the clusters. Additionally, we propose an unsupervised cluster labeling method that effectively extracts meaningful keywords to describe each cluster. We have tested our improvements on seven different text datasets that include both new and publicly available datasets. Based on our experiments on these datasets, we have found that our proposed improvements successfully overcome the drawbacks of spherical k-means in significantly reduced computation time. Furthermore, we have qualitatively verified the performance of the proposed cluster labeling method by extracting descriptive keywords of the clusters from these datasets.},
	author = {Hyunjoong Kim and Han Kyul Kim and Sungzoon Cho},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113288},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Spherical k-means, Document clustering, k-means initialization, Sparse vector projection, Clustering labeling},
	pages = {113288},
	title = {Improving spherical k-means for document clustering: Fast initialization, sparse centroid projection, and efficient cluster labeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420301135},
	volume = {150},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420301135},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113288}}

@article{TANG2022118062,
	abstract = {With rapid development of socio-economics, the task of discovering functional zones becomes critical to better understand the interactions between social activities and spatial locations. In this paper, we propose a framework to discover the real functional zones from the biased and extremely sparse Point of Interests (POIs). To cope with the bias and sparsity of POIs, the unbiased inner influences between spatial locations and human activities are introduced to learn a balanced and dense latent region representation. In addition, a spatial location based clustering method is also included to enrich the spatial information for latent region representation and enhance the region functionality consistency for the fine-grained region segmentation. Moreover, to properly annotate the various and fine-grained region functionalities, we estimate the functionality of the regions and rank them by the differences between the normalized POI distributions to reduce the inconsistency caused by the fine-grained segmentation. Thus, our whole framework is able to properly address the biased categories in sparse POI data and explore the true functional zones with a fine-grained level. To validate the proposed framework, a case study is evaluated by using very large real-world users GPS and POIs data from city of Raleigh. The results demonstrate that the proposed framework can better identify functional zones than the benchmarks, and, therefore, enhance understanding of urban structures with a finer granularity under practical conditions.},
	author = {Wen Tang and Alireza Chakeri and Hamid Krim},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118062},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Functional zones discovering, Latent region representation learning, Sparse and bias POIs, GPS data, Conditional random field clustering, Function annotation},
	pages = {118062},
	title = {Discovering urban functional zones from biased and sparse points of interests and sparse human activities},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012672},
	volume = {207},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422012672},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118062}}

@article{AMADORDOMINGUEZ2021115731,
	abstract = {Hyper-personalization policies entail a considerable improvement regarding previous personalization approaches. However, they present several issues that need to be addressed, such as minimal explainability and privacy invasion. A hierarchical Multi-Agent System (MAS) is presented in this work to provide a solution to these concerns. The system is formulated as a hybrid approach, where some of the agents work autonomously, while the user input triggers the remaining. At the autonomous level, a set of Virtual Identities (VIs) representing different user profiles interact with Black-Box Hyper-Personalization Online Systems (BBHOS), gathering a set of targeted responses. Associative patterns and profile aggregations can then be inferred from the analysis of these responses. In the user-triggered level, the real user is virtualized as an identity that represents their features. The virtual identity serves as an intermediary between the personalization system and the real user. This virtualization hinders the personalization service from extracting sensitive contextual information about the real user, protecting their privacy. The results obtained by the user identity on its interaction with the personalization service are then analyzed, adjusting the content of the response to fit the user's requests instead of their features. A use case on the functioning of the analysis of search engines is presented to illustrate the complete behavior of the proposed architecture.},
	author = {Elvira Amador-Dom{\'\i}nguez and Emilio Serrano and Daniel Manrique},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115731},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-agent system, Virtual identities, Personalization},
	pages = {115731},
	title = {A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421011118},
	volume = {186},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421011118},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115731}}

@article{NUGUMANOVA2022117179,
	abstract = {This work describes automatic term extraction approach based on the combination of the probabilistic topic modelling (PTM) and non-negative matrix factorization (NMF). Topic modeling algorithms including NMF-based ones do not require expensive and time-consuming manual annotations for domain terms, but only a corpus of domain documents. The topics emerge from the corpus documents without any supervision as sets of most probable words. This work is aimed to investigate how fully and precisely these most probable words from topics can reflect domain terminology. We run a series of experiments on the novel, qualitatively annotated dataset ACTER that was first used in the TermEval 2020 Shared Task. We compare five different NMF algorithms and four different NMF initializations when changing the number of topics extracted from documents and the number of most probable words extracted from topics in order to determine optimal combinations for best performance of term extraction. Finally, we compare the obtained optimal combinations of NMF with the competitive methods in TermEval 2020 and prove that our approach is second only to two much more sophisticated, domain-dependent supervised methods.},
	author = {Aliya Nugumanova and Darkhan Akhmed-Zaki and Madina Mansurova and Yerzhan Baiburin and Almasbek Maulit},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117179},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Automatic term extraction, Probabilistic topic modeling, NMF, Unsupervised term extraction, ACTER dataset, TermEval shared task},
	pages = {117179},
	title = {NMF-based approach to automatic term extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422005668},
	volume = {199},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422005668},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117179}}

@article{GOZUACIK2021115388,
	abstract = {Social media platforms are considered one of the most effective intermediaries for companies to interact with consumers. Social media-based decision support systems for the marketing domain are highly developed, but product development and innovation-oriented studies remain limited. This study offers a novel approach which utilises opinion retrieval theme along with sentiment analysis to support the decision-making process for product analysis and development. To achieve this aim, we propose an end-to-end social media-based opinion retrieval system and utilise machine learning and natural language processing techniques. Google Glass is chosen as a use-case as this product was unable to achieve its commercial targets despite its superior technological offerings. We design a multi-task deep neural network architecture for the training of sentiment prediction and opinion detection tasks. We first divide the tweets containing certain useful opinions and suggestions into two categories based on their sentiment labels. The negative tweets are analysed to identify product-related concerns, whereas the positive and neutral tweets are used to extract innovative ideas and identify new use cases for product development. We visualise and interpret the clusters of keywords extracted from each sentiment label group. Apart from methodological contributions, this study offers practical contributions for the next generations of smart glasses.},
	author = {Necip Gozuacik and C. Okan Sakar and Sercan Ozcan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115388},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deep learning, Feedback retrieval, Natural language processing, Opinion mining, Sentiment analysis, Text analytics},
	pages = {115388},
	title = {Social media-based opinion retrieval for product analysis using multi-task deep neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008125},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421008125},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115388}}

@article{JOSHI2023118442,
	abstract = {In this paper, we propose DeepSumm, a novel method based on topic modeling and word embeddings for the extractive summarization of single documents. Recent summarization methods based on sequence networks fail to capture the long range semantics of the document which are encapsulated in the topic vectors of the document. In DeepSumm, our aim is to utilize the latent information in the document estimated via topic vectors and sequence networks to improve the quality and accuracy of the summarized text. Each sentence is encoded through two different recurrent neural networks based on probabilistic topic distributions and word embeddings, and then a sequence to sequence network is applied to each sentence encoding. The outputs of the encoder and the decoder in the sequence to sequence networks are combined after weighting using an attention mechanism and converted into a score through a multi-layer perceptron network. We refer to the score obtained through the topic model as Sentence Topic Score (STS) and to the score generated through word embeddings as Sentence Content Score (SCS). In addition, we propose Sentence Novelty Score (SNS) and Sentence Position Score (SPS) and perform a weighted fusion of the four scores for each sentence in the document to compute a Final Sentence Score (FSS). The proposed DeepSumm framework was evaluated on the standard DUC 2002 benchmark and CNN/DailyMail datasets. Experimentally, it was demonstrated that our method captures both the global and the local semantic information of the document and essentially outperforms existing state-of-the-art approaches for extractive text summarization with ROUGE-1, ROUGE-2, and ROUGE-L scores of 53.2, 28.7 and 49.2 on DUC 2002 and 43.3, 19.0 and 38.9 on CNN/DailyMail dataset.},
	author = {Akanksha Joshi and Eduardo Fidalgo and Enrique Alegre and Laura Fern{\'a}ndez-Robles},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118442},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text summarization, Extractive, Seq2seq, Attention networks, Topic models},
	pages = {118442},
	title = {DeepSumm: Exploiting topic models and sequence to sequence networks for extractive text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015391},
	volume = {211},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015391},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118442}}

@article{CAMPOS2022117510,
	abstract = {Massive Open Online Courses (MOOCs) have been widely disseminated due to the arrival of Web 2.0. However, the growth of MOOCs brings some difficulties for students in choosing suitable courses in these ecosystems. In recent years, some recommendation systems emerged to solve this problem but remain limited since they do not identify the student's prior knowledge broadly or the student's goals. To overcome this limitation, this work proposes the Fragmented Recommendation for MOOCs Ecosystems (FReME), a recommendation system to suggest parts of courses from multiple providers (i.e., Khan Academy, Udemy, and edX). FReME is based on the student profile and on the MOOCs ecosystems perspective to balance the ecological environment and strengthen interactions. Moreover, we differ from the current recommendation systems since our method identifies and reduces the students' knowledge gap optimizing the learning process. Experimental results conducted with a dataset integrating 3 MOOCs providers and 19 students demonstrated that the implemented techniques are more consistent than other approaches. Finally, it was verified through precision, utility, novelty, and confidence that our recommendations are 62,24% accurate, 68.89% useful, 72.81% reliable, and present new content in 99.12% of cases. These results validate that FReME supports students in reducing their knowledge gap.},
	author = {Rodrigo Campos and Rodrigo Pereira dos Santos and Jonice Oliveira},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117510},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Online education systems, Content-based recommendation, Topic modeling, Non-negative matrix factorization, Unsupervised machine learning},
	pages = {117510},
	title = {Providing recommendations for communities of learners in MOOCs ecosystems},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422008375},
	volume = {205},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422008375},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117510}}

@article{ZIHAYAT2021114910,
	abstract = {A patent gives the owner of an invention the exclusive rights to make, use and sell their invention. Before a new patent application is filed, patent lawyers are required to engage in Prior Art Search to determine the likelihood that an invention is novel, valid or to make sense of the domain. To perform this search, existing platforms utilize keywords and Boolean Logic, which disregards the syntax and semantics of natural language and thus, making the search extremely difficult. Consequently, studies regarding semantics using neural embeddings exist, but these only consider a narrow number of unidirectional words. In this study, we propose an end-to-end framework to consider bidirectional semantics, syntax and the thematic nature of natural language for prior art search. The proposed framework goes beyond keywords as input queries and takes a patent as the input. The contributions of this paper is twofold; adapting pre-trained embedding models (e.g., BERT) to address the semantics and syntax of language, followed by the second component, which exploits topic modeling to build a diversified answer that covers all themes across domains of the input patent. We evaluate the performance of the proposed framework on the CLEF-IP 2011 benchmark dataset and a real-world dataset obtained from Google patent repository and show that the proposed framework outperforms existing methods and returns meaningful results for a given patent.},
	author = {Morteza Zihayat and Rochelle Etwaroo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114910},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Question answering, Prior art search, Pre-trained embeddings, Topic modeling, Search diversification, Sensemaking},
	pages = {114910},
	title = {A non-factoid question answering system for prior art search},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421003511},
	volume = {177},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421003511},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114910}}

@article{SHI2022116538,
	abstract = {Public concern detection provides potential guidance to the authorities for crisis management before or during a pandemic outbreak. Detecting people's concerns and attention from online social media platforms has been widely acknowledged as an effective approach to relieve public panic and prevent a social crisis. However, detecting concerns in time from massive volumes of information in social media turns out to be a big challenge, especially when sufficient manually labelled data is in the absence during public health emergencies, e.g., COVID-19. In this paper, we propose a novel end-to-end deep learning model to identify people's concerns and the corresponding relations based on Graph Convolutional Networks and Bi-directional Long Short Term Memory integrated with Concern Graphs. Except for the sequential features from BERT embeddings, the regional features of tweets can be extracted by the Concern Graph module, which not only benefits the concern detection but also enables our model to be high noise-tolerant. Thus, our model can address the issue of insufficient manually labelled data. We conduct extensive experiments to evaluate the proposed model by using both manually labelled tweets and automatically labelled tweets. The experimental results show that our model can outperform the state-of-the-art models on real-world datasets.},
	author = {Jingli Shi and Weihua Li and Sira Yongchareon and Yi Yang and Quan Bai},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116538},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Concern detection, COVID-19, Auto concern extraction, Concern graph, Graph Convolutional Network},
	pages = {116538},
	title = {Graph-based joint pandemic concern and relation extraction on Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000379},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000379},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116538}}

@article{SAKSHI2023119028,
	abstract = {Context
Although recognition works on mathematical expressions have been explored for four decades, the current literature and trends are varied and frequently influenced by distinct emerging methods and technology. This situation instigates the necessity of an organized review to provide heedful insight into research trends and patterns currently prevailing in the domain of mathematical expression recognition (MER).
Objective
To identify and associate (semantic mapping) the leading research zones, core research areas, and research trends steering in the MER domain. Identifying prominent recognition models based on extracted research areas. To develop the development chart from extracted research trends for directing the future works in this direction.
Method
A manual and automatic search has been performed across the reputed digital libraries for corpus formation. The formulated corpus is used for topic modeling, and Latent Dirichlet Allocation is deployed for information modeling for achieving defined objectives.
Result
The corpus of 325 research papers published from 1967 to 2021 has been processed using LDA. The five major research areas and ten research trends are identified. Leading research area is ``Segmentation and Classification Procedures'', and the trend with the highest related publications is ``Contextual and Graph-based recognition''. ``Attention and Deep Networks'' has emerged as the newborn trend, and the identified newborn, young, and matured trends impetrate more exploration from the MER research community.},
	author = {Sakshi and Vinay Kukreja},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.119028},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Mathematical expressions, Research trends, Pattern recognition, Latent dirichlet allocation, Topic modelling},
	pages = {119028},
	title = {Recent trends in mathematical expressions recognition: An LDA-based analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422020462},
	volume = {213},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422020462},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.119028}}

@article{ALTINELGIRGIN2021114599,
	abstract = {Inspired by the importance of social media, a Social Network Opinion Leaders (SNOL) system has been proposed in this paper. The purpose of this system is to identify topic-based opinion leaders of social media. In order to accomplish this goal, several steps have been taken, such as data collection, data processing, data analysis, data classification, ranking of topic-based opinion leaders, and evaluation. The SNOL system has two main parts. In the first part, collected tweets are classified by semantic kernels for topic-based analysis. In the second part, leadership scores are given to each user in the network according to topic modeling and user modeling results. Leadership scores are then calculated with the formula generated and opinion leaders are determined for each category. Experiments are performed on data gathered from Twitter including 17,234,924 tweets from 38,727 users. The evaluation of opinion leader detection is a difficult job since there is no standard method for identifying opinion leaders. Therefore, the evaluation of the results of this study has been done using two different methods, retweet count and spread score, to prove that the suggested methodology outperforms the PageRank algorithm. The results have also been evaluated considering the user-topic sentiment correlation of the retrieved lists. Furthermore, SNOL has been compared against some opinion leader detection methods previously presented in the literature. The experimental results show that SNOL generates remarkably higher performance than the PageRank algorithm and other existing algorithms in the literature for nearly all topics and all selected top N opinion leaders.},
	author = {Berna {Altınel Girgin}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114599},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social network analysis, Opinion leader detection, Flow of influence, Sentiment polarity score, PageRank algorithm, Semantic kernels},
	pages = {114599},
	title = {Ranking influencers of social networks by semantic kernels and sentiment information},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000403},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000403},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114599}}

@article{SINGHCHAUHAN2020113673,
	abstract = {Social networking sites have a wealth of user-generated unstructured text for fine-grained sentiment analysis regarding the changing dynamics in the marketplace. In aspect-level sentiment analysis, aspect term extraction (ATE) task identifies the targets of user opinions in the sentence. In the last few years, deep learning approaches significantly improved the performance of aspect extraction. However, the performance of recent models relies on the accuracy of dependency parser and part-of-speech (POS) tagger, which degrades the performance of the system if the sentence doesn't follow the language constraints and the text contains a variety of multi-word aspect-terms. Furthermore, lack of domain and contextual information is again an issue to extract domain-specific, most relevant aspect terms. The existing approaches are not capable of capturing long term dependencies for noun phrases, which in turn fails to extract some valid aspect terms. Therefore, this paper proposes a two-step mixed unsupervised model by combining linguistic patterns with deep learning techniques to improve the ATE task. The first step uses rules-based methods to extract the single word and multi-word aspects, which further prune domain-specific relevant aspects using fine-tuned word embedding. In the second step, the extracted aspects in the first step are used as label data to train the attention-based deep learning model for aspect-term extraction. The experimental evaluation on the SemEval-16 dataset validates our approach as compared to the most recent and baseline techniques.},
	author = {Ganpat {Singh Chauhan} and Yogesh {Kumar Meena} and Dinesh Gopalani and Ravi Nahta},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113673},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect extraction, Aspect-level sentiment analysis, Attention model, Deep learning, LSTM, Unsupervised learning},
	pages = {113673},
	title = {A two-step hybrid unsupervised model with attention mechanism for aspect extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420304978},
	volume = {161},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420304978},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113673}}

@article{ZHAO2022118335,
	abstract = {Scientific paper summarization aims at generating a short and concise digest while preserving important information of the original document. Currently, scientific paper summarization faces two main challenges. First, inter-sentence relations are hard to learn, especially in the case of long-form scientific papers. Second, structural information of the well-structured scientific papers has not been fully exploited. To overcome the above two challenges, we propose a novel Heterogeneous Tree structure-based extractive Summarization (HetTreSum) model, where each document is modeled as a tree structure to learn inter-sentence relations and structural information of the original document is incorporated, enabling the tree structure to have a global perspective of the whole document. Then an iterative updating strategy is presented to interactively refine nodes of the tree structure for better contextualized representations, which can further enhance summarization performance. Experimental results on PubMed and arXiv datasets show that our proposed HetTreeSum model achieves significantly advanced performance compared with various scientific paper summarization models.},
	author = {Jintao Zhao and Libin Yang and Xiaoyan Cai},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118335},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Scientific paper summarization, Heterogeneous tree structure, Inter-sentence relations, Structural information, Iterative updating strategy},
	pages = {118335},
	title = {HetTreeSum: A Heterogeneous Tree Structure-based Extractive Summarization Model for Scientific Papers},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014580},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014580},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118335}}

@article{RAHIMI2020113770,
	abstract = {A human is capable of understanding and classifying a text but a computer can understand the underlying semantics of a text when texts are represented in a way comprehensible by computers. The text representation is a fundamental stage in natural language processing (NLP). One of the main drawbacks of existing text representation approaches is that they only utilize one aspect or view of a text e.g. They only consider texts by their words while the topic information can be extracted from text as well. The term-document and document-topic matrix are two views of a text and contain complementary information. We use the strength of both views to extract a richer representation. In this paper, we propose three different text representation methods with the help of these two matrices and tensor factorization to utilize the power of both views. The proposed approach (Tens-Embedding) was applied in the tasks of text classification, sentence-level and document-level sentiment analysis and text clustering wherein the conducted experiments on 20newsgroups, R52, R8, MR and IMDB datasets indicated the superiority of the proposed method in comparison with other document embedding techniques.},
	author = {Zahra Rahimi and Mohammad Mehdi Homayounpour},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113770},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Text classification, Text representation, Document embeddings, Tensor factorization, Topic modeling},
	pages = {113770},
	title = {Tens-embedding: A Tensor-based document embedding method},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305947},
	volume = {162},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420305947},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113770}}

@article{ANTONAKAKI2021114006,
	abstract = {Twitter is the third most popular worldwide Online Social Network (OSN) after Facebook and Instagram. Compared to other OSNs, it has a simple data model and a straightforward data access API. This makes it ideal for social network studies attempting to analyze the patterns of online behavior, the structure of the social graph, the sentiment towards various entities and the nature of malicious attacks in a vivid network with hundreds of millions of users. Indeed, Twitter has been established as a major research platform, utilized in more than ten thousands research articles over the last ten years. Although there are excellent review and comparison studies for most of the research that utilizes Twitter, there are limited efforts to map this research terrain as a whole. Here we present an effort to map the current research topics in Twitter focusing on three major areas: the structure and properties of the social graph, sentiment analysis and threats such as spam, bots, fake news and hate speech. We also present Twitter's basic data model and best practices for sampling and data access. This survey also lays the ground of computational techniques used in these areas such as Graph Sampling, Natural Language Processing and Machine Learning. Along with existing reviews and comparison studies, we also discuss the key findings and the state of the art in these methods. Overall, we hope that this survey will help researchers create a clear conceptual model of Twitter and act as a guide to expand further the topics presented.},
	author = {Despoina Antonakaki and Paraskevi Fragopoulou and Sotiris Ioannidis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114006},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social networks, Twitter, Survey, Social graph, Sentiment analysis, Spam, Bots, Fake news, Hate speech},
	pages = {114006},
	title = {A survey of Twitter research: Data model, graph structure, sentiment analysis and attacks},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030779X},
	volume = {164},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030779X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114006}}

@article{HACOHENKERNER2022117140,
	abstract = {Author profiling from text documents has become a popular task in latest years, in natural language applications. Author profiling is important for various domains such as advertising, marketing, forensics, and security. This survey focuses on profiling age and gender, the two features, which are probably the most researched profile attributes. In this paper, we present an overview of representative studies and datasets of the field (including those organized by PAN) with several significant leaps. Due to the increasing use of deep learning (DL) methods in recent years, we have also reviewed several DL systems that profile authors' age and gender. Most age and gender datasets contain blog posts or Twitter messages written in English, Spanish or Arabic. There are also several relevant datasets written in Dutch, Italian, Portuguese, Turkish, and Russian. There is no consistency and no uniformity in the datasets concerning to the number and types of their documents, the division into training, dev, and test sets, the types of the applied preprocessing methods, and the quality measures used to evaluate the classification results. A prominent interesting finding is that the best age accuracy results are not as high as we might have expected taking into account relatively simple types of classification especially by gender (only 2 categories) when a large number of teams have competed over the years. Another interesting finding that repeats itself in various classification tasks is that classical ML methods are still better than DL methods for age and gender classification tasks. Most classical systems used word unigrams and bigrams and character 3--4-5-grams. Several systems also used various types of stylistic features. While many earlier systems did not apply preprocessing methods, most recent systems applied several preprocessing methods, e.g., lowercase conversion and replacement of various strings (e.g., URLs, LF characters, and User Mentions). We also suggest several potential future issues in age and gender profiling research.},
	author = {Yaakov HaCohen-Kerner},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117140},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Age classification, Author profiling, Deep learning, Gender classification, Supervised machine learning, Text classification},
	pages = {117140},
	title = {Survey on profiling age and gender of text authors},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742200536X},
	volume = {199},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742200536X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117140}}

@article{AYO2021114762,
	abstract = {The key challenges for automatic hate-speech classification in Twitter are the lack of generic architecture, imprecision, threshold settings and fragmentation issues. Most studies used binary classifiers for hate speech classification, but these classifiers cannot really capture other emotions that may overlap between positive or negative class. Hence, a probabilistic clustering model for hate speech classification in twitter was developed to tackle problems with hate speech classification. A metadata extractor was used to collect tweets containing hate speech keywords and a crowd-sourced experts was employed to label the collected hate tweets into two categories: hate speech and non-hate speech. Features representation was done with Term Frequency- Inverse Document Frequency (TF-IDF) model and enhanced with topics inferred by a Bayes classifier. A rule-based clustering method was used to automatically classify real-time tweets into the correct topic clusters. Fuzzy logic was then used for hate speech classification using semantic fuzzy rules and a score computation module. From the evaluation results, it was observed that the developed model performed better in hate speech detection with F1-sore of 0.9256 using a 5-fold cross validation. Similarly, the developed model for hate speech classification performed better with F1-score of 91.5 compared to related models. The developed model also indicates a more perfect test having an AUC of 0.9645, when compared to similar methods. The Paired Sample t-Test validated the efficiency of the developed model for hate speech classification.},
	author = {Femi Emmanuel Ayo and Olusegun Folorunso and Friday Thomas Ibharalu and Idowu Ademola Osinuga and Adebayo Abayomi-Alli},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114762},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Twitter, Hate speech, Fuzzy logic, Combinatorial algorithm, Bayesian function, Sentiment analysis},
	pages = {114762},
	title = {A probabilistic clustering model for hate speech classification in twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002037},
	volume = {173},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002037},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114762}}

@article{ALAMI2021114652,
	abstract = {Humans must easily handle the vast amounts of data being generated by the revolution of information technology. Thus, Automatic Text summarization has been applied to various domains in order to find the most relevant information and make critical decisions quickly. In the context of Arabic, text summarization techniques suffer from several problems. First, most existing methods do not consider the context or domain to which the document belongs. Second, the majority of the existing approaches are based on the traditional bag-of-words representation, which involves high dimensional and sparse data, and makes it difficult to capture relevant information. Third, research in Arabic Text summarization is fairly small and only recently compared to that on Anglo-Saxon and other languages due to the shortage of Arabic corpora, resources, and automatic processing tools. In this paper, we try to overcome these limitations by proposing a new approach using documents clustering, topic modeling, and unsupervised neural networks in order to build an efficient document representation model. First, a new document clustering technique using Extreme learning machine is performed on large text collection. Second, topic modeling is applied to documents collection in order to identify topics present in each cluster. Third, each document is represented in a topic space by a matrix where rows represent the document sentences and columns represent the cluster topics. The generated matrix is then trained using several unsupervised neural networks and ensemble learning algorithms in order to build an abstract representation of the document in the concept space. Important sentences are ranked and extracted according to a graph model with a redundancy elimination component. The proposed approach is evaluated on Essex Arabic Summaries Corpus and compared against other Arabic text summarization approaches using ROUGE measure. Experimental results showed that the models trained on topic representation learn better representations and improve significantly the summarization performance. In particular, ensemble learning models demonstrated an important improvement on Rouge recall and promising results on F-measure.},
	author = {Nabil Alami and Mohammed Meknassi and Noureddine En-nahnahi and Yassine {El Adlouni} and Ouafae Ammor},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114652},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Arabic text summarization, Natural language processing, Deep learning, Neural networks, Clustering, Topic modeling},
	pages = {114652},
	title = {Unsupervised neural networks for automatic Arabic text summarization using document clustering and topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000932},
	volume = {172},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000932},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114652}}

@article{XIA2022118143,
	abstract = {The Fintech mobile payment platform is expanding rapidly; this expansion, in turn, creates numerous risks. There is an urgent need to better understand these risks and to spur more secure payment behavior. This research aims to develop knowledge graphs of the mobile payment platform based on deep learning for risk analysis and policy inferences. We identify entities from collected policy documents, extract the relationships among the entities, and draw a risk knowledge graph on mobile payments. The use of unsupervised semi-automatic knowledge acquisition, we argue, can reduce the risk of mobile payment caused by a lack of knowledge. A significant benefit of this method is that risk knowledge can be acquired without supervision. Unlike other models, the absence of manual labeling allows for the relation extraction of triples to be unsupervised, while the previous triplet extraction was supervised. Compared with other unsupervised models, the precision of our model is improved, and the recall is the same as that of previous unsupervised shutdown extraction.Unsupervised relationship extraction can extract text relationships quickly and on a large scale, saving human resources for labeling.This method offers a potential solution to a fundamental problem; the content and quantity of policy documents exceed organizations' and individuals' ability to understand them. Our approach suggests the viability of developing a national policy risk knowledge graph to help mobile payment platforms understand national policies and reduce platforms' operational risks while allowing users to quickly learn the risks of mobile payments and minimize the impact of those risks.},
	author = {Huosong Xia and Yuan Wang and Jeffrey Gauthier and Justin Zuopeng Zhang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118143},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Fintech, Mobile payment, Deep learning, Knowledge graph},
	pages = {118143},
	title = {Knowledge graph of mobile payment platforms based on deep learning: Risk analysis and policy implications},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013264},
	volume = {208},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422013264},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118143}}

@article{DUAN2020113540,
	abstract = {Classification of investors' sentiments in stock message boards has attracted a great deal of attention. Since the messages are usually short, we propose a semi-supervised learning method to make full use of the features in both train and test messages. The generative emotion model takes message, emotion and words into consideration simultaneously. Based on the facts that words are of different ability in discriminating sentiments, they are categorized into three classes in the model with different emotion strength. Training the generative model can transform the messages into emotion vectors which finally feeds to a sentiment classifier. The experiment results show that the proposed model and learning method are efficient for modeling sentiment in short text, and by properly selecting the amount of train data and the percent of test samples, we can achieve higher classification accuracy than traditional ones. The results indicate that the generative model is effective for short message sentiment classification, and provides a significant approach for the implementation of semi-supervised learning which is a typical expert and intelligent information processing method.},
	author = {Jiangjiao Duan and Banghui Luo and Jianping Zeng},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113540},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Sentiment analysis, Generative model, Semi-supervised learning, Stock message board},
	pages = {113540},
	title = {Semi-supervised learning with generative model for sentiment classification of stock messages},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030364X},
	volume = {158},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030364X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113540}}

@article{CAO2022115977,
	abstract = {The increasing online reviews play an essential role in the e-commerce platform, which profoundly affects the purchase decisions of consumers. However, rampant dishonest sellers manipulate other buyers or robots to post deceptive reviews for profit. Recently, the detection of deceptive reviews has attracted general research attention, which mainly comprises two directions, traditional methods based on statistics and intelligent methods based on neural networks. These methods use a single feature or multiple features for classifier design. To make full use of different features for better feature representation of detecting deceptive reviews, this paper proposes a new feature fusion strategy and verifies its performance by comparing it with other feature fusion strategies. First, we utilize three independent models for feature extraction: the TextCNN, the Bidirectional Gated Recurrent Unit (GRU), and the Self-Attention are used to learn local semantic features, temporal semantic features, and weighted semantic features of reviews, respectively. Secondly, after obtaining different feature representations from the fully connected layers of these three models, we concatenate them together to form the final documental representation. Finally, we use a full connection layer and the sigmoid function to further learn and complete deceptive review detection. Experiments on three balanced and unbalanced in-domain small datasets (hotel, restaurant, doctor) and mixed-domain datasets show that our model is superior to baselines. Experiments on large-scale data with various imbalanced proportions verify the effectiveness of our method. We also analyze the results of different datasets from the perspective of part of speech to improve the model's interpretability.},
	author = {Ning Cao and Shujuan Ji and Dickson K.W. Chiu and Maoguo Gong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115977},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive reviews detection, Separated training, Convolutional neural network, Recurrent neural network, Self attention},
	pages = {115977},
	title = {A deceptive reviews detection model: Separated training of multi-feature learning and classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421013270},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421013270},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115977}}

@article{DAHIR2021114909,
	abstract = {Query Expansion (QE) approaches that involve the reformulation of queries by adding new terms to the initial user query, are intended to ameliorate the vocabulary mismatch between the query keywords and the documents' in Information Retrieval Systems (IRS). One big issue in QE is the selection of the right candidate terms for expansion. For this purpose Linked Data can be used, as a valuable resource, for providing additional expansion features such as the values of sub- and super classes of resources. The underlying research question is whether interlinked data and vocabulary items provide features which can be taken into account for query expansion. In this paper, we introduced a new QE approach that aimed at improving IRS by using the well-known distribution based method Bose-Einstein statistics (Bo1) as well as Linked Data from the knowledge base DBpedia using different numbers of expansion terms. We evaluated the effectiveness of each method individually as well as their combinations using two Text REtrieval Conference (TREC) test collections. Our approach has lead to significant improvement in terms of precision, recall, Mean Average Precision (MAP) at rank 10, and normalized Discounted Cumulative Gain (nDCG) at different ranks compared to Pseudo Relevance Feedback (PRF) that we used as a baseline. The results show that the inclusion of semantic annotations clearly improves the retrieval performance over the baseline method.},
	author = {Sarah Dahir and Abderrahim {El Qadi} and Hamid Bennis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114909},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information retrieval, Query expansion, DBpedia spotlight, Term distribution, Language model},
	pages = {114909},
	title = {Query expansion based on term distribution and DBpedia features},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100350X},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100350X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114909}}

@article{KAUR2020113350,
	abstract = {Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24%, 95.64%, 95.87% precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.},
	author = {Sawinder Kaur and Parteek Kumar and Ponnurangam Kumaraguru},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113350},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Clickbait, News, Classifier, Features, Social media},
	pages = {113350},
	title = {Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420301755},
	volume = {151},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420301755},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113350}}

@article{VIDANAGAMA2022117869,
	abstract = {Majority of customers and manufacturers who tend to purchase and trade via e-commerce websites primarily rely on reviews before making purchasing decisions and product improvements. Deceptive reviewers consider this opportunity to write fake reviews to mislead customers and manufacturers. This calls for the necessity of identifying fake reviews before making them available for decision making. Accordingly, this research focuses on a fake review detection method that incorporates review-related features including linguistic features, Part-of-Speech (POS) features, and sentiment analysis features. A domain feature ontology is used in the feature-level sentiment analysis and all the review-related features are extracted and integrated into the ontology. The fake review detection is enhanced through a rule-based classifier by inferencing the ontology. Due to the lack of a labeled dataset for model training, the Mahalanobis distance method was used to detect outliers from an unlabeled dataset where the outliers were selected as fake reviews for model training. The performance measures of the rule-based classifier were improved by integrating linguistic features, POS features, and sentiment analysis features, in spite of considering them separately.},
	author = {D.U. Vidanagama and A.T.P. Silva and A.S. Karunananda},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117869},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Domain ontology, Rule-based classifier, Outliers, Feature-level sentiment analysis, Review-related features},
	pages = {117869},
	title = {Ontology based sentiment analysis for fake review detection},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742201123X},
	volume = {206},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742201123X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117869}}

@article{ELIGUZEL2022117433,
	abstract = {Due to the rapid incline in the number of documents along with social media usage, text categorization has become an important concept. There are tasks required to be fulfilled during the text categorization, such as extracting useful data from different perspectives, reducing the high feature space dimension, and improving effectiveness. In order to accomplish these tasks, feature selection, and feature extraction gain importance. This paper investigates how to solve feature selection and extraction problems. Also, this study aims to decide which topics are the focus of a document. Moreover, the Twitter data-set is utilized as a document and an Uncapacitated P-Median Problem (UPMP) is applied to make clustering. In this study, UPMP is used on Twitter data collection for the first time to collect clustered tweets. Therefore, a novel hybrid genetic bat algorithm (HGBA) is proposed to solve the UPMP for our case. The proposed novel approach is applied to analyze the Twitter data-set of the Nepal earthquake. The first part of the analysis includes the data pre-processing stage. The Latent Dirichlet Allocation (LDA) method is applied to the pre-processed text. After that, a similarity (distance) matrix is generated by utilizing the Jensen Shannon Divergence (JSD) model. The study's main goal is to use Twitter to assess the needs of victims during and after a disaster. To evaluate the applicability of the proposed approach, experiments are conducted on the OR-Library data-set. The results demonstrate that the proposed approach successfully extracts topics and categorizes text.},
	author = {Nazmiye Elig{\"u}zel and Cihan {\c C}etinkaya and T{\"u}rkay Dereli},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117433},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bat algorithm, Feature extraction, Feature selection, Genetic algorithm, Uncapacitated P-median problem, Text categorization},
	pages = {117433},
	title = {A novel approach for text categorization by applying hybrid genetic bat algorithm through feature extraction and feature selection methods},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422007709},
	volume = {202},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422007709},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117433}}

@article{JIA2022116405,
	abstract = {Social network analysis is a fundamental problem inherent in various applications, which can be handled mainly based on a graph model given in advance. However, it is generally ignored in most existing studies that the social networks may only contain users and no users' connections are explicitly available. Then the connections between users play a considerable role in building the graph, and it is necessary to calculate users' similarities. Traditional methods of user similarity calculation are extensively based on the topics of text content because they can effectively reflect users' interests. Nevertheless, these methods mostly ignore the importance of time series in the texts, where the texts with time series can reveal the activity trends of users in social networks. In this work, we explore a new problem of building a social network graph over text with time series. Our basic idea is that social media users are more similar if they have similar text semantics in similar time sequences. To obtain the semantics of text with time series, we extract topic words of each user from the corresponding text with our proposed Time-Biterm Topic Model (T-BTM), which improves the BTM model by taking the time-topic distribution into account. On this basis, we further propose a novel time series-based graph model with text, called Text with Time series for Graph (TT-Graph) model, which explicitly considers the user similarity and time series similarity. With the TT-Graph model, we propose novel methods for topic detection, community detection, and link prediction in social network analysis. Extensive experiments demonstrate that topic detection, community detection and link prediction can be effectively conducted on the TT-Graph model, and the credibility of our model can be proved.},
	author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116405},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social network graph, Text semantics, Time series, User similarity},
	pages = {116405},
	title = {TT-graph: A new model for building social network graphs from texts with time series},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421016948},
	volume = {192},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421016948},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116405}}

@article{REN2021107093,
	abstract = {Generating long macro reports from a piece of breaking news is quite a challenging task. Essentially, this task is a long text generation problem from short text. Apparently, the difficulty of this task lies in the logic inference of human beings. To address this issue, this paper proposes a novel hybrid deep generative neural model which first learns the outline of the input news and then generates macro financial reports from the learnt outline. In the outline generation component, we generate the outline text using the framework of Pointer-Generator network with attention mechanism. In the target report generation component, we generate the macro financial reports by the revised VAE model. To train our end-to-end model, we have collected the experimental dataset containing over one hundred thousand pairs of news-report data. Extensive experiments are then evaluated on this dataset. The proposed model achieves the SOTA performance against both the baseline models and the state-of-the-art models with respect to evaluation criteria BLEU, ROUGE and human scores. Although the readability of the generated reports by our approach is better than that of the rest models, it remains an open problem which needs further efforts in the future.},
	author = {Yunpeng Ren and Wenxin Hu and Ziao Wang and Xiaofeng Zhang and Yiyuan Wang and Xuan Wang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107093},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Financial data mining, Text generation, Natural language generation},
	pages = {107093},
	title = {A hybrid deep generative neural model for financial report generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003567},
	volume = {227},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121003567},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107093}}

@article{HE2020106228,
	abstract = {The classical high-order rating distance model which aims to minimize not only (i) the difference between the estimated and real ratings of the same user--item pair (i.e., the first-order rating distance), but also (ii) the difference between the estimated and real rating difference of the same user across different items (i.e., the second-order rating distance), and use stochastic gradient descent to solve this convex optimization problem in recommender systems, has good performance in prediction accuracy. However, when the manually set parameter for the second-order rating difference is fixed, this model will not converge as the size of dataset increasing, and its performance on efficiency is slow compared with the matrix factorization method. Aiming at improving such model's adaptability and efficiency, we propose an improved high-order rating distance model with omitting rules based on slack variable, in which the static parameter used to balance the first-order rating distance and the second-order rating distance is replaced by a data-scale sensitive function. We choose Newton method to solve the convex recommendation optimization problem defined in this paper instead of stochastic gradient descent. Our model not only achieves the adaptability by eliminating several static parameters for module balancing, reduces the computation complexity, but also accelerates the optimization function convergence speed. We provide solid theoretical support and conduct comprehensive experiments on four real-world datasets. Experimental results show the proposed model has good performance in terms of prediction accuracy and efficiency.},
	author = {Yifan He and Haitao Zou and Hualong Yu and Shang Zheng and Shang Gao},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106228},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Slack variable, High-order rating distance, Newton method},
	pages = {106228},
	title = {Adaptive and efficient high-order rating distance optimization model with slack variable},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304366},
	volume = {205},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304366},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106228}}

@article{DELCARMENRODRIGUEZHERNANDEZ2021106740,
	abstract = {In the Artificial Intelligence (AI) field, and particularly within the area of Machine Learning (ML), recommender systems have attracted significant research attention. These systems attempt to alleviate the increasing information overload that users can experience in the current Big Data era, by providing personalized recommendations of items that they may find relevant. Besides, given the importance of mobile computing, these systems have evolved to consider also the dynamic context of the mobile users (location, time, weather conditions, etc.) to offer them more appropriate suggestions and information while on the move. In this paper, we provide an extensive survey of recent advances towards intelligent mobile Context-Aware Recommender Systems (mobile CARS) from an information management perspective, with an emphasis on mobile computing and AI techniques, along with an analysis of existing research gaps and future research directions. We focus on approaches that go beyond just considering the location of the user and exploit also other context information. In this study, we have identified that deep learning approaches are promising artificial intelligence models for mobile CARS. Additionally, in a near future, we expect a higher prominence of push-based recommendation solutions where at least part of the recommendation engine could be executed in the mobile devices, which could share data and tasks in a distributed way.},
	author = {Mar{\'\i}a {del Carmen Rodr{\'\i}guez-Hern{\'a}ndez} and Sergio Ilarri},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106740},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Context-Aware Recommender Systems, Mobile computing, Context-aware computing, Personalization, Information management},
	pages = {106740},
	title = {AI-based mobile context-aware recommender systems from an information management perspective: Progress and directions},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121000034},
	volume = {215},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121000034},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106740}}

@article{ABEBE2020104817,
	abstract = {Various methods have been put forward to perform automatic social-based event detection and description. Yet, most of them do not capture the semantic meaning embedded in online social media data, which are usually highly heterogeneous and unstructured, and do not identify event relationships (e.g., car accident temporally occurs after storm, and geographically occurs near soccer match). To address this problem, we introduce a generic Social-based Event Detection, Description, and Linkage framework titled SEDDaL, taking as input: a collection of social media objects from heterogeneous sources (e.g., Flickr, YouTube, and Twitter), and producing as output a collection of semantically meaningful events interconnected with spatial, temporal, and semantic relationships. The latter are required as the building blocks for event-based Collective Knowledge (CK) organization, where CK underlines the combination of all known data, information, and metadata concerning a given concept or event. SEDDaL consists of four main modules for: i) describing social media objects in a generic Metadata Representation Space Model (MRSM) consisting of three composite dimensions: temporal, spatial, and semantic, ii) evaluating the similarity between social media objects' descriptions following MRSM, iii) detecting events from similar social media objects using an adapted unsupervised learning algorithm, where events are represented as clusters of objects in MRSM, and iv) identifying directional, metric, and topological relationships between events following MRSM's dimensions. We believe this is the first study to provide a generic model for describing semantic-aware events and their relationships extracted from social metadata on the Web. Experimental results confirm the quality and potential of our approach.},
	author = {Minale A. Abebe and Joe Tekli and Fekade Getahun and Richard Chbeir and Gilbert Tekli},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.06.025},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social media, Metadata, Semantics, Similarity evaluation, Event detection, Event relationships, Collective knowledge},
	pages = {104817},
	title = {Generic metadata representation framework for social-based event detection, description, and linkage},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302928},
	volume = {188},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302928},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.06.025}}

@article{WEN2020106344,
	abstract = {Traditional recommender systems encounter several challenges such as data sparsity and unexplained recommendation. To address these challenges, many works propose to exploit semantic information from review data. However, these methods have two major limitations in terms of the way to model textual features and capture textual interaction. For textual modeling, they simply concatenate all the reviews of a user/item into a single review. However, feature extraction at word/phrase level can violate the meaning of the original reviews. As for textual interaction, they defer the interactions to the prediction layer, making them fail to capture complex correlations between users and items. To address those limitations, we propose a novel Hierarchical Text Interaction model (HTI) for rating prediction. In HTI, we propose to model low-level word semantics and high-level review representations hierarchically. The hierarchy allows us to exploit textual features at different granularities. To further capture complex user--item interactions, we propose to exploit semantic correlations between each user--item pair at different hierarchies. At word level, we propose an attention mechanism specialized to each user--item pair, and capture the important words for representing each review. At review level, we mutually propagate textual features between the user and item, and capture the informative reviews. The aggregated review representations are integrated into a collaborative filtering framework for rating prediction. Experiments on five real-world datasets demonstrate that HTI outperforms state-of-the-art models by a large margin. Further case studies provide a deep insight into HTI's ability to capture semantic correlations at different levels of granularities for rating prediction.},
	author = {Jiahui Wen and Jingwei Ma and Hongkui Tu and Mingyang Zhong and Guangda Zhang and Wei Yin and Jian Fang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106344},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Hierarchical neural network, Interactive networks, Review texts, Rating prediction},
	pages = {106344},
	title = {Hierarchical text interaction for rating prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305013},
	volume = {206},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305013},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106344}}

@article{ABUALIGAH2022108833,
	abstract = {Text document clustering is to divide textual contents into clusters or groups. It received wide attention due to the vast amount of daily data from the Web. In the last decade, Meta-Heuristic (MH) techniques have been adopted to solve clustering problems. Motivated by that, the authors introduce a reliable version of the newly developed MH algorithm called Arithmetic Optimization Algorithm (AOA). Math arithmetic operators inspire the AOA: multiplication, subtraction adding, and division. The AOA showed good performance in several global problems; nonetheless, it suffers from entrapment in local optima in complicated and high dimensional problems. Therefore, this paper proposes an improved version of AOA for the text document clustering problem. The Improved AOA (IAOA) introduces an integration between Opposition-based learning (OBL) and Levy flight distribution (LFD) with AOA to tackle the limitations of the traditional AOA. The IAOA is examined with different UCI datasets for the text clustering problems and assessed with a set of CEC2019 benchmark functions as a global optimization algorithm with extensive comparison to existing optimization algorithms. Overall, experimental results show the superiority of the proposed IAOA compared to several optimization algorithms. Moreover, the proposed IAOA is compared with twenty-one state-of-the-art methods using thirty-one benchmark text datasets, and the results proved the superiority of the proposed IAOA.},
	author = {Laith Abualigah and Khaled H. Almotairi and Mohammed A.A. Al-qaness and Ahmed A. Ewees and Dalia Yousri and Mohamed Abd Elaziz and Mohammad H. Nadimi-Shahraki},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108833},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Arithmetic Optimization Algorithm (AOA), Opposition-based learning, Levy flight, Text clustering, CEC2019 problems},
	pages = {108833},
	title = {Efficient text document clustering approach using multi-search Arithmetic Optimization Algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003975},
	volume = {248},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003975},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108833}}

@article{LI2021107359,
	abstract = {Key elements refer to the elements that play a crucial role in disseminating information in social networks. The influence discovery of key elements can guide a series of works, such as public opinion control, user recommendation, and marketing promotion. Recently, there have been many studies on the influence of elements, but at present, many methods focus on either the influence discovery of key elements of different types or the dynamic influence discovery of a certain type of element alone, and rarely consider the combination of the two. Therefore, this study proposes a key element discovery algorithm based on a ternary association graph and representation learning, which can detect the influence of paths, users, and user groups. Additionally, the changes of different types of key elements can be analyzed according to the influence of elements in each stage of topic communication.},
	author = {Qian Li and Meiling Li and Xu Shi and Bin Wu and Yunpeng Xiao},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107359},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social networks, Hotspot topic, Key elements influence, Representation learning, Ternary association graph},
	pages = {107359},
	title = {A key elements influence discovery scheme based on ternary association graph and representation learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121006213},
	volume = {229},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121006213},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107359}}

@article{LI2021106948,
	abstract = {Online recommender systems generally suffer from severe data sparsity problems, and this are particularly prevalent in newly launched systems that do not have sufficient amounts of data. Cross-domain recommendations can provide us with some new ideas for assisting with user recommendations in sparse target domains by transferring knowledge from a source domain with rich data. In this paper, a deep sparse autoencoder prediction model based on adversarial learning for cross-domain recommendations (DSAP-AL) is proposed to improve the accuracy of rating predictions in similar cross-domain recommender systems. Specifically, joint matrix factorization and adversarial network learning models are adopted to integrate and align user and item latent factor spaces in a unified pattern. Then, a deep sparse autoencoder is represented and modeled by transferring the latent factors and interlayer weights. Furthermore, a domain factor adaptation algorithm is proposed to capture robust user and item factors, and the learned regularization constraints are added to the objective function, thereby alleviating the data sparsity issue. Experimental results on four real-world datasets demonstrate that, even without overlapping entities (users or items) in the source and target domains, the proposed DSAP-AL method achieves competitive performance relative to other state-of-the-art individual and cross domain approaches. Moreover, the DSAP-AL model is not only effective for scenarios with sparse data but also robust for noise-containing recommendations.},
	author = {Yakun Li and Jiadong Ren and Jiaomin Liu and Yixin Chang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106948},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Data sparsity, Sparse autoencoder, Adversarial learning, Recommender systems, Matrix factorization},
	pages = {106948},
	title = {Deep sparse autoencoder prediction model based on adversarial learning for cross-domain recommendations},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121002112},
	volume = {220},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121002112},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106948}}

@article{PONZA2020105051,
	abstract = {Many text mining tasks, such as clustering, classification, retrieval, and named entity linking, benefit from a measure of relatedness between entities in a knowledge graph. We present a thorough study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph. To facilitate this study, we introduce a new dataset with human judgments of entity relatedness. No clear dominance is seen between measures based on textual similarity and graph proximity. Some of the better measures involve expensive global graph computations. We propose a new, space-efficient, computationally lightweight, two-stage framework for relatedness computation. In the first stage, a small weighted subgraph is dynamically grown around the two query entities; in the second stage, relatedness is derived based on computations on this subgraph. Our system shows better agreement with human judgment than existing proposals both on the new dataset and on an established one. Our framework also shows improvements with respect to the state-of-the-art on three different extrinsic evaluations in the domains of ranking entity pairs, entity linking, and synonym extraction.},
	author = {Marco Ponza and Paolo Ferragina and Soumen Chakrabarti},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105051},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Entity relatedness, Wikipedia, Knowledge graph},
	pages = {105051},
	title = {On Computing Entity Relatedness in Wikipedia, with Applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119304447},
	volume = {188},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119304447},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105051}}

@article{DIGIROLAMO2021106563,
	abstract = {Current Online Social Networks represent a means for the continuous generation and distribution of information, which is slightly changed when moving from a user to another during the traversing of the network. Such an amount of information can overcome the capacity of a single user to manage it, so it would be useful to reduce it so that the user is able to have a summary of the information flowing the network. To this aim, it is of crucial importance to detect events within such an information stream, composing of the most representative words containing in each information instance, representing the event described by the set of tweet categorized together. There is a vast literature on off-line event detection on data-sets acquired from online social networks, but a similar solid set of approaches is missing if the detection has to be done on-line, which is demanding by the current applications. The driving idea described in this paper is to realize on-line clustering of tweets by leveraging on evolutionary game theory and the replicator dynamics, which have been used with success in many classification problems and/or multiobjective optimizations. We have adapted and enhanced a evolutionary clustering from the literature to meet the needs of on-line tweet clustering. Such a solution has been implemented according to the Kappa architectural model and assessed against state-of-the art approaches showing higher values of topic and keyword recall on two realistic data-sets.},
	author = {Rocco {Di Girolamo} and Christian Esposito and Vincenzo Moscato and Giancarlo Sperl{\'\i}},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106563},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Event detection, Tweet categorization, Online social networks, Online clustering, Game theory, Dempster--Shafer theory},
	pages = {106563},
	title = {Evolutionary game theoretical on-line event detection over tweet streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120306924},
	volume = {211},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120306924},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106563}}

@article{CAO2020106114,
	abstract = {Due to the short attention span and instant gratification phenomenon, micro-videos are growing exponentially while gaining more and more concerns. Yet the sheer number of micro-videos leads to severe information overload issues, making it difficult for users to identify their desired micro-videos. The hashtag, mainly utilized in the domain of the microblog or the image, is the indicator or the core idea of the target content and can be applied to various information retrieval scenarios (e.g., search, browse, and categorization). So far, however, little attention has been paid to perform the hashtag recommendation for micro-videos via harnessing multiple modalities. In this article, we devise a neural network-based solution, LOGO (short for ``muLti-mOdal-based hashtaG recOmmendation''), to recommend hashtags for micro-videos by utilizing multiple modalities. The proposed LOGO approach first represents each modality as the combination of sequential units in it, weighted by the attention mechanism. In this way, the sequential and attentive features are captured simultaneously. After that, the LOGO integrates the representations of all modalities via a multi-view representation learning framework, which projects the representations into a common space under the restriction of the modality similarity. Ultimately, the LOGO feed the projections of three modalities in the common space and the embeddings of hashtags into a customized neural collaborative filtering framework to perform the hashtag recommendation. Extensive experiments on the scope of both overall performance comparison and micro-level analyses have well-justified the effectiveness and rationality of our proposed approach.},
	author = {Da Cao and Lianhai Miao and Huigui Rong and Zheng Qin and Liqiang Nie},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106114},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Micro-videos, Hashtag recommendation, Multiple modalities, Deep neural network},
	pages = {106114},
	title = {Hashtag our stories: Hashtag recommendation for micro-videos via harnessing multiple modalities},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120303798},
	volume = {203},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120303798},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106114}}

@article{ARAQUE2020105184,
	abstract = {Moral rhetoric plays a fundamental role in how we perceive and interpret the information we receive, greatly influencing our decision-making process. Especially when it comes to controversial social and political issues, our opinions and attitudes are hardly ever based on evidence alone. The Moral Foundations Dictionary (MFD) was developed to operationalize moral values in the text. In this study, we present MoralStrength, a lexicon of approximately 1,000 lemmas, obtained as an extension of the Moral Foundations Dictionary, based on WordNet synsets. Moreover, for each lemma it provides with a crowdsourced numeric assessment of Moral Valence, indicating the strength with which a lemma is expressing the specific value. We evaluated the predictive potentials of this moral lexicon, defining three utilization approaches of increased complexity, ranging from lemmas' statistical properties to a deep learning approach of word embeddings based on semantic similarity. Logistic regression models trained on the features extracted from MoralStrength, significantly outperformed the current state-of-the-art, reaching an F1-score of 87.6% over the previous 62.4% (p-value <0.01), and an average F1-Score of 86.25% over six different datasets. Such findings pave the way for further research, allowing for an in-depth understanding of moral narratives in text for a wide range of social issues.},
	author = {Oscar Araque and Lorenzo Gatti and Kyriaki Kalimeri},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105184},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Moral foundations, Moral values, Lexicon, Twitter data, Natural language processing, Machine learning},
	pages = {105184},
	title = {MoralStrength: Exploiting a moral lexicon and embedding similarity for moral foundations prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511930526X},
	volume = {191},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511930526X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105184}}

@article{TOMER2022108108,
	abstract = {A novel text summarization framework referred to as Skip-Though Vector and Bi-encoder Based Automatic Text Summarization (STV--BEATS) is proposed in this paper. STV--BEATS utilizes --- (a) skip-though vector to generate sentence-based embedding; and (b) Long Short-Term Memory (LSTM) based deep autoencoder to reduce dimensions of skip thought vectors. STV--BEATS works in the conjunction of extractive and abstractive summarization models to enhance the overall quality of the results. For each sentence, relevance and novelty metrics are calculated on the intermediate representation of the deep autoencoder to generate the final sentence score. The highly scored sentences are selected to generate an extractive summary. On the other hand, the abstractive part is composed of two encoders and a decoder which works as --- (a) the first GRU-based bi-directional encoder and decoder work as basic sequence-to-sequence model on the extractive summary; and (b) the second GRU-based unidirectional encoder is used for fine encoding. Extensive computer experiments are conducted to determine the effectiveness of the STV--BEATS. Three standard benchmark datasets, namely, CNN/Daily Mail, DUC-2004, and DUC-2002 are used during experiments. Further, recall-oriented understudy for gisting evaluation (ROUGE) is used for validation of the STV--BEATS. Result reveals that the proposed STV--BEATS is capable of effective text summarization and achieves substantially better results over the state-of-the-art models.},
	author = {Minakshi Tomer and Manoj Kumar},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108108},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Abstractive text summarization, auto encoder, extractive text summarization, primary encoder, secondary encoder, skip thought vector},
	pages = {108108},
	title = {STV-BEATS: Skip Thought Vector and Bi-Encoder based Automatic Text Summarizer},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011680},
	volume = {240},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011680},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108108}}

@article{CHEN2022110000,
	abstract = {As more and more conversation-oriented streaming videos become available, streaming platforms have gradually taken the place of traditional media for people to access information. Still, conversation-oriented streaming videos are often lengthy and people are reluctant to view the whole video. In this research, we investigated highlight extraction on conversation-oriented streaming videos. Previous highlight extraction methods analyzed visual features of videos and are therefore unable to deal with conservation-oriented streaming videos whose highlights are related to streamer discourses and viewer responses. For this reason, the proposed highlight extraction method called COHETS does not evaluate visual features but rather simultaneously examines textual streams of streamer discourses and viewer messages to extract meaningful highlights. Experiments based on real world streaming data demonstrate that streamer discourses and viewer responses via their feedback messages are useful for extracting highlights of conversation-oriented streaming videos. Also, our designed position enrichment and message attention techniques effectively distill the embeddings of the two textual streams and lead to extraction results that are superior to those of state-of-the-art deep learning-based highlight extraction methods and extraction-based text summarization methods.},
	author = {Chien Chin Chen and Liang-Wei Lo and Sheng-Jie Lin},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.110000},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Supervised learning, Natural language processing, Video highlight extraction},
	pages = {110000},
	title = {COHETS: A highlight extraction method using textual streams of streaming videos},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122010930},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122010930},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.110000}}

@article{ORTEGABUENO2022107597,
	abstract = {Making machines understand language and reasoning on it has been one of the most challenging problems addressed by Artificial Intelligent researchers. This challenge increases when figurative language is used for communicating complex meanings, intentions, emotions and attitudes in creative and funny ways. In fact, sentiment analysis approaches struggle when facing irony, satire and other figurative languages, particularly those where the explanation of a prediction might arguably be as necessary as the prediction itself. This paper describes a new model MvAttLSTM based on deep learning for irony and satire detection in tweets written in distinct Spanish variants. The proposed model is based on an attentive-LSTM informed with three additional views learned from distinct perspectives. We investigate two strategies to pass these views into MvAttLSTM. We perform an extensive evaluation on three corpora, one for irony detection and two for satire detection. Moreover, in order to study the robustness of our proposed model, we investigate its performance on humor recognition. Experiments confirm that the proposed views help our model to improve its performance. Moreover, they show that affective information benefits our model to detect irony and satire. In particular, a first analysis of the results highlights the discriminating power of emotional features obtained from SenticNet and SEL lexicon. Overall, our system achieves the state-of-the-art performance in irony and satire detection in Spanish variants and competitive results in humor recognition.},
	author = {Reynier Ortega-Bueno and Paolo Rosso and Jos{\'e} E. {Medina Pagola}},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107597},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Irony and satire, Attention mechanism, Linguistic features, Contextualized pre-trained embedding, Fusing representation, Spanish variants, Figurative language},
	pages = {107597},
	title = {Multi-view informed attention-based model for Irony and Satire detection in Spanish variants},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121008595},
	volume = {235},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121008595},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107597}}

@article{PRADHAN2020106181,
	abstract = {Manually selecting appropriate scholarly venues is becoming a tedious and time-consuming task for researchers due to many reasons that include relevance, scientific impact, and research visibility. Sometimes, high-quality papers get rejected due to mismatch between the area of the paper and the scope of the journal. Recommending appropriate academic venues can, therefore, enable researchers to identify and take part in relevant conferences and publish in journals that matter the most. A researcher may certainly know of a few leading venues for her specific field of interest. However, a venue recommendation system becomes particularly helpful when exploring a new domain or when more options are needed. Due to high dimensionality and sparsity of text data, and complex semantics of the natural language, journal identification presents difficult challenges. We propose a novel and unified architecture that contains a Bi-directional LSTM (Bi-LSTM) and a Hierarchical Attention Network (HAN) to address the above problems. We call the proposed architecture modularized Hierarchical Attention-based Scholarly Venue Recommender system (HASVRec), which only requires the abstract, title, keywords, field of study, and author of a new paper along with its past publication record to recommend scholarly venues. Experiments on the DBLP-Citation-Network V11 dataset exhibit that our proposed approach outperforms several state-of-the-art methods in terms of accuracy, F1, nDCG, MRR, average venue quality, and stability.},
	author = {Tribikram Pradhan and Abhinav Gupta and Sukomal Pal},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106181},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender system, Bidirectional LSTM, Attentive pooling, Deep learning, Hierarchical Attention Network (HAN)},
	pages = {106181},
	title = {HASVRec: A modularized Hierarchical Attention-based Scholarly Venue Recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304135},
	volume = {204},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304135},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106181}}

@article{DENHAM2020105114,
	abstract = {The rise in the Internet of Things (IoT) and other sensor networks has created many vertically-distributed and high-velocity data streams that require specialized algorithms for true distributed data mining. This paper proposes a novel Hierarchical Distributed Stream Miner (HDSM) that learns relationships between the features of separate data streams with minimal data transmission to central locations. Experimental evaluation demonstrates significant improvements in classification accuracy over previously proposed distributed stream-mining approaches while minimizing data transmission and computational costs. HDSM's potential for dynamically trading off accuracy with computational resource costs is also demonstrated.},
	author = {Benjamin Denham and Russel Pears and M. Asif Naeem},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105114},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Distributed data stream mining, Vertically-distributed data, Online classification},
	pages = {105114},
	title = {HDSM: A distributed data mining approach to classifying vertically distributed data streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119304836},
	volume = {189},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119304836},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105114}}

@article{YUE2020106206,
	abstract = {Observed rating data in Web2.0 applications concerns user attributes and rating scores, which explicitly reflects users' overall evaluation on events, products and various informative items. However, the unobservable user preference is critical for personalized services, precise marketing, accurate advertising, etc. In this paper, by adopting Bayesian network (BN) with a latent variable as the knowledge framework to describe user preference using the latent variable, we propose user preference Bayesian network (UPBN) to represent dependence relations among the latent and observed variables. By incorporating the classic expectation maximization (EM) algorithm and scoring & search idea for learning a BN, we focus on UPBN construction from rating data, i.e., the learning of probability parameters and graphical structure. To make UPBN fit the rating data, we first give the constraints of structure and parameters in terms of inherence dependencies among user preference, latent variable and characteristics of EM. Consequently, we present a parallel and constraint induced algorithm for UPBN construction based on EM, structural EM (SEM) and Bayesian information criterion. To deal with the large amount of iterations of probability computations and guarantee the efficiency of model construction, we implement our algorithms upon Spark for the massive intermediate results and large scale rating datasets. Experimental results show the expressiveness of UPBN for preference modeling and the efficiency of model construction, and also demonstrate that UPBN outperforms some state-of-the-art models for user preference estimation and rating prediction.},
	author = {Kun Yue and Xinran Wu and Liang Duan and Shaojie Qiao and Hao Wu},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106206},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Rating data, User preference, Latent variable, Bayesian network, Expectation maximization, Spark},
	pages = {106206},
	title = {A parallel and constraint induced approach to modeling user preference from rating data},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304251},
	volume = {204},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304251},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106206}}

@article{ZHANG2022108006,
	abstract = {Recently, text-to-Image (T2I) generation has been well developed by improving synthesis authenticity, text-consistency and generation diversity. However, large amount of pairwise image--text data required restricts generalization of synthesis models only to its pre-trained language. In this paper, a cross-lingual pre-training method is proposed to adapt target low-resource language to pre-trained generative models. As far as we known, this is the first time that arbitrary input languages could access T2I generation. This joint encoding scheme fulfills both universal and visual semantic alignment. With any prepared GAN-based T2I framework, pre-trained source encoder model could be easily fine-tuned to construct target encoder model and hence entirely enable transfer of T2I synthesis ability between languages. After that, a semantic-level alignment independent of source T2I structure is established to guarantee optimal text consistency and detail generation. Different from monolingual T2I methods that apply discriminator to enhance generation quality, we use an adversarial training scheme that optimizes the sentence-level alignment along with the word-level alignment with a self-attention mechanism. Considering of training for low-resource languages lack of parallel texts in practice, target input embedding is designed available for zero-shot learning. Experimental results prove robustness of the proposed cross-lingual T2I pre-training on multiple downstream generative models and target languages applied.},
	author = {Han Zhang and Suyi Yang and Hongqing Zhu},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108006},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Cross-lingual pre-training, Text-to-image synthesis, Universal contextual word vector space, Semantic alignment, Joint adversarial training},
	pages = {108006},
	title = {CJE-TIG: Zero-shot cross-lingual text-to-image generation by Corpora-based Joint Encoding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011138},
	volume = {239},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011138},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108006}}

@article{XU2022107839,
	abstract = {Quantifying and predicting the long-term impact of both scientific papers and individual authors have important implications for many academic policy decisions, from identifying emerging trends to assessing the merits of proposals for potential funding. This paper presents SI-HDGNN, a novel heterogeneous dynamical graph neural network that explicitly models a heterogeneous, weighted, directed and attributed academic graph, enabling a prediction of the cumulative scientific impact of papers and authors by a specifically designed aggregation method. Unlike the existing feature-based or homogeneous approaches, SI-HDGNN addresses the problem by capturing the temporal--structural characteristics of the papers and authors as well as their complex interactions and long-term dependencies. Extensive experiments conducted on three large-scale multidisciplinary academic datasets demonstrate its superior performance in predicting the long-term scientific impact of both scientific papers and authors.},
	author = {Xovee Xu and Ting Zhong and Ce Li and Goce Trajcevski and Fan Zhou},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107839},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Scientific impact prediction, Heterogeneous information network, Graph neural network, Information diffusion, Science of science},
	pages = {107839},
	title = {Heterogeneous dynamical academic network for learning scientific impact propagation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121010285},
	volume = {238},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121010285},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107839}}

@article{YANG2020105214,
	abstract = {Last decade has witnessed a drastically increasing development of smart devices, while related mobile applications have emerged significantly in people's daily life. As such, understanding the pattern of mobile application usage and related online behavior is of great importance for a variety of purposes, such as application engineering, resource optimization, and marketing. Existing research of online usage discovery includes surveys from end-users, application provider-related analysis, and usage log mining. These works, however, suffer from some limitations, such as lacking of user socio-economics background, insufficient coverage and sample bias, etc. A novel and comprehensive application-usage profiling algorithm, termed as TAG, is proposed in this study to investigate online behavior. The proposed algorithm consists of three major steps: (i) T-step: representing usage data as a Term Frequency-Inverse Document Frequency based matrix; (ii) A-step: applying Alternating Least Squares factorization technique to reduce data sparseness and dimension; and last (iii) G-step: utilizing a smoothed Gaussian Mixture Model for clustering purpose. The performance of the proposed TAG algorithm is evaluated, taking a national dataset generated from 31,280 devices and 30,155 applications over 30 months as an example. Experimental results demonstrate that the proposed algorithm outperforms existing methods via forming accurate usage groups from school-level online behavior. As such, the superior clustering outcome demonstrates the flexibility and applicability of the proposed work for understanding online pattern using complex application usage data. Resultant knowledge can in turn be used to inform decision making and improve application development.},
	author = {Jie Yang and Jun Ma and Sarah K. Howard},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105214},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {User online behavior, Mobile applications, Alternating least squares, Smooth Gaussian mixture model},
	pages = {105214},
	title = {Usage profiling from mobile applications: A case study of online activity for Australian primary schools},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119305441},
	volume = {191},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119305441},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105214}}

@article{ZHU2022108741,
	abstract = {Bug localization, which aims to locate buggy source code files for given bug reports, is a crucial yet challenging software-mining task. Despite remarkable success, the state of the art falls short in handling (1) bug reports with diverse characteristics and (2) programs with wildly different behaviors. In response, this paper proposes a graph-based neural model BLoco for automated bug localization. To be specific, our proposed model decomposes bug reports into several bug clues to capture bug-related information from various perspectives for highly diverse bug reports. To understand the program in depth, we first design a code hierarchical network structure, Code-NoN, based on basic blocks to represent source code files. Correspondingly, a multilayer graph neural network is tailored to capture program behaviors from the Code-NoN structure of each source code file. Finally, BLoco further incorporates a bi-affine classifier to comprehensively predict the relationship between the bug reports and source files. Extensive experiments on five large-scale real-world projects demonstrate that the proposed model significantly outperforms existing techniques.},
	author = {Ziye Zhu and Hanghang Tong and Yu Wang and Yun Li},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108741},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Software mining, Bug localization, Bug report, Program behavior, Hierarchical network, Network of networks},
	pages = {108741},
	title = {Enhancing bug localization with bug report decomposition and code hierarchical network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003483},
	volume = {248},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003483},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108741}}

@article{ZHANG2021107135,
	abstract = {Incorporating extra attributes of customer reviews, such as user and product information, to align text representations to each attribute has improved the sentiment polarity classification performance. Existing works only treated such attributes separately thus ignored the interactive information between these attributes. In this paper, we proposed an interactive attributes attention model that considered all attributes to be relevant and investigated the interactive relationships in and across separate features to improve the sentiment classification performance for customer reviews. In addition to the local text encoder, three more interactive attribute encoders, including user--product, user--text, and product--text encoders, are applied to extract implicit information to align attribute features to text representations with a bilinear interaction instead of self-attention. To better integrate different information, a multiloss objective function is used to further improve the performance. The comparative experiments on the IMDB, Yelp, and Amazon datasets show that the proposed model achieves significant improvements in the effects of the bilinear interactions in and across attributes and local text features.},
	author = {You Zhang and Jin Wang and Xuejie Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107135},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Text classification, Sentence representation, Bilinear attention},
	pages = {107135},
	title = {Personalized sentiment classification of customer reviews via an interactive attributes attention model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003981},
	volume = {226},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121003981},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107135}}

@article{YANG2021106687,
	abstract = {Review-based recommendation algorithms can alleviate the data sparsity issue in collaborative filtering by combining user ratings and reviews in model learning. However, most existing methods simplify the feature extraction process from reviews by assuming that different granularities of information (e.g., word, review, and feature) are equally important, which cannot optimally leverage the most important information and thus achieves suboptimal recommendation accuracy. Besides, many existing works directly regard text features as users or items representations, which may not be enough to make precise representations due to the large amount of redundant information in reviews. To tackle the two problems mentioned above, we propose a deep learning-based method named Hierarchical Attention Network Oriented Towards Crowd Intelligence (HANCI). First, HANCI replaces the commonly-used topic models or CNN text processor with an RNN text processor in review feature extraction, which can fully exploit the advantages of the sequential dependencies of reviews by using the whole hidden layers of the bidirectional LSTM as outputs. Second, HANCI weighs the importance of features guided by crowd intelligence to more accurately represent each user on each item, and vice versa. Third, HANCI utilizes a hierarchical attention network based on multi-level review text analysis to extract more precise user preferences and item latent features, so that HANCI can explore the importance of words, the usefulness of reviews and the importance of features to achieve more accurate recommendation. Extensive experiments on three public datasets show that HANCI outperforms the state-of-the-art review-based recommendation algorithms in accuracy and meanwhile provides insightful explanations.},
	author = {Chao Yang and Weixin Zhou and Zhiyu Wang and Bin Jiang and Dongsheng Li and Huawei Shen},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106687},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Crowd intelligence, Explainable recommendation, Hierarchical attention, Review representation, Recommender system},
	pages = {106687},
	title = {Accurate and Explainable Recommendation via Hierarchical Attention Network Oriented Towards Crowd Intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120308169},
	volume = {213},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120308169},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106687}}

@article{BERNABEMORENO2020105236,
	abstract = {The latest development in cognitive technologies are helping us understand emotions and sentiments with unprecedented precision. Polarity detection is the key enabler to sentiment analysis and typically relies on experimental dictionaries, where terms are assigned polarity scores, yet lacking contextual information and based on human inputs and conventions. In this article, we present a novel approach to automatically extract a polarity dictionary from a particular domain, the stock market, without human intervention and addressing the scaling and thresholding problem. Our approach tracks the price changes of particular stocks over time, using it as a guiding polarity value. The magnitude of the price variation for a particular stock is then attributed to the financial news about this stock in corresponding period of time and that is what we use as our working corpus. On top of that, we derive the so-called binned corpus and apply the well-known TF--IDF information retrieval techniques to compute the TF--IDF value for each term. These values are then disseminated within the neighbourhood of each term based on the embeddings-enabled cosine distance. After introducing the problem and providing the background information, we thoroughly describe our method and all the components required to implement the system. Last but not least, we assign the terms to fuzzy linguistic labels and provide a volatility metric indicating how reliable our scores are depending on their distribution of occurrences in the corpus. To show how our approach works, we implement it for the Euro Stoxx 50 from January 2018 to March 2019 and discuss the results compared with typical approaches, pointing out potential improvements for further research work.},
	author = {J. Bernab{\'e}-Moreno and A. Tejeda-Lorente and J. Herce-Zelaya and C. Porcel and E. Herrera-Viedma},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105236},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Polarity extraction, Word embeddings, Information retrieval, Contextual bias, Fuzzy polarity},
	pages = {105236},
	title = {A context-aware embeddings supported method to extract a fuzzy sentiment polarity dictionary},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119305556},
	volume = {190},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119305556},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105236}}

@article{PRADHAN2020105784,
	abstract = {In academia, researchers collaborate with their peers to improve the quality of research and thereby enhance academic profiles. However, information overload in big scholarly data poses a challenge in identifying potential researchers for fruitful collaboration. In this article, we introduce a multi-level fusion-based model for collaborator recommendation, DRACoR (Deep learning and Random walk based Academic Collaborator Recommender). DRACoR fuses deep learning and biased random walk model to provide the recommendation for potential collaborators that share similar research interests at the peer level. We run a topic model on abstracts and Doc2Vec on titles on year-wise publications to capture the dynamic research interests of researchers. Author--author cosine similarity is computed from the feature vectors extracted from abstracts and titles and is then used to weigh edges in the author--author graph (AAG). We also aggregate various meta-path features with profile-aware features to bias the random walk behavior. Finally, we employ a random walk with restart(RWR) to recommend top N collaborators where the edge weights are used to bias the random walker's behavior. Extensive experiments on DBLP and hep-th datasets demonstrate the effectiveness of our proposed DRACoR model against various state-of-the-art methods in terms of precision, recall, F1-score, MRR, and nDCG.},
	author = {Tribikram Pradhan and Sukomal Pal},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105784},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Collaborator recommendation, Meta-path analysis, Random walk with restart (RWR), Topic modeling, Rank-based fusion, LSTM model},
	pages = {105784},
	title = {A multi-level fusion based decision support system for academic collaborator recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301817},
	volume = {197},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301817},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105784}}

@article{MOHSIN2022107711,
	abstract = {Effective bug classification and assignment to relevant developers improves the efficiency of software management. However, textually dependent approaches produce inconsistent results on varying datasets, while approaches that depend upon multi-source data can produce dataset conflicts and inaccuracy. Accordingly, we introduce a model based on Self-Paced Association augmentation and Node embedding (SPAN), which uses an effective combination of textually dependent and independent bug categorization to produce consistent results, followed by a bug assignment mechanism to prevent conflicts. To this end, we present a novel unified classifier and assignment model that exploits the connections between nodes in the Software Bug Report Network (SBRNet) to identify the target features. The model is capable of accurately categorizing bugs in a self-paced manner with association augmentation. Finally, we present an approach that assigns the most appropriate developer for bug resolution through SBRNet node information embedding. Our deep two-step self-paced solution is capable of categorizing software bugs with improved accuracy, while still utilizing fewer features. Results reveal that our model is more effective (up to 98% classification accuracy and 96% for bug assignment) when compared to its counterparts.},
	author = {Hufsa Mohsin and Chongyang Shi and Shufeng Hao and He Jiang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107711},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Bug classification, Bug triage, Association augmentation, Graph embedding, Bug assignment, Bug report analysis},
	pages = {107711},
	title = {SPAN: A self-paced association augmentation and node embedding-based model for software bug classification and assignment},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121009606},
	volume = {236},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121009606},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107711}}

@article{WANG2020106433,
	abstract = {Social voting is an emerging new feature in online social platforms, through which users can express their attitudes and opinions towards various interested subjects. Since both social relations and textual content decide the votes propagation, the diverse sources present opportunities and challenges for recommender systems. In this paper, we jointly consider these two factors for the online voting recommendation. First, we conduct feature learning on the vote content. Note that the vote questions are usually short and contain informal expressions, existing text mining methods cannot handle it well. We propose a novel topic-enhanced word embedding (TEWE) method, which learns the word vectors by considering both token-level semantics and document-level mixture topics. Second, we propose two Joint Topic-Semantic-aware Social Matrix Factorization (JTS-MF) models, which fuse social relations and textual content for the vote recommendation. Specifically, JTS-MF1 directly identifies the interaction strength to calculate the similarity among users and votes, while JTS-MF2 aims to preserve inter-user and inter-vote similarities during matrix factorization. Extensive experimental results on real online voting dataset show the effectiveness of our approaches against several state-of-the-art baselines. JTS-MF1 and JTS-MF2 models surpass the matrix factorization based method, with 25.4% and 57.1% improvements in the top-1 recall, and 59.12% and 25.1% improvements in the top-10 recall.},
	author = {Jia Wang and Hongwei Wang and Miao Zhao and Jiannong Cao and Zhuo Li and Minyi Guo},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106433},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Online voting, Recommender systems, Topic-enhanced word embedding, Matrix factorization},
	pages = {106433},
	title = {Joint Topic-Semantic-aware Social Matrix Factorization for online voting recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305621},
	volume = {210},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305621},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106433}}

@article{XU2021106858,
	abstract = {In this paper, we study abstractive summarization for product reviews in the recommender systems, which aims to generate condensed text for online reviews. The summary generation is not only relevant with the content of the review itself but should be fully aware of the intrinsic features of the corresponding user and product, i.e., personalization, which are helpful to identify the saliency information in the reviews. Therefore, we propose a Rating-boosted Abstractive Review Summarization with personalized generation (RARS). In our approach, we first propose a neural review-level attention model to effectively learn user preference embedding and product characteristic embedding from their history reviews. Then, we design a personalized decoder to generate the personalized summary, which utilizes the representations of the user and the product to calculate saliency scores for words in the input review to guide the summary generation process. In addition, the rating information can explicitly indicate the sentiment opinion, hence we jointly optimize the summary generation and rating prediction through a multi-task framework, where the two tasks inherently share user preference embedding and product characteristics embedding. Extensive experiments on four datasets show that our model can effectively improve the performance of both review summarization and rating prediction.},
	author = {Hongyan Xu and Hongtao Liu and Wang Zhang and Pengfei Jiao and Wenjun Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106858},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Summary generation, Rating prediction, Recommender system},
	pages = {106858},
	title = {Rating-boosted abstractive review summarization with neural personalized generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001210},
	volume = {218},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001210},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106858}}

@article{LENG2020105600,
	abstract = {Deep neural network needs large amount of data for training, to obtain more data, many simple data augmentation algorithms have been proposed. In this paper, we propose a LDA-based data augmentation algorithm to extend the training set. The proposed LDA-based data augmentation algorithm uses the topic model LDA to detect the key audio words in the recordings, and further to detect the key audio events and non-key audio events for each recording; with the detected key-audio-event segments, for each acoustic scene class, the probability distribution of key-audio-event's occurrence numbers, the probability distribution of key-audio-event's locations under each occurrence number and the probability distribution of key-audio-event's durations under each occurrence number is counted, and then the new recordings are generated according to these probability distributions. Experiments are done on the public TUT acoustic scenes 2016 dataset, and the experimental results show that compared with the other simple data augmentation algorithms, the proposed LDA-based data augmentation algorithm is more stable and effective, it can get better generalization ability for different kinds of neural network on different datasets.},
	author = {Yan Leng and Weiwei Zhao and Chan Lin and Chengli Sun and Rongyan Wang and Qi Yuan and Dengwang Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105600},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Acoustic scene classification, Topic model, LDA, Key audio event, Non-key audio event},
	pages = {105600},
	title = {LDA-based data augmentation algorithm for acoustic scene classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120300733},
	volume = {195},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120300733},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105600}}

@article{DU2021107247,
	abstract = {The prediction of event propagation has received extensive attention from the knowledge discovery community for applications such as social network analysis. The data describing these phenomena are multidimensional asynchronous event data that affect each other and show complex dynamic patterns in the continuous-time domain. The study of these dynamic processes and the mining of their potential correlations provide a foundation for the application of event propagation forecasting. However, conventional forecasting methods often make strong assumptions about the generative processes of the event data that may or may not reflect the reality, and the strong parametric assumptions also restrict the expressive power of the respective processes. Therefore, it is difficult to capture both the temporal and spatial effects of past event sequences. Most of the existing methods capture the intensity function of the Hawkes processes conditioned only on the historical events while ignoring the spatial information and the influences among different events. In this work, we propose the Astrologer, a graph neural Hawkes process that can capture the propagation of events from historical events on graph. The underlying idea of Astrologer is to incorporate the conditional intensity function of the Hawkes processes with a graph convolutional recurrent neural network. Using both synthetic and real-world datasets, we show that, Astrologer can learn the dynamics of event propagation without knowing the actual parametric forms. Astrologer can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.},
	author = {Haizhou Du and Yan Zhou and Yunpu Ma and Shiwei Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107247},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Hawkes process, Graph neural network, Forecasting, Spatio-temporal events},
	pages = {107247},
	title = {Astrologer: Exploiting graph neural Hawkes process for event propagation prediction with spatio-temporal characteristics},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121005098},
	volume = {228},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121005098},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107247}}

@article{DESSI2022109945,
	abstract = {Science communication has a number of bottlenecks that include the rising number of published research papers and its non-machine-accessible and document-based paradigm, which makes the exploration, reading, and reuse of research outcomes rather inefficient. Recently, Knowledge Graphs (KG), i.e., semantic interlinked networks of entities, have been proposed as a new core technology to describe and curate scholarly information with the goal to make it machine readable and understandable. However, the main drawback of the use of such a technology is that researchers are asked to manually annotate their research papers and add their contributions within the KGs. To address this problem, in this paper we propose SCICERO, a novel KG generation approach that takes in input text from research articles and generates a KG of research entities. SCICERO uses Natural Language Processing techniques to parse the content of scientific papers to discover entities and relationships, exploits state-of-the-art Deep Learning Transformer models to make sense and validate extracted information, and uses Semantic Web best practices to formally represent the extracted entities and relationships, making the written content of research papers machine-actionable. SCICERO has been tested on a dataset of 6.7M papers about Computer Science generating a KG of about 10M entities. It has been evaluated on a manually generated gold standard of 3,600 triples that cover three Computer Science subdomains (Information Retrieval, Natural Language Processing, and Machine Learning) obtaining remarkable results.},
	author = {Danilo Dess{\'\i} and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109945},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Knowledge graph, Scholarly domain, Scientific facts, Artificial intelligence},
	pages = {109945},
	title = {SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
	volume = {258},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109945}}

@article{GAO2020105418,
	abstract = {Network embedding aims to map vertices in a complex network into a continuous low-dimensional vector space. Meanwhile, the original network structure and inherent properties must be preserved. Most of the existing methods merely focus on preserving local structural features of vertices, whereas they largely ignore the community patterns and rich attribute information. For example, the title of papers in an academic citation network could imply their research directions, which are potentially valuable in seeking more meaningful representations of these papers. In this paper, we propose a Community-oriented Attributed Network Embedding (COANE) framework, which can smoothly incorporate the community information and text contents of vertices into network embedding. We design a margin-based random walk procedure on the network coupled with flexible margins among communities, which limit the scope of random walks. Inspired by the analogy between vertex sequences and documents, the statistical topic model is adopted to extract community features in the network. Furthermore, COANE integrates textual semantics into representations through the topic model while preserving their structural correlations. Experiments on real-world networks indicate that our proposed method outperforms six state-of-the-art network embedding approaches on network visualization, vertex classification and link prediction.},
	author = {Yuan Gao and Maoguo Gong and Yu Xie and Hua Zhong},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105418},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Representation learning, Attributed network embedding (ANE), Community detection, Topic model},
	pages = {105418},
	title = {Community-oriented attributed network embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306513},
	volume = {193},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306513},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105418}}

@article{JEONG2021106659,
	abstract = {In the real world, group recommendation that recommends items to a set of users (i.e., a group) is a challenging problem since it is very difficult to ensure the satisfaction of all group members with different preferences. Many existing group recommendation systems commonly use aggregation methods, which are insufficient to model group behavior where preference of a user as an individual is often changed when he/she is a member of a group. Some recent methods attempt to reflect this dynamic group behavior. However, they still have limitations in capturing complex relationships between items and group members. Moreover, previous approaches do not fully utilize useful context information available, and only use rating data. Reflecting context information together with ratings helps resolve a well-known data sparsity problem in group recommendation. In this paper, we propose a novel group recommendation framework called Dynamic Group behavior modeling that utilizes Context information for group recommendation(DGC). In DGC, we newly develop dynamic group behavior modeling that enables summarization of complex patterns in group decision making processes. To apply context information, firstly, we extract relevant context information from a heterogeneous information network (HIN) that contains rich information between various entities. Then, context information is properly applied to a group recommendation model by using semi-supervised learning that is composed of a supervised loss for label prediction and an unsupervised loss for context prediction. Experimental results show that our method provides significant performance improvement over other existing methods.},
	author = {Hyun Ji Jeong and Kwang Hee Lee and Myoung Ho Kim},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106659},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	pages = {106659},
	title = {DGC: Dynamic group behavior modeling that utilizes context information for group recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307887},
	volume = {213},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120307887},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106659}}

@article{PARK2020104825,
	abstract = {An essential challenge in aspect term sentiment classification using deep learning is modeling a tailor-made sentence representation towards given aspect terms to enhance the classification performance. To seek a solution to this, we have two main research questions: (1) Which factors are vital for a sentiment classifier? (2) How will these factors interact with dataset characteristics? Regarding the first question, harmonious combination of location attention and content attention may be crucial to alleviate semantic mismatch problem between aspect terms and opinion words. However, location attention does not reflect the fact that critical opinion words usually come left or right of corresponding aspect terms, as implied in the target-dependent method although not well elucidated before. Besides, content attention needs to be sophisticated to combine multiple attention outcomes nonlinearly and consider the entire context to address complicated sentences. We merge all these significant factors for the first time, and design two models differing a little in the implementation of a few factors. Concerning the second question, we suggest a new multifaceted view on the dataset beyond the current tendency to be somewhat indifferent to the dataset in pursuit of a universal best performer. We then observe the interaction between factors of model architecture and dimensions of dataset characteristics. Experimental results show that our models achieve state-of-the-art or comparable performances and that there exist some useful relationships such as superior performance of bi-directional LSTM over one-directional LSTM for sentences containing multiple aspects and vice versa for sentences containing only one aspect.},
	author = {Hyun-jung Park and Minchae Song and Kyung-Shik Shin},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.06.033},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Aspect-based sentiment analysis, Sentiment classification, Deep learning, LSTM, GRU, Attention},
	pages = {104825},
	title = {Deep learning models and datasets for aspect term sentiment classification: Implementing holistic recurrent attention on target-dependent memories},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119303004},
	volume = {187},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119303004},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.06.033}}

@article{PENG2022109933,
	abstract = {In Community Question Answering (CQA) websites, expert finding aims to seek relevant experts for answering questions. The core of expert finding is to match candidate experts and target questions precisely. Most existing methods usually learn a single feature vector for the expert from the historically answered questions, and then match the target question, which would lose fine-grained and low-level semantic matching information. In this paper, instead of matching with a unified expert embedding, we propose an expert finding method with a multi-grained hierarchical matching framework, named EFHM. Specifically, we design a word-level and question-level match encoder to learn the fine-grained semantic matching between each historical answered question and target question, and then propose an expert-level match encoder to learn an overall expert feature for matching the target question. Through the hierarchical matching mechanism, our model has the potential to capture the comprehensive relevance between candidate experts and target questions. Experimental results on six real-world CQA datasets demonstrate that the proposed method could achieve better performance than existing state-of-the-art methods.},
	author = {Qiyao Peng and Wenjun Wang and Hongtao Liu and Yinghui Wang and Hongyan Xu and Minglai Shao},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109933},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Expert finding, Hierarchical matching, Personalized, Community question answering},
	pages = {109933},
	title = {Towards comprehensive expert finding with a hierarchical matching network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122010267},
	volume = {257},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122010267},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109933}}

@article{PESSUTTO2020105339,
	abstract = {In the last few years, there has been growing interest in aspect-based sentiment analysis, which deals with extracting, clustering, and rating the overall opinion about the features of the entity being evaluated. Techniques for aspect extraction can produce an undesirably large number of aspects --- with many of those relating to the same product feature. Hence, aspect clustering becomes necessary. Current solutions for aspect clustering are monolingual, but in many practical situations, reviews for a given entity are available in several languages, calling for multilingual integration. In this article, we address the novel task of multilingual aspect clustering, which aims at grouping semantically related aspects extracted from reviews written in several languages. Our method is unsupervised and relies on the contextual information of the aspects, which is represented by word embeddings. This representation allied with a suitable similarity measure allows clustering related aspects. Our experiments on two datasets with five languages each showed that our unsupervised clustering technique achieves results that outperform monolingual baselines adapted to work with multilingual data. We also show the benefits of the multilingual approach compared to using languages in isolation.},
	author = {Lucas Rafael Costella Pessutto and Danny Suarez Vargas and Viviane P. Moreira},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105339},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Aspect-based sentiment analysis, Multilingual aspect clustering, Unsupervised learning, Word embeddings},
	pages = {105339},
	title = {Multilingual aspect clustering for sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306070},
	volume = {192},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306070},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105339}}

@article{DAI2022107659,
	abstract = {Text classification is an important and classical problem in natural language processing. Recently, Graph Neural Networks (GNNs) have been widely applied in text classification and achieved outstanding performance. Despite the success of GNNs on text classification, existing methods are still limited in two main aspects. On the one hand, transductive methods cannot easily adapt to new documents. Since transductive methods incorporate all documents into their text graph, they need to reconstruct the whole graph and retrain their system from scratch when new documents come. However, this is not applicable to real-world situations. On the other hand, many state-of-the-art algorithms ignore the quality of text graphs, which may lead to sub-optimal performance. To address these problems, we propose a Graph Fusion Network (GFN), which can overcome these limitations and boost text classification performance. In detail, in the graph construction stage, we build homogeneous text graphs with word nodes, which makes the learning system capable of making inference on new documents without rebuilding the whole text graph. Then, we propose to transform external knowledge into structural information and integrate different views of text graphs to capture more structural information. In the graph reasoning stage, we divide the process into three steps: graph learning, graph convolution, and graph fusion. In the graph learning step, we adopt a graph learning layer to further adapt text graphs. In the graph fusion step, we design a multi-head fusion module to integrate different opinions. Experimental results on five benchmarks demonstrate the superiority of our proposed method.},
	author = {Yong Dai and Linjun Shou and Ming Gong and Xiaolin Xia and Zhao Kang and Zenglin Xu and Daxin Jiang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107659},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Graph Neural Networks, Text classification, External knowledge, Graph fusion},
	pages = {107659},
	title = {Graph Fusion Network for Text Classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121009217},
	volume = {236},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121009217},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107659}}

@article{CHEN2021107521,
	abstract = {Travel recommendation is very critical to helping users quickly find products or services that they are interested in. The key to travel recommender systems is learning user shopping intentions, which are expressed through various supervision signals, such as the clicked products and their titles. Existing travel recommendation methods commonly infer user intentions from click behaviors on travel products. However, remarkable keywords in the product title, such as departure, destination, travel time, hotel, and transportation are paid less attention. To this end, we hypothesize that modeling click sequences and product keywords in title jointly would result in a more holistic representation of a product and towards more accurate recommendations. Thus, we propose a TRKG (short for Travel Recommendation with Keywords Generation) model, which fulfills the travel recommendation and keywords generation tasks simultaneously. To generate explainable outputs, unlike most previous approaches that regard the product title as a hidden feature vector, TRKG regards keywords in the product title as an additional supervision signal. Meanwhile, TRKG integrates the long-term and short-term user preferences in the travel recommendation component and the keywords generation component. To evaluate the proposed model, we constructed datasets from a large tourism e-commerce website in China. Extensive experiments demonstrate that the proposed method yields significant improvements over state-of-the-art methods.},
	author = {Lei Chen and Jie Cao and Guixiang Zhu and Youquan Wang and Weichao Liang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107521},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommendation system, Travel recommendation, Keywords generation, Deep learning, Multi-task learning},
	pages = {107521},
	title = {A multi-task learning approach for improving travel recommendation with keywords generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121007838},
	volume = {233},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121007838},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107521}}

@article{LIU2021106660,
	abstract = {Matrix factorization-based collaborative filtering, learning user and item latent features, has been one of the powerful recommendation techniques. Due to its simply modeling of user--item interactions by inner product of two vectors as a linear model, its efficiency needs an improvement. Neural Network-based matrix factorization has been proposed to deal with this issues. Usually, these methods are proposed on clean data, but in real applications, there are possibly unexpected noises and outliers, due to many subjective or objective reasons. The noisy instances would disturb the learning of normal instances and thus cause adverse affect as the model would also be easily over-fitted. Thus, we propose an enhanced neural matrix factorization model by introducing a self-paced learning (SPL) schema, which can automatically distinguish noisy instances and learn the model mostly based on clean instances. The main contribution of our model is that we design a bounded SPL learning schema with a parameter to control how many instances will be finally induced in the model learning. Thus, different from traditional SPL that gradually selects instances until all are selected, the bounded SPL mechanism tries to learn the model mainly on clean data and exclude noisy instances. The effectiveness of proposed method on collaborative filtering is demonstrated by extensive experiments on three widely used datasets.},
	author = {Zhen Liu and Xiaodong Feng and Yecheng Wang and Wenbo Zuo},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106660},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommendation, Deep learning, Noisy and outlier corruption, Instance weighting, Self-paced learning},
	pages = {106660},
	title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307899},
	volume = {213},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120307899},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106660}}

@article{LI2021106846,
	abstract = {Automatic keyphrase extraction algorithms aim to identify words and phrases that contain the core information in documents. As online scholarly resources have become widespread in recent years, better keyphrase extraction techniques are required to improve search efficiency. We present two features, keyphrase semantic diversity and keyphrase coverage, to overcome limitations of existing methods for unsupervised keyphrase extraction. Keyphrase semantic diversity is the degree of semantic variety in the extraction result, which is introduced to avoid extracting synonym phrases that contain the same high-score candidate. Keyphrase coverage refers to candidates' representativeness of other words in documents. We propose an unsupervised keyphrase extraction method called TripleRank, which evaluates three features: word position (a sensitive feature for academic documents) and two innovative features mentioned above. The architecture of TripleRank includes three sub-models that score the three features and a summing model. Though involving multiple models, there is no typical iteration process in TripleRank; hence, the computational cost is relatively low. TripleRank has led the experiment results on four academic datasets compared to four state-of-the-art baseline models, which confirmed the influence of keyphrase semantic diversity and keyphrase coverage and proved the efficiency of our method.},
	author = {Tuohang Li and Liang Hu and Hongtu Li and Chengyu Sun and Shuai Li and Ling Chi},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106846},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Keyphrase extraction, Keyphrase semantic diversity, Keyphrase coverage, Unsupervised approach},
	pages = {106846},
	title = {TripleRank: An unsupervised keyphrase extraction algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512100109X},
	volume = {219},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512100109X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106846}}

@article{ZHOU2020105695,
	abstract = {Due to the prevalence of social media sites, users are allowed to conveniently share their ideas and activities anytime and anywhere. Therefore, these sites hold substantial real-world event related data. Different from traditional social event detection methods which mainly focus on single-media, multi-modal social event detection aims at discovering events in vast heterogeneous data such as texts, images and video clips. These data denote real-world events from multiple dimensions simultaneously so that they can provide comprehensive and complementary understanding of social event. In recent years, multi-modal social event detection has attracted intensive attentions. This paper concentrates on conducting a comprehensive survey of extant works. Two current attempts in this field are firstly reviewed: event feature learning and event inference. Particularly, event feature learning is a pre-requisite because of its ability on translating social media data into computer-friendly numerical form. Event inference aims at deciding whether a sample belongs to a social event. Then, several public datasets in the community are introduced and the comparison results are also provided. At the end of this paper, a general discussion of the insights is delivered to promote the development of multi-modal social event detection.},
	author = {Han Zhou and Hongpeng Yin and Hengyi Zheng and Yanxia Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105695},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multi-modality, Social event detection, Feature learning, Event inference},
	pages = {105695},
	title = {A survey on multi-modal social event detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301271},
	volume = {195},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301271},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105695}}

@article{AN2022107623,
	abstract = {The growing popularity of location-based social networks gives rise to a tremendous amount of social check-ins data, which are broadly used in previous studies to produce dense venue representations for various trajectory mining tasks. In this work, we focus on the interpretability of venue representations, an essential property that existing methods fail to provide. We propose two novel models to generate interpretable and easy-to-understand venue representations. The first model, CEM, is a category-aware (a category may be a restaurant, a mall, etc.) check-in embedding model and generates venue and category representations by capturing the sequential patterns of check-in records. With the second model, XEM, each dimension of the venue representation corresponds to a semantic anchor (i.e., a category) and can be interpreted as a coherent topic. We conduct extensive experiments using real-world check-in datasets for venue similarity computation and venue semantic annotation, and empirically show that introducing interpretability to the venue representations improves the performance of various downstream tasks.},
	author = {Ning An and Meng Chen and Li Lian and Peng Li and Kai Zhang and Xiaohui Yu and Yilong Yin},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107623},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Venue semantic representation, Interpretable, Embedding learning, Semantic mapping, Check-ins},
	pages = {107623},
	title = {Enabling the interpretability of pretrained venue representations using semantic categories},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121008856},
	volume = {235},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121008856},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107623}}

@article{BEHPOUR2021106907,
	abstract = {Identifying the trending topics in journals and conferences is valuable for understanding the role of authors, institutions, and funding agencies in the progression of knowledge produced in the field. However, many available clustering methods do not accommodate a desire for temporally clustered results that are typical of trends, in part because time of publication is often neglected as a feature. As a demonstration of how time can be emphasized in trend detection, we use a novel approach of introducing a weighted temporal feature to bias a topic clustering toward articles in a similar time frame; this is performed over a set of finance journal abstracts from 1974 to 2020. Latent Dirichlet Allocation (LDA) is used to parameterize each abstract, followed by dimensionality reduction using Singular Value Decomposition (SVD). We detect trending finance topics that are not identifiable when we use a standard clustering approach with no temporal bias. To identify trending topics, we utilize a metric of the silhouette score divided by the standard deviation of clusters over time. We then isolate topics identified by this metric and validate them using expert judgment. Our clustering strategy using temporal bias can be readily utilized in other fields for discovering the rise and fall of trends.},
	author = {Sahar Behpour and Mohammadmahdi Mohammadi and Mark V. Albert and Zinat S. Alam and Lingling Wang and Ting Xiao},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106907},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Text mining, Trend detection, Temporal biased clustering, Machine learning},
	pages = {106907},
	title = {Automatic trend detection: Time-biased document clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001702},
	volume = {220},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001702},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106907}}

@article{MARTIN2022109265,
	abstract = {Our society produces and shares overwhelming amounts of information through Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern in most countries. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is a crucial step to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi-automated fact-checking and hoaxes propagation analysis that can be used to implement applications designed for both the general public and for fact-checking organisations. FacTeR-Check implements three different modules relying on the XLM-RoBERTa Transformer architecture to evaluate semantic similarity, to calculate natural language inference and to build search queries through automatic keywords extraction and Named-Entity Recognition. The three modules have been validated using state-of-the-art benchmark datasets, exhibiting good performance in all of them. Besides, FacTeR-Check is employed to collect and label a dataset, called NLI19-SP, composed of more than 40,000 tweets supporting or denying 60 hoaxes related to COVID-19, released publicly. Finally, an analysis of the data collected in this dataset is provided, which allows to obtain a deep insight of how disinformation operated during the COVID-19 pandemic in Spanish-speaking countries.},
	author = {Alejandro Mart{\'\i}n and Javier Huertas-Tato and {\'A}lvaro Huertas-Garc{\'\i}a and Guillermo Villar-Rodr{\'\i}guez and David Camacho},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109265},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Misinformation, Transformers, COVID-19, Hoax, Natural language inference, Semantic similarity},
	pages = {109265},
	title = {FacTeR-Check: Semi-automated fact-checking through semantic similarity and natural language inference},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122006323},
	volume = {251},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122006323},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109265}}

@article{LIAO2022108665,
	abstract = {Attributed graph clustering is an important task for grouping the nodes in a graph. In recent years, the algorithms based on graph convolutional networks (GCN) have achieved promising performance. However, almost all existing methods ignore that the nonlinearity between two consecutive GCN layers is unnecessary for improving the performance of attributed graph clustering, and may even harm the efficiency of the model. In this paper, we propose a novel deep linear graph attention model for attributed graph clustering (DLGAMC), which consists of an attention-based aggregation module and a similarity preserve module. Specifically, we simply exploit cosine similarity to construct the attention for aggregation, which does not need to learn extra attention parameters. It is worth noting that the attention we designed not only explicitly considers the similarity of attribute information, but also implicitly takes into account the local graph structure. To select the proper order of aggregation, we propose an adaptive strategy to evaluate the smoothness of node representations, where intra-cluster distance and inter-cluster distance are the key indicators in this process. To learn node representations for clustering, we design a similarity preserve module to preserve local similarity and global dissimilarity of the smooth features obtained by multiple aggregations, which is different from the ideas in reconstruction-based methods. Finally, k-means is performed on the learned representations to obtain the cluster partition. Experiments on several datasets show that our algorithm achieves great performance in attributed graph clustering tasks.},
	author = {Huifa Liao and Jie Hu and Tianrui Li and Shengdong Du and Bo Peng},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108665},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Attributed graph clustering, Linear model, Attention-based fusion, Graph neural network, Graph representation learning},
	pages = {108665},
	title = {Deep linear graph attention model for attributed graph clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003057},
	volume = {246},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003057},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108665}}

@article{WANDABWA2021107249,
	abstract = {Consumption of content in short-text microblogs is necessitated to a large extent by individual users and their friendship network interests. Based on the dynamism in the data throughput on such platforms, e.g., Twitter, prevailing conditions are bound to determine the nature of consumed or disseminated content. Therefore, semantic interests differ over time even for individual users. Detecting this semantic change over time is integral in mapping user profiles over a time period, especially in microblogs where only the extrinsic user profile identifiers provide metadata that seldom evolve. This is vital in serving relevant third-party content as well as in the computation of topical interest variations over time. In essence, current, and relevant topics of interest to a user on such a platform may not be representative of the same users' interests a few months later. In our quest to identify the most user-representative interests at any given time, each topical term was modelled as the inner product between word embeddings and a time-based embedding representation of assigned topics at varied time periods. The model was fitted onto tweets as time-series documents. To validate the model, changes in the extracted user-representative interests over time were semantically weighed against a mirrored, time-variant dataset. Interest weights across the time-variant datasets were computed and validated in five sub-topics for a period spanning two and a half years. Linearity in the relationships between the test and validation sets could be identified, more so in emerging topics. A Pearson correlation coefficient as high as 0.871 was achieved in interest change verification over the tested period.},
	author = {Herman M. Wandabwa and M. Asif Naeem and Farhaan Mirza and Russel Pears},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107249},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {User profiling, Text mining, Neural Networks, Information retrieval, Short-text microblogs},
	pages = {107249},
	title = {Multi-interest semantic changes over time in short-text microblogs},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121005116},
	volume = {228},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121005116},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107249}}

@article{LI2021107163,
	abstract = {Understanding word-level emotion in terms of both category and intensity has always been considered an essential step in addressing text emotion classification tasks. Existing studies have mainly adopted the categorical lexicons that are tagged by predefined emotion taxonomies to link affective words with discrete emotions. However, in these lexicons, emotion tags are restricted to a specific set of basic emotions. Moreover, the emotional intensity is ignored, making these methods less flexible and less informative. This paper proposes a novel method to generate a word-level emotion distribution (WED) vector by incorporating domain knowledge and dimensional lexicon. The proposed method can link a word with more generic and fine-grained emotion taxonomies with quantitatively computed intensities. We propose two schemas to utilize the WED vector implicitly and explicitly to facilitate classification. The implicit approach implements a rule-based conversion strategy to augment the information in the label space. The explicit approach exploits WED as an emotional word embedding to enhance the sentiment feature. We conduct extensive experiments on seven multiclass datasets. The results indicate that both proposed schemas produce competitive results compared with the state-of-the-art baselines.},
	author = {Zongxi Li and Haoran Xie and Gary Cheng and Qing Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107163},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotion classification, Emotional embedding, VAD, Emotional lexicon},
	pages = {107163},
	title = {Word-level emotion distribution with two schemas for short text emotion classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004263},
	volume = {227},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121004263},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107163}}

@article{PENTA2021107342,
	abstract = {Document clustering is a powerful method with numerous applications, where the core idea is to group text into smaller and more manageable pieces of semantically related information. While there has been progress in the research community on improving the quality of clusters, the extraction of information that explains the semantic content of the clusters is still mostly a manual activity, as it requires the inspection of sample documents. In this work, we propose three main cluster explanation approaches that extend the current state of the art, which is mostly based on predicting a label for each cluster (i.e. cluster labelling). The first approach is based on scores extracted from the word distributions; the second is based on augmenting the score-based explanations using external knowledge-bases, and the third one combines the first two methods to exploit the knowledge coming from a labelled dataset. We also discuss the limitations of the current metric presented in the literature and extend it to evaluate our approaches using ground truth. We also provided an extensive set of evaluations to verify the effectiveness of the proposed algorithms, by running multiple experiments to compare our approaches with different baselines. Results indicate that improved explanations are possible and that linking external knowledge can provide more general cluster descriptions. Semi-supervised approaches also demonstrated meaningful insight. User studies were also conducted highlighting that users prefer specialized explanations.},
	author = {Antonio Penta and Anandita Pal},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107342},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Document clustering, Text analytics, Explainability, Cluster summarization, Cluster labelling, Clusters explanations},
	pages = {107342},
	title = {What is this Cluster about? Explaining textual clusters by extracting relevant keywords},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121006043},
	volume = {229},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121006043},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107342}}

@article{LIU2020106435,
	abstract = {Nowadays, social network sites (SNSs) have been significant platforms for content sharing in our daily life. With the emergence of different kinds of social network sites and users' diverse needs for content sharing, their content sharing practices are generally taken place in multiple SNSs. To construct models that can characterize users' content sharing practices in a composite context constituted by multiple social network sites (cross-site user generated content modeling) has been an emerging research topic in web data mining and human behavior research. However, previous methods such as Dirichlet Multinomial Mixture model (DMM), Biterm Topic Model (BTM), Twitter-LDA and MultiLDA have limited representation ability or are based on unreliable assumption, which cannot characterize the user generated content (UGC) accurately from the perspective of multiple SNSs. In this paper, we first conduct an empirical study to investigate the characteristics of users' content sharing practices in cross-site context, based on which we propose a more reliable cross-site UGC model named CrossSite-LDA (C-LDA). We then evaluate the performances of the C-LDA model with four state-of-the-art models based on the two data sets sampled from Weibo--Douban and Facebook--Twitter. Results show that the C-LDA has better performances in perplexity, word coherence, topic KL divergence, UCI and UMass metrics compared with existing models, which suggests its superior accuracy on modeling users' content characteristics in cross-site context.},
	author = {Baoxi Liu and Peng Zhang and Tun Lu and Ning Gu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106435},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multiple social network sites, User generated content modeling, Topic model, Weibo, Douban},
	pages = {106435},
	title = {A reliable cross-site user generated content modeling method based on topic model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305645},
	volume = {209},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305645},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106435}}

@article{YANG2022108488,
	abstract = {Keyphrase generation is an important fundamental task of natural language processing, which can help users quickly obtain valuable information from a large number of documents especially when they are facing with informal social media text. Existing Recurrent Neural Network (RNN) based keyphrase generation approaches cannot properly model the dependency structure of the informal text, which is often implicit between those distant words and plays an important role in extracting salient information. To obtain core features of text, we apply Graph Convolutional Network (GCN) on document-level graph to capture dependency structure information. The GCN-based node representations are further fed into a predictor network to provide potential candidates for copying mechanism. Moreover, we utilize a novel variational selector network to determine the final selection probability of each word in a phrase, which relies on its probabilities of copying from a given document and being generated from a vocabulary. Eventually, we introduce an enhancement mechanism to maximize the mutual information between document and generated keyphrase, thus ensuring the consistency between them. Experiment results show that our model outperforms previous state-of-the-art baselines on three social datasets, including Weibo, Twitter and StackExchange.},
	author = {Peng Yang and Yanyan Ge and Yu Yao and Ying Yang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108488},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Keyphrase generation, Sequence-to-sequence, Graph convolutional network, Mutual information},
	pages = {108488},
	title = {GCN-based document representation for keyphrase generation enhanced by maximizing mutual information},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122002076},
	volume = {243},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122002076},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108488}}

@article{LI2021106827,
	abstract = {Because of its efficiency, word embedding has been widely used in many natural language processing and text modeling tasks. It aims to represent each word by a vector so such that the geometry between these vectors can capture the semantic correlations between words. An ambiguous word can often have diverse meanings in different contexts, a quality which is called polysemy. The bulk of studies aimed to generate only one single embedding for each word, whereas a few studies have made a small number of embeddings to present different meanings of each word. However, it is hard to determine the exact number of senses for each word, as meanings depend on contexts. To address this problem, this paper proposes a novel adaptive cross-contextual word embedding (ACWE) method for capturing the word polysemy in different contexts based on topic modeling, in which the word polysemy is defined over a latent interpretable semantic space. The proposed ACWE consists of two main parts, in the first of which an unsupervised cross-contextual probabilistic word embedding model is designed to obtain the global word embeddings, and each word is represented by an embedding in the unified latent semantic space. Based on the global word embeddings, an adaptive cross-contextual word embedding process is then devised in the second part to learn the local embeddings for each polysemous word in different contexts. In fact, a word embedding is adaptively adjusted and updated with respect to different contexts to generate different word embeddings tailored to the corresponding contexts. The proposed ACWE is validated on two datasets collected from Wikipedia and IMDb on different tasks including word similarity, polysemy induction, semantic interpretability, and text classification. Experimental results indicate that ACWE does not only outperform the established word embedding methods, which consider word polysemy on six popular benchmark datasets, but it also yields competitive performance compared with state-of-the-art deep learning-based approaches without considering polysemy. Moreover, the proposed ACWE significantly improves the performances of text classification both in precision and F1, and the visualizations of the semantics of words demonstrate the feasibility and advantage of the proposed ACWE model on polysemy.},
	author = {Shuangyin Li and Rong Pan and Haoyu Luo and Xiao Liu and Gansen Zhao},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106827},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Word polysemy, Representation learning, Adaptive word embeddings, Tailored word embedding, Topic modeling, Semantic learning},
	pages = {106827},
	title = {Adaptive cross-contextual word embedding for word polysemy with unsupervised topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121000903},
	volume = {218},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121000903},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106827}}

@article{QIN2021107160,
	abstract = {Emotion distribution learning aims to annotate unlabeled instances with a set of emotion categories and their strengths. Non-negative Matrix Tri-Factorization (NMTF) introduces an association matrix between document clusters and word clusters to help the domain adaptation task in emotion distribution learning. Nevertheless, many prior cross-domain emotion distribution learning methods had two major deficiencies. First, they hypothesize that there is a one-to-one correspondence between document clusters and emotion labels. In their experiments, the number of document clusters depends on the number of labels. Second, the prior work does not endow models with adequate constraints. In the real scenario of cross-domain emotion distribution learning, there are potential constraints that may improve the performance of such models. In order to address these problems, we propose a constrained optimization approach based on NMTF for cross-domain emotion distribution learning. In our model, the relationship between document clusters and emotion labels is not always one-to-one. A novel content-based constraint is also endowed based on the hypothesis that documents belonging to the same clusters must have similar content. We solve the optimization problem by an alternately iterative algorithm and show the proof of convergence. Experiments on 12 real-world cross-domain emotion distribution learning tasks validate the effectiveness of our method.},
	author = {Xiaorui Qin and Yufu Chen and Yanghui Rao and Haoran Xie and Man Leung Wong and Fu Lee Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107160},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotion distribution learning, Non-negative Matrix Tri-Factorization, Domain adaptation, Content-based constraint},
	pages = {107160},
	title = {A constrained optimization approach for cross-domain emotion distribution learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004238},
	volume = {227},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121004238},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107160}}

@article{QIAN2020105684,
	abstract = {Feature selection used for dimensionality reduction of the feature space plays an important role in multi-label learning where high-dimensional data are involved. Although most existing multi-label feature selection approaches can deal with the problem of label ambiguity which mainly focuses on the assumption of uniform distribution with logical labels, it cannot be applied to many practical applications where the significance of related label for every instance tends to be different. To deal with this issue, in this study, label distribution learning covered with a certain real number of labels is introduced to design a model for the labeling-significance. Nevertheless, multi-label feature selection is limited to handling only labels consisting of logical relations. In order to solve this problem, combining the random variable distribution with granular computing, we first propose a label enhancement algorithm to transform logical labels in multi-label data into label distribution with more supervised information, which can mine the hidden label significance from every instance. On this basis, to remove some redundant or irrelevant features in multi-label data, a label distribution feature selection algorithm using mutual information and label enhancement is developed. Finally, the experimental results show that the performance of the proposed method is superior to the other state-of-the-art approaches when dealing with multi-label data.},
	author = {Wenbin Qian and Jintao Huang and Yinglong Wang and Wenhao Shu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105684},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Feature selection, Multi-label data, Granular computing, Label enhancement, Mutual information},
	pages = {105684},
	title = {Mutual information-based label distribution feature selection for multi-label learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301210},
	volume = {195},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301210},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105684}}

@article{XU2021107225,
	abstract = {Textual emotion analysis is a challenging research topic in the field of natural language processing (NLP), which plays an important role in related NLP tasks, such as opinion mining and personalized recommendation. Existing research on emotion analysis has focused mostly on detecting types of emotions, and has solved problems using classification-based methods. Recently, fine-grained emotion analysis has attracted the attention of researchers for probing the essential elements of emotions, such as the causes, experiencers and results of emotion events, which could help further elucidate textual emotions in more depth. In this paper, we focus on the task of emotion cause extraction, aiming to recognize the causes in sentences that provoke certain emotions. We propose a two-stage supervised ranking method for accurately extracting the emotion causes based on information retrieval techniques. In the first stage, we measure the complexity of provoked emotions using query performance predictors to distinguish the number of causes for each emotion in contexts. In the second stage, we incorporate the emotion complexity into learning an autoencoder-enhanced ranking model for accurately extracting the causal clauses. We also extract abundant emotion-level clause features for clause representations as the learning samples. We evaluate the proposed method on an existing dataset for emotion cause extraction and demonstrate that our method significantly outperforms the state-of-the-art baseline methods. The proposed method is effective in extracting textual emotion causes in sentences, which can greatly benefit in-depth emotion analysis for effective cognitive computing.},
	author = {Bo Xu and Hongfei Lin and Yuan Lin and Kan Xu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107225},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotion cause extraction, Sentiment analysis, Ranking model, Natural language processing},
	pages = {107225},
	title = {Two-stage supervised ranking for emotion cause extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004871},
	volume = {228},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121004871},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107225}}

@article{LI2020105436,
	abstract = {Text representation, a crucial step for text mining and natural language processing, concerns about transforming unstructured textual data into structured numerical vectors to support various machine learning and data mining algorithms. For document classification, one classical and commonly adopted text representation method is Bag-of-Words (BoW) model. BoW represents document as a fixed-length vector of terms, where each term dimension is a numerical value such as term frequency or tf-idf weight. However, BoW simply looks at surface form of words. It ignores the semantic, conceptual and contextual information of texts, and also suffers from high dimensionality and sparsity issues. To address the aforementioned issues, we propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from external knowledge base, then conceptualizes words and phrases in the document into higher level semantics (i.e. concepts) in a probabilistic manner, and eventually represents a document as a distributed vector in the learned concept space. By utilizing background knowledge from knowledge base, BoC representation is able to provide more semantic and conceptual information of texts, as well as better interpretability for human understanding. We also propose Bag-of-Concept-Clusters (BoCCl) model which clusters semantically similar concepts together and performs entity sense disambiguation to further improve BoC representation. In addition, we combine BoCCl and BoW representations using an attention mechanism to effectively utilize both concept-level and word-level information and achieve optimal performance for document classification.},
	author = {Pengfei Li and Kezhi Mao and Yuecong Xu and Qi Li and Jiaheng Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105436},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Natural language processing, Text representation, Document classification, Knowledge base, Interpretability},
	pages = {105436},
	title = {Bag-of-Concepts representation for document classification based on automatic knowledge acquisition from probabilistic knowledge base},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306604},
	volume = {193},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306604},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105436}}

@article{ELAKROUCHI2021106650,
	abstract = {An extremely competitive business environment requires every company to monitor its competitors and anticipate future opportunities and risks, creating a dire need for competitive intelligence. In response to this need, foresight study became a prominent field, especially the concept of weak signal detection. This research area has been widely studied for its utility, but it is limited by the need of human expert judgments on these signals. Moreover, the increase in the volume of information on the Internet through blogs and web news has made the detection process difficult, which has created a need for automation. Recent studies have attempted topic modeling techniques, specifically latent Dirichlet allocation (LDA), for automating the weak signal detection process; however, these approaches do not cover all parts of the process. In this study, we propose a fully automatic LDA-based weak signal detection method, consisting of two filtering functions: the weakness function aimed at filtering topics, which potentially contains weak signals, and the potential warning function, which helps to extract only early warning signs from the previously filtered topics. We took this approach with a famous daily web news dataset, and we could detect the risk of the COVID19 pandemic at an early stage.},
	author = {Manal {El Akrouchi} and Houda Benbrahim and Ismail Kassou},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106650},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Weak signals, Topic modeling, Latent Dirichlet allocation},
	pages = {106650},
	title = {End-to-end LDA-based automatic weak signal detection in web news},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307796},
	volume = {212},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120307796},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106650}}

@article{WAN2022109551,
	abstract = {Data sparsity and cold start are two critical issues which need to be addressed in recommender systems (RSs). Currently, most methods address these issues by applying user history files or some side information to improve the user model and complete the rating matrix. However, such methods cannot perform well when labeled data is scarce or unavailable. In this paper, we propose a dual learning-based recommendation approach (DLRA). DLRA can trigger initial recommendation and improve the quality of recommendations by using the duality characteristics of RSs, even when the available labeled information is scarce. Specifically, DLRA regards the recommendation task as two independent subtasks --- primal task and dual task, and these two tasks show strong duality in DLRA. The primal task is item-centered which aims to find users who can rate high for items, while the dual task is user-centered that aims to recommend the most favorite items to users. These two tasks have strong dualities in terms of the recommendation space, selection probability and recommendation basis. Based on these dualities, we design three dual learning strategies to couple the whole recommendation process and realize the self-tuning and self-improvement of each task model, and finally optimize the whole recommendation model. Based on the dataset of Movielens and BookCrossing, we simulate data sparsity and cold start recommendation scenarios, the experimental results show that DLRA achieves substantial improvement when the labeled data is scare, and it outperforms other hybrid recommendation approaches and deep learning strategies with a smaller predictive error as well as better recommendation accuracy.},
	author = {Shanshan Wan and Ying Liu and Dongwei Qiu and James Chambua and Zhendong Niu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109551},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender system, Dual learning, Data sparsity, Duality, Hybrid filtering recommendation},
	pages = {109551},
	title = {A dual learning-based recommendation approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122007791},
	volume = {254},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122007791},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109551}}

@article{LIU2021106917,
	abstract = {In many reality networks, nodes contain rich text attribute information that exhibits significant role in describing the properties of them and relationship between them. The integration of structural and textual information is beneficial to downstream network analysis tasks. In this work, we present an Enhanced Textual Information Network Embedding model, called ETINE, for learning network embeddings with not only global structural information but also deep semantic relationship between nodes. Specifically, we formulate the optimization of our proposed structure-based and text-based loss functions as a matrix approximation problem. Moreover, to enhance the efficiency and robustness of the proposed method, we propose to optimize the loss functions with an efficient randomized singular value decomposition (RSVD) method. Extensive experiments on four benchmarks demonstrate that our model outperforms other state-of-the-art baselines in multi-class node classification, network reconstruction and node clustering tasks.},
	author = {Wenfeng Liu and Maoguo Gong and Zedong Tang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106917},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Network embedding, Textual information networks, Structural proximity, Textual proximity, Matrix approximation},
	pages = {106917},
	title = {ETINE: Enhanced Textual Information Network Embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001805},
	volume = {220},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001805},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106917}}

@article{LIU2022108495,
	abstract = {In many fields, how to catch the related-topic Web resources is crucial. As a vertical search method, focused crawler has received great attention in recent years. Currently, most focused crawlers consider multiple evaluating factors of the hyperlinks and use the weighted sum approach to compute the priorities of unvisited hyperlinks. However, the proper weighted coefficients are hard to determine, and their unsuitable values may even cause the direction of crawlers to deviate seriously from the topic. To overcome this issue, this article builds a multi-objective optimization model based on Web text and link structure and designs a crawler framework called the Web space evolution (WSE), where a hyperlink bank whose radius is gradually increased is introduced to extend the search scape of crawlers in Web space. To improve the uniformity and diversity of hyperlinks, a nearest and farthest candidate solution method is combined with the fast non-dominated sorting to choose Pareto-optimal solutions (hyperlinks). A domain ontology based on the formal concept analysis is applied to establish the topic model. By incorporating the WSE and the domain ontology into the focused crawling, a novel focused crawler called FCWSEO is proposed to collect topic-relevant webpages. The experimental results on the rainstorm disaster domain show that the FCWSEO outperforms other focused crawler strategies in terms of the quantity and quality of retrieved relevant webpages.},
	author = {Jingfa Liu and Xin Li and Qiansheng Zhang and Guo Zhong},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108495},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Focused crawler, Web space evolution, Multi-objective optimization, Pareto optimal, Ontology},
	pages = {108495},
	title = {A novel focused crawler combining Web space evolution and domain ontology},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512200212X},
	volume = {243},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512200212X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108495}}

@article{YE2022108699,
	abstract = {Hyperbolic embedding aims to reveal the hidden space by capturing most of the properties observed in real networks. Most existing hyperbolic embedding models map to learn the representation vectors while preserving the microscopic adjacency, which cannot accurately represent the mesoscopic community structures. To this end, this study proposes a Community Preserving Hyperbolic Embedding model (CPHE). Specifically, we regularize the likelihood function of hyperbolic embedding by adding to the community co-occurrence relation (CCR). We then construct a closed loop for node embedding and community detection. Thus, the representation vectors and CCR alternately update. Finally, to avoid the distortion of the representing community, an equivalent majorization based on the sum of linear ratios programming is achieved for the numerical solution. Application experiments are implemented on both synthetic and real-world networks to evaluate the embedding performance. The accuracy and normalized mutual information (NMI) of community detection improved by approximately 3% and 2.4%, respectively, and the area under the receiver operating characteristic curve (AUROC) of link prediction improved by approximately 1.3%, demonstrating the advantages of the proposed model.},
	author = {Dongsheng Ye and Hao Jiang and Ying Jiang and Qiang Wang and Yulin Hu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108699},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Hyperbolic geometry, Network embedding, Community embedding},
	pages = {108699},
	title = {Community preserving mapping for network hyperbolic embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003227},
	volume = {246},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003227},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108699}}

@article{ZHOU2020105458,
	abstract = {Real-world networks usually consist of a large number of interacting, multi-typed components which are usually referred as heterogeneous information networks (HIN). HIN that associated with various attributes on nodes is defined as attributed HIN (or AHIN). Clustering is a fundamental task for HIN and AHIN. However, most of the current existing methods focus on single type nodes and there is very limited existing work that groups objects of different types into the same cluster. This is largely due to the reasons that object similarities can either be attribute-based or link-based between same type of nodes and it is challenging to incorporate both in a unified framework. To bridge this gap, in this paper, we propose a framework, namely Cross Multi-Type Objects Clustering in Attributed Heterogeneous Information Network, or CMOC-AHIN, to integrate both the attribute information and multi-type node clustering in a principled way. We empirically show superior performances of CMOC-AHIN on three large scale challenging data sets and also summarize insights on the performances compared to other state-of-the-arts methodologies.},
	author = {Sheng Zhou and Jiajun Bu and Zhen Zhang and Can Wang and Lingzhou Ma and Jianfeng Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105458},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Heterogeneous information network, Clustering, Attributed network},
	pages = {105458},
	title = {Cross Multi-Type Objects Clustering in Attributed Heterogeneous Information Network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306719},
	volume = {194},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306719},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105458}}

@article{QIN2020105750,
	abstract = {In recent years, many efforts based on deep learning have been made to address the issue of text tagging. However, these work generally neglect to consider the neighborhood effect which may help improve the accuracy of predictions. For this, we present a neighborhood-aware deep model for text tagging (NATT). A neural component which combines bi-directional recurrent neural network and self-attention mechanism, is firstly selected as the text encoder to encode the target document into one feature vector. Then, k-nearest-neighbor documents of the target document are identified and encoded into feature vectors one by one with the same text encoder. Simultaneously, an independent attention module is introduced to aggregate these neighboring documents into a special feature vector, which will represent features of the neighborhood. Finally, the two feature vectors are fused to match the embedding vectors of tags. To optimize the NATT model, we build the objective function with pairwise hinge loss and specially develop a neighborhood-aware negative sampling strategy to form training data. Experimental results on four datasets demonstrate that NATT outperforms some state-of-the-art neural models. Additionally, NATT is economical on achieving the best results with less training epochs and a smaller number of nearest neighbors.},
	author = {Shaowei Qin and Hao Wu and Rencan Nie and Jun He},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105750},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Neighborhood-aware, Negative sampling, Deep neural networks, Text tagging},
	pages = {105750},
	title = {Deep model with neighborhood-awareness for text tagging},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301623},
	volume = {196},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301623},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105750}}

@article{GANGAVARAPU2020105321,
	abstract = {In hospitals, caregivers are trained to chronicle the subtle changes in the clinical conditions of a patient at regular intervals, for enabling decision-making. Caregivers' text-based clinical notes are a significant source of rich patient-specific data, that can facilitate effective clinical decision support, despite which, this treasure-trove of data remains largely unexplored for supporting the prediction of clinical outcomes. The application of sophisticated data modeling and prediction algorithms with greater computational capacity have made disease prediction from raw clinical notes a relevant problem. In this paper, we propose an approach based on vector space and topic modeling, to structure the raw clinical data by capturing the semantic information in the nursing notes. Fuzzy similarity based data cleansing approach was used to merge anomalous and redundant patient data. Furthermore, we utilize eight supervised multi-label classification models to facilitate disease (ICD-9 code group) prediction. We present an exhaustive comparative study to evaluate the performance of the proposed approaches using standard evaluation metrics. Experimental validation on MIMIC-III, an open database, underscored the superior performance of the proposed Term weighting of unstructured notes AGgregated using fuzzy Similarity (TAGS) model, which consistently outperformed the state-of-the-art structured data based approach by 7.79% in AUPRC and 1.24% in AUROC.},
	author = {Tushaar Gangavarapu and Aditya Jayasimha and Gokul S. Krishnan and Sowmya Kamath S.},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105321},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Clinical decision support systems, Disease prediction, Healthcare analytics, ICD-9 code group prediction, Machine learning, Natural language processing},
	pages = {105321},
	title = {Predicting ICD-9 code groups with fuzzy similarity based supervised multi-label classification of unstructured clinical nursing notes},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119305982},
	volume = {190},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119305982},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105321}}

@article{KAUR2022108014,
	abstract = {Lately, cross-modal retrieval has attained plenty of attention due to enormous multi-modal data generation every day in the form of audio, video, image, and text. One vital requirement of cross-modal retrieval is to reduce the heterogeneity gap among various modalities so that one modality's results can be efficiently retrieved from the other. So, a novel unsupervised cross-modal retrieval framework based on associative learning has been proposed in this paper where two traditional SOMs are trained separately for images and collateral text and then they are associated together using the Hebbian learning network to facilitate the cross-modal retrieval process. Experimental outcomes on a popular Wikipedia dataset demonstrate that the presented technique outshines various existing state-of-the-art approaches.},
	author = {Parminder Kaur and Avleen Kaur Malhi and Husanbir Singh Pannu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108014},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Self organizing maps, Cross-modal retrieval, Hebbian learning, Zernike moments, Machine learning},
	pages = {108014},
	title = {Hybrid SOM based cross-modal retrieval exploiting Hebbian learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011175},
	volume = {239},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011175},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108014}}

@article{YERA2022109216,
	abstract = {The need of increasing trustworthiness and transparency in artificial intelligence (AI)-based systems that adhere ethical principles of respect for human autonomy, prevention of harm, fairness, and explainability; has boosting the development of systems that incorporate such issues as a key component. Recommender systems (RSs) are included in such AI-based systems, because they use intelligent algorithms for providing the most suitable items to active users according to other users' preferences. The RSs success is based on how much customers trust on the system, therefore recommendation explainability has become a crucial dimension for RSs adoption in real-world scenarios. Among the different successful applications of RS, it is remarkable the recent and exponential importance of recommendations for health and wellness areas. Hence, this paper aims at exploring, adapting and applying explanations for nutrition/recipes recommendations, that not only explain why the recommendation is enjoyable but also, it is aware of how healthy is the recommendation. Among the different methodologies to explain recommendations, this paper is focused on post-hoc explainability approaches and its adaptation, application and evaluation for nutrition/recipes recommendation. Eventually, it is included a comprehensive experimental study for characterizing the strengths and weaknesses of such explainability approaches in the recipe recommendation context.},
	author = {Raciel Yera and Ahmad A. Alzahrani and Luis Mart{\'\i}nez},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109216},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Explainable recommendation, Cooking recipes, Post-hoc explanation, Trustworthiness},
	pages = {109216},
	title = {Exploring post-hoc agnostic models for explainable cooking recipe recommendations},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122006050},
	volume = {251},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122006050},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109216}}

@article{VENUGOPALAN2022108668,
	abstract = {Aspect level sentiment analysis is a fine-grained task in sentiment analysis. It extracts aspects and their corresponding sentiment polarity from opinionated text. The first subtask of identifying the opinionated aspects is called aspect extraction, which is the focus of the work. Social media platforms are an enormous resource of unlabeled data. However, data annotation for fine-grained tasks is quite expensive and laborious. Hence unsupervised models would be highly appreciated. The proposed model is an unsupervised approach for aspect term extraction, a guided Latent Dirichlet Allocation (LDA) model that uses minimal aspect seed words from each aspect category to guide the model in identifying the hidden topics of interest to the user. The guided LDA model is enhanced by guiding inputs using regular expressions based on linguistic rules. The model is further enhanced by multiple pruning strategies, including a BERT based semantic filter, which incorporates semantics to strengthen situations where co-occurrence statistics might fail to serve as a differentiator. The thresholds for these semantic filters have been estimated using Particle Swarm Optimization strategy. The proposed model is expected to overcome the disadvantage of basic LDA models that fail to differentiate the overlapping topics that represent each aspect category. The work has been evaluated on the restaurant domain of SemEval 2014, 2015 and 2016 datasets and has reported an F-measure of 0.81, 0.74 and 0.75 respectively, which is competitive in comparison to the state of art unsupervised baselines and appreciable even with respect to the supervised baselines.},
	author = {Manju Venugopalan and Deepa Gupta},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108668},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Aspect term extraction, Guided LDA, BERT, Semantic similarity},
	pages = {108668},
	title = {An enhanced guided LDA model augmented with BERT based semantic strength for aspect term extraction in sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003069},
	volume = {246},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003069},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108668}}

@article{AYETIRAN2021106902,
	abstract = {Several language applications often require word semantics as a core part of their processing pipeline either as precise meaning inference or semantic similarity. Multi-sense embeddings (m-se) can be exploited for this important requirement. m-se seeks to represent each word by their distinct senses in order to resolve the conflation of meanings of words as used in different contexts. Previous works usually approach this task by training a model on a large corpus and often ignore the effect and usefulness of the semantic relations offered by lexical resources. However, even with large training data, coverage of all possible word senses is still an issue. In addition, a considerable percentage of contextual semantic knowledge are never learned because a huge amount of possible distributional semantic structures are never explored. In this paper, we leverage the rich semantic structures in WordNet using a graph-theoretic walk technique over word senses to enhance the quality of multi-sense embeddings. This algorithm composes enriched texts from the original texts. Furthermore, we derive new distributional semantic similarity measures for m-se from prior ones. We adapt these measures to word sense disambiguation (wsd) aspect of our experiment. We report evaluation results on 11 benchmark datasets involving wsd and Word Similarity tasks and show that our method for enhancing distributional semantic structures improves embeddings quality on the baselines. Despite the small training data, it achieves state-of-the-art performance on some of the datasets.},
	author = {Eniafe Festus Ayetiran and Petr Sojka and V{\'\i}t Novotn{\'y}},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106902},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multi-sense embeddings, Graph walk, Language generation, Distributional semantics, Distributional structures, Word sense disambiguation, Knowledge-based systems, Word similarity, Semantic applications},
	pages = {106902},
	title = {EDS-MEMBED: Multi-sense embeddings based on enhanced distributional semantic structures via a graph walk over word senses},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001659},
	volume = {219},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001659},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106902}}

@article{FACCHINETTI2022109266,
	abstract = {Systematic Literature Review (SLR) is nowadays a challenging task due to the large number of papers that typically compose the scientific material of the topic to review. Recently, a lot of research effort has been devoted to automate, even partially, the stages of an SLR. This paper proposes the design and implementation of a workflow and a set of tools -- called slr-kit -- to support key tasks in an SLR. The proposed approach leverages a semi-supervised strategy, in which time-consuming processes are carried out using automatic tools, whereas manual tasks have been optimized by carefully designed support tools to reduce the overall required effort. Important parts of the workflow include the extraction of key terms directly from the abstracts of the papers to survey, and the subsequent topic modeling that allows for a thematic clustering of the corpus of papers. In the proposed workflow, the former task is carried out by exploiting a novel tool, called FAst WOrd Classifier (FAWOC). The latter, instead, is designed to be automatically carried out by leveraging an ad-hoc solution based on the application of the Latent Dirichlet Allocation (LDA) algorithm. The result of the process consists in a set of statistics regarding the relationship among papers, topics, and their trend of publication on journals and conference proceedings. The validity of the method is demonstrated with an application to a dataset related to the scientific field of NLP, while its accuracy is assessed by the manual examination of the results by domain experts.},
	author = {Tullio Facchinetti and Guido Benetti and Davide Giuffrida and Antonino Nocera},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109266},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Natural language processing, Topic modeling, Systematic literature review, Tagging, Performance evaluation},
	pages = {109266},
	title = {slr-kit: A semi-supervised machine learning framework for systematic literature reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122006335},
	volume = {251},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122006335},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109266}}

@article{ZHAO2022108550,
	abstract = {Currently, sequence/graph-to-sequence models for abstractive dialogue summarization are being studied extensively. However, previous methods strive to integrate complex events spanning multiple utterances, and the generated summaries are often filled with incorrect facts. In this study, we first utilize the speaker-aware structure to model the information interaction process in the dialogue, which shows an excellent ability to settle the cross-sentence dependency. Then, we incorporate the factual representations via a dual-copy decoder to obtain summaries conditioned on both the tokens from source sequences and the factual knowledge from our designed fact graph, which enhances the factual consistency for dialogue summarization. We also propose some fact-level factual consistency metrics. Adequate experimental results demonstrate that our model outperforms the state-of-the-art baselines by a significant margin on the SAMSum and DialSumm datasets. A comprehensive analysis also proves the effectiveness of our model. Furthermore, human judges confirm that the outputs of our model contain more informative and faithful information.},
	author = {Lulu Zhao and Weiran Xu and Chunyun Zhang and Jun Guo},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108550},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Abstractive dialogue summarization, Speaker-aware structure, Dual-copy, Factual consistency},
	pages = {108550},
	title = {Leveraging speaker-aware structure and factual knowledge for faithful dialogue summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512200243X},
	volume = {245},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512200243X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108550}}

@article{GONZALEZSANTOS2021107113,
	abstract = {Topic modeling is a growing field within the area of text analysis that extracts underlying topics from document collections. Several objectives can be simultaneously considered when designing an approach for topic modeling. A multi-objective optimization approach based on the swarm intelligence of a bee colony (MOABC, Multi-Objective Artificial Bee Colony) has been designed, implemented, and tested. This new approach has been evaluated by using documents from the Reuters-21578 and TagMyNews datasets. Three objective functions (coherence, coverage, and perplexity) and three multi-objective metrics (hypervolume, set coverage, and distance to the ideal point) have been considered in two topic scenarios. Results show that MOABC provides relevant improvements with respect to LDA (Latent Dirichlet Allocation, the most used approach for topic modeling) and MOEA/D (Multi-Objective Evolutionary Algorithm based on Decomposition, the only multi-objective approach published to date). This demonstrates that the multi-criteria nature of topic modeling should be exploited with multi-objective optimization approaches.},
	author = {Carlos Gonz{\'a}lez-Santos and Miguel A. Vega-Rodr{\'\i}guez and Carlos J. P{\'e}rez},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107113},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Artificial bee colony, Evolutionary computing, Latent Dirichlet allocation, Multi-objective optimization, Text analysis, Topic modeling},
	pages = {107113},
	title = {Addressing topic modeling with a multi-objective optimization approach based on swarm intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003762},
	volume = {225},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121003762},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107113}}

@article{ALI2020106438,
	abstract = {Researchers face millions of research papers on various digital libraries. Therefore, finding relevant research work that meets the preferences of a researcher is a challenging task. Hence, different paper recommendation models have been proposed to address this issue. However, these models lack in exploiting prominent information factors, namely: papers' citations proximity, authors' information, papers' topical relevance, venues' information, researchers' preference dynamics, and labels information to produce quality recommendations. Additionally, these models encounter problems such as cold start papers and data sparsity. To overcome these problems, this paper presents a weighted probabilistic paper recommendation model termed as PR-HNE, which jointly learns researchers' and papers' dynamics by encoding information from six information networks into a joint latent space. Specifically, it captures papers' citation proximity, authors' collaboration proximity, venues' information, labeled information, and topical relevance to generate personalized paper recommendations. Compared to state-of-the-art models, the results generated by PR-HNE over publicly available datasets prove 4% and 6% improvement in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) metrics, respectively. Further, in the cold-start papers problem, the proposed model produced 8% better recall score than its counterparts.},
	author = {Zafar Ali and Guilin Qi and Khan Muhammad and Bahadar Ali and Waheed Ahmed Abro},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106438},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Paper recommendation, Citation recommendation, Heterogeneous network embedding, Neural networks, Cold-start},
	pages = {106438},
	title = {Paper recommendation based on heterogeneous network embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305670},
	volume = {210},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305670},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106438}}

@article{ZHU2021106511,
	abstract = {Travel package recommendation is a critical task in the tourism e-commerce recommender systems. Recently, an increasing number of studies proposed various travel package recommendation algorithms to improve Online Travel Agencies (OTAs) service, such as collaborative filtering-based, matrix factorization-based and neural network-based methods. Despite their value, however, the main challenges that incorporating complex descriptive information of the travel packages and capturing complicated users' long-term preferences for fine-grained travel package recommendation are still not fully resolved. In terms of these issues, this paper propose a novel model named Neural Attentive Travel package Recommendation (NATR) for tourism e-commerce by combining users' long-term preferences with short-term preferences. Specifically, NATR mainly contains two core modules, namely, travel package encoder and user encoder. The travel package encoder module is developed to learn a unified travel package representation by an attentive multi-view learning approach including word-level and view-level attention mechanisms. The user encoder module is designed to study long-term and short-term preference of the user by Bidirectional Long Short-Term Memory (Bi-LSTM) neural networks with package-level attention mechanism. In addition, we further adopt a gated fusion approach to coalesce these two kinds of preferences for learning high-quality the user's representation. Extensive experiments are conducted on a real-life tourism e-commerce dataset, the results demonstrate the proposed model yields significant performance advantages over several competitive methods. Further analyses from different attention weights provide insights of attentive multi-view learning and gated fusion network, respectively.},
	author = {Guixiang Zhu and Youquan Wang and Jie Cao and Zhan Bu and Shuxin Yang and Weichao Liang and Jingting Liu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106511},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Travel recommendations, Sequential behaviors, Neural networks, Personalized attention},
	pages = {106511},
	title = {Neural Attentive Travel package Recommendation via exploiting long-term and short-term behaviors},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120306407},
	volume = {211},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120306407},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106511}}

@article{GUO2021107454,
	abstract = {Text matching is a fundamental and critical problem in natural language understanding (NLU), where multi-level semantics matching is the most challenging task. Human beings can always leverage their semantic knowledge, while neural computer systems first learn sentence semantic representations and then perform text matching based on learned representation. However, without sufficient semantic information, computer systems will not perform very well. To bridge the gap, we propose a novel Frame-based Multi-level Semantics Representation (FMSR) model, which utilizes frame knowledge to extract multi-level semantic information within sentences explicitly for the text matching task. Specifically, different from existing methods that only rely on the sophisticated architectures, FMSR model, which leverages both frame and frame elements in FrameNet, is designed to integrate multi-level semantic information with attention mechanisms to learn better sentence representations. Our extensive experimental results show that FMSR model performs better than the state-of-the-art technologies on two text matching tasks.},
	author = {Shaoru Guo and Yong Guan and Ru Li and Xiaoli Li and Hongye Tan},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107454},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Text matching, Frame semantics, Multi-level semantic representation},
	pages = {107454},
	title = {Frame-based Multi-level Semantics Representation for text matching},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121007164},
	volume = {232},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121007164},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107454}}

@article{DECAMPOS2020105337,
	abstract = {In the information age we are living in today, not only are we interested in accessing multimedia objects such as documents, videos, etc. but also in searching for professional experts, people or celebrities, possibly for professional needs or just for fun. Information access systems need to be able to extract and exploit various sources of information (usually in text format) about such individuals, and to represent them in a suitable way usually in the form of a profile. In this article, we tackle the problems of profile-based expert recommendation and document filtering from a machine learning perspective by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested. The experts will then be represented by means of multi-faceted profiles. Our experiments show that this is a valid technique to improve the performance of expert finding and document filtering.},
	author = {Luis M. {de Campos} and Juan M. Fern{\'a}ndez-Luna and Juan F. Huete and Luis Redondo-Exp{\'o}sito},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105337},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Clustering, Content-based recommendation, Expert finding, Filtering, User profiling},
	pages = {105337},
	title = {Automatic construction of multi-faceted user profiles using text clustering and its application to expert recommendation and filtering problems},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306069},
	volume = {190},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306069},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105337}}

@article{LIANG2022108050,
	abstract = {Temporal dynamics such as short term and long term effects, recency effects, periodic and seasonal temporal factors in information networks are of great importance for many real-world applications. However, existing network embedding learning approaches mainly focus on semantic information or temporal phenomenon such as recency or dynamic process. They failed to have the capability of incorporating multiple temporal factors/phenomenon in information networks. To bridge the gap, this paper proposes a general time-aware network representation learning framework TNE for temporal applications. TNE contains a temporally annotated network TAN, a temporally annotated meta-path based random walk method, and a self-supervised embedding learning approach. We introduce temporal nodes and relations to existing information networks to construct TAN that can incorporate multiple temporal factors. We propose a temporally annotated meta-path based random walk approach to form a time-aware hybrid neighbourhood context that considers both semantic and temporal factors. Based on the time-aware context, self-supervised representation learning approaches are used to simultaneously preserve both semantic and temporal factors in embeddings. Extensive experiments of two large scale real-life datasets show that the proposed framework is effective in various temporal applications such as temporal similarity search and temporal recommendations.},
	author = {Huizhi Liang and Thanet Markchom},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108050},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Representation learning, Heterogeneous information network, User profiling, Network embeddings, Temporal dynamics},
	pages = {108050},
	title = {TNE: A general time-aware network representation learning framework for temporal applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011382},
	volume = {240},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011382},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108050}}

@article{XU2020106391,
	abstract = {Rumors can be propagated across online microblogs at a relatively low cost, but result in a series of major problems in our society. Traditional rumor detection approaches focus on exploring various propagation patterns or data interactions between a source microblog and its subsequent reactions. It is obvious that this causes missing interaction on rumor detection, especially in the absence of retweets or reactions. According to the communication theory of Allport and Postman (1947), Chorus (1953) and Rosnow (1988), the topic of a post can help determine its potential of being a rumor or not. Therefore, we develop a novel topic-driven rumor detection (TDRD) framework to determine whether a post is a rumor only according to its source microblog. Specifically, we first automatically perform topic classification on source microblogs, and then we successfully incorporate the predicted topic vector of the source microblogs into rumor detection. Our extensive experimental results demonstrate that our TDRD significantly outperforms state-of-the-art methods on both two English and two Chinese benchmark datasets.},
	author = {Fan Xu and Victor S. Sheng and Mingwen Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106391},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Rumor detection, Topic vector, Latent dirichlet allocation (LDA), Source microblogs},
	pages = {106391},
	title = {Near real-time topic-driven rumor detection in source microblogs},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305281},
	volume = {207},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305281},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106391}}

@article{DONG2022108954,
	abstract = {A significant remaining challenge for existing recommender systems is that users may not trust recommender systems for either inaccurate recommendation or lack of explanation. Thus, it becomes critical to embrace a trustworthy recommender system. This survey provides a systematic summary of three categories of trust issues in recommender systems: social-aware recommender systems, which leverage users' social trust relationships; robust recommender systems, which filter untruthful information, noises and enhance attack resistance; and explainable recommender systems, which provide explanations of the recommended items. We focus on the work based on deep learning techniques, which is an emerging area in the recommendation research.},
	author = {Manqing Dong and Feng Yuan and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108954},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Deep learning, Systematic survey},
	pages = {108954},
	title = {A survey for trust-aware recommender systems: A deep learning perspective},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122004622},
	volume = {249},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122004622},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108954}}

@article{ZOU2022107927,
	abstract = {The recent deep cross-modal hashing (DCMH) has achieved superior performance in effective and efficient cross-modal retrieval and thus has drawn increasing attention. Nevertheless, there are still two limitations for most existing DCMH methods: (1) single labels are usually leveraged to measure the semantic similarity of cross-modal pairwise instances while neglecting that many cross-modal datasets contain abundant semantic information among multi-labels. (2) several DCMH methods utilized the multi-labels to supervise the learning of hash functions. Nevertheless, the feature space of multi-labels suffers the weakness of sparse, resulting in sub-optimization for the hash functions learning. Thus, this paper proposed a multi-label modality enhanced attention-based self-supervised deep cross-modal hashing (MMACH) framework. Specifically, a multi-label modality enhanced attention module is designed to integrate the significant features from cross-modal data into multi-labels feature representations, aiming to improve its completion. Moreover, a multi-label cross-modal triplet loss is defined based on the criterion that the feature representations of cross-modal pairwise instances with more common categories should preserve higher semantic similarity than other instances. To the best of our knowledge, the multi-label cross-modal triplet loss is the first time designed for cross-modal retrieval. Extensive experiments on four multi-label cross-modal datasets demonstrate the effectiveness and efficiency of our proposed MMACH. Moreover, the MMACH also achieved superior performance and outperformed several state-of-the-art methods on the task of cross-modal retrieval. The source code of MMACH is available at https://github.com/SWU-CS-MediaLab/MMACH.},
	author = {Xitao Zou and Song Wu and Nian Zhang and Erwin M. Bakker},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107927},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Deep cross-modal hashing, Attention mechanism, Multi-label semantic learning},
	pages = {107927},
	title = {Multi-label modality enhanced attention based self-supervised deep cross-modal hashing},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512101073X},
	volume = {239},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512101073X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107927}}

@article{LIU2020105918,
	abstract = {Email data has unique characteristics, involving multiple topics, lengthy replies, formal language, high variance in length, high duplication, anomalies, and indirect relationships that distinguish it from other social media data. In order to better model Email documents and to capture complex sentiment structures in the content, we develop a framework for document-level multi-topic sentiment classification of Email data. Note that, a large volume of labeled Email data is rarely publicly available. We introduce an optional data augmentation process to increase the size of datasets with synthetically labeled data to reduce the probability of overfitting and underfitting during the training process. To generate segments with topic embeddings and topic weighting vectors as inputs for our proposed model, we apply both latent Dirichlet allocation topic modeling and semantic text segmentation to post-process Email documents. Empirical results obtained with multiple sets of experiments, including performance comparison against various state-of-the-art algorithms with and without data augmentation and diverse parameter settings, are analyzed to demonstrate the effectiveness of our proposed framework.},
	author = {Sisi Liu and Kyungmi Lee and Ickjai Lee},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105918},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment classification, Email sentiment, Multi-topic sentiment, Bidirectional LSTM, Data augmentation},
	pages = {105918},
	title = {Document-level multi-topic sentiment classification of Email data with BiLSTM and data augmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120302574},
	volume = {197},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120302574},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105918}}

@article{DHELIM2020106227,
	abstract = {With the emergence of online social networks and microblogging websites, user interest mining has been an active research topic for the past few years. However, most of the existing works suffer from two significant drawbacks, firstly, they focus on the user's explicit content and social network structure to predicate the user's interests, neglecting the fact that the user's personality might be a rich source to infer the topical interests. Secondly, they represent the user's content using the bag-of-words model that ignores the chronological order of the posted content, hence the predicted interests might contain outdated topics that the user does not interest anymore. In this paper, we propose a novel user interest mining system based on Big Five personality traits and dynamic interests. To prove the effectiveness of incorporating the user's personality traits in the interest mining process, we have implemented a social network for news sharing and conducted different experiments on the collected data. The experiment results show that considering personality traits can increase the precision and recall of interest mining systems, as well as can help to tackle the cold start problem.},
	author = {Sahraoui Dhelim and Nyothiri Aung and Huansheng Ning},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106227},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Interest mining, Personality computing, User modeling, Interest graph, Personality traits, Big five, User interest, Social computing},
	pages = {106227},
	title = {Mining user interest based on personality-aware hybrid filtering in social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304354},
	volume = {206},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304354},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106227}}

@article{CHEN2020105546,
	abstract = {In order to facilitate human decision making, trust evaluation has received widespread attention in many fields, especially for online services. Most of the existing methods consider trust in a person as a value which does not vary across different scenarios without any attention to the distinction of domains or communities where trust is derived. However, the notion of context is a significant and indispensable factor for trust evaluation in practice. Due to the lack of the consideration of context, traditional methods cannot resolve the issue that arises when a highly trustworthy person in one domain is likely to dominate the results of trust assessment in others where the person is in fact less authoritative. To solve this problem, in this paper, we develop a general approach to accomplish topic-sensitive trust evaluation by considering the context of trust. We first propose a general framework which presents the well-organized architecture of topic-sensitive trust evaluation in online communities. Then, a user-topic model is proposed to automatically extract topic data from user-generated content based on the Labeled Latent Dirichlet Allocation (LLDA) model. To compare the topic differences between users, we design a topic coverage function for revealing their trust relationships in diverse topics. Moreover, we employ two traditional methods and extend them to accomplish trust prediction for people with multiple domain knowledge. Experiments based on a real-world dataset show that extended topic-sensitive approaches are more adaptive and accurate than those topic-free trust evaluation approaches, especially when the trust application scenario features multiple topics.},
	author = {Xu Chen and Yuyu Yuan and Mehmet Ali Orgun and Lilei Lu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105546},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Topic-sensitive analysis, Trust evaluation, Trust propagation, Context-dependency, Labeled LDA},
	pages = {105546},
	title = {A topic-sensitive trust evaluation approach for users in online communities},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120300435},
	volume = {194},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120300435},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105546}}

@article{LOPEZ2021107455,
	abstract = {Opinion summarisation is concerned with generating structured summaries of multiple opinions in order to provide insightful knowledge to end users. We present the Aspect Discovery for OPinion Summarisation (ADOPS) methodology, which is aimed at generating explainable and structured opinion summaries. ADOPS is built upon aspect-based sentiment analysis methods based on deep learning and Subgroup Discovery techniques. The resultant opinion summaries are presented as interesting rules, which summarise in explainable terms for humans the state of the opinion about the aspects of a specific entity. We annotate and release a new dataset of opinions about a single entity on the restaurant review domain for assessing the ADOPS methodology, and we call it ORCo. The results show that ADOPS is able to generate interesting rules with high values of support and confidence, which provide explainable and insightful knowledge about the state of the opinion of a certain entity.},
	author = {Miguel L{\'o}pez and Eugenio Mart{\'\i}nez-C{\'a}mara and M. Victoria Luz{\'o}n and Francisco Herrera},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107455},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Opinion summarisation, Deep learning, Aspect extraction, Subgroup discovery, Interesting rules},
	pages = {107455},
	title = {ADOPS: Aspect Discovery OPinion Summarisation Methodology based on deep learning and subgroup discovery for generating explainable opinion summaries},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121007176},
	volume = {231},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121007176},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107455}}

@article{JUNG2022110020,
	abstract = {The shared interest among existing research topics matures over time until it emerges as a topic of its own. This paper detects emerging topics as well as general predictor models spanning multiple research domains through the network-based topic evolution approach, which offers additional topic evolution capabilities such as extrapolation of data and separation of topic transition and correlation. Topics are represented as their neighbors in the past, or ancestors, and their structural properties are used to train binary classification models in capturing the materialization of such topics. The entirety of 197 million publications within the Microsoft Academic Graph was used to build multiple datasets, where machine learning algorithms were trained with structural features resulting in over 0.98 area under the precision--recall curve. General topic emergence predictor equations are then proposed based on the models trained specifically for each domain, which were able to capture a common pattern shared by emerging topics in general.},
	author = {Sukhwan Jung and Aviv Segev},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.110020},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Topic prediction, Scientometrics, Knowledge management, Machine learning},
	pages = {110020},
	title = {Identifying a common pattern within ancestors of emerging topics for pan-domain topic emergence prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122011133},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122011133},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.110020}}

@article{KUNDU2021106535,
	abstract = {Here, we propose a topic sensitive hybrid expertise retrieval system in community question answering services. We introduce three new expertise signatures: knowledge, reputation, and authority. These signatures consider the questions, and hence, their answerers from a topic sensitive perspective. We estimate the knowledge of an answerer on a new question based on the previously answered subset of questions with similar topic distributions to the new question. The reputation of an answerer, moreover, is derived from the qualities of previously answered questions by the answerer with similar distributions of topics. Furthermore, we propose a topic sensitive authority model. It considers some topic related information associated with questions and the relationships among their answerers. We compare the proposed method with 26 existing methods on 4 real-world datasets using 5 performance measures. It outperforms the comparing algorithms in 91.73% (477 out of 520) cases.},
	author = {Dipankar Kundu and Rajat Kumar Pal and Deba Prasad Mandal},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106535},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Community question answering, Expertise retrieval, Social network analysis, Topic sensitive model},
	pages = {106535},
	title = {Topic sensitive hybrid expertise retrieval system in community question answering services},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512030664X},
	volume = {211},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512030664X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106535}}

@article{XU2021113525,
	abstract = {The online review systems of digital platforms feature a variety of designs. This study examines how closed-form evaluations and open-ended textual comment options affect customers' online review writing behaviors and reflect satisfaction with hotels. We find that direct and spillover effects exist. These refer to the effects of customers' closed-form evaluations of a product or service attribute on their reviews of the same attribute and other attributes in their open-ended comments, respectively. Regarding the direct effect, positive closed-form evaluations of attributes reduce customers' behaviors of writing details about the same attributes in open-ended comments, but negative closed-form evaluations of attributes increase that behavior. The increasing effect is greater than the reducing effect. Regarding the spillover effect, closed-form evaluations of certain attributes increase customers' behaviors of writing details about other attributes in open-ended comments. Additionally, we find that the heterogeneity effect lies in the different roles of closed-form evaluations and open-ended textual comments in reflecting customers' overall satisfaction. Open-ended comments about the advantages of the attributes have a more significant effect than the closed-form evaluations of the same attributes in reflecting overall customer satisfaction, but closed-form evaluations better reflect customers' low overall satisfaction with the disadvantages of the attributes. The direct, spillover, and heterogeneity effects on customers' evaluations and comments regarding independent hotels are higher than those effects on evaluations and comments regarding chain hotels. The findings provide insights for companies to design review systems, understand customer perceptions, and use the positive electronic word-of-mouth effect.},
	author = {Xun Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113525},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Online customer reviews, Review systems, Closed-form evaluation, Textual comments, Customer satisfaction},
	pages = {113525},
	title = {Closed-form evaluations and open-ended comment options: How do they affect customer online review behavior and reflect satisfaction with hotels?},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362100035X},
	volume = {145},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362100035X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113525}}

@article{KAZMAIER2020113304,
	abstract = {The increased exposure of the average citizen and customer to polarised content from various sources has been of significant consequence for companies and governmental organisations. Such content has, for example, served as a catalyst for violent uprisings and shifts in stock market prices. The collection and study of opinion have therefore become a necessity in many industries. Due to the vast nature of such data, manual approaches to this problem are no longer feasible. Several computational approaches have been proposed within the field of sentiment analysis, which successfully address many aspects of this problem, such as the classification of data into one of several sentiment categories. The research in the field is lacking, however, with respect to the integration and application of these techniques in practice, as well as their incorporation into the decision-making process of affected entities. In this paper, a generic framework for sentiment analysis is proposed, with a focus on facilitating the model development process for a user in a manner such that good performance may be achieved irrespective of the problem domain, as well as facilitating a flexible, exploratory analysis of model results in combination with existing structured attributes in order to gain actionable insights. The objective of the framework is to aid organisations in successfully leveraging unstructured, opinion-bearing data in combination with structured data sources to inform decision making.},
	author = {Jacqueline Kazmaier and Jan H. {van Vuuren}},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113304},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Sentiment analysis, Machine learning, Natural language processing, Decision Support Systems},
	pages = {113304},
	title = {A generic framework for sentiment analysis: Leveraging opinion-bearing data to inform decision making},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620300592},
	volume = {135},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620300592},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113304}}

@article{DUTTA2022113662,
	abstract = {Text databases have grown tremendously in number, size, and volume over the last few decades. Optical Character Recognition (OCR) software is used to scan the text and make them available in online repositories. The OCR transcription process is often not accurate resulting in large volumes of garbled text in the repositories. Spell correction and other post-processing of OCR text often prove to be very expensive and time-consuming. While it is possible to rely on the OCR model to assess the quality of text in a corpus, many natural language processing and information retrieval tasks prefer the extrinsic evaluation of the effect of noise on the task at hand. This paper examines the effect of noise on the unsupervised ranking of person name entities by first populating a list of person names using an out-of-the-box Named Entity Recognition (NER) software, extracting content-based features for the identified entities, and ranking them using a novel unsupervised Kernel Density Estimation (KDE) based ranking algorithm. This generative model has the ability to learn rankings using the data distribution and therefore requires limited manual intervention. Empirical results are presented on a carefully curated parallel corpus of OCR and clean text and ``in the wild'' using a large real-world corpus. Experiments on the parallel corpus reveals that even with a reasonable degree of noise in the dataset, it is possible to generate ranked lists using the KDE algorithm with a high degree of precision and recall. Furthermore, since the KDE algorithm has comparable performance to state-of-the-art unsupervised rankers, using it on real-world corpora is feasible. The paper concludes by reflecting on other methods for enhancing the performance of the unsupervised algorithm on OCR text such as cleaning entity names, disambiguating names concatenated to one another and correcting OCR errors that are statistically significant in the corpus.},
	author = {Haimonti Dutta and Aayushee Gupta},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113662},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Unsupervised ranking, Kernel density estimation, OCR noise, Named entity recognition},
	pages = {113662},
	title = {PNRank: Unsupervised ranking of person name entities from noisy OCR text},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362100172X},
	volume = {152},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362100172X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113662}}

@article{CHENG2022113864,
	abstract = {Priming is challenging when consumers start shortlisting products before the final purchase. This is because this shortlisting process is performed in multiple user sessions online across time, the shortlist does not stay as a static list, and product comparison in this stage uses the heuristics internal to individual consumers. The goal of this study is two folds: (1) to approximate user heuristics after B&B product shortlisting using NLP and deep learning techniques, and (2) to identify optimized deep learning models for the representation of key elements of consumer heuristics. This offers an extension of the priming theory into product comparison and shortlisting stages that were traditionally difficult for marketers to tap into. By analyzing the B&B product information repeated visited in user sessions, the formation of shortlists is identified and products in the shortlists can then be compared. Subsequent priming and promotions can therefore be performed closer to the actual purchase. Our study also provides marketers keywords and their associated activated words relevant for crafting marketing messages. As these activation words are extracted from the B&B sites and product reviews that the users had visited repeatedly in long-term tracking sessions, they are analogous to effects produced from user participatory design, an approach popular in the IT world. Our work shows that opportunities for marketing decision support, especially into the shortlisting phase, are now possible through machine learning techniques. Both theoretical and practical implications are provided.},
	author = {Li-Chen Cheng and Kuanchin Chen},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113864},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Text mining, Session-based recommendation, Priming theory, -commerce},
	note = {Business and Government Applications of Text Mining & Natural Language Processing (NLP) for Societal Benefit},
	pages = {113864},
	title = {Mining longitudinal user sessions with deep learning to extend the boundary of consumer priming},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362200135X},
	volume = {162},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362200135X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113864}}

@article{LI2022113863,
	abstract = {Extracting useful knowledge from multimodal data is the core of many multimedia applications, such as recommendation systems, and cross-modal retrieval. In this paper, we propose a label-based multimodal topic (LB-MMT) model to jointly model text and image data tagged with multiple labels. Specifically, we use the labels as supervised information to generate the text and image data. In the LB-MMT model, we assume that the textual words and visual words related to each text and image are drawn from a mixture of latent topics, where each topic is represented as a group of textual words and visual words. Moreover, we introduce multiple topics for each label, to build the top-down relationship from label to text and image. To investigate the effectiveness of the proposed approach, we conduct extensive experiments on a real-world multimodal dataset with labels. The results show the proposed approach obtains superior performances on topic coherence and label prediction compared with previous competitors. In addition, we show that our model yields interesting insights about multimodal topics. The proposed model provides important practical implications, e.g., designing more attractive multimodal contents for marketers.},
	author = {Hao Li and Yang Qian and Yuanchun Jiang and Yezheng Liu and Fan Zhou},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113863},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Multimodal data, Topic modeling, Label data, Supervised model, Image representation},
	pages = {113863},
	title = {A novel label-based multimodal topic model for social media analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622001348},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622001348},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113863}}

@article{BISWAS2022113651,
	abstract = {Online hacker communities are meeting spots for aspiring and seasoned cybercriminals where they engage in technical discussions, share exploits and relevant hacking tools to be used in launching cyber-attacks on business organizations. Sometimes, the affected organizations can detect these attacks in advance, with the help of cyber-threat intelligence derived from the explicit and implicit features of hacker communication in these forums. Herein, we proposed a novel text-mining based cyber-risk assessment and mitigation framework, which performs the following critical tasks. (i) Cyber-risk Assessment - to identify hacker expertise (i.e., newbie, beginner, intermediate, and advanced) using explicit and implicit features applying various classification algorithms. Among these features, cybersecurity keywords, sharing of attachments, and sentiments emerged as significant. Further, we found that expert hackers demonstrate leadership in the online forums that eventually serve as communities of practice. Consequently, novice hackers gradually develop their cyber-attack skills through prolonged observations, interactions, and external influences in this social learning process. (ii) Cyber-risk mitigation -- computes financial impact for every {hacker expertise, attack-type} combination, and then by ranking them on a {likelihood, impact} decision-matrix to prioritize mitigation strategies in affected organizations. Through these novel recommendations, our framework can guide managers to decide on appropriate cybersecurity controls using an {expected loss, probability, attack-type, hacker expertise} metric against financial losses due to cyber-attacks.},
	author = {Baidyanath Biswas and Arunabha Mukhopadhyay and Sudip Bhattacharjee and Ajay Kumar and Dursun Delen},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113651},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Information security, Cyber risks, Hacker forum, Machine learning, Sentiment analysis},
	pages = {113651},
	title = {A text-mining based cyber-risk assessment and mitigation framework for critical analysis of online hacker forums},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923621001615},
	volume = {152},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923621001615},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113651}}

@article{ROEDER2022113770,
	abstract = {Other than banks, non-financial companies also continuously monitor and analyze their credit risk exposure to avoid possible counterparty defaults. Credit default swaps are commonly used financial instruments that provide information on a counterparty's creditworthiness. Although this metric can provide crucial insights, the underlying price dynamics often remain unknown and require further explanation. Data-driven decision-making is a key concept for identifying these reasons and supporting and justifying decisions. In this paper, we provide such justifications by applying sentiment and topic analysis to company-related financial analyst reports. While the contents of financial news have been analyzed in the past, analyst reports can offer additional insights, as seasoned analysts use them to disseminate in-depth research to experienced investors. This analysis examines 3386 analyst reports covering constituents of the Dow Jones Industrial Average Index in the period from 2009 to 2020. The results suggest that even when established credit risk indicators and financial news are considered, the sentiment and a subset of topics are correlated with changes in the credit default swap spread, indicating a fundamental relationship between quantitative risk metric and analyst reports. We find that analyst reports contain information related to the change in credit default swap spreads, an insight that helps to improve our understanding of existing risk assessments. The outcome indicates that banks or corporate risk managers can benefit from complementing established financial metrics and even financial news data with new insights derived from analyst reports.},
	author = {Jan Roeder and Matthias Palmer and Jan Muntermann},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113770},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Credit risk, Data-driven decision-making, Unstructured data, Text mining, Sentiment analysis, Topic mining},
	pages = {113770},
	title = {Data-driven decision-making in credit risk management: The information value of analyst reports},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000410},
	volume = {158},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000410},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113770}}

@article{DEHGHAN2020113425,
	abstract = {Expert finding in Community Question Answering (CQA) networks such as Stack Overflow is a practical issue facing a challenging problem called vocabulary gap. A widely used approach to overcome this problem is translation model. Different from prior works that only consider the relevancy of translations to a query, we intend to diversify query translations for better coverage of query topics. In this work, we have utilized the idea of clustering to group relevant translations to a given query into different clusters and then select representatives from each cluster as a set of diverse translations. We have proposed two new approaches to cluster translations. In the first one, the Mutual Information was primarily utilized as a similarity measure during clustering. In the second approach, the relevant translations are embedded in a topic space and then clustered in that space. After clustering, we propose two batch and sequential methods to select a diverse set of translations from the resultant clusters. The batch method selects the top most relevant translations from each cluster proportional to the relevancy of that cluster to the user query. The sequential one is an iterative method that looks for the most diverse set of translations considering the previously selected ones. Finally, to rank users, a regression model was utilized to learn how expert and non-expert users differ in using a set of diverse translations in their documents. Experiments on a large dataset generated from Stack Overflow demonstrate that the proposed methods improve the ranking performance over baselines in the expert finding.},
	author = {Mahdi Dehghan and Ahmad Ali Abin and Mahmood Neshati},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113425},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Expert finding, Question answering, Stack overflow, Translations diversification},
	pages = {113425},
	title = {An improvement in the quality of expert finding in community question answering networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620301809},
	volume = {139},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620301809},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113425}}

@article{WANG2020113171,
	abstract = {We investigate multiple disease risk prediction modeling, aimed at assessing future disease risks for an individual who is ready for discharge after hospitalization. We propose a novel framework that combines directed disease network and recommendation system techniques to substantially enhance multiple disease risk predictive modeling. Firstly, a directed disease network considering temporal information is developed. Then based on this directed disease network, we look into different disease risk score computing approaches. We validate the proposed approaches with two real-world datasets from two independent hospitals. The predicted results can be promisingly utilized as a reference for medical experts to offer effective healthcare guidance for both inpatients and outpatients. The proposed framework can also be utilized for developing an innovative tool that helps individuals create and maintain a better healthcare plan over time.},
	author = {Tingyan Wang and Robin G. Qiu and Ming Yu and Runtong Zhang},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113171},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Multiple-disease risk prediction, Health risk assessment, Disability adjusted life year, Directed network, Disease temporal relations},
	pages = {113171},
	title = {Directed disease networks to facilitate multiple-disease risk assessment modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619302003},
	volume = {129},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619302003},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113171}}

@article{LIU2021113609,
	abstract = {Understanding customer experience is an essential part of service operations for sustaining business in the sharing economy. We investigate the relationship among customer experience, perceived value, and customer loyalty from a stimulus-organism-response (S-O-R) perspective. Using a dataset of 4166 listings in a leading Chinese accommodation-sharing platform and text mining and econometric methods to analyze online customer reviews, we find that customer experience manifests in the physical environment and human interaction dimensions. The results show a positive association among customer experience, perceived value, and customer loyalty. Notably, the physical environment and human interaction are equally important in influencing customers' value judgments about their consumption experience. Moreover, perceived value has a stronger positive effect on attitudinal loyalty than on behavioral loyalty. This study adds new insights into the customer experience-perceived value-customer loyalty path by showing that the physical environment and human interaction have the same importance in affecting perceived value and identifying the subtle difference between attitudinal and behavioral loyalty influenced by perceived value in the accommodation-sharing economy. Furthermore, these findings provide managerial insights for service operations management and marketing strategy planning.},
	author = {Fuzhen Liu and Kee-Hung Lai and Jiang Wu and Wenjing Duan},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113609},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Customer experience, Perceived value, Attitudinal loyalty, Behavioral loyalty, Sharing economy},
	pages = {113609},
	title = {Listening to online reviews: A mixed-methods investigation of customer experience in the sharing economy},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923621001196},
	volume = {149},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923621001196},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113609}}

@article{ZHANG2020113288,
	abstract = {Trust plays an important role in sharing transactions on short-term rental platforms. However, the impact of host self-description on trust perception and whether trust perception can influence purchase behavior remain under-studied. Therefore, a text analytics framework was proposed to research the relationships among host self-description, trust perception and purchase behavior on Airbnb. Specifically, a deep-learning-based method was designed to automatically code trust perception of host self-descriptions. And the linguistic and semantic features of description texts were extracted with text mining methods. The estimated order quantity was used to quantify purchase behavior. Then, the influence of linguistic and semantic features on trust perception was identified, and the relationship between trust perception and purchase behavior was also verified. The empirical analysis derives the following findings: i. The readability of self-description is positively associated with trust perception; ii. Perspective taking expressed in self-description is also helpful; iii. Excessive positive sentiment expression can raise barriers to trust building; iv. Paying more attention to family relationship, openness, service and travel experience in self-description would be helpful; v. Trust perception can promote purchases. These findings can help hosts write better self-description, which contributes to trust building and purchases on short-term rental platforms.},
	author = {Le Zhang and Qiang Yan and Leihan Zhang},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113288},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Self-description, Trust perception, Purchase behavior, Airbnb, Text mining},
	pages = {113288},
	title = {A text analytics framework for understanding the relationships among host self-description, trust perception and purchase behavior on Airbnb},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620300439},
	volume = {133},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620300439},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113288}}

@article{KUNDU2020113164,
	abstract = {Here, we propose a preference enhanced hybrid expertise retrieval (PEHER) system in community question answering services. PEHER consists of three segments, namely, preferability estimator, authority estimator, and expertise estimator. The preferability estimator utilizes the textual information to determine both intra-profile and inter-profile preferences of answerers for each term. The intra-profile preferences consider the preference of a term using the answering history of a given answerer. The inter-profile preferences incorporate the preferences of all answerers for a term. These preferences are then used to determine the preferability of each answerer for each of the archived questions. The authority estimator considers the textual familiarity between each archived question and the profile of each answerer as the weight of the associated link in the network. The expertise estimator is composed of three blocks, namely, question similarity finder, proficiency estimator, and expert list generator. The question similarity finder finds the similarities between the new question and each of the archived questions. The proficiency estimator uses the said similarities of the archived questions along with their preferabilities to decide the proficiencies of answerers for the new question. Finally, the expert list generator considers the authorities and proficiencies to generate a list of experts for a given question. We compare PEHER with twenty existing methods on four real-world datasets using five performance measures. We find that PEHER outperforms the comparing algorithms in 92.00% (368 out of 400) cases.},
	author = {Dipankar Kundu and Rajat Kumar Pal and Deba Prasad Mandal},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113164},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Community question answering, Expertise retrieval, Hybrid systems, Social network analysis},
	pages = {113164},
	title = {Preference enhanced hybrid expertise retrieval system in community question answering services},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619301939},
	volume = {129},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619301939},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113164}}

@article{GOLDBERG2022113751,
	abstract = {In recent years, online reviews have offered a rich new medium for consumers to express their opinions and feedback. Product designers frequently aim to consider consumer preferences in their work, but many firms are unsure of how best to harness this online feedback given that textual data is both unstructured and voluminous. In this study, we use text mining tools to propose a method for rapid prioritization of online reviews, differentiating the reviews pertaining to innovation opportunities that are most useful for firms. We draw from the innovation and entrepreneurship literature and provide an empirical basis for the widely accepted attribute mapping framework, which delineates between desirable product attributes that firms may want to capitalize upon and undesirable attributes that they may need to remedy. Based on a large sample of reviews in the countertop appliances industry, we demonstrate the performance of our technique, which offers statistically significant improvements relative to existing methods. We validate the usefulness of our technique by asking senior managers at a large manufacturing firm to rate a selection of online reviews, and we show that the selected attribute types are more useful than alternative reviews. Our results offer insight in how firms may use online reviews to harness vital consumer feedback.},
	author = {David M. Goldberg and Alan S. Abrahams},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113751},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Online reviews, Text mining, Data mining, Innovation, Business intelligence},
	pages = {113751},
	title = {Sourcing product innovation intelligence from online reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000227},
	volume = {157},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000227},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113751}}

@article{YANG2022113813,
	abstract = {Depression is a leading mental health problem affecting 300 million people globally. Recent studies show that social networks provide a tremendous potential for mental health professionals as a source of supplemental information about their patients. This study presents a methodological framework for clinical decision support systems (CDSSs) through analysis of social network data to distinguish the language usage of individuals with early signs of depression (i.e., contrast language analysis). By analyzing the contrast language patterns of different user groups, we are able to uncover constructive and actionable insights into the pain points and characteristics of users with signs of depression as decision support mechanisms for clinicians during intervention, (early) diagnosis and treatment plans. First, we discover terms that represent contrasting language by analyzing the percentage difference of terms in two user groups, labeled as''depressed'' and''non-depressed'' for ease of reference. Second, by building topic models based on social network contents, the topic-level contrast features are discovered. Finally, we consider the structure of the social network to discover the network-level contrast features. To illustrate the effectiveness of the proposed framework, we present a case study on early depression detection using a real-world dataset. The proposed framework has methodological contributions in enhancing the features and functionalities of CDSS for clinicians. It also contributes to evidence-based health research through cost-effective data and analytical insights that can supplement or improve the traditional survey and time-consuming interview methods.},
	author = {Xingwei Yang and Alexandra Joukova and Anteneh Ayanso and Morteza Zihayat},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113813},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Contrast language analysis, Clinical Decision Support System (CDSS), Depression detection, Social network},
	pages = {113813},
	title = {Social influence-based contrast language analysis framework for clinical decision support systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000847},
	volume = {159},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000847},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113813}}

@article{KUMAR2022113792,
	abstract = {The COVID-19 pandemic has had a severe impact on mankind, causing physical suffering and deaths across the globe. Even those who have not contracted the virus have experienced its far-reaching impacts, particularly on their mental health. The increased incidences of psychological problems, anxiety associated with the infection, social restrictions, economic downturn, etc., are likely to aggravate with the virus spread and leave a longer impact on humankind. These reasons in aggregation have raised concerns on mental health and created a need to identify novel precursors of depression and suicidal tendencies during COVID-19. Identifying factors affecting mental health and causing suicidal ideation is of paramount importance for timely intervention and suicide prevention. This study, thus, bridges this gap by utilizing computational intelligence and Natural Language Processing (NLP) to unveil the factors underlying mental health issues. We observed that the pandemic and subsequent lockdown anxiety emerged as significant factors leading to poor mental health outcomes after the onset of COVID-19. Consistent with previous works, we found that psychological disorders have remained pre-eminent. Interestingly, financial burden was found to cause suicidal ideation before the pandemic, while it led to higher odds of depressive (non-suicidal) thoughts for individuals who lost their jobs. This study offers significant implications for health policy makers, governments, psychiatric practitioners, and psychologists.},
	author = {Rahul Kumar and Shubhadeep Mukherjee and Tsan-Ming Choi and Lalitha Dhamotharan},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113792},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Mental health, Depression, Suicidal ideation, Natural language processing, Social-media, COVID-19, Pandemic},
	note = {Business and Government Applications of Text Mining & Natural Language Processing (NLP) for Societal Benefit},
	pages = {113792},
	title = {Mining voices from self-expressed messages on social-media: Diagnostics of mental distress during COVID-19},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362200063X},
	volume = {162},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362200063X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113792}}

@article{XU2020113162,
	abstract = {With the rapid development of information technology, platform-facilitated collaborative consumption has recently become attractive to consumers. A comparative study of consumers' online review behavior and its impact on overall satisfaction and demand in the accommodation-sharing economy and the hotel industry indicates that consumers' perceptions and behavior change gradually with changes in the level of sharing---from no sharing when staying in hotel rooms to intensive sharing when sharing rooms through collaborative consumption. Online consumer reviews focus on product and service attributes, and the influential factors of customer satisfaction and demand differ when consumers are at different accommodation-sharing levels. Not all attributes described in online reviews influence overall customer satisfaction. With a higher level of sharing, consumers' valuation changes from more to less tangible attributes. Consumers at a higher sharing level care more about social interaction and economic value than consumers at a lower sharing level. Transaction costs, particularly the information search and acquisition costs, play an important role in influencing customer purchase decisions in the sharing economy. Consumers refer to direct information for tangible attributes and to previous consumers' online reviews for intangible attributes to familiarize themselves with details before making purchase decisions. Our study provides implications that help platforms and hosts better target consumer segments with different sharing levels and more effectively utilize online reviews to generate positive electronic word of mouth to enhance consumer demand and the performance of platform economics.},
	author = {Xun Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113162},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Sharing economy, Customer satisfaction, Product and service attributes, Online reviews},
	pages = {113162},
	title = {How do consumers in the sharing economy value sharing? Evidence from online reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619301915},
	volume = {128},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619301915},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113162}}

@article{XU2021113467,
	abstract = {The on-demand economy has prospered with the rapid development of digital platforms. Many customers use on-demand service platforms to order services and then post online reviews. Using text-mining approaches, this study examines customers' online review-writing behavior and their overall satisfaction with restaurants in the context of on-demand food service. We use customers' overall ratings in their reviews to measure their overall satisfaction. We find that customers comment on the main service provider, the restaurants, and the auxiliary service providers, which include the drivers and on-demand service platform; in their online customer reviews, which are posted on the restaurants' web pages on the on-demand service platform. From text regressions, we found the determinants of customer satisfaction with the restaurants through their online reviews. There is a spillover effect from the performance of auxiliary providers on customer satisfaction with the main provider. That is, the performance of the drivers and the platform affects customers' overall satisfaction with the restaurants. In addition, we find that a higher cost of the order makes customers comment more on the attributes offered by the restaurants to show their overall satisfaction. Further, we find the type of listed merchants categorized by their properties (i.e., chain or independent) and participation in the platform programs affect the influence of the various attributes, offered by different providers, on customer satisfaction with the main provider. The findings shed light on the determinants of customers' overall satisfaction and urge improvement in collaboration and coordination between various participants in the on-demand service context.},
	author = {Xun Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113467},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {On-demand service, Customer satisfaction, Online customer reviews, Spillover effect},
	pages = {113467},
	title = {What are customers commenting on, and how is their satisfaction affected? Examining online reviews in the on-demand food service context},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620302220},
	volume = {142},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620302220},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113467}}

@article{LI2022113755,
	abstract = {For narrative products such as movies, books, and TV shows, electronic word of mouth (eWOM) can be a double-edged sword. It provides consumers with useful information while potentially revealing the storyline, that is, spoiling the surprise of what will happen. Prior studies have focused on the impact of spoilers on consumers' experiences through psychological experiments. However, the relationship between spoilers and narrative product sales has rarely been empirically explored. To fill this gap, the current study explores the impact of spoilers on the movie box office revenue and how this impact evolves over time. We collected 279,433 reviews of 465 films on a leading community website in China and constructed a dynamic generalized method of moments (GMM) model with instrumental variables to empirically examine the spoiler effect in the movie market. Our findings indicate that spoilers have a negative influence on the movie box office revenue. However, this impact is limited to the first 6 days after the movie is released. We also discover that spoilers have a stronger negative effect on narrative-based movies than non-narrative-based movies. Furthermore, eWOM volume and eWOM variance negatively moderate the spoiler effect on the box office revenue, but the moderating effect of eWOM valence is not significant. These findings can deepen movie industry decision makers' and platform providers' understanding of spoilers, helping them devise more feasible eWOM operation strategies.},
	author = {Yang Li and Xin (Robert) Luo and Kai Li and Xiaobo Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113755},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Spoilers, eWOM, Movie reviews, Box office revenue, Narrative},
	pages = {113755},
	title = {Exploring the spoiler effect in the digital age: Evidence from the movie industry},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000264},
	volume = {157},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000264},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113755}}

@article{ZHANG2022113765,
	abstract = {The task of detecting fraudulent reviewers is of great importance to E-commerce platforms. Existing research has invested much effort into developing comprehensive features and advanced techniques to detect fraudulent reviewers. However, most of these studies have ignored the data imbalance problem inherent in fraudulent reviewer detection: non-fraudulent reviewers are the majority, while fraudulent reviewers are the minority in real practice. To fill this gap, we propose a novel approach called ImDetector to detect fraudulent reviewers while handling data imbalance based on weighted latent Dirichlet allocation (LDA) and Kullback--Leibler (KL) divergence. Specifically, we develop a weighted LDA model to extract the latent topics of reviewers distributed on the review features. Asymmetric KL divergence is adopted to make the similarity measure between reviewers biased toward the fraudulent minority when using the K-nearest-neighbor for classification. By mapping the reviewers to the latent topics of features derived from the weighted LDA model and measuring the similarities between reviewers using asymmetric KL divergence, the data imbalance problem in fraudulent reviewer detection is alleviated. Extensive experiments on the Yelp.com dataset demonstrate that the proposed ImDetector approach is superior to the state-of-the-art techniques used for fraudulent reviewer detection. We also explain the experimental results and present the managerial implications of this paper.},
	author = {Wen Zhang and Rui Xie and Qiang Wang and Ye Yang and Jian Li},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113765},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {-commerce, Fraudulent reviewer detection, Imbalanced data, Weighted LDA, Kullback--Leibler divergence},
	pages = {113765},
	title = {A novel approach for fraudulent reviewer detection based on weighted topic modelling and nearest neighbors with asymmetric Kullback--Leibler divergence},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000367},
	volume = {157},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000367},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113765}}

@article{ZHENG2020113369,
	abstract = {Product defects are a major concern for manufacturers and customers. Detecting product defects is vital for manufacturers to prevent enormous product failure costs. As the surge of social media is in vogue, social media data become an important information source for manufacturers to collect defect information. In this study, we propose a novel probabilistic graphic model to discover defects from social media data. We first use three filters, namely, sentiment filter, component-symptom filter and similarity filter, to select informative data. Second, we analyze the remaining data via the proposed probabilistic graphic model and identify defect-related data. Our method provides detailed defect information including defect types, defective components and defect symptoms which is omitted by previous research. A case study in the automobile industry validates the effectiveness and superior performance of our method compared to prior approaches.},
	author = {Lu Zheng and Zhen He and Shuguang He},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113369},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Product defect detection, Social media data, Probabilistic graphic model, Text analysis},
	pages = {113369},
	title = {A novel probabilistic graphic model to detect product defects from social media data},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362030124X},
	volume = {137},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362030124X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113369}}

@article{WANG2021113465,
	abstract = {Comparable entity identification plays an essential role in the decision making of both consumers and firms in competitive environment. In contrast to traditional cooccurrence approaches, this paper proposes a novel method, namely, ICE (identifying comparable entities) for effectively identifying comparable entities from web search logs, which are online user-generated contents that reflect users' attention and preferences. ICE consists of two stages: the formulation of directly and indirectly associative relations, followed by a generative procedure that is designed for deriving a broad set of candidate entities that are indirectly associative with a specified focal entity; and a deep-learning-based semantic analysis with a word embedding procedure for measuring the similarities between entity profiles so as to target comparable entities from the candidate set. Extensive experiments show that ICE outperforms several baseline methods in the identification of accurate, broad and novel comparable entities with suitable rankings.},
	author = {Liye Wang and Jin Zhang and Guoqing Chen and Dandan Qiao},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113465},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Comparable entity identification, Web search logs, Indirectly associative relation, Semantic analysis},
	pages = {113465},
	title = {Identifying comparable entities with indirectly associative relations and word embeddings from web search logs},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620302207},
	volume = {141},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620302207},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113465}}

@article{LI2020241,
	abstract = {In cross-language question retrieval (CLQR), users employ a new question in one language to search the community question answering (CQA) archives for similar questions in another language. In addition to the ranking problem in monolingual question retrieval, one needs to bridge the language gap in CLQR. The existing adversarial models for cross-language learning normally rely on a single adversarial component. Since natural languages consist of units of different abstract levels, we argue that crossing the language gap adaptatively on different levels with multiple adversarial components should lead to smoother text representation and better CLQR performance. To this end, we first encode questions into multi-layer representations of different abstract levels with a CNN based model which enhances conventional models with diverse kernel shapes and the corresponding pooling strategy so as to capture different aspects of a text segment. We then impose a set of adversarial components on different layers of question representation so as to decide the appropriate abstract levels and their role in performing cross-language mapping. Experimental results on two real-world datasets demonstrate that our model outperforms state-of-the-art models for CLQR, which is on par with the strong machine translation baselines and most monolingual baselines.},
	author = {Bo Li and Xiaodong Du and Meng Chen},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-language question retrieval, Adversarial learning, Community question answering},
	pages = {241-252},
	title = {Cross-language question retrieval with multi-layer representation and layer-wise adversary},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300359},
	volume = {527},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300359},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.035}}

@article{SHEN202063,
	abstract = {Hierarchical classification (HC) is effective when categories are organized hierarchically. However, the blocking problem makes the effect of hierarchical classification greatly reduced. Blocking means that samples are easily getting misclassified in high-level classifiers so that the samples are blocked at the high-level of the hierarchy. This issue is caused by the inconsistency between the artificially defined hierarchy and the actual hierarchy of the raw data. Another issue is that it is flippant to strictly process data following the hierarchy. Therefore, special treatment is required for some uncertain data. To address the first issue, we learn category relationships and modify the hierarchy. To address the second issue, we introduce three-way decisions (3WD) to targetedly deal with the ambiguous data. We extend original studies and propose two HC models based on 3WD, collectively referred to as TriHC, for carefully modifying the hierarchy to alleviate the blocking problem. The proposed TriHC model learns new category hierarchies by the following three steps: (1) mining category relations; (2) modifying category hierarchies according to the latent category relations; and (3) using 3WD to divide observed objects into three regions: positive region, boundary region, and negative region, and making decisions based on different strategies. Specifically, based on different category relation mining methods, there are two versions of TriHC, cross-level blocking priori knowledge based TriHC (CLPK-TriHC) and expert classifier based TriHC (EC-TriHC). The CLPK-TriHC model defines a cross-level blocking distribution matrix to mine the category relations between the higher and lower levels. To better exploit category hierarchical relations, the EC-TriHC model builds expert classifiers using topic model to learn latent category topics. Experimental results validate that the proposed methods can simultaneously reduce the blocking and improve the classification accuracy.},
	author = {Wen Shen and Zhihua Wei and Qianwen Li and Hongyun Zhang and Duoqian Miao},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.02.020},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical classification, Blocking reduction, Three-way decisions, Category relation mining, Topic model},
	pages = {63-76},
	title = {Three-way decisions based blocking reduction models in hierarchical classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300955},
	volume = {523},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300955},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.02.020}}

@article{KHAN202269,
	abstract = {Knowledge Graph Embedding (KGE)-enhanced recommender systems are effective in providing accurate and personalized recommendations in diverse application scenarios. However, such techniques that exploit entire embedded Knowledge Graph (KG) without data relevance approval constraints fail to stop noise penetration into the data. Additionally, approaches that pay no heed to tackle semantic relations among entities remain unable to effectively capture semantical structure of Heterogeneous Information Graph (HIG). Therefore, in this paper, we propose Similarity Attributed Graph-embedding Enhancement (SAGE) approach to model similarity-aware semantic connections among entities according to their triplets' granularity. SAGE is a novel Knowledge Graph Embedding Enhancement (KGEE) method that constructs Entity-relevance-based Similarity-attributed Subgraph (ESS) to remove noise from the underlying data. It propagates interactions-enhanced knowledge over ESS to learn higher-order semantic connections among entities; and simultaneously utilizes feedbacks to enhance the interactions and regularize the model to highlight influential targets (nodes). Further, it samples influential targets in KG, independently move their preferences to the Local Central Nodes (LCN) of current influential areas, and streamline the collected information from all LCN to the main unit. Finally, a prediction module is used to determine generalized preferences for recommendation. We performed extensive experiments on benchmark datasets to evaluate the performance of SAGE where it outperformed the state-of-the-art methods with significant improvements in effectively providing the desired explainable recommendations.},
	author = {Nasrullah Khan and Zongmin Ma and Aman Ullah and Kemal Polat},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.124},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Knowledge graph, KGE and KGEE, Recommender systems, Similarity, Interactions, Features},
	pages = {69-95},
	title = {Similarity attributed knowledge graph embedding enhancement for item recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522010441},
	volume = {613},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522010441},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.124}}

@article{ZHANG2020306,
	abstract = {With the fast development of online E-commerce Websites and mobile applications, users' auxiliary information as well as products' textual information can be easily collected to form a vast amount of training data. Therefore, research efforts are urgently needed to make customized recommendations using such large but sparse data. Deep recommendation model is a natural choice for this research issue. However, most existing approaches try to investigate either user's auxiliary information such as age and zipcode, or item's textual information such as product descriptions, reviews or comments. Therefore, it is desired to see whether user's auxiliary information and item's textual information could be modeled simultaneously. This paper proposes a novel approach which is essentially a hybrid probabilistic matrix factorization model. Particularly, it has two sub components. One component tries to predict user's rating scores by capturing user's personal preferences extracted from auxiliary information. Another component tries to model item's textual attractiveness to different users via a proposed attention based convolutional neural network. We then propose a global objective function and optimize these two sub components under a unified framework. Extensive experiments are performed on five real-world datasets, i.e., ML-100K, ML-1M, ML-10M, AIV and Amazon sub dataset. The promising experimental results have demonstrated the superiority of our proposed approach when compared with both baseline models and state-of-the-art deep recommendation approaches, i.e., PMF, CDL, CTR, ConvMF, ConvMF+ and D-Attn with respect to RMSE criterion.},
	author = {Xiaofeng Zhang and Huijie Liu and Xiaoyun Chen and Jingbin Zhong and Di Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.044},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Probabilistic matrix factorization, Deep learning, Recommendation systems},
	pages = {306-316},
	title = {A novel hybrid deep recommendation system to differentiate user's preference and item's attractiveness},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300554},
	volume = {519},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300554},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.044}}

@article{BAI2022,
	abstract = {With the widespread development of the internet, multi-view text documents have become increasingly common, which has led to extensive research on multi-view text document modeling. As opposed to traditional single-view document modeling, which treats each document independently and learns each document as a single topic representation, the views of multi-view text documents have complicated correlation relationships that include both the global and local underlying topical information. In this study, we introduce a deep generative model for multi-view document modeling known as Hierarchical Variational Auto-Encoder (HVAE), which combines the advantages of the probability generative model for learning interpretable latent information and the deep neural network for efficient parameter inference. Specifically, a set of hierarchical topic representations is learned for each multi-view document to capture the document-level global topical information and view-level local topical information for each view. A two-level hierarchical topic inference network is investigated as the encoder network of HVAE, which is designed using an aligned variational auto-encoder, to learn the hierarchical topic representations. Subsequently, multi-view documents are generated through a two-layered generation network, considering both the view-level local and document-level global topic representations. Experiments on three real datasets of different scales for various tasks demonstrate the satisfactory results of the proposed method.},
	author = {Ruina Bai and Ruizhang Huang and Yongbin Qin and Yanping Chen and Chuan Lin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.10.052},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {deep variational inference, variational auto-encoder, multi-view document modeling, hierarchical topic representation, probability generative model},
	title = {HVAE: A Deep Generative Model via Hierarchical Variational Auto-Encoder for Multi-view Document Modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522011732},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522011732},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.10.052}}

@article{DAU20201279,
	abstract = {With the developments of e-commerce websites, user textual review has become an important source of information for improving the performance of recommendation systems, as they contain fine-grained users' opinions that generally reflect their preference towards products. However, most of the classical recommender systems (RSs) often ignore such user opinions and therefore fail to precisely capture users' specific sentiments on products. Although a few of the approaches have attempted to utilize fine-grained users' opinions for enhancing the accuracy of recommendation systems to some extent, most of these methods basically rely on handcrafted and rule-based approaches that are generally known to be time-consuming and labour-intensive. As such, their application is limited in practice. Thus, to overcome the above problems, this paper proposes a recommendation system that utilizes aspect-based opinion mining (ABOM) based on the deep learning technique to improve the accuracy of the recommendation process. The proposed model consists of two parts: ABOM and rating prediction. In the first part, we use a multichannel deep convolutional neural network (MCNN) to better extract aspects and generate aspect-specific ratings by computing users' sentiment polarities on various aspects. In the second part, we integrate the aspect-specific ratings into a tensor factorization (TF) machine for the overall rating prediction. Experimental results using various datasets show that our proposed model achieves significant improvements compared with the baseline methods.},
	author = {Aminu Da'u and Naomie Salim and Idris Rabiu and Akram Osman},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.10.038},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Aspect Based Opinion Mining, Sentiment classification, Deep CNN, Tensor factorization, Recommendation systems, Ratings prediction},
	pages = {1279-1292},
	title = {Recommendation system exploiting aspect-based opinion mining with deep learning method},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519310060},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519310060},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.10.038}}

@article{XIAO2021262,
	abstract = {Topic detection aims to discover valuable topics from the massive online news. It can help people to capture what is happening in real world and alleviate the burden of information overload. It also has great significance since the online news is experiencing an explosive growth. Topic detection is typically transformed into a document clustering problem, whose core idea is to cluster news documents that report on the same topic to the same group based on document similarity. Due to the complex structure and long length of news documents, the similarity measurement of news is very challenging. Existing term-based methods represent news documents based on a set of informative keywords in the document with a vector space model (VSM) and then the relationship between documents is calculated by cosine similarity. However, VSM ignores the relationship between words and has sparse semantics, which leads to low precision of topic detection. In recent years, the probabilistic methods and the graph analytical methods have been proposed for topic detection. However, both of them have high time complexity. To cope with these problems, we first present a novel document representation approach based on graphical decomposition, which decomposes each news document into different semantic units and then relationship between the semantic units is constructed to form a capsule semantic graph (CSG). The CSG can retain the relationship between words and alleviate the sparse semantics compared to VSM representation. We next introduce the graph kernel to measure the similarity between the CSGs based on their substructures. Finally, we use an incremental clustering method to cluster the news documents, in which the documents are represented by CSGs and the similarity between documents is calculated by graph kernel. The experiment results on three standard datasets show that our method obtains higher precision, recall and F1 score than several state-of-the-art methods. Moreover, the experiment results on a large news dataset show that our CSG-SM has lower time complexity than probabilistic methods and graph analytical methods.},
	author = {Kejing Xiao and Zhaopeng Qian and Biao Qin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.04.029},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Topic detection, Word co-occurrence, Capsule semantic graph, Graph kernel, Community detection},
	pages = {262-277},
	title = {A graphical decomposition and similarity measurement approach for topic detection from online news},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552100356X},
	volume = {570},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552100356X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.04.029}}

@article{YANG2021185,
	abstract = {The investigation of user preferences through user comments has attracted significant attention. Although topic models have been verified as useful tools to facilitate the understanding of textual contents, they cannot be directly applied to accomplish this task because of two problems. The first problem is the severe data sparsity suffered by user comments because they are generally short. The second problem is the mixture of opinions from both user comments and the original documents the users commented on. To simultaneously solve the data sparsity problem and explore clean user preferences, we propose an author co-occurring topic model (AOTM) for normal documents and their short user comments. By considering authorship, AOTM allows each author of short texts to have a probability distribution over a set of topics represented only short texts. Accordingly, the individual user preferences can be investigated based on these author-level distributions. We verify the performance of AOTM using two news article datasets and one e-commerce dataset. Extensive experiments demonstrate that the AOTM outperforms several state-of-the-art methods in topic learning and topic representation of documents. The potential usage of AOTM in exploring individual user preferences is further illustrated by drawing user portraits and predicting user posting behaviors.},
	author = {Yang Yang and Feifei Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.04.060},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Authorship, Short texts, Topic model, User preference},
	pages = {185-199},
	title = {Author topic model for co-occurring normal documents and short texts to explore individual user preferences},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521003893},
	volume = {570},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521003893},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.04.060}}

@article{ZHAO2021283,
	abstract = {Current research on subset selection for opinion analysis assumes that their methods can retrieve the opinions expressed in documents from general text features. However, such relaxed conditions can hardly maintain the performance of the analysis in opinion mining, especially when given strict limitations on the subset size. In this paper, we propose a framework for opinion subset selection. This framework can select a small set of instances from original data to convey a subjective representation for opinion classification and regression. Compared with our framework, the conventional submodular based subset selection approach cannot capture the fine-grained opinion features expressed in the corpus. Specifically, we propose a monotone non-decreasing score function and a framework based on topic modeling and submodular maximization for filtering irrelevant information and selecting the subsets. Our work further introduces an opinion-sensitive algorithm for optimizing the proposed function for opinion subset construction. We perform extensive experiments and comparative analysis of different subset selection methods in this work. The experimental result shows that the proposed opinion subset selection framework can compress the original text training set and preserve the test set's classification and regression metric performance at the same time.},
	author = {Yang Zhao and Tommy W.S. Chow},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.12.083},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Subset selection, Opinion mining, Opinion summarization, Submodular function optimization, Opinion subset selection, Sentiment analysis},
	pages = {283-306},
	title = {Opinion subset selection via submodular maximization},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000141},
	volume = {560},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000141},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.12.083}}

@article{PRADHAN2021212,
	abstract = {Scholarly venue recommendation is an emerging field due to a rapid surge in the number of scholarly venues concomitant with exponential growth in interdisciplinary research and cross collaboration among researchers. Finding appropriate publication venues is confronted as one of the most challenging aspects in paper publication as a larger proportion of manuscripts face rejection due to a disjunction between the scope of the venue and the field of research pursued by the research article. We present CLAVER--an integrated framework of Convolutional Layer, bi-directional LSTM with an Attention mechanism-based scholarly VEnue Recommender system. The system is the first of its kind to integrate multiple deep learning-based concepts, that requires only the abstract and the title of a manuscript to identify academic venues. An extensive and exhaustive set of experiments conducted on the DBLP dataset certify that the postulated model CLAVER performs better than most of the modern techniques as entrenched by standard metrics such as stability, accuracy, MRR, average venue quality, precision@k, nDCG@k and diversity.},
	author = {Tribikram Pradhan and Prashant Kumar and Sukomal Pal},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.12.024},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Recommendation system, Convolution neural network, Long short-term memory (LSTM), Attention mechanism, Deep learning},
	pages = {212-235},
	title = {CLAVER: An integrated framework of convolutional layer, bidirectional LSTM with attention mechanism based scholarly venue recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520311890},
	volume = {559},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520311890},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.12.024}}

@article{COSTA2021226,
	abstract = {An innovative model-based approach to coupling text clustering and topic modeling is introduced, in which the two tasks take advantage of each other. Specifically, the integration is enabled by a new generative model of text corpora. This explains topics, clusters and document content via a Bayesian generative process. In this process, documents include word vectors, to capture the (syntactic and semantic) regularities among words. Topics are multivariate Gaussian distributions on word vectors. Clusters are assigned corresponding topic distributions as their semantics. Content generation is ruled by text clusters and topics, which act as interacting latent factors. Documents are at first placed into respective clusters, then the semantics of these clusters is then repeatedly sampled to draw document topics, which are in turn sampled for word-vector generation. Under the proposed model, collapsed Gibbs sampling is derived mathematically and implemented algorithmically with parameter estimation for the simultaneous inference of text clusters and topics. A comparative assessment on real-world benchmark corpora demonstrates the effectiveness of this approach in clustering texts and uncovering their semantics. Intrinsic and extrinsic criteria are adopted to investigate its topic modeling performance, whose results are shown through a case study. Time efficiency and scalability are also studied.},
	author = {Gianni Costa and Riccardo Ortale},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.01.019},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Document clustering, Topic modeling, Word embeddings, Bayesian text analysis},
	pages = {226-240},
	title = {Jointly modeling and simultaneously discovering topics and clusters in text corpora using word vectors},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000463},
	volume = {563},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000463},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.01.019}}

@article{QIN202237,
	abstract = {Online reviews play an important role in consumers' purchasing decisions. However, many online reviews confuse consumers when they wish to make a purchase but lack experience. To solve the problem of product ranking based on online reviews, two important issues must be addressed: sentiment analysis and product ranking based on multi-criteria decision-making (MCDM) methods. Therefore, this paper proposes an integrated MCDM method for product ranking through online reviews based on evidential reasoning (ER) theory and stochastic dominance (SD) rules. First, online reviews are preprocessed to obtain product attributes and weight values. Then, we use naive Bayes (NB), logistic regression (LR), and support vector machines (SVM) for the sentiment analysis of online reviews, and the results of the three classifiers are aggregated using ER theory. In addition, according to the confidence distribution matrix of sentiment orientations, SD rules are used to determine the stochastic dominance relations between pairwise alternatives for each attribute. Furthermore, we use the stochastic multi-criteria acceptability analysis (SMAA)-PROMETHEE method to obtain the final product ranking results and conduct sensitivity analysis. Finally, a case study on ranking computer products from JD Mall through online reviews is provided to illustrate the validity of the proposed method.},
	author = {Jindong Qin and Mingzhi Zeng},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.070},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Online reviews, Product ranking, Evidential reasoning, Stochastic dominance, SMAA-PROMETHEE},
	pages = {37-61},
	title = {An integrated method for product ranking through online reviews based on evidential reasoning theory and stochastic dominance},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522009811},
	volume = {612},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522009811},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.070}}

@article{HOU2022215,
	abstract = {Previous online policy opinion analyses based on social media data have focused on topic detection and sentiment classification of policy opinion after a given period following policy implementation. These approaches are limited and inefficient because they provide no opportunity to change citizens' opinions once they have been formed. Furthermore, incorporating auxiliary information to enrich semantic representations is vital and challenging due to limited texts, and a lack of both semantic information and strict syntactic structure. Therefore, we propose a novel framework to extract and integrate multidimensional features from user-related and policy-related social media information and predict policy comment polarity in the policy release phase. First, we construct four machine learning models for model-induced features to capture topic-related and opinion-related features and identify the policy-opinion nexus. In addition, we integrate basic and behavioral user features. Then, we leverage multidimensional features to construct a stacked learning model for predicting the policy opinion. Finally, we conduct experiments on 20 policy comment datasets to demonstrate that our prediction framework can effectively predict public opinion about a policy once it is released. Our model provides key insights into policy opinions in advance and can enable policymakers to engage in better policy communication before opinion formation.},
	author = {Wenju Hou and Ying Li and Yijun Liu and Qianqian Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.004},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Policy opinion, Sentiment prediction, Deep learning, Feature engineering},
	pages = {215-234},
	title = {Leveraging multidimensional features for policy opinion sentiment prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522008842},
	volume = {610},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522008842},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.004}}

@article{CHAI20221029,
	abstract = {Crowdsourcing plays a vital role in today's AI industry. However, existing crowdsourcing research mainly focuses on those simple tasks that are often formulated as label classification, while complex open-ended tasks such as question answering and translation have not received much attention. Such tasks usually have open solution spaces and non-unique true answers, which pose great challenges for designing effective crowdsourcing algorithms. In this work, we are concerned specifically with complex text annotation crowdsourcing tasks, where each answer of a task is in the form of free text. We propose an error consistency-based approach to inferring a satisfying result from a set of open-ended answers. First, each answer is represented with two vectors that capture the local word collocation and the global sentence semantics respectively. Second, the true answer is approximated by the sum of the answer vectors weighted by the reciprocals of their respective errors. Third, an algorithm called AEC (Aggregation based on Error Consistency) is designed to infer the aggregated result by maximizing the consistency of the errors of an answer in two vector spaces. Experimental results on two datasets demonstrate the effectiveness of our approach.},
	author = {Lei Chai and Hailong Sun and Zizhe Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.07.001},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Crowdsourcing, Open-ended text annotation, Answer aggregation, Annotation error consistency},
	pages = {1029-1044},
	title = {An error consistency based approach to answer aggregation in open-ended crowdsourcing},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522006909},
	volume = {608},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522006909},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.07.001}}

@article{WU2020100,
	abstract = {Text is one of the most common unstructured data, and usually, the most primary task in text mining is to transfer the text into a structured representation. However, the existing text representation models split the complete semantic unit and neglect the order of words, finally lead to understanding bias. In this paper, we propose a novel phrase-based text representation method that takes into account the integrity of semantic units and utilizes vectors to represent the similarity relationship between texts. First, we propose HPMBP (Hierarchical Phrase Mining Based on Parsing) which mines hierarchical phrases by parsing and uses BOP (Bag Of Phrases) to represent text. Then, we put forward three phrase embedding models, called Phrase2Vec, including Skip-Phrase, CBOP (Continuous Bag Of Phrases), and GloVeFP (Global Vectors For Phrase Representation). They learn the phrase vector with semantic similarity, further obtain the vector representation of the text. Based on Phrase2Vec, we propose PETC (Phrase Embedding based Text Classification) and PETCLU (Phrase Embedding based Text Clustering). PETC utilizes the phrase embedding to get the text vector, which is fed to a neural network for text classification. PETCLU gets the vectorization expression of text and cluster center by Phrase2Vec, furthermore extends the K-means model for text clustering. To the best of our knowledge, it is the first work that focuses on the phrase-based English text representation. Experiments show that the introduced Phrase2Vec outperforms state-of-the-art phrase embedding models in the similarity task and the analogical reasoning task on Enwiki, DBLP, and Yelp dataset. PETC is superior to the baseline text classification methods in the F1-value index by about 4%. PETCLU is also ahead of the prevalent text clustering methods in entropy and purity indicators. In summary, Phrase2Vec is a promising approach to text mining.},
	author = {Yongliang Wu and Shuliang Zhao and Wenbin Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.12.031},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Text representation, Phrase mining, Phrase embedding, Parsing, Text classification, Text clustering},
	pages = {100-127},
	title = {Phrase2Vec: Phrase embedding based on parsing},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519311429},
	volume = {517},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519311429},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.12.031}}

@article{ASGHARI2022184,
	abstract = {A primary focus of the healthcare industry is to improve patient experience and quality of service. Practitioners and health workers are generating large volumes of text that are captured in Electronic Medical Records, clinical reports, and publications. Additionally, patients post millions of comments on social media related to healthcare, on diverse topics such as hospital services, disease symptoms, and drugs effects. Unifying various data sources can guide physicians and healthcare workers to avoid unnecessary, irrelevant information and expedite access to helpful information. The main challenge to creating Biomedical Natural Language Understanding is the lack of standard datasets and the extensive computational resources needed to develop different models. This paper proposes a model trained on low-tier GPU computers, producing comparable results to larger models like BioBERT. We propose BINER, a Biomedical Named Entity Recognition architecture using limited data and computational resources.},
	author = {Mohsen Asghari and Daniel Sierra-Sosa and Adel S. Elmaghraby},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.04.037},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Natural Language Processing, Named entity recognition, Deep learning, Biomedical text, Transfer Learning, Computational efficiency},
	pages = {184-200},
	title = {BINER: A low-cost biomedical named entity recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522003838},
	volume = {602},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522003838},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.04.037}}

@article{CHEN2021343,
	abstract = {Network representation learning (NRL) aims at modeling network graph by encoding vertices and edges into a low-dimensional space. These learned representations can be used for subsequent applications, such as vertex classification and link prediction. Negative Sampling (NS) is the most widely used method for boosting the performance of NRL. However, most of the existing work only randomly draws negative samples based on vertex frequencies, i.e., the vertices with higher frequency are more likely to be drawn, which ignores the situation that the sampled one may not be a true negative sample, thus, lead to undesirable embeddings. In this paper, we propose a new negative sampling method, called Hierarchical Negative Sampling (HNS), which is able to model the latent structures of vertices and learn the relations among them. During sampling, HNS can draw more appropriate negative samples and thereby obtain better performance on network embeddings. Firstly, we theoretically demonstrate the superiority of HNS over NS. And then we use experimental results to show that our proposed method outperforms the state-of-the-art models on vertex classification tasks at different training scales in real-world networks.},
	author = {Junyang Chen and Zhiguo Gong and Wei Wang and Weiwen Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.07.015},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical negative sampling, Network representation learning, Network embeddings},
	pages = {343-356},
	title = {HNS: Hierarchical negative sampling for network representation learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520306770},
	volume = {542},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520306770},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.07.015}}

@article{ZHOU20221030,
	abstract = {Multi-topic sentiment analysis, which aims to identify the topics and classify their corresponding sentiment, is of great value in understanding consumers' behaviour and improving services. Because of the high cost of manual annotation of the datasets, topic model-based approaches that model the joint distributions of both topics and sentiments have been studied previously. Some studies proposed models that leverage the prior knowledge derived from the pre-trained word embeddings and have proven effective. However, most of the existing models are based on the assumption that words and topics are conditionally independent, ignoring the dependency relations among them. Additionally, the fine-tuning of the pre-trained word embeddings to incorporate the contextual information is also neglected in these models. This could result in the ambiguous representations of topics. In this paper, we propose a novel weakly-supervised graph-based joint sentiment topic model (W-GJST) that integrates an edge-gated graph convolutional network (E-GCN) into a joint sentiment-topic model. An importance sampling-based training method is proposed to learn the contextual representations of topics and words efficiently. Additionally, a self-training multi-topic classifier is designed for the multi-label topic identification. Experiments on two benchmark datasets demonstrate the superiority of the proposed W-GJST compared to the baseline models in terms of topic modelling, topic identification and topic-sentiment identification.},
	author = {Tao Zhou and Kris Law and Douglas Creighton},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.07.126},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multi-topic sentiment analysis, Edge-gated graph convolutional network, Joint sentiment-topic model, Self-training},
	pages = {1030-1051},
	title = {A weakly-supervised graph-based joint sentiment topic model for multi-topic sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522008192},
	volume = {609},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522008192},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.07.126}}

@article{KONG202039,
	abstract = {Sentiment classification is an important research task in Natural Language Processing. To fulfill this type of classification, previous works have focused on leveraging task-specific features. However, they only notice part of the related features. Also, state-of-the-art methods based on neural networks often ignore traditional features. This paper proposes a novel text sentiment classification method that learns the representation of texts by hierarchically incorporating multiple features. More specifically, we design different representations for sentiment words according to the polarity of labeled texts and whether negation exists; we distinguish words with different part-of-speech tags; emoticons, if there are, are to optimize the word vectors obtained in the previous step; apart from word embeddings, character embeddings are also trained. We use a deep neural network to get a sentence-level representation from both word and character sequence. For documents with at least two sentences, we use a hierarchical structure and design a rule to give more weight to import sentences empirically to get a document-level representation. Experimental results on open datasets demonstrate that our method could effectively improve the sentiment classification performance compared with the basic models and state-of-the-art methods.},
	author = {Li Kong and Chuanyi Li and Jidong Ge and FeiFei Zhang and Yi Feng and Zhongjin Li and Bin Luo},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.012},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment classification, Deep learning, Feature engineering},
	pages = {39-55},
	title = {Leveraging multiple features for document sentiment classification},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552030013X},
	volume = {518},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552030013X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.012}}

@article{LEI202098,
	abstract = {Recommendation system (RS) is designed to provide personalized services based on the users' historical data. It has been applied in various fields and is expected to recommend the suitable services for the different kinds of users. Considering the importance of individual privacy, current users gradually tend not to expose personal information. This means RS may face the highly sparse datasets in the fields of cloud security. In general, the accuracy of recommendation will be improved with the growth of individual data, but the cold start problem is exactly in this contradictory phenomenon: this question evolves to produce sufficiently accurate recommendation result under the data scarcity problem. RS has to recommend services for the rarely historical data users and the latent users might drain along with the production of counter effects. To alleviate data scarcity problem in cloud security environment, this work is to introduce similar domain knowledge based on the transfer learning. Besides, the content and location based methods have been proved that these ideas work under this situation. So, this work also employs latent dirichlet allocation (LDA) to analysis the service descriptions and explore the relationship between the content and location information. In this framework, the suitable combination of LDA and word2vec models will balance the accuracy and speed which benefit service recommendation particularly. The related experiments demonstrate the effectiveness on the real word dataset. It can be found that the transfer learning based word2vec model shows the potentiality to explore the relationship between topic words, and improve the LDA algorithm from the content relationship. This proves that in both cold start environment and warm start environment, the proposed algorithm is more robust than other model-based state-of-art methods.},
	author = {Chao Lei and Hongjun Dai and Zhilou Yu and Rui Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.10.004},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Service recommendation, Transfer learning, Cloud security},
	pages = {98-111},
	title = {A service recommendation algorithm with the transfer learning based matrix factorization to improve cloud security},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309582},
	volume = {513},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519309582},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.10.004}}

@article{DONG2020203,
	abstract = {There has been considerable interest in transforming unstructured social tagging data into structured knowledge for semantic-based retrieval and recommendation. Research in this line mostly exploits data co-occurrence and often overlooks the complex and ambiguous meanings of tags. Furthermore, there have been few comprehensive evaluation studies regarding the quality of the discovered knowledge. We propose a supervised learning method to discover subsumption relations from tags. The key to this method is quantifying the probabilistic association among tags to better characterise their relations. We further develop an algorithm to organise tags into hierarchies based on the learned relations. Experiments were conducted using a large, publicly available dataset, Bibsonomy, and three popular, human-engineered or data-driven knowledge bases: DBpedia, Microsoft Concept Graph, and ACM Computing Classification System. We performed a comprehensive evaluation using different strategies: relation-level, ontology-level, and knowledge base enrichment based evaluation. The results clearly show that the proposed method can extract knowledge of better quality than the existing methods against the gold standard knowledge bases. The proposed approach can also enrich knowledge bases with new subsumption relations, having the potential to significantly reduce time and human effort for knowledge base maintenance and ontology evolution.},
	author = {Hang Dong and Wei Wang and Frans Coenen and Kaizhu Huang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.04.002},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Knowledge discovery, Knowledge base enrichment, Ontology learning, Social tagging, Probabilistic association analysis, Classification},
	pages = {203-220},
	title = {Knowledge base enrichment by relation learning from social tagging data},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520303017},
	volume = {526},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520303017},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.04.002}}

@article{WANG2021762,
	abstract = {A recommender system utilizes information filtering techniques to help users obtain accurate information effectively and efficiently. The existing recommender systems, however, recommend items based on the overall ratings or the click-through rate, and emotions expressed by users are neglected. Conversely, the cold-start problem and low model scalability are the two main problems with recommender systems. The cold-start problem is encountered when the system lacks initial rating. Low model scalability indicates that a model is incapable of coping with high-dimensional data. These two problems may mislead the recommender system, and thus, users will not be satisfied with the recommended items. A hybrid recommender system is proposed to mitigate the negative effects caused by these problems. Additionally, ontologies are applied to integrate the extracted features into topics to reduce dimensionality. Topics mentioned in the items are displayed in the form of a topic map, and users can refer to these similar items for further information.},
	author = {Hei-Chia Wang and Hsu-Tung Jhou and Yu-Shan Tsai},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.04.015},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hybrid recommender system, Cold start, Social network, Ontology, Sentiment analysis},
	pages = {762-778},
	title = {Adapting topic map and social influence to the personalized hybrid recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518302718},
	volume = {575},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518302718},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.04.015}}

@article{CHAUDHURI2022,
	abstract = {Extraneous growth of scientific information over the Internet makes the searching task non-trivial and as a consequence researchers are facing difficulties in finding relevant papers from the millions of research papers in digital repositories. The research paper recommendation systems have been advocated to address this problem. The existing research paper recommendation systems lack in exploiting prominent information of papers, such as relevancy with the current time, novelty, scientific contribution, writing complexity of the papers, etc. Further, the existing models emphasize only on user's preference rather than user's intention that may change with time. Furthermore, the existing models do not consider a sound ranking strategy to unleash the personalization aspect and relevancy of papers. This work aims to address the existing limitations and proposes a systematic hidden attribute-based recommendation engine (SHARE). SHARE utilizes a feature engineering technique to unfold valuable insights of papers through multiple hidden features. These features are used as a context for users as well as multiple criteria for ranking papers. Additionally, SHARE predicts a user's intention beyond the user's preference to capture the dynamic notion of a user. Finally, a novel ranking strategy is proposed to retrieve personalized and the most important papers. SHARE is flexible for recommending both old and new users. In order to evaluate the effectiveness of SHARE both user studies and system evaluations were performed. Experimental results substantiate the efficacy of the proposed approach and are comparable to the existing systems.},
	author = {Arpita Chaudhuri and Monalisa Sarma and Debasis Samanta},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.09.064},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Research paper recommendation, Content-based recommendation, User modeling, Multi-criteria analysis, Hybrid ranking, Personalized recommendation},
	title = {SHARE: Designing Multiple Criteria-Based Personalized Research Paper Recommendation System},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552201115X},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552201115X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.09.064}}

@article{ZHENG2022211,
	abstract = {Friend link prediction is an important research problem in recommender systems. Existing network embedding and knowledge embedding methods mainly consider the structural relationships of entities, ignoring the explanatory role of text contents. In this paper, we present an explainable friend link recommendation method that leverages direct and indirect similarity of user pairs via fusion embedding of heterogeneous context information. In social networks, while considering user content interests, first, a fusion user embedding method was developed by incorporating external knowledge semantics. Second, for a user pair, we calculate their direct similar relationship using fusion user embeddings. Additionally, based on intermediate neighbors, we compute their indirect similar relationship by using an attention mechanism, which explains neighbors' bootable and transitive influences for learning the social relationship of user pairs. Then, a hybrid personalized and neighbor attention model for friend link prediction was proposed by considering direct and indirect factors. Finally, experiments were conducted on the Sina Weibo datasets, which indicates that the proposed method effectively predicts the friends of users and provides a good interpretation for link prediction recommendation results.},
	author = {Jianxing Zheng and Zifeng Qin and Suge Wang and Deyu Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.03.010},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Explainable friend link prediction, Attention mechanism, Fusion user embedding, Heterogeneous context information},
	pages = {211-229},
	title = {Attention-based explainable friend link prediction with heterogeneous context information},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522002092},
	volume = {597},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522002092},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.03.010}}

@article{DAS2021279,
	abstract = {Nowadays, the increase in criminal activities has resulted in a massive generation of crime reports describing the details of the crime incidents. Analyzing these reports for crime type prediction helps the law enforcement agencies deal with crime prevention strategies. But it is quite a demanding and difficult task to consider these reports individually and determine their crime types. In the proposed work, an efficient classifier has been designed to analyze the crime reports which not only predict the crime types of the reports but at the same time upgrades itself with the help of new crime reports. Therefore, this task demands an incremental supervised learning technique that continuously learns the existing classifier based on the new set of reports and information already extracted from the old set of reports. Developing an incremental classifier infuses the knowledge that keep coming from the newly generated reports and help in increasing the report-discriminating power of the classifier. In this work, we have applied a Bi-objective Particle Swarm Optimization technique for generating an efficient incremental classifier for classifying and predicting the crime reports dynamically. Crime reports of different countries, such as India, the United States of America, and the United Arab Emirates, have been collected from online classified newspapers to measure the performance of the proposed as well as some state-of-the-art classifiers. Also, the method has been evaluated based on an unbiased police witness narrative crime reports and finally, a statistical test has been performed using all four considered datasets to measure the statistical significance of the proposed methodology.},
	author = {Priyanka Das and Asit Kumar Das and Janmenjoy Nayak and Danilo Pelusi and Weiping Ding},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.02.002},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Crime report prediction, Particle Swarm Optimization, Incremental classifier, Named entity recognition, Multi-objective optimization},
	pages = {279-303},
	title = {Incremental classifier in crime prediction using bi-objective Particle Swarm Optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521001225},
	volume = {562},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521001225},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.02.002}}

@article{PHAN2021243,
	abstract = {Social networks are a very popular channel for people to communicate with, to find, to reference other users before making decisions, especially those concerning purchase. How can users' opinions within social networks be used in making decisions cost-effective and reliable? In this paper, we propose an approach for supporting decision-making based on measuring the user satisfaction level by analyzing the sentiment of aspects and mining the fuzzy decision trees. Our proposal has been proved to overcome some of the disadvantages of previous methods. Specifically, we consider the fuzzy sentiments of users for aspects and the effects of user satisfaction, dissatisfaction, and hesitation for decision-making. The proposed method comprises four main stages. The first stage identifies a topic, which the user is interested. In the second stage, aspects of the topic and their sentiments within tweets are extracted. At the third stage, the user satisfaction level is calculated according to each kind of sentiment identified in the second step. Finally, a decision matrix is constructed, and the fuzzy decision tree is built to generate a set of rules for supporting users in decision-making. The experiments using tweets show that the proposed method achieves promising results regarding the accuracy and gained information.},
	author = {Huyen Trang Phan and Ngoc Thanh Nguyen and Van Cuong Tran and Dosam Hwang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.01.008},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment analysis, Decision-making, Fuzzy decision tree, User satisfaction level, Fuzzy sentiment phrase, BiLSTM-CRF model},
	pages = {243-273},
	title = {An approach for a decision-making support system based on measuring the user satisfaction level on Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000098},
	volume = {561},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000098},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.01.008}}

@article{NAKAMURA2021482,
	abstract = {Most work on musical score models (a.k.a.musical language models) for music transcription has focused on describing the local sequential dependence of notes in musical scores and failed to capture their global repetitive structure, which can be a useful guide for transcribing music. Focusing on rhythm, we formulate several classes of Bayesian Markov models of musical scores that describe repetitions indirectly using the sparse transition probabilities of notes or note patterns. This enables us to construct piece-specific models for unseen scores with an unfixed repetitive structure and to derive tractable inference algorithms. Moreover, to describe approximate repetitions, we explicitly incorporate a process for modifying the repeated notes/note patterns. We apply these models as prior musical score models for rhythm transcription, where piece-specific score models are inferred from performed MIDI data by Bayesian learning, in contrast to the conventional supervised construction of score models. Evaluations using the vocal melodies of popular music showed that the Bayesian models improved the transcription accuracy for most of the tested model types, indicating the universal efficacy of the proposed approach. Moreover, we found an effective data representation for modelling rhythms that maximizes the transcription accuracy and computational efficiency.},
	author = {Eita Nakamura and Kazuyoshi Yoshii},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.04.100},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Music transcription, Musical language model, Bayesian modelling, Symbolic music processing, Musical rhythm},
	pages = {482-500},
	title = {Musical rhythm transcription based on Bayesian piece-specific score models capturing repetitions},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521004412},
	volume = {572},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521004412},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.04.100}}

@article{WANG2021136,
	abstract = {Cross-modal hashing has been intensively studied to efficiently retrieve multi-modal data across modalities. Supervised cross-modal hashing methods leverage the labels of training data to improve the retrieval performance. However, most of these methods still assume that the semantic labels of training data are ideally complete and noise-free. This assumption is too optimistic for real multi-modal data, whose label annotations are, in essence, error-prone. To achieve effective cross-modal hashing on multi-modal data with noisy labels, we introduce an end-to-end solution called Noise-robust Deep Cross-modal Hashing (NrDCMH). NrDCMH contains two main components: a noise instance detection module and a hash code learning module. In the noise detection module, NrDCMH firstly detects noisy training instance pairs based on the margin between the label similarity and feature similarity, and specifies weights to pairs using the margin. In the hash learning module, NrDCMH incorporates the weights into a likelihood loss function to reduce the impact of instances with noisy labels and to learn compatible deep features by applying different neural networks on multi-modality data in a unified end-to-end framework. Experimental results on multi-modal benchmark datasets demonstrate that NrDCMH performs significantly better than competitive methods with noisy label annotations. NrDCMH also achieves competitive results in `noise-free' scenarios.},
	author = {Runmin Wang and Guoxian Yu and Hong Zhang and Maozu Guo and Lizhen Cui and Xiangliang Zhang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.09.030},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-modal hashing, Noise labels, Deep learning, Feature similarity, Label similarity},
	pages = {136-154},
	title = {Noise-robust Deep Cross-Modal Hashing},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521009610},
	volume = {581},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521009610},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.09.030}}

@article{FENG202279,
	abstract = {As one of the prevalent topic mining methods, neural topic modeling has attracted a lot of interests due to the advantages of low training costs and strong generalisation abilities. However, the existing neural topic models may suffer from the feature sparsity problem when applied to short texts, due to the lack of context in each message. To alleviate this issue, we propose a Context Reinforced Neural Topic Model (CRNTM), whose characteristics can be summarized as follows. First, by assuming that each short text covers only a few salient topics, the proposed CRNTM infers the topic for each word in a narrow range. Second, our model exploits pre-trained word embeddings by treating topics as multivariate Gaussian distributions or Gaussian mixture distributions in the embedding space. Extensive experiments on two benchmark short corpora validate the effectiveness of the proposed model on both topic discovery and text classification.},
	author = {Jiachun Feng and Zusheng Zhang and Cheng Ding and Yanghui Rao and Haoran Xie and Fu Lee Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.05.098},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Neural topic model, Short texts, Context reinforcement},
	pages = {79-91},
	title = {Context reinforced neural topic modeling over short texts},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522005369},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522005369},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.05.098}}

@article{LIU2020227,
	abstract = {Question retrieval is an extremely important research field in Community Question Answering (CQA). Most existing question retrieval methods depend on semantic analysis of questions, whose effectiveness suffers from the short texts of the noise words in the question corpus. In order to recommend the questions with more advanced knowledge to users, the influence of the questions' popularity should be considered during retrieving questions. To make retrieved questions with both similar semantics and high popularity, we propose an Integrated Retrieval Framework for Similar Questions named Word-semantic Embedded Label Clustering -- LDA with Question Life Cycle (WELQLC-QR), consisting of Word-semantic Embedded Label Clustering -- LDA (WEL) and Question Life Cycle Optimization Similar Question List Strategy (QLC). Firstly, WEL is proposed for question retrieval from the perspective of semantic matching. It not only overcomes the problem of over-generalization of the semantic information extracted by topic models when facing short questions with multi-levels labels, but also avoids the influence of noise vocabularies during semantic extracting of the questions. Then, based on the internal factors (i.e., the number of comments and answers to the question) and external factors (i.e., programming language ranking information) of questions, QLC constructs a popularity-predicted model to optimize the similar question set searched by WEL, making the final retrieval results both semantically similar and popular. Finally, experiments are conducted on CQADupStack dataset, and results show that the MRR@N of WELQLC-QR model has an average increase of 8.99%, 8.3%, 4.74% and 3.56% compared with that of L-LDA, LC-LDA, BM25 and Word2vec, respectively.},
	author = {Yue Liu and Aihua Tang and Zhibin Sun and Weize Tang and Fei Cai and Chengjin Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.05.014},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {CQA, Question retrieval, Product life cycle, Semantic representation},
	pages = {227-245},
	title = {An integrated retrieval framework for similar questions: Word-semantic embedded label clustering -- LDA with question life cycle},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520304060},
	volume = {537},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520304060},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.05.014}}

@article{ETEMADI2022,
	abstract = {Finding a qualified individual who can independently answer a question on a community question answering platform is becoming more challenging due to the increasing multidisciplinary nature of posted questions. As such, finding a group of experts to collaboratively answer the questions is of paramount importance. To this end, we propose a novel approach to form teams of experts who can collectively answer new questions. The proposed approach, called team2box, learns neural embedding representations based on the content of the posted questions, experts' engagement with these questions, and past expert collaboration history in order to form a team to answer the posted question. It embeds experts and questions as points and existing teams as regions within the embedding space. Therefore, team2box forms a team whose members (1) collectively cover the knowledge required to answer a question, (2) have successful past experience in jointly answering similar questions, and (3) can work efficiently together to answer the question. Extensive experiments on real-life datasets from Stack Exchange show that team2box outperforms the state-of-the-art by discovering teams with on average 38.97% more covering the skills required to answer new questions and employing experts with collectively a high expertise level.},
	author = {Roohollah Etemadi and Morteza Zihayat and Kuan Feng and Jason Adelman and Ebrahim Bagheri},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.09.036},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Team Formation, Network Embedding, Learn to Ranking, Skill Coverage},
	title = {Embedding-based Team Formation for Community Question Answering},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522010829},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522010829},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.09.036}}

@article{LU202087,
	abstract = {Heaven Ape (HAPE) is an integrated big knowledge graph platform supporting the construction, management, and operation of large to massive scale knowledge graphs. Its current version described in this paper is a prototype, which consists of three parts: a big knowledge graph knowledge base, a knowledge graph browser on the client side, and a knowledge graph operating system on the server side. The platform is programmed in two high level scripting languages: JavaScript for programming the client side functions and Python for the server side functions. For making the programming more suitable for big knowledge processing and more friendly to knowledge programmers, we have developed two versions of knowledge scripting languages, namely K-script-c and K-script-s, for performing very high level knowledge programming of client resp. server side functions. HAPE borrows ideas from some well-known knowledge graph processing techniques and also invents some new ones as our creation. As an experiment, we transformed a major part of the DBpedia knowledge base and reconstructed it as a big knowledge graph. It works well in some application tests and provides acceptable efficiency.},
	author = {Ruqian LU and Chaoqun FEI and Chuanqing WANG and Shunfeng GAO and Han QIU and Songmao ZHANG and Cungen CAO},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.08.051},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Big knowledge, Big knowledge system, Big knowledge graph, Knowledge graph browser, Knowledge graph operating system, Knowledge scripting language, Big knowledge security},
	pages = {87-103},
	title = {HAPE: A programmable big knowledge graph platform},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551930800X},
	volume = {509},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002551930800X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.08.051}}

@article{KHODABAKHSH20221,
	abstract = {The focus of our work is the ad hoc table retrieval task, which aims to rank a list of structured tabular objects in response to a user query. Given the importance of this task, various methods have already been proposed in the literature that focus on syntactic, semantic and neural representations of tables for determining table relevance. However, recent works have highlighted queries that are consistently difficult for baseline methods to satisfy, referred to as hard queries. For this reason, the objectives of this paper include: (1) effectively satisfying hard queries by proposing three classes of qualitative measures, namely coherence, interpretability and exactness, (2) offering a systematic approach to interpolate these three classes of measures with each other and with baseline table retrieval methods, and (3) performing extensive experiments using a range of baseline retrieval methods to show the feasibility of the proposed measures for hard queries. We demonstrate that the consideration of the proposed qualitative measures will lead to improved performance for hard queries on a range of state-of-the-art ad hoc table retrieval baselines. We further show that our proposed measures are synergistic and will lead to even higher performance improvements over the baselines when interpolated with each other. The improvements measure up to 22.94% on the Semantic Table Retrieval (STR) method with an NDCG@20 of 0.5, which is superior to the performance of any state-of-the-art baseline for hard queries in the ad hoc table retrieval task.},
	author = {Maryam Khodabakhsh and Ebrahim Bagheri},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.05.080},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Qualitative measures, Ad hoc table retrieval, Deep contextual embeddings},
	pages = {1-26},
	title = {Qualitative measures for ad hoc table retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522005126},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522005126},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.05.080}}

@article{HUANG202018,
	abstract = {Co-clustering aims to explore coherent patterns by simultaneously clustering samples and features of data. Several co-clustering methods have been proposed in the past decades. However, in real-world applications, datasets are often with multiple modalities or composed of multiple representations (i.e., views), which provide different yet complementary information. Hence, it is essential to develop multi-view co-clustering models to solve the multi-view application problems. In this paper, a novel multi-view co-clustering method based on bipartite graphs is proposed. To make use of the duality between samples and features of multi-view data, a bipartite graph for each view is constructed such that the co-occurring structure of data can be extracted. The key point of utilizing the bipartite graphs to deal with the multi-view co-clustering task is to reasonably integrate these bipartite graphs and obtain an optimal consensus one. As for this point, the proposed method can learn an optimal weight for each bipartite graph automatically without introducing an additive parameter as previous methods do. Furthermore, an efficient algorithm is proposed to optimize this model with theoretically guaranteed convergence. Extensive experimental results on both toy data and several benchmark datasets have demonstrated the effectiveness of the proposed model.},
	author = {Shudong Huang and Zenglin Xu and Ivor W. Tsang and Zhao Kang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.09.079},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multi-view learning, Co-clustering, Bipartite graph learning, Auto-weighted strategy},
	pages = {18-30},
	title = {Auto-weighted multi-view co-clustering with bipartite graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309302},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519309302},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.09.079}}

@article{TERRAGNI2020581,
	abstract = {Relational topic models (RTM) have been widely used to discover hidden topics in a collection of networked documents. In this paper, we introduce the class of Constrained Relational Topic Models (CRTM), a semi-supervised extension of RTM that, apart from modeling the structure of the document network, explicitly models some available domain knowledge. We propose two instances of CRTM that incorporate prior knowledge in the form of document constraints. The models smooth the probability distribution of topics such that two constrained documents can either share the same topics or denote distinct themes. Experimental results on benchmark relational datasets show significant performances of CRTM on a semi-supervised document classification task.},
	author = {Silvia Terragni and Elisabetta Fersini and Enza Messina},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.09.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Constrained Relational Topic Models, Semi-supervised model, Latent Dirichlet Allocation, Domain knowledge},
	pages = {581-594},
	title = {Constrained Relational Topic Models},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519308850},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519308850},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.09.039}}

@article{JUNIOR2021116,
	abstract = {With the advent of online social networks, people became more eager to express and share their opinions and sentiment about all kinds of targets. The overwhelming amount of opinion texts soon attracted the interest of many entities (industry, e-commerce, celebrities, etc.) that were interested in analyzing the sentiment people express about what they produce or communicate. This interest has led to the surge of the sentiment analysis (SA) field. One of the most studied subfields of SA is polarity detection, which is the problem of classifying a text as positive, negative, or neutral. This classification problem is difficult to solve automatically, and many hand-adjusted resources are needed to overcome the difficulties in detecting sentiment from text. These resources include hand-adjusted textual features as well as lexicons. Deciding which resource and which combination of resources are more appropriate to a given scenario is a time-consuming trial-and-error process. Thus, in this work, we propose the use of Genetic Programming (GP) as a tool for automatically choosing, combining, and classifying sentiment from text. We propose a series of functions that allow GP to deal with preprocessing tasks, handcrafted features, and automatic weighting of lexicons for a given training set. Our experiments show that our GP solution is competitive and sometimes better than SVM and superior to na{\"\i}ve Bayes, logistic regression, and stochastic gradient descent, which are methods used in SA competitions.},
	author = {Airton Bordin Junior and N{\'a}dia F{\'e}lix F. {da Silva} and Thierson Couto Rosa and Celso G.C. Junior},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.01.025},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment analysis, Genetic programming, Lexicon, Classifiers},
	pages = {116-135},
	title = {Sentiment analysis with genetic programming},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000529},
	volume = {562},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000529},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.01.025}}

@article{ZHONG2021178,
	abstract = {As semi-supervised feature selection is becoming much more popular among researchers, many related methods have been proposed in recent years. However, many of these methods first compute a similarity matrix prior to feature selection, and the matrix is then fixed during the subsequent feature selection process. Clearly, the similarity matrix generated from the original dataset is susceptible to the noise features. In this paper, we propose a novel adaptive discriminant analysis for semi-supervised feature selection, namely, SADA. Instead of computing a similarity matrix first, SADA simultaneously learns an adaptive similarity matrix S and a projection matrix W with an iterative process. Moreover. we introduce the ℓ2,p norm to control the sparsity of S by adjusting p. Experimental results show that S will become sparser with the decrease of p. The experimental results for synthetic datasets and nine benchmark datasets demonstrate the superiority of SADA, in comparison with 6 semi-supervised feature selection methods.},
	author = {Weichan Zhong and Xiaojun Chen and Feiping Nie and Joshua Zhexue Huang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.02.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Feature selection, Semi-supervised feature selection, Discriminant analysis},
	pages = {178-194},
	title = {Adaptive discriminant analysis for semi-supervised feature selection},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521001778},
	volume = {566},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521001778},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.02.035}}

@article{LI20221023,
	abstract = {In the continental law system, it is appropriate for judges to find relevant laws and consider rules defined in them when dealing with legal cases. Therefore, recommending relevant laws quickly and accurately based on case content is crucial in improving the efficiency of case processing. There have been researched works of recommender systems in various fields, but few of them lucubrates systems that recommend statutes for cases. To the best of our knowledge, there is no research on recommending statutes by modeling the relationship between case content and law content with interpretable hand-crafted features. In this paper, we define five novel types of features for calculating relevance between a case and a statute for resorting all statutes retrieved through collaborative filtering for the input case. Both pair-wise and list-wise ranking models are trained based on all these features for re-ranking the statutes list. Besides, we also test the combinations of different learning algorithms and popular pre-trained language models. Experimental results show that adopting the proposed novel features in pair-wise ranking achieves the best performance. It improves the recommendation recall of the Top 1 statute by almost 5% compared with the collaborative filtering approach.},
	author = {Chuanyi Li and Jidong Ge and Kun Cheng and Bin Luo and Victor Chang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.06.042},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Statute recommender, Learning to rank, Text matching, Text relation modeling},
	pages = {1023-1040},
	title = {Statute recommendation: Re-ranking statutes by modeling case-statute relation with interpretable hand-crafted features},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522006363},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522006363},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.06.042}}

@article{LIANG202194,
	abstract = {Explainable recommendation systems (ERSs) have attracted increasing attention from researchers, which generate high-quality recommendations with intuitive explanations to help users make appropriate decisions. However, most of the existing ERSs are designed with an offline setting, which can hardly adjust their models using the online feedback instantly for improved performance. To overcome the limitations of ERSs with the offline setting, we propose a novel online setting for ERSs and devise an effective model called O3ERS in this online setting, which can perform online learning with good scalability and rigorous theoretical guides for better online recommendations and online explanations. O3ERS also addresses two challenging problems in real scenarios, namely, the sparsity and delay of online explanations' feedback as well as the partialness and insufficiency of online recommendations' feedback. Specifically, O3ERS not only instantly leverages the knowledge learned from the recommendations' feedback to adjust the sparse and delayed explanations' feedback for better explanations but also utilizes a novel exploitation--exploration strategy that incorporates the explanations' feedback to adjust the partial and insufficient recommendations' feedback for better recommendations. Our theoretical analysis and empirical studies on one simulated and two real-world datasets show that our model outperforms the state-of-the-art models in online scenarios remarkably.},
	author = {Qianqiao Liang and Xiaolin Zheng and Yan Wang and Mengying Zhu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.12.070},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Explainable recommendation systems, Online learning, Factorization bandit},
	pages = {94-115},
	title = {O3ERS: An explainable recommendation system with online learning, online recommendation, and online explanation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520312366},
	volume = {562},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520312366},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.12.070}}

@article{LIU2021129,
	abstract = {Facing the increasingly fierce competition, app developers have to update features of their products continually. In this process, developers need to not only consider users' demands but also pay attention to what other similar apps do so that they can stay one step ahead in the competition. App stores provide large-scale useable data for achieving this goal while how to use them efficiently becomes a new challenge for developers. In this paper, we aim at helping developers make feature updating strategies of their apps by analyzing data of similar products in App stores. Firstly, we identify similar apps by using texts in app descriptions and UI. Then, we gain and integrate the information of updated features in these apps from their release texts. Furthermore, we match reviews with the related updated features, which helps developers predict the payback if they adopt a similar updating strategy. To validate the proposed approach, we conducted a series of experiments based on Google Play. The results show that our approach can analyze the data reasonably and provide useful information for developers making feature updating strategy in the evolution of their own products.},
	author = {Huaxiao Liu and Yihui Wang and Yuzhou Liu and Shanquan Gao},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.08.050},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {App store mining, Feature extraction, Reviews analysis, App evolution},
	pages = {129-151},
	title = {Supporting features updating of apps by analyzing similar products in App stores},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521008562},
	volume = {580},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521008562},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.08.050}}

@article{HU2022239,
	abstract = {A method to enhance Web service clustering is proposed in this paper. Since current service clustering methods usually face low quality of service representation vectors and lack consideration of service collaboration, we try to provide an improved topic model to generate high-quality service representation vectors and design a service clustering method to integrate function similarity and collaboration similarity. First, by introducing feature word extraction and probability distribution correction into GSDMM, we present a model called TE-GSDMM (topic enhanced Gibbs sampling algorithm for the Dirichlet Multinomial Mixture model). Then, a service collaboration graph is put forward to model cooperation relationships and generate service collaboration vectors. Collaboration similarity is assessed by the similarity of service collaboration vectors. Finally, the K-means++ algorithm is employed to cluster Web services by evaluating service function similarity and collaboration similarity. Experiments show that TE-GSDMM outperforms other topic models in generating high-quality service representation vectors for service clustering. Moreover, service clustering performance is further improved by integrating collaboration similarity. Thus, the proposed method effectively enhances Web service clustering by improving the quality of service representation vectors and integrating service collaboration similarity.},
	author = {Qiang Hu and Jiaji Shen and Kun Wang and Junwei Du and Yuyue Du},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.11.087},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Web service, Service clustering, Topic model, Service collaboration graph, GSDMM},
	pages = {239-260},
	title = {A Web service clustering method based on topic enhanced Gibbs sampling algorithm for the Dirichlet Multinomial Mixture model and service collaboration graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521012159},
	volume = {586},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521012159},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.11.087}}

@article{HAN2020177,
	abstract = {As high-resolution remote-sensing (HRRS) images have become increasingly widely available, scene classification focusing on the smart classification of land cover and land use has also attracted more attention. However, mainstream methods encounter a severe problem in that many annotation samples are required to obtain an ideal model for scene classification. In the remote sensing community, there is no dataset with a comparative scale to ImageNet (which contains over 14 million images) to meet the sample requirements of the convolutional neural network (CNN)-based methods. In addition, labeling new images is both labor intensive and time consuming. To address these problems, we present a new generative adversarial network (GAN)-based remote-sensing image generation method (GAN-RSIGM) that can be applied to create high-resolution annotated samples for scene classification. In GAN-RSIGM, the Wasserstein distance is used to measure the difference between the generator distribution and the real data distribution. This addresses the problem of the gradient disappearing during sample generation, and distinctly promotes a generator distribution close to the real data distribution. An auxiliary classifier is added to the discriminator, guiding the generator to produce consistent and distinct images. With regard to the network structure, the discriminator and the generator are implemented by stacking residual blocks, which further stabilize the training process of the GAN-RSIGM. Extensive experiments were conducted to evaluate the proposed method with two public HRRS datasets. The experimental results demonstrated that the proposed method could achieve satisfactory performance for high-quality annotation sample generation, scene classification, and data augmentation.},
	author = {Wei Han and Lizhe Wang and Ruyi Feng and Lang Gao and Xiaodao Chen and Ze Deng and Jia Chen and Peng Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.06.018},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Generative adversarial network, Scene classification, High-resolution remote sensing, Sample generation},
	pages = {177-194},
	title = {Sample generation based on a supervised Wasserstein Generative Adversarial Network for high-resolution remote-sensing scene classification},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552030606X},
	volume = {539},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552030606X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.06.018}}

@article{FROLOV2020595,
	abstract = {This paper proposes a novel method, referred to as ParGenFS, for finding a most specific generalization of a query set represented by a fuzzy set of topics assigned to leaves of the rooted tree of a taxonomy. The query set is generalized by ``lifting'' it to one or more ``head subjects'' in the higher ranks of the taxonomy. The head subjects should cover the query set, with the possible addition of some ``gaps'', taxonomy nodes covered by the head subject but irrelevant to the query set. To decrease the numbers of gaps, we admit some ``offshoots'', nodes belonging to the query set but not covered by a head subject. The method globally minimizes the total number of head subjects, gaps and offshoots, each suitably weighted. Our algorithm is applied to the structural analysis and description of a collection of 17,685 abstracts of research papers published in 17 Springer journals related to Data Science for the 20-year period 1998--2017. Our taxonomy of Data Science (TDS) is extracted from the Association for Computing Machinery Computing Classification System 2012 (ACM-CCS), a six-level hierarchical taxonomy manually developed by a team of ACM experts. The TDS also includes a number of additional leaves that we added to cater for recent developments not represented in the ACM-CCS taxonomy. We find fuzzy clusters of leaf topics over the text collection, using specially developed machinery. Three of the clusters are indeed thematic, relating to the Data Science sub-areas of (a) learning, (b) information retrieval, and (c) clustering. These three clusters are then lifted in the TDS using ParGenFS, which allows us to draw some conclusions about tendencies in developments in these areas.},
	author = {Dmitry Frolov and Susana Nascimento and Trevor Fenner and Boris Mirkin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.09.082},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical taxonomy, Parsimony, Generalization, Additive fuzzy cluster, Spectral clustering, Annotated suffix tree},
	pages = {595-615},
	title = {Parsimonious generalization of fuzzy thematic sets in taxonomies applied to the analysis of tendencies of research in data science},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309454},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519309454},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.09.082}}

@article{LI2022186,
	abstract = {Sentence representation approaches have been widely used and proven to be effective in many text modeling tasks and downstream applications. Many recent proposals are available on learning sentence representations based on deep neural frameworks. However, these methods are pre-trained in open domains and depend on the availability of large-scale data for model fitting. As a result, they may fail in some special scenarios, where data are sparse and embedding interpretations are required, such as legal, medical, or technical fields. In this paper, we present an unsupervised learning method to exploit representations of sentences for some closed domains via topic modeling. We reformulate the inference process of the sentences with the corresponding contextual sentences and the associated words, and propose an effective context-enhanced process called the bi-Directional Context-enhanced Sentence Representation Learning (bi-DCSR). This method takes advantage of the semantic distributions of the nearby contextual sentences and the associated words to form a context-enhanced sentence representation. To support the bi-DCSR, we develop a novel Bayesian topic model to embed sentences and words into the same latent interpretable topic space called the Hybrid Priors Topic Model (HPTM). Based on the defined topic space by the HPTM, the bi-DCSR method learns the embedding of a sentence by the two-directional contextual sentences and the words in it, which allows us to efficiently learn high-quality sentence representations in such closed domains. In addition to an open-domain dataset from Wikipedia, our method is validated using three closed-domain datasets from legal cases, electronic medical records, and technical reports. Our experiments indicate that the HPTM significantly outperforms on language modeling and topic coherence, compared with the existing topic models. Meanwhile, the bi-DCSR method does not only outperform the state-of-the-art unsupervised learning methods on closed domain sentence classification tasks, but also yields competitive performance compared to these established approaches on the open domain. Additionally, the visualizations of the semantics of sentences and words demonstrate the interpretable capacity of our model.},
	author = {Shuangyin Li and Weiwei Chen and Yu Zhang and Gansen Zhao and Rong Pan and Zhenhua Huang and Yong Tang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.05.113},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentence representations learning, Closed domains, Bayesian sentence embedding, Bi-directional context-enhanced, Semantic interpretability, Topic modeling},
	pages = {186-210},
	title = {A context-enhanced sentence representation learning method for close domains with topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522005540},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522005540},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.05.113}}

@article{HUYNHTHE2020112,
	abstract = {Recently, skeleton-based human action recognition has received more interest from industrial and research communities for many practical applications thanks to the popularity of depth sensors. A large number of conventional approaches, which have exploited handcrafted features with traditional classifiers, cannot learn high-level spatiotemporal features to precisely recognize complex human actions. In this paper, we introduce a novel encoding technique, namely Pose-Transition Feature to Image (PoT2I), to transform skeleton information to image-based representation for deep convolutional neural networks (CNNs). The spatial joint correlations and temporal pose dynamics of an action are exhaustively depicted by an encoded color image. For learning action models, we fine-tune end-to-end a pre-trained network to thoroughly capture multiple high-level features at multi-scale action representation. The proposed method is benchmarked on several challenging 3D action recognition datasets (e.g., UTKinect-Action3D, SBU-Kinect Interaction, and NTU RGB+D) with different parameter configurations for performance analysis. Outstanding experimental results with the highest accuracy of 90.33% on the most challenging NTU RGB+D dataset demonstrate that our action recognition method with PoT2I outperforms state-of-the-art approaches.},
	author = {Thien Huynh-The and Cam-Hao Hua and Trung-Thanh Ngo and Dong-Seong Kim},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.10.047},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Pose-transition feature to image (PoT2I) encoding technique, Depth camera, Human action recognition, Deep convolutional neural networks},
	pages = {112-126},
	title = {Image representation of pose-transition feature for 3D skeleton-based action recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519310151},
	volume = {513},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519310151},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.10.047}}

@article{SONG2020138,
	abstract = {Anomaly detection is an important application field of evolutionary algorithm. Unlike traditionly anomaly detection, group anomaly detection aims to discover the anomalous aggregate behaviors in data points. Over past decades, a large number of promising methods have been successfully applied for group anomaly detection. However, they inherently neglect the correlations among groups in data points, limiting their abilities. This paper presents a correlated hierarchical generative model, which can model the intricate correlations hidden in groups by introducing a logistic normal distribution to capture the correlations among groups. With the proposed model, we construct a full variational Bayesian framework, which can data-adaptively optimize the model parameters of the proposed model. The model is designed and trained using Genetic Algorithm (GA), which helps automating the use of generative model. Further, a new score function is proposed as an anomaly criterion to estimate final anomaly groups in data points. Several experiments on synthetic data and real astronomical star data from Sloan Digital Sky Survey demonstrate the effectiveness of proposed method compared with the-state-of-art methods, in terms of average accurac (AP) and area under the Receiver Operating Characteristic(ROC) curve(AUC).},
	author = {Wanjuan Song and Wenyong Dong and Lanlan Kang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.03.110},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Group correlation, Genetic algorithm, Anomaly group detection, Logistic normal distribution, Variational inference},
	pages = {138-149},
	title = {Group anomaly detection based on Bayesian framework with genetic algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520302905},
	volume = {533},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520302905},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.03.110}}

@article{ZHOU2022809,
	abstract = {Cross-lingual information retrieval (CLIR) methods have quickly made the transition from translation-based approaches to semantic-based approaches. In this paper, we examine the limitations of current unsupervised neural CLIR methods, especially those leveraging aligned cross-lingual word embedding (CLWE) spaces. At the moment, CLWEs are normally constructed on the monolingual corpus of bilingual texts through an iterative induction process. Homonymy and polysemy have become major obstacles in this process. On the other hand, contextual text representation methods often fail to outperform static CLWE methods significantly for CLIR. We propose a method utilizing a novel neural generative model with Wasserstein autoencoders to learn neural topic-enhanced CLWEs for CLIR purposes. Our method requires minimal or no supervision at all. On the CLEF test collections, we perform a comparative evaluation of the state-of-the-art semantic CLWE methods along with our proposed method for neural CLIR tasks. We demonstrate that our method outperforms the existing CLWE methods and multilingual contextual text encoders. We also show that our proposed method obtains significant improvements over the CLWE methods based upon representative topical embeddings.},
	author = {Dong Zhou and Wei Qu and Lin Li and Mingdong Tang and Aimin Yang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.06.081},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-Lingual Information Retrieval, Cross-lingual Word Embeddings, Neural Generative Models, Word Embedding Models},
	pages = {809-824},
	title = {Neural topic-enhanced cross-lingual word embeddings for CLIR},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522006752},
	volume = {608},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522006752},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.06.081}}

@article{BAEK2022235,
	abstract = {Graph mining has been widely studied to analyze real-world graph properties and applied to various applications. In particular, graph subspace clustering performance, defined as partitioning high-dimensional graph data into several clusters by finding minimum weights for the edges, has been consistently improved by exploiting deep learning algorithms with Euclidean features extracted from Euclidean domains (image datasets). Most subspace clustering algorithms tend to extract features from the Euclidean domain to identify graph characteristics and structures, and hence are limited for real-world data applications in non-Euclidean domains. This paper proposes a self-supervised deep geometric subspace clustering algorithm optimized for non-Euclidean high-dimensional graph data by emphasizing spatial features and geometric structures while simultaneously reducing redundant nodes and edges. Quantitative and qualitative experimental results verified the proposed approach is effective for graph clustering compared with previous state-of-the-art algorithms on public datasets.},
	author = {Sangwon Baek and Gangjoon Yoon and Jinjoo Song and Sang Min Yoon},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.006},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Subspace clustering, Graph mining, Deep geometric learning},
	pages = {235-245},
	title = {Self-supervised deep geometric subspace clustering network},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552200888X},
	volume = {610},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552200888X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.006}}

@article{ANNAMORADNEJAD2022144,
	abstract = {With thousands of new questions posted every day on popular Q&A websites, there is a need for automated and accurate software solutions to replace manual moderation. In this paper, we address the critical drawbacks of crowdsourcing moderation actions in Q&A communities and demonstrate the ability to automate moderation using the latest machine learning models. From a technical point, we propose a multi-view approach that generates three distinct feature groups that examine a question from three different perspectives: 1) question-related features extracted using a BERT-based regression model; 2) context-related features extracted using a named-entity-recognition model; and 3) general lexical features derived using statistical and analytical methods. As a last step, we train a gradient boosting classifier to predict a moderation action. For evaluation purposes, we created a new dataset consisting of 60,000 Stack Overflow questions classified into three choices of moderation actions. Based on cross-validation on the novel dataset, our approach reaches 95.6% accuracy as a multiclass task and outperforms all state-of-the-art and previously-published models. Our results clearly demonstrate the high influence of our feature generation components on the overall success of the classifier.},
	author = {Issa Annamoradnejad and Jafar Habibi and Mohammadamin Fazli},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.03.085},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Automatic moderation, User-generated content, Community question answering, Multi-view learning, Decision support system, Stack overflow},
	pages = {144-154},
	title = {Multi-view approach to suggest moderation actions in community question answering sites},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522003127},
	volume = {600},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522003127},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.03.085}}

@article{GAO2022170,
	abstract = {Food recommendation has attracted increasing attentions to various food-related applications and services. The food recommender models aim to match users' preferences with recipes, where the key lies in the representation learning of users and recipes. However, ranging from early content-based filtering and collaborative filtering methods to recent hybrid methods, the existing work overlooks the various food-related relations, especially the ingredient-ingredient relations, leading to incomprehensive representations. To bridge this gap, we propose a novel model Food recommendation with Graph Convolutional Network (FGCN), which exploits ingredient-ingredient, ingredient-recipe, and recipe-user relations deeply. FGCN employs the information propagation mechanism and adopts multiple embedding propagation layers to model high-order connectivity across different food-related relations and enhance the representations. Specifically, we develop three types of information propagation: (1) ingredient-ingredient information propagation, (2) ingredient-recipe information propagation, and (3) recipe-user information propagation. To validate the effectiveness and rationality of FGCN, we conduct extensive experiments on a real-world dataset. The results show that the proposed FGCN outperforms the state-of-the-art baselines. Further in-depth analyses reveal that FGCN could alleviate the sparsity issue in food recommendation.},
	author = {Xiaoyan Gao and Fuli Feng and Heyan Huang and Xian-Ling Mao and Tian Lan and Zewen Chi},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.10.040},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Food recommendation, Food-related relations, Graph convolutional network, Information propagation, High-order connectivity},
	pages = {170-183},
	title = {Food recommendation with graph convolutional network},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521010549},
	volume = {584},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521010549},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.10.040}}

@article{YANG202241,
	abstract = {Traditional topic classification usually adopts the closed-world assumption that all the test topics have been seen in training. However, in open dynamic environments, the potential new topics may appear in testing due to the evolution of text data over time. Considering the uncertainty and multi-granularity of dynamic text data, such open topic classification needs to detect unseen topics by mining the boundary region continually, and incrementally update the previous models by knowledge accumulation. To address these challenge issues, this paper introduces a unified framework of three-way multi-granularity learning to open topic classification based on the fusion of three-way decision and granular computing. First, we propose the multilevel granular structure of tasks from the temporal-spatial multi-granularity perspective. Then, we construct an adaptive decision boundary and use the centroids and the corresponding radius to discover unknowns by the reject option. Subsequently, we further explore the unknown topics by three-way enhanced clustering and the uncertain instances will be re-investigated in the next stage. Besides, we design a built-in knowledge base represented as the centroid of each topic to store the topic knowledge. Finally, the experiments are conducted to compare the performances of proposed models and the efficiency of knowledge accumulation with classic models.},
	author = {Xin Yang and Yujie Li and Dan Meng and Yuxuan Yang and Dun Liu and Tianrui Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.11.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Three-way decision, Multi-granularity learning, Open topic, Uncertainty, Knowledge accumulation},
	pages = {41-57},
	title = {Three-way multi-granularity learning towards open topic classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521011555},
	volume = {585},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521011555},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.11.035}}

@article{XIAO20211,
	abstract = {Link prediction is one of the core problems in social network analysis. Considering the complexity of features in social networks, we propose a link prediction method based on feature representation and fusion. Firstly, based on the sparseness and high-dimensionality of network structure, network embedding is applied to represent the network structure as low-dimensional vectors, which identifies the spatial relationships and discovers the relevance among users. Second, owing to the diversity and complexity of text semantics, the user text is converted into vectors by word embedding models. As user behaviors can reflect the dynamic change of links, a time decay function is introduced to process the text vector to quantify the impact of user text on link establishment. Meanwhile, to simplify the complexity, we choose the top-k relevant users for each user. Finally, due to the attention mechanism can improve the expression of user's interests in text information, a link prediction method with attention-based convolutional neural network is proposed. By fusing and mining structural and text features, the purpose of synthetically predict link is finally achieved. Experimental results show that the proposed model can effectively improve the performance of link prediction.},
	author = {Yunpeng Xiao and Rui Li and Xingyu Lu and Yanbing Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.09.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Link prediction, Social networks, Network embedding, Convolutional neural network, Feature fusion},
	pages = {1-17},
	title = {Link prediction based on feature representation and fusion},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520309452},
	volume = {548},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520309452},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.09.039}}

@article{LIU2022395,
	abstract = {Expert finding is an important research field in community question answering (CQA). Traditional expert finding methods mainly exploit topic analysis and authority calculation methods to identify high-quality experts in certain fields. To avoid recommending questions to those experts who do not display the willingness or ability to provide high-quality answers, user interest drift and user quality should be considered. This study proposes a novel method named high-quality domain expert finding in CQA based on multi-granularity semantic analysis and interest drift (HQExpert). Firstly, HQExpert considers different semantic granularities by employing two models, a coarse-grained topic model LC-LDA and a fine-grained model (BERT), to capture the domain information of questions and users more accurately. Secondly, to address the diverse interests of the users, a user interest drift model in HQExpert is developed to dynamically represent the changes in the interests of the users at different periods. In addition, a user quality model is developed to further optimize the professional level of the user, finding experts who can provide high-quality answers and are interested in the current question. Finally, extensive experiments on two datasets from different domains demonstrate that the proposed HQExpert model can significantly improve the accuracy of finding high-quality experts.},
	author = {Yue Liu and Weize Tang and Zitu Liu and Lin Ding and Aihua Tang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.02.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Community question answering, Domain expert finding, Semantic analysis, Interest drift, User quality},
	pages = {395-413},
	title = {High-quality domain expert finding method in CQA based on multi-granularity semantic analysis and interest drift},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522001748},
	volume = {596},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522001748},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.02.039}}

@article{WAN2020243,
	abstract = {The Latent Dirichlet Allocation (LDA) model, which is a document-level probabilistic model, has been widely used in topic modeling. However, an essential issue of the LDA is its shortage in identifying co-occurrence relationships (e.g., aspect-aspect, aspect-opinion, etc.) in sentences. To address the problem, we propose an association constrained LDA (AC-LDA) for effectively capturing the co-occurrence relationships. Specifically, based on the basic features of the syntactic structure in product reviews, we formalize three major types of word association combinations and then carefully design corresponding identifications. For reducing the influence of global aspect words on the local distribution, we apply an important constraint on global aspects. Finally, the constraint and related association combinations are merged into the LDA to guide the topic-words allocation in the learning process. Based on the experiments on real-world product review data, we demonstrate that our model can effectively capture the relationships hidden in local sentences and further increase the extraction rate of fine-grained aspects and opinion words. Our results confirm the superiority of the AC-LDA over the state-of-the-art methods in terms of the extraction accuracy. We also verify the strength of our method in identifying irregularly appeared terms, such as non-aspect opinions, low-frequency words, and secondary aspects.},
	author = {Changxuan Wan and Yun Peng and Keli Xiao and Xiping Liu and Tengjiao Jiang and Dexi Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.036},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Aspect words, Association constraint, LDA model, Opinion words},
	pages = {243-259},
	title = {An association-constrained LDA model for joint extraction of product aspects and opinions},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300360},
	volume = {519},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300360},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.036}}

@article{BALLESTER2022101224,
	abstract = {Approaches for estimating the similarity between individual publications are an area of long-standing interest in the scientometrics and informetrics communities. Traditional techniques have generally relied on references and other metadata, while text mining approaches based on title and abstract text have appeared more frequently in recent years. In principle, topic models have great potential in this domain. But, in practice, they are often difficult to employ successfully, and are notoriously inconsistent as latent space dimension grows. In this manuscript we identify the three properties all usable topic models should have: robustness, descriptive power and reflection of reality. We develop a novel method for evaluating the robustness of topic models and suggest a metric to assess and benchmark descriptive power as number of topics scale. Employing that procedure, we find that the neural-network-based paragraph embedding approach seems capable of providing statistically robust estimates of the document--document similarities, even for topic spaces far larger than what is usually considered prudent for the most common topic model approaches.},
	author = {Omar Ballester and Orion Penner},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101224},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Scientometrics, Topic modelling, Stability, Robustness, Similarity, Informetrics},
	number = {1},
	pages = {101224},
	title = {Robustness, replicability and scalability in topic modelling},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772100095X},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772100095X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101224}}

@article{XIE2021101201,
	abstract = {Scholar performance assessment plays an important role in reward evaluation, funding allocation, and promotion and recruitment decisions. However, raw publication counts and raw citation count-based scholar performance assessment indicators, such as H-index or author citations, have shortcomings; for example, they ignore the impact of different citation patterns under different research topics, leading to authorship credit inflation due to full citation allocation to each author in multi-author publications. This study proposes a new scholar performance assessment indicator called the normalized scholar academic productivity (NSAP) indicator, which overcomes the issues posed by raw citation counts and publication counts-related scholar performance indicators by considering diverse aspects of scholar research achievements. The NSAP indicator considers the research topic, author sequence and author role in the author list, field-normalized journal impact when allocating citation counts to scholars, and published time. The research topic is generated by the co-keyword embedding and semantic relatedness of each keyword in order to make NSAP topic-dependent; the author sequence and role affect authorship credit allocation strategy; and field-normalized journal impact was used to assign different weights on raw publication counts and citation counts. Finally, awardees of the Derek de Solla Price Memorial Medal and the Association for Information Science and Technology's awards were used to evaluate the validity of NSAP for calculating scholar performance assessment. Results reveal outstanding topic-related scholar performance assessment properties compared to raw citation count indicators, such as H-index, author citations, and cited-by counts (i.e., total number of citing authors).},
	author = {Qing Xie and Xinyuan Zhang and Min Song},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101201},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Scholar performance assessment, Topic-dependent indicator, Authorship credit allocation, Field-normalized journal impact, Derek de Solla price memorial medal, The association for information science and technology's awards},
	number = {4},
	pages = {101201},
	title = {A network embedding-based scholar assessment indicator considering four facets: Research topic, author credit allocation, field-normalized journal impact, and published time},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000729},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000729},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101201}}

@article{ZHOU2021101162,
	abstract = {The global spread of COVID-19 has caused pandemics to be widely discussed. This is evident in the large number of scientific articles and the amount of user-generated content on social media. This paper aims to compare academic communication and social communication about the pandemic from the perspective of communication preference differences. It aims to provide information for the ongoing research on global pandemics, thereby eliminating knowledge barriers and information inequalities between the academic and the social communities. First, we collected the full text and the metadata of pandemic-related articles and Twitter data mentioning the articles. Second, we extracted and analyzed the topics and sentiment tendencies of the articles and related tweets. Finally, we conducted pandemic-related differential analysis on the academic community and the social community. We mined the resulting data to generate pandemic communication preferences (e.g., information needs, attitude tendencies) of researchers and the public, respectively. The research results from 50,338 articles and 927,266 corresponding tweets mentioning the articles revealed communication differences about global pandemics between the academic and the social communities regarding the consistency of research recognition and the preferences for particular research topics. The analysis of large-scale pandemic-related tweets also confirmed the communication preference differences between the two communities.},
	author = {Qingqing Zhou and Chengzhi Zhang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101162},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {COVID-19, Global pandemic, Academic communication, Social communication, Topic mining, Sentiment analysis},
	number = {3},
	pages = {101162},
	title = {Breaking community boundary: Comparing academic and social communication preferences regarding global pandemics},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772100033X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772100033X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101162}}

@article{ALSUDAIS2021101139,
	abstract = {While several aspects of open research software libraries have been studied, their in-code citation practices remain an unexplored area. In-code citations are citations of published scientific contributions added in the programming source code of research software libraries. In this paper, 73 such libraries are analyzed to determine the availability and consistency of in-code citations and reference sections. Findings indicate that 54 (73.9 %) of these libraries have at least one in-code citation. However, 47 had at least one incomplete citation and 89.3 % of libraries with several citations used multiple formats for citations. For reference sections, 36 of the libraries investigated in this study had at least one section. Still, inconsistencies in formats were also present as 83.3 % of the libraries with two or more sections used multiple formats for the sections, which may prevent automated programmers from indexing and collecting the list of references. Most importantly, this study investigates the availability of a systematic method that allows for the linking of references and in-code citations. Findings indicate that only six of the libraries had such a method, although many did not fully implement the method.},
	author = {Abdulkareem Alsudais},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101139},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Research software, In-code citation, Software citation, Open software, Citation networks},
	number = {2},
	pages = {101139},
	title = {In-code citation practices in open research software libraries},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000109},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000109},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101139}}

@article{HAJIBABAEI2022101275,
	abstract = {Gender disparity in science is one of the most focused debating points among authorities and the scientific community. Over the last few decades, numerous initiatives have endeavored to accelerate gender equity in academia and research society. However, despite the ongoing efforts, gaps persist across the world, and more measures need to be taken. Using social network analysis, natural language processing, and machine learning, in this study, we comprehensively analyzed gender-specific patterns in the highly interdisciplinary and evolving field of artificial intelligence for the period of 2000--2019. Our findings suggest an overall increasing rate of mixed-gender collaborations. From the observed gender-specific collaborative patterns, the existence of disciplinary homophily at both dyadic and team levels is confirmed. However, a higher preference was observed for female researchers to form homophilous collaborative links. Our core-periphery analysis indicated a significant positive association between having diverse collaboration and scientific performance and experience. We found evidence in support of expecting the rise of new female superstar researchers in the artificial intelligence field.},
	author = {Anahita Hajibabaei and Andrea Schiffauerova and Ashkan Ebadi},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101275},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Gender disparity, Interdisciplinary research, Artificial intelligence, Research performance, Collaboration},
	number = {2},
	pages = {101275},
	title = {Gender-specific patterns in the artificial intelligence scientific ecosystem},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772200027X},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772200027X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101275}}

@article{XU2020101014,
	abstract = {In the modern world, science and technology jointly determine the evolutionary path of scientific innovation, with an increasingly close relationship between them. Therefore, it is important to study the identification method of the innovation path, based on the linkage of topics in science and technology. This study focuses on connected topics utilizing bibliometric analysis, thereby exploring the identification method for innovation paths based on the linkage of scientific and technological topics. The internal mechanism of knowledge dissemination and the relationship between science and technology are revealed and described in detail by measuring the linkage of knowledge units. For practical bibliometric analyses, research papers and patent literature were used to characterize scientific research and technological research to reveal the innovation path for the interaction of science and technology quantitatively, automatically, and visually. Experimental study shows that analysis of the topic-linked path of science and technology, along with the integration of multi-relationships, can effectively identify important science- and technology-related topics in a field in the evolution process, and help grasp the key points of basic research and applied research.},
	author = {Haiyun Xu and Jos Winnink and Zenghui Yue and Ziqiang Liu and Guoting Yuan},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101014},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Evolution path, Scientific innovation, Science-technology linkage, Knowledge dissemination},
	number = {2},
	pages = {101014},
	title = {Topic-linked innovation paths in science and technology},
	url = {https://www.sciencedirect.com/science/article/pii/S175115771930210X},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115771930210X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101014}}

@article{ZHANG2020101032,
	abstract = {Compared with journal articles, books can provide broader, deeper and more comprehensive information, and often have higher expertise and academic depth. However, most researches on book assessment focus on measuring academic value of books (e.g. citations analysis) or identifying attitudes of readers (e.g. book review mining), depth and breadth reflected by book contents is neglected. Therefore, in this paper, we measure books' depth and breadth by mining books' tables of contents, so as to enrich resources and methods for book assessment research, help users understand book contents quickly and improve efficiency of book selection. Specifically, we measured books' depth and breadth based on books' tables of contents via two levels: topic level and feature level. Firstly, we obtained topic-level metrics by identifying topics expressed in tables of contents and calculating topic distributions. Then, we got feature-level results via feature extraction and feature distribution calculation. Finally, we compared depth and breadth metrics and other book assessment metrics. Experimental results reveal that, books' depth and breadth at two levels are different, and substantial differences between disciplines and book types are obvious. In addition, books' depth and breadth can provide alternative and supplementary information for assessing multi-dimensional values of books.},
	author = {Chengzhi Zhang and Qingqing Zhou},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101032},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Book impact assessment, Depth and breadth analysis, Topic extraction, Feature extraction},
	number = {2},
	pages = {101032},
	title = {Assessing books' depth and breadth via multi-level mining on tables of contents},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719302238},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719302238},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101032}}

@article{LEE2021101126,
	abstract = {This study aims at representing research patterns of bibliographic entities (e.g., scholars, papers, and venues) with a fixed-length vector. Bibliographic network structures rooted in the entities are incredibly diverse, and this diversity increases in the outstanding entities. Thus, despite their significant volume, the outstanding entities obtain minimal learning opportunities, whereas low-performance entities are over-represented. This study solves the problem by representing the patterns of the entities rather than depicting individual entities in a precise manner. First, we describe structures rooted in the entities using the Weisfeiler--Lehman (WL) relabeling process. Each subgraph generated by the relabeling process provides information on the scholars, kinds of papers they published, standards of venues in which the papers were published, and types of their collaborators. We assume that a subgraph depicts the research patterns of bibliographic entities, such as the preference of a scholar in choosing either a few highly impactful papers or numerous papers of moderate impact. Then, we simplify the subgraphs according to multiple levels of detailedness. Original subgraphs represent the individuality of the entities, and simplified subgraphs represent the entities sharing the same research patterns. In addition, simplified subgraphs balance the learning opportunities of high- and low-performance entities by co-occurring with both types of entities. We embed the subgraphs using the Skip-Gram method. If the results of the embedding represent the research patterns of the entities, the obtained vectors should be able to represent various aspects of the research performance in both the short-term and long-term durations regardless of the performances of the entities. Therefore, we conducted experiments for predicting 23 performance indicators during four time periods for four performance groups (top 1%, 5%, 10%, and all entities) using only the vector representations. The proposed model outperformed the existing network embedding methods in terms of both accuracy and variance.},
	author = {O-Joun Lee and Hyeon-Ju Jeon and Jason J. Jung},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101126},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Bibliographic network embedding, Skewed distribution, Multi-resolution representation learning, Level-wise simplification, Outstanding scholars},
	number = {1},
	pages = {101126},
	title = {Learning multi-resolution representations of research patterns in bibliographic networks},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772030643X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772030643X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101126}}

@article{AMON2022101284,
	abstract = {In competitive research environments, scholars have a natural interest to maximize the prestige associated with their scientific work. In order to identify factors that might help them address this goal more effectively, the scientometric literature has tried to link linguistic and meta characteristics of academic papers to the associated degree of scientific prestige, conceptualized as cumulative citation counts. In this paper, we take an alternative approach that instead understands scientific prestige in terms of the rankings of the journals that the articles appeared in, as such rankings are routinely used as surrogate research quality indicators. For the purpose of determining the most important drivers of suchlike prestige, we use state-of-the-art text mining tools to extract 344 interpretable features from a large corpus of over 200,000 journal articles in economics. We then estimate beta regression models to investigate the relationship between these predictors and a cross-sectionally standardized version of SCImago Journal Rank (SJR) in multiple topically homogeneous clusters. In so doing, we also reinvestigate the bafflegab theory, according to which more prestigious research papers tend to be less readable, in a methodologically novel way. Our results show the consistently most informative predictors to be associated with the length of the paper, the span of coreference chains in its full text, the deployment of a personal and moderately informal writing style, the ``density'' of the article in terms of sentences per page, international and institutional collaboration in research teams and the references cited in the paper. Moreover, we identify various linguistic intricacies that matter in the association between readability and scientific prestige, which suggest this relationship to be more complicated than previously assumed.},
	author = {Julian Amon and Kurt Hornik},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101284},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Research impact, SJR indicator, NLP, Readability, Gradient boosting, GLMLSS},
	number = {2},
	pages = {101284},
	title = {Is it all bafflegab? -- Linguistic and meta characteristics of research articles in prestigious economics journals},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000360},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000360},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101284}}

@article{REHS2021101166,
	abstract = {Author-level scientometric indicators are an important tool in individual and institutional-based research assessment and require high-quality author-publication profiles. To address this need, our study developed a robust supervised machine learning approach in combination with graph community detection methods to disambiguate author names in the Web of Science publication database. We used the unique author identifier Researcher ID to retrieve true authorship data of 1,904 scientists and trained a random forest and a logistic regression classifier on 1.2 million corresponding publication pairs with authors that share the same last name and first name initial. To do this, we reviewed a vast set of paper and author characteristics and randomly included missing data to make our machine learning robust to quality changes of new publication data. In the application on an unseen test set, we achieved F1 scores of 0.82 in the random forest and 0.75 in the logistic regression model. Subsequently, we evaluate feature performance and apply the infomap graph community detection algorithm to identify all publications belonging to an author. The community detection results in reasonable cluster metrics (Mean K-Metric in logistic regression-based model = 0.78 and = 0.81 in random forest-based model). Finally, we test our algorithm on a large surname-initial block (``Muller, M.'') and demonstrate speed and predictive performance.},
	author = {Andreas Rehs},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101166},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Author name disambiguation, Machine learning, Pairwise classification, Random forest, Community detection, Web of science},
	number = {3},
	pages = {101166},
	title = {A supervised machine learning approach to author disambiguation in the Web of Science},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000377},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000377},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101166}}

@article{DOGAN2020101076,
	abstract = {The effective representation of the relationship between the documents and their contents is crucial to increase classification performance of text documents in the text classification. Term weighting is a preprocess aiming to represent text documents better in Vector Space by assigning proper weights to terms. Since the calculation of the appropriate weight values directly affects performance of the text classification, in the literature, term weighting is still one of the important sub-research areas of text classification. In this study, we propose a novel term weighting (MONO) strategy which can use the non-occurrence information of terms more effectively than existing term weighting approaches in the literature. The proposed weighting strategy also performs intra-class document scaling to supply better representations of distinguishing capabilities of terms occurring in the different quantity of documents in the same quantity of class. Based on the MONO weighting strategy, two novel supervised term weighting schemes called TF-MONO and SRTF-MONO were proposed for text classification. The proposed schemes were tested with two different classifiers such as SVM and KNN on 3 different datasets named Reuters-21578, 20-Newsgroups, and WebKB. The classification performances of the proposed schemes were compared with 5 different existing term weighting schemes in the literature named TF-IDF, TF-IDF-ICF, TF-RF, TF-IDF-ICSDF, and TF-IGM. The results obtained from 7 different schemes show that SRTF-MONO generally outperformed other schemes for all three datasets. Moreover, TF-MONO has promised both Micro-F1 and Macro-F1 results compared to other five benchmark term weighting methods especially on the Reuters-21578 and 20-Newsgroups datasets.},
	author = {Turgut Dogan and Alper Kursat Uysal},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101076},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Text classification, Supervised term weighting, Max-occurrence, Non-occurrence},
	number = {4},
	pages = {101076},
	title = {A novel term weighting scheme for text classification: TF-MONO},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157720300705},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157720300705},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101076}}

@article{HUANG2021101145,
	abstract = {Policy documents have become increasingly valuable in the field of bibliometrics because they contain important information such as the intentions and behaviors of policymakers. Policy instruments are the central elements of policy documents; therefore, identifying core policy instruments can help researchers in the field better understand the important methodological measures taken by government organizations to achieve specific economic or social goals. However, existing identification methods often focus on the effectiveness of a policy instrument along one dimension (e.g., economic indicators), while ignoring the relationship between individual policy instruments. This paper attempts to fill this gap by designing a network-based framework incorporating structural holes theory to identify the core policy instruments implied in the policy documents. We first identify ``policy target-policy instrument'' patterns in relevant policy documents and then establish a ``policy target-policy instrument'' network that maps onto real-world policy systems. Finally, using structural holes theory, we identify core policy instruments and analyze the policy mix system upon this basis. We use China's nuclear energy policy as a case study to evaluate the proposed approach. Our proposed method is useful for quantitatively analyzing complex policy systems and for identifying core policy instruments and targets within them.},
	author = {Cui Huang and Chao Yang and Jun Su},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101145},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Bibliometrics, Policy documents, Structural holes, Policy instruments, ``Policy target-policy instrument'', Patterns},
	number = {2},
	pages = {101145},
	title = {Identifying core policy instruments based on structural holes: A case study of China's nuclear energy policy},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772100016X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772100016X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101145}}

@article{WANG2021101214,
	abstract = {Recent research has shifted to investigating knowledge integration in an interdisciplinary field and measuring the interdisciplinarity. Conventional citation analysis does not consider the context of citations, which limits the understanding of interdisciplinary knowledge integration. This study introduces a novel analytical framework to characterize interdisciplinary knowledge integration by both the content, i.e., integrated knowledge phrases (IKPs), and location of citances (i.e., citing sentences) in addition to citations. Seven knowledge categories are used to classify IKPs, including Research Subject, Theory, Research Methodology, Technology, Human Entity, Data, and Others. The eHealth field is explored as an exemplar interdisciplinary field in the case study. The result reveals that the ranks of source disciplines quantified by the integrated knowledge phrases are different from those by citations, especially in terms of average knowledge integration density. The distributions of the IKPs over the knowledge categories differ among source disciplines, indicating their different contributions to knowledge integration of eHealth field. The knowledge from adjacent disciplines is integrated into the field faster than that from other disciplines. Knowledge distributions over sections of articles are also different among source disciplines, and a correlation between knowledge categories and the sections they were used is observed. The analytical framework offers a way to better understand an interdisciplinary field by disclosing the characteristics of interdisciplinary knowledge integration from the perspective of knowledge content and usage.},
	author = {Shiyun Wang and Jin Mao and Kun Lu and Yujie Cao and Gang Li},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101214},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Knowledge flow, Citation age, Interdisciplinary research, Citation context, eHealth},
	number = {4},
	pages = {101214},
	title = {Understanding interdisciplinary knowledge integration through citance analysis: A case study on eHealth},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000857},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000857},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101214}}

@article{EBADI2020101018,
	abstract = {Research and development activities are regarded as one of the most influencing factors of the future of a country. Large investments in research can yield a tremendous outcome in terms of a country's overall wealth and strength. However, public financial resources of countries are often limited which calls for a wise and targeted investment. Scientific publications are considered as one of the main outputs of research investment. Although the general trend of scientific publications is increasing, a detailed analysis is required to monitor the research trends and assess whether they are in line with the top research priorities of the country. Such focused monitoring can shed light on scientific activities evolution as well as the formation of new research areas, thus helping governments to adjust priorities, if required. But monitoring the output of the funded research manually is not only very expensive and difficult, it is also subjective. Using structural topic models, in this paper we evaluated the trends in academic research performed by federally funded Canadian researchers during the time-frame of 2000--2018, covering more than 140,000 research publications. The proposed approach makes it possible to objectively and systematically monitor research projects, or any other set of documents related to research activities such as funding proposals, at large-scale. Our results confirm the accordance between the performed federally funded research projects and the top research priorities of Canada.},
	author = {Ashkan Ebadi and St{\'e}phane Tremblay and Cyril Goutte and Andrea Schiffauerova},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101018},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Text mining, Topic modeling, Machine learning, Funded research, Publications, Government research priorities, Canada},
	number = {2},
	pages = {101018},
	title = {Application of machine learning techniques to assess the trends and alignment of the funded research output},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157718301901},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157718301901},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101018}}

@article{CHUNG2021101103,
	abstract = {Scouting young and talented human resources with firm-specific domain knowledge has a great impact on performance and sustainable growth among technology-based firms. Previous studies have proposed key researcher identification and recommendation approaches, but few studies have focused on identifying prospective human resources---young and talented people suitable for a firm's technology strategy. Thus, this study proposes an inventor profile mining approach for identifying such human resources. The proposed approach is as follows: 1) collecting patent data related to a target firm and preprocessing candidate inventors' patents; 2) identifying the inventors' technology fields and measuring their invention performance and career; 3) generating performance-career portfolio maps for invention fields to identify prospective inventors aligned with the target firm's technology development directions. We show that this approach can identify prospective inventors through a case study and statistical validation. This approach is expected to be used as a human resources management tool by technology-based firms to help them identify and scout young and talented human resources the most suitable for technology strategies.},
	author = {Jaemin Chung and Namuk Ko and Hyeonsu Kim and Janghyeok Yoon},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101103},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Inventor profile mining, Human resource, Scouting, Technology-based firm, Patent analysis},
	number = {1},
	pages = {101103},
	title = {Inventor profile mining approach for prospective human resource scouting},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772030328X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772030328X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101103}}

@article{JUNG2022101320,
	abstract = {Topic emergence detection aids in pinpointing prominent topics within a given domain, providing practical insights into all interested parties on where to focus the limited resources. This paper employs the network-based topic evolution approach to overcome limitations in text-based topic evolution, providing prospective topic emergence prediction capabilities by representing emergent topics by their ancestors. A descendant-aware clustering algorithm is proposed to generate non-exhaustive and overlapping clusters, utilizing the pace of collaborations and structural similarities between topics with iterative edge removal and addition processes. Over 100 datasets specific to a research topic were extracted from the Microsoft Academic Graph dataset for the experiments, where the proposed algorithm consistently outperformed existing clustering algorithms in generating clusters with a higher likelihood of being ancestors to an emergent topic up to three years in the future. Regression-based cluster filtering using five structural cluster features and topic cluster qualities showed that the prediction performance can be enhanced by automatically classifying undesirable clusters from previously known data. The results showed that the proposed algorithm can enhance topic emergence predictions on a wide range of research domains regardless of their maturities, popularities, and magnitudes without having access to the data in the predicted year, paving a road to prospective predictions on emergent topics.},
	author = {Sukhwan Jung and Aviv Segev},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101320},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Topic evolution, Topic prediction, Clustering, Topic emergence prediction, Scientometrics},
	number = {3},
	pages = {101320},
	title = {DAC: Descendant-aware clustering algorithm for network-based topic emergence prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000724},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000724},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101320}}

@article{JUNG2020101040,
	abstract = {Topic modeling methods aim to extract semantic topics from unstructured documents, and topic evolution is one of its branches seeking to analyze how temporal topics in a set of documents evolve and has shown successful identification of content transitions within static topics over time; yet, the inherent limitations of topic modeling methods inhibit traditional topic evolution methods from highlighting topical correlations between different, dynamic topics. The authors propose an alternative topic modeling method conscious of the topical correlation in the academic domain by introducing the notion of the common interest authors (CIA11CIA: Common Interest Authors), defining a topic as a set of shared common research interests of a researcher group. Publication records related to the Human Computer Interaction field were extracted from the Microsoft Academic Graph dataset, with virtual reality as the target field of research. The result showed that the proposed alternative topic modeling is capable of successfully model coherent topics regardless of the topic size with only the meta-data of the document set, indicating that the alternative approach is not only capable of allowing topic correlation analysis during the topic evolution but also able to generate coherent topics at the same time.},
	author = {Sukhwan Jung and Wan Chul Yoon},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101040},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Topic modeling, Bibliographic network, Topic evolution, Scientometric},
	number = {3},
	pages = {101040},
	title = {An alternative topic model based on Common Interest Authors for topic evolution analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719303517},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719303517},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101040}}

@article{CHEN2022101281,
	abstract = {Main Path Analysis (MPA) is widely used to trace the developmental trajectory of a technological field through a citation network. The citation-based traversal weight is usually utilized to cherry-pick the most significant path. However, the theme of documents along a main path may not be so coherent, and it is very possible to miss the main paths of significant sub-fields overall in a domain. Furthermore, the global path search algorithm in conventional MPA also suffers from high space complexity due to the exhaustive strategy. To address these limitations, a new method, named as semantic MPA (sMPA), is proposed by leveraging semantic information in two steps of candidate path generation and main path selection. In the meanwhile, the resulting source code can be freely accessed. To demonstrate the advantages of our method, extensive experiments are conducted on a patent dataset pertaining to lithium-ion battery in electric vehicle. Experimental results show that our sMPA is capable of discovering more knowledge flows from important sub-fields, and improving the topical coherence of candidate paths as well.},
	author = {Liang Chen and Shuo Xu and Lijun Zhu and Jing Zhang and Haiyun Xu and Guancan Yang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101281},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Main path analysis, Developmental trajectory, Patent mining, Topic coherence, Lithium-ion battery},
	number = {2},
	pages = {101281},
	title = {A semantic main path analysis method to identify multiple developmental trajectories},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000335},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000335},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101281}}

@article{HUANG2022101317,
	abstract = {This research proposes a new approach that considers citation relevance in main path analysis (MPA). Traditional MPA assumes that all citations have equal weight, but in practice treating every citation equally may not find the main paths that truthfully reflect the knowledge flow in a target science field. To address the issue, this study suggests taking the level of relevance among documents into consideration. For demonstration purposes, the level of relevance is determined by similarity in both citation structure and key phrases among documents. The approach not only achieves convergence of development trajectories, but also helps frame the topics on the main paths to a specific concept from a wide range of research domains. This study takes health interoperability fields as the demonstration case to show the effects of converging the trajectories toward a target domain.},
	author = {Chen-Hao Huang and John S. Liu and Mei Hsiu-Ching Ho and Tzu-Chuan Chou},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101317},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Citation relevance, Literature review, Main path analysis, Health interoperability standards},
	number = {3},
	pages = {101317},
	title = {Towards more convergent main paths: A relevance-based approach},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000694},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000694},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101317}}

@article{KIM2022101242,
	abstract = {Main path analysis (MPA) is the most widely accepted approach to tracing knowledge transfer in a research field. In this study, we extracted multiple longest paths from the multidisciplinary academic field's citation network and integrating topic modeling to the extracted paths. We consider three main aspects of trajectory analysis when analyzing the represented documents through the extracted paths: emergence, authority, and topic dynamics. For path extraction, we adopt the longest path algorithm that consists of the following three steps: 1) topological sort, 2) edge relaxation, and 3) multiple path extraction. For topic integration into multiple paths, we employ latent Dirichlet allocation (LDA) by utilizing the topic-document matrix that LDA derives to select an article's topic from the citation network, where each article is labeled with the topic that is assigned with the highest topical probability for that article. We conduct a series of experiments to examine the results on a dataset from the field of healthcare informatics that PubMed provides.},
	author = {Erin H.J. Kim and Yoo Kyung Jeong and YongHwan Kim and Min Song},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101242},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Citation analysis, Healthcare informatics, Longest path, Main path analysis, Topic modeling},
	number = {1},
	pages = {101242},
	title = {Exploring scientific trajectories of a large-scale dataset using topic-integrated path extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721001139},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721001139},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101242}}

@article{KIM2022101255,
	abstract = {This study explores the topic-based interdisciplinarity in the research domain of literacy. A text corpus of keywords was generated through a deep keyword generation model from abstracts of 346,387 articles published in 296 disciplines from 1917 to 2021. Dirichlet-Multinomial Regression topic modeling, interdisciplinarity indices, and network analysis were employed to analyze the collected corpus. Topic modeling uncovered 15 dominant research topics in the literacy field, as well as their up-and-down trends from 2000 to 2021. For each topic, keywords were then replaced with disciplines, and interdisciplinarity was measured using four indices: variety, balance, disparity, and diversity. Finally, the interdisciplinarity of each topic, connectivity between topics, and topic trends were comprehensively analyzed on the keyword co-occurrence network. Our methodology reaches beyond connectivity limited to a few disciplines and provides insight into the direction of collaboration between disciplines centered on a research domain. Moreover, the study's deep keyword generation model has methodological implications for forming a corpus spanning numerous disciplines as a bottom-up approach.},
	author = {Hyeyoung Kim and Hyelin Park and Min Song},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101255},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Disciplinarity, Interdisciplinary cooperation, Topic diversity, Keyword generation, DMR topic modeling, Deep learning},
	number = {2},
	pages = {101255},
	title = {Developing a topic-driven method for interdisciplinarity analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000074},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000074},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101255}}

@article{QIAN2020101047,
	abstract = {Detecting what type of knowledge constitutes a discipline, tracking how the knowledge changes, and understanding why the changes are triggered are the key issues in analyzing scientific development from a macro perspective, which is usually analyzed by the topic of evolution. However, traditional methods assume that the disciplinary structure is flat with only one-layer topics, rather than a tree-like structure with hierarchical topics, which leads to the inability of existing methods to effectively examine the details of the evolution, such as the interactions between different research directions. In this paper, we take artificial intelligence (AI) as a case in which we study its hierarchical structural evolution, more precisely inspecting disciplinary development, by analyzing 65,887 AI-related research papers published during a 10-year period from 2009 to 2018. From a hierarchical topic model that can construct a topic-tree with multi-layer organizations, we design a visual analysis model for the topic-tree to systematically and visually investigate how knowledge transfers along the topic-tree and how the topic-tree changes over time. Moreover, some assistant indicators are employed to help in the exploration of the complicated structural evolution. Then, we discover the latent relationship between the sub-structures within a topic as well as the triggering reason for the knowledge migration. Based on these results, we conclude that different topics have different development patterns and that the recent artificial intelligence revolution stems from the interactions among the different topics.},
	author = {Yue Qian and Yu Liu and Quan Z. Sheng},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101047},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Topic evolution, Artificial intelligence, Hierarchical knowledge structure, Nonnegative matrix factorization, Evolutionary patterns, Visual analysis approach},
	number = {3},
	pages = {101047},
	title = {Understanding hierarchical structural evolution in a scientific discipline: A case study of artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719302925},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719302925},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101047}}

@article{JIAN2021108100,
	abstract = {As the Internet confronts the multimedia explosion, it becomes urgent to investigate personalized recommendation for alleviating information overload and improving users' experience. Most personalized recommendation approaches pay their attention to collaborative filtering over users' interactions, which suffers greatly from the highly sparse interactions. In image recommendation, visual correlations among images that users consumed provide a piece of intrinsic evidence to reveal users' interests. It inspires us to investigate image recommendation over the dense visual graph of images instead of the sparse user interaction graph. In this paper, we propose a semantic manifold modularization-based ranking (MMR) for image recommendation. MMR leverages the dense visual manifold to propagate users' historical records and infer user-image correlations for image recommendation. Especially, it constrains interest propagation within semantic visual compact groups by manifold modularization to make a tradeoff between users' personality and graph smoothness in propagation. Experimental results demonstrate that user-consumed visual correlations play actively to capture users' interests, and the proposed MMR can infer user-image correlations via visual manifold propagation for image recommendation.},
	author = {Meng Jian and Jingjing Guo and Chenlin Zhang and Ting Jia and Lifang Wu and Xun Yang and Lina Huo},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108100},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Manifold propagation, Modularization, Image recommendation, User interest},
	pages = {108100},
	title = {Semantic manifold modularization-based ranking for image recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321002879},
	volume = {120},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321002879},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108100}}

@article{TALAVERA2020107330,
	abstract = {Developing tools to understand and visualize lifestyle is of high interest when addressing the improvement of habits and well-being of people. Routine, defined as the usual things that a person does daily, helps describe the individuals' lifestyle. With this paper, we are the first ones to address the development of novel tools for automatic discovery of routine days of an individual from his/her egocentric images. In the proposed model, sequences of images are firstly characterized by semantic labels detected by pre-trained CNNs. Then, these features are organized in temporal-semantic documents to later be embedded into a topic models space. Finally, Dynamic-Time-Warping and Spectral-Clustering methods are used for final day routine/non-routine discrimination. Moreover, we introduce a new EgoRoutine-dataset, a collection of 104 egocentric days with more than 100.000 images recorded by 7 users. Results show that routine can be discovered and behavioural patterns can be observed.},
	author = {Estefania Talavera and Carolin Wuerich and Nicolai Petkov and Petia Radeva},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107330},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Routine, Egocentric vision, Lifestyle, Behaviour analysis, Topic modelling},
	pages = {107330},
	title = {Topic modelling for routine discovery from egocentric photo-streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301333},
	volume = {104},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301333},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107330}}

@article{PATRO2021107586,
	abstract = {In this paper, we propose a probabilistic framework for solving the task of `Visual Dialog'. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable.},
	author = {Badri N. Patro and Anupriy and Vinay P. Namboodiri},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107586},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {CNN, LSTM, Uncertainty, Aleatoric uncertainty, Epistemic uncertainty vision and language, Visual dialog, VQA, Answer generation, Question generation, Bayesian deep learning},
	pages = {107586},
	title = {Probabilistic framework for solving visual dialog},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320303897},
	volume = {110},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320303897},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107586}}

@article{ZHANG2022108217,
	abstract = {Cross-modal retrieval has become a hot research topic in both computer vision and natural language processing areas. Learning intermediate common space for features of different modalities has become one of mainstream methods. In this paper, we propose a novel multi-task framework based on feature separation and reconstruction (mFSR) for cross-modal retrieval based on common space learning methods, which introduces feature separation module to deal with information asymmetry between different modalities, and introduces image and text reconstruction module to improve the quality of feature separation module. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that feature separation and specific information reconstruction can significantly improve the baseline performance of cross-modal image-caption retrieval.},
	author = {Li Zhang and Xiangqian Wu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108217},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Cross-modal retrieval, Feature separation, Image reconstruction, Text reconstruction},
	pages = {108217},
	title = {Multi-task framework based on feature separation and reconstruction for cross-modal retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321003988},
	volume = {122},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321003988},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108217}}

@article{XIE2020107205,
	abstract = {With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted.},
	author = {Lin Xie and Feifei Lee and Li Liu and Koji Kotani and Qiu Chen},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107205},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Scene recognition, Patch feature encoding, Spatial layout pattern learning, Discriminative region detection, Convolutional neural networks, Deep learning},
	pages = {107205},
	title = {Scene recognition: A comprehensive survey},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032030011X},
	volume = {102},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032030011X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107205}}

@article{DENITTO2020107318,
	abstract = {Biclustering can be defined as the simultaneous clustering of rows and columns in a data matrix and it has been recently applied to many scientific scenarios such as bioinformatics, text analysis and computer vision to name a few. In this paper we propose a novel biclustering approach, that is based on the concept of dominant-set clustering and extends such algorithm to the biclustering problem. In more detail, we propose a novel encoding of the biclustering problem as a graph so to use the dominant set concept to analyse rows and columns simultaneously. Moreover, we extend the Dominant Set Biclustering approach to facilitate the insertion of prior knowledge that may be available on the domain. We evaluated the proposed approach on a synthetic benchmark and on two computer vision tasks: multiple structure recovery and region-based correspondence. The empirical evaluation shows that the method achieves promising results that are comparable to the state-of-the-art and that outperforms competitors in various cases.},
	author = {M. Denitto and M. Bicego and A. Farinelli and S. Vascon and M. Pelillo},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107318},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Biclustering, Dominant set, Replicator dynamics, Prior knowledge},
	pages = {107318},
	title = {Biclustering with dominant sets},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301217},
	volume = {104},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301217},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107318}}

@article{WANG2022108230,
	abstract = {Node clustering aims to partition the vertices in a graph into multiple groups or communities. Existing studies have mostly focused on developing deep learning approaches to learn a latent representation of nodes, based on which simple clustering methods like k-means are applied. These two-step frameworks for node clustering are difficult to manipulate and usually lead to suboptimal performance, mainly because the graph embedding is not goal-directed, i.e., designed for the specific clustering task. In this paper, we propose a clustering-directed deep learning approach, Deep Neighbor-aware Embedded Node Clustering (DNENC for short) for clustering graph data. Our method focuses on attributed graphs to sufficiently explore the two sides of information in graphs. It encodes the topological structure and node content in a graph into a compact representation via a neighbor-aware graph autoencoder, which progressively absorbs information from neighbors via a convolutional or attentional encoder. Multiple neighbor-aware encoders are stacked to build a deep architecture followed by an inner-product decoder for reconstructing the graph structure. Furthermore, soft labels are generated to supervise a self-training process, which iteratively refines the node clustering results. The self-training process is jointly learned and optimized with the graph embedding in a unified framework, to benefit both components mutually. Experimental results compared with state-of-the-art algorithms demonstrate the good performance of our framework.},
	author = {Chun Wang and Shirui Pan and Celina P. Yu and Ruiqi Hu and Guodong Long and Chengqi Zhang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108230},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Attributed graph, Node clustering, Graph attention network, Graph convolutional network, Network representation},
	pages = {108230},
	title = {Deep neighbor-aware embedding for node clustering in attributed graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321004118},
	volume = {122},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321004118},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108230}}

@article{GUO2022108334,
	abstract = {Graph clustering based on embedding aims to divide nodes with higher similarity into several mutually disjoint groups, but it is not a trivial task to maximumly embed the graph structure and node attributes into the low dimensional feature space. Furthermore, most of the current advanced methods of graph nodes clustering adopt the strategy of separating graph embedding technology and clustering algorithm, and ignore the potential relationship between them. Therefore, we propose an innovative end-to-end graph clustering framework with joint strategy to handle the complex problem in a non-Euclidean space. In terms of learning the graph embedding, we propose a new variational graph auto-encoder algorithm based on the Graph Convolution Network (GCN), which takes into account the boosting influence of joint generative model of graph structure and node attributes on the embedding output. On the basis of embedding representation, we implement a self-training mechanism through the construction of auxiliary distribution to further enhance the prediction of node categories, thereby realizing the unsupervised clustering mode. In addition, the loss contribution of each cluster is normalized to prevent large clusters from distorting the embedding space. Extensive experiments on real-world graph datasets validate our design and demonstrate that our algorithm has highly competitive in graph clustering over state-of-the-art methods.},
	author = {Lin Guo and Qun Dai},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108334},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Graph convolution neural network, Variational graph embedding, Graph clustering, Variational graph auto-encoder},
	pages = {108334},
	title = {Graph Clustering via Variational Graph Embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321005148},
	volume = {122},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321005148},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108334}}

@article{CHEN2022108849,
	abstract = {Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.},
	author = {Lifei Chen and Haiyan Wu and Wenxuan Kang and Shengrui Wang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108849},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Sequence representation, Hidden Markov model, State clustering, Hierarchical model selection, Activity recognition},
	pages = {108849},
	title = {Symbolic sequence representation with Markovian state optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322003302},
	volume = {131},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322003302},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108849}}

@article{ZHENG2020107126,
	abstract = {Business information networks involve diverse users and rich content and have emerged as important platforms for enabling business intelligence and business decision making. A key step in an organizations business intelligence process is to cluster users with similar interests into social audiences and discover the roles they play within a business network. In this article, we propose a novel machine-learning approach, called CBIN, that co-clusters business information networks to discover and understand these audiences. The CBIN framework is based on co-factorization. The audience clusters are discovered from a combination of network structures and rich contextual information, such as node interactions and node-content correlations. Since what defines an audience cluster is data-driven, plus they often overlap, pre-determining the number of clusters is usually very difficult. Therefore, we have based CBIN on an overlapping clustering paradigm with a hold-out strategy to discover the optimal number of clusters given the underlying data. Experiments validate an outstanding performance by CBIN compared to other state-of-the-art algorithms on 13 real-world enterprise datasets.},
	author = {Yu Zheng and Ruiqi Hu and Sai-fu Fung and Celina Yu and Guodong Long and Ting Guo and Shirui Pan},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2019.107126},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Machine learning, Clustering, Business information networks, Social networks},
	pages = {107126},
	title = {Clustering social audiences in business information networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319304273},
	volume = {100},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320319304273},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2019.107126}}

@article{BOSE2021107784,
	abstract = {Soft computing provides the framework for dealing with the uncertainty and imprecision inherent in real-life applications. Soft computing has become a long-standing notable paradigm for medical image processing. A typical fuzzy clustering uses the fuzzy membership function. Nevertheless, there is an alternative membership representation, known as typicality or possibilistic membership. Unlike fuzzy membership that is probabilistic in nature, typicality represents an absolute membership and it is the degree of belonging of an object to a class that does not depend on its distances from the other classes. However, both fuzzy membership and typicality play important role in assigning membership to an object. This study proposes a novel clustering model that creates a vague environment enriched with the concept of fuzzy membership and typicality, while the use of type-reduction plays an essential role in capturing all the vagueness present in the data set. The proposed model is called type-reduced vague possibilistic fuzzy clustering (TVPFC), and we use MRI images to demonstrate its superior robustness over that of FCM (fuzzy c-means), PCM (possibilistic c-means), VCM (vague c-means) and IPFCM (interval-valued possibilistic fuzzy c-means).},
	author = {Ankita Bose and Kalyani Mali},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107784},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Fuzzy membership, Typicality, Vague set, Type-reduction, Medical images},
	pages = {107784},
	title = {Type-reduced vague possibilistic fuzzy clustering for medical images},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320305872},
	volume = {112},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320305872},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107784}}

@article{KURBAN2022108621,
	abstract = {In this paper, we propose a new temporal template approach for action recognition and person identification based on motion sequence information in masked depth video streams obtained from RGB-D data. This new representation creates a membership function that models the change in motion based on the correlation between frames that occur during motion flow. The energy images created with this function emphasize the intervals of motion with more change, while the intervals with less change are suppressed. To understand the distinctive features, the obtained energy images by using the proposed function are given as input to the convolutional neural networks and different handcrafted classifiers. The proposed method was observed on the BodyLogin, NATOPS, and SBU Kinect datasets and compared with the existing temporal templates and recent methods. The results indicate that the proposed method provides both higher performance and better motion representation.},
	author = {Onur Can Kurban and Nurullah Calik and T{\"u}lay Yildirim},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108621},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Motion recognition, Human recognition, Correlation coefficients, Deep learning, Behavioral biometrics},
	pages = {108621},
	title = {Human and action recognition using adaptive energy images},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322001029},
	volume = {127},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322001029},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108621}}

@article{JOO2020107514,
	abstract = {This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the inverse cumulative distribution function of the Gamma distribution, which is a component of the Dirichlet distribution. This approximation on a new prior led an investigation on the component collapsing, and DirVAE revealed that the component collapsing originates from two problem sources: decoder weight collapsing and latent value collapsing. The experimental results show that 1) DirVAE generates the result with the best log-likelihood compared to the baselines; 2) DirVAE produces more interpretable latent values with no collapsing issues which the baselines suffer from; 3) the latent representation from DirVAE achieves the best classification accuracy in the (semi-)supervised classification tasks on MNIST, OMNIGLOT, COIL-20, SVHN, and CIFAR-10 compared to the baseline VAEs; and 4) the DirVAE augmented topic models show better performances in most cases.},
	author = {Weonyoung Joo and Wonsung Lee and Sungrae Park and Il-Chul Moon},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107514},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Representation learning, Variational autoencoder, Deep generative model, Multi-modal latent representation, Component collapse},
	pages = {107514},
	title = {Dirichlet Variational Autoencoder},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320303174},
	volume = {107},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320303174},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107514}}

@article{HU2021107734,
	abstract = {Cross-modal retrieval aims at retrieving relevant points across different modalities, such as retrieving images via texts. One key challenge of cross-modal retrieval is narrowing the heterogeneous gap across diverse modalities. To overcome this challenge, we propose a novel method termed as Cross-modal discriminant Adversarial Network (CAN). Taking bi-modal data as a showcase, CAN consists of two parallel modality-specific generators, two modality-specific discriminators, and a Cross-modal Discriminant Mechanism (CDM). To be specific, the generators project diverse modalities into a latent cross-modal discriminant space. Meanwhile, the discriminators compete against the generators to alleviate the heterogeneous discrepancy in this space, i.e., the generators try to generate unified features to confuse the discriminators, and the discriminators aim to classify the generated results. To further remove the redundancy and preserve the discrimination, we propose CDM to project the generated results into a single common space, accompanying with a novel eigenvalue-based loss. Thanks to the eigenvalue-based loss, CDM could push as much discriminative power as possible into all latent directions. To demonstrate the effectiveness of our CAN, comprehensive experiments are conducted on four multimedia datasets comparing with 15 state-of-the-art approaches.},
	author = {Peng Hu and Xi Peng and Hongyuan Zhu and Jie Lin and Liangli Zhen and Wei Wang and Dezhong Peng},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107734},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Adversarial learning, Cross-modal representation learning, Cross-modal retrieval, Discriminant adversarial network, Cross-modal discriminant mechanism, Latent common space},
	pages = {107734},
	title = {Cross-modal discriminant adversarial network},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320305379},
	volume = {112},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320305379},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107734}}

@article{SELOSSE2020107315,
	abstract = {Recently, different studies have demonstrated the use of co-clustering, a data mining technique which simultaneously produces row-clusters of observations and column-clusters of features. The present work introduces a novel co-clustering model to easily summarize textual data in a document-term format. In addition to highlighting homogeneous co-clusters as other existing algorithms do we also distinguish noisy co-clusters from significant co-clusters, which is particularly useful for sparse document-term matrices. Furthermore, our model proposes a structure among the significant co-clusters, thus providing improved interpretability to users. The approach proposed contends with state-of-the-art methods for document and term clustering and offers user-friendly results. The model relies on the Poisson distribution and on a constrained version of the Latent Block Model, which is a probabilistic approach for co-clustering. A Stochastic Expectation-Maximization algorithm is proposed to run the model's inference as well as a model selection criterion to choose the number of co-clusters. Both simulated and real data sets illustrate the efficiency of this model by its ability to easily identify relevant co-clusters.},
	author = {Margot Selosse and Julien Jacques and Christophe Biernacki},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107315},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Co-Clustering, Document-term matrix, Latent block model},
	pages = {107315},
	title = {Textual data summarization using the Self-Organized Co-Clustering model},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301199},
	volume = {103},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301199},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107315}}

@article{JEON2022108592,
	abstract = {We address the data association problem and propose a Bayesian approach based on a mixture of Gaussian Processes (GPs) having two key components, the assignment probabilities and the GPs. In the proposed approach, the two key components are simultaneously updated according to observations through an efficient Expectation-Maximization (EM) algorithm that we develop. The proposed approach is thus more adaptive to the observations than the existing approaches for data association. To validate the performance of the proposed approach, we provide experimental results with real data sets as well as two synthetic data sets. We also provide a theoretical analysis to show the effectiveness of the Bayesian update.},
	author = {Younghwan Jeon and Ganguk Hwang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108592},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Gaussian processes, Bayesian models, Variational inference, Expectation maximization},
	pages = {108592},
	title = {Bayesian mixture of gaussian processes for data association problem},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322000735},
	volume = {127},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322000735},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108592}}

@article{REN2020107297,
	abstract = {Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed.},
	author = {Yan Ren and Adams Wai Kin Kong and Licheng Jiao},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107297},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Image cosegmentation, Video cosegmentation},
	pages = {107297},
	title = {A survey on image and video cosegmentation: Methods, challenges and analyses},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301011},
	volume = {103},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301011},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107297}}

@article{LI2021107824,
	abstract = {Targeting at boosting business revenue, purchase prediction based on user behavior is crucial to e-commerce. However, it is not a well-explored topic due to a lack of relevant datasets. Specifically, no public dataset provides both price and discount information varying on time, which play an essential role in the user's decision making. Besides, existing learn-to-rank methods cannot explicitly predict the purchase possibility for a specific user-item pair. In this paper, we propose a two-step graph-based model, where the graph model is applied in the first step to learn representations of both users and items over click-through data, and the second step is a classifier incorporating the price information of each transaction record. To evaluate the model performance, we propose a transaction-based framework focusing on the purchased items and their context clicks, which contain items that a user is interested in but fails to choose after comparison. Our experiments show that exploiting the price and discount information can significantly enhance prediction accuracy.},
	author = {Zongxi Li and Haoran Xie and Guandong Xu and Qing Li and Mingming Leng and Chi Zhou},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.107824},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Purchase prediction, Graph-based method, e-commerce, Transaction-level data},
	pages = {107824},
	title = {Towards purchase prediction: A transaction-based setting and a graph-based method leveraging price information},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032100011X},
	volume = {113},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032100011X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.107824}}

@article{WANG2020107479,
	abstract = {Multimodal hashing methods have gained considerable attention in recent years due to their effectiveness and efficiency for cross-modal similarity searches. Existing multimodal hashing methods either learn unified hash codes for different modalities or learn individual hash codes for each modality and then explore cross-correlations between them. Generally, learning unified hash codes tends to preserve the shared properties of multimodal data and learning individual hash codes tends to preserve the specific properties of each modality. There remains a crucial bottleneck regarding how to learn hash codes that simultaneously preserve the shared properties and specific properties of multimodal data. Therefore, we present a joint and individual matrix factorization hashing (JIMFH) method, which not only learns unified hash codes for multimodal data to preserve their common properties but also learns individual hash codes for each modality to retain its specific properties. The proposed JIMFH learns unified hash codes by joint matrix factorization, which jointly factorizes all modalities into a shared latent semantic space. In addition, JIMFH learns individual hash codes by individual matrix factorization, which separately factorizes each modality into a modal-specific latent semantic space. Finally, unified hash codes and individual hash codes are combined to obtain the final hash codes. In this way, hash codes learned by JIMFH can preserve both the shared properties and specific properties of multimodal data, and therefore the retrieval performance is enhanced. Comprehensive experiments show that the proposed JIMFH performs much better than many state-of-the-art methods on cross-modal retrieval applications.},
	author = {Di Wang and Quan Wang and Lihuo He and Xinbo Gao and Yumin Tian},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107479},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Hashing, Multimodal, Retrieval, Cross-modal, Matrix factorization},
	pages = {107479},
	title = {Joint and individual matrix factorization hashing for large-scale cross-modal retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032030282X},
	volume = {107},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032030282X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107479}}

@article{WANG2022108512,
	abstract = {Due to the huge commercial interests behind online reviews, a tremendous amount of spammers manufacture spam reviews for product reputation manipulation. To further enhance the influence of spam reviews, spammers often collaboratively post spam reviews within a short period of time, the activities of whom are called collective opinion spam campaign. The goals and members of the spam campaign activities change frequently, and some spammers also imitate normal purchases to conceal the identity, which makes the spammer detection challenging. In this paper, we propose an unsupervised network embedding-based approach to jointly exploiting different types of relations, e.g., direct common behavior relation, and indirect co-reviewed relation to effectively represent the relevances of users for detecting the collective opinion spammers. The average improvements of our method over the state-of-the-art solutions on dataset AmazonCn and YelpHotel are [14.09%,12.04%] and [16.25%,12.78%] in terms of AP and AUC, respectively.},
	author = {Ziyang Wang and Wei Wei and Xian-Ling Mao and Guibing Guo and Pan Zhou and Sheng Jiang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108512},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Spam detection, Collective spammer, Network embedding, Signed network},
	pages = {108512},
	title = {User-based network embedding for opinion spammer detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321006889},
	volume = {125},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321006889},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108512}}

@article{TANG2022108787,
	abstract = {In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning, which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CAT's main components.},
	author = {Xudong Tang and Chao Dong and Wei Zhang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108787},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Text clustering, Contrastive learning, Representation learning},
	pages = {108787},
	title = {Contrastive author-aware text clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322002680},
	volume = {130},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322002680},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108787}}

@article{LIU2020107053,
	abstract = {Online activity recognition which aims to detect and recognize activity instantly from a continuous video stream is a key technology in human-robot interaction. However, the partial activity observation problem, mainly due to the incomplete sequence acquisition, makes it greatly challenging. This paper proposes a novel approach, named Multi-stage Adaptive Regression (MAR), for online activity recognition with the main focus on addressing the partial observation problem. Specifically, the MAR framework delicately assembles overlapped activity observations to improve its robustness against arbitrary activity segments. Then multiple score functions corresponding to each specific performance stage are collaboratively learned via a adaptive label strategy to enhance its power of discriminating similar partial activities. Moreover, the Online Human Interaction (OHI) database is constructed to evaluate the online activity recognition in human interaction scenarios. Extensive experimental evaluations on the Multi-Modal Action Detection (MAD) database and the OHI database show that the MAR method achieves an outstanding performance over the state-of-the-art approaches.},
	author = {Bangli Liu and Haibin Cai and Zhaojie Ju and Honghai Liu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2019.107053},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Online activity recognition, Interaction recognition, Partial observation, Adaptive regression},
	pages = {107053},
	title = {Multi-stage adaptive regression for online activity recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319303553},
	volume = {98},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320319303553},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2019.107053}}

@article{ZHANG2022108661,
	abstract = {Graph convolutional network (GCN) is an effective neural network model for graph representation learning. However, standard GCN suffers from three main limitations: (1) most real-world graphs have no regular connectivity and node degrees can range from one to hundreds or thousands, (2) neighboring nodes are aggregated with fixed weights, and (3) node features within a node feature vector are considered equally important. Several extensions have been proposed to tackle the limitations respectively. This paper focuses on tackling all the proposed limitations. Specifically, we propose a new node-feature convolutional (NFC) layer for GCN. The NFC layer first constructs a feature map using features selected and ordered from a fixed number of neighbors. It then performs a convolution operation on this feature map to learn the node representation. In this way, we can learn the usefulness of both individual nodes and individual features from a fixed-size neighborhood. Experiments on three benchmark datasets show that NFC-GCN consistently outperforms state-of-the-art methods in node classification.},
	author = {Li Zhang and Heda Song and Nikolaos Aletras and Haiping Lu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108661},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Graph, Representation learning, Graph convolutional networks, Convolutional neural networks},
	pages = {108661},
	title = {Node-Feature Convolution for Graph Convolutional Networks},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032200142X},
	volume = {128},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032200142X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108661}}

@article{WEI2021107636,
	abstract = {Sequence labeling is a fundamental task in natural language processing and has been widely studied. Recently, RNN-based sequence labeling models have increasingly gained attentions. Despite superior performance achieved by learning the long short-term (i.e., successive) dependencies, the way of sequentially processing inputs might limit the ability to capture the non-continuous relations over tokens within a sentence. To tackle the problem, we focus on how to effectively model successive and discrete dependencies of each token for enhancing the sequence labeling performance. Specifically, we propose an innovative attention-based model (called position-aware self-attention, i.e., PSA) as well as a well-designed self-attentional context fusion layer within a neural network architecture, to explore the positional information of an input sequence for capturing the latent relations among tokens. Extensive experiments on three classical tasks in sequence labeling domain, i.e.,  part-of-speech (POS) tagging, named entity recognition (NER) and phrase chunking, demonstrate our proposed model outperforms the state-of-the-arts without any external knowledge, in terms of various metrics.},
	author = {Wei Wei and Zanbo Wang and Xianling Mao and Guangyou Zhou and Pan Zhou and Sheng Jiang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107636},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Equence labeling, Self-attention, Discrete context dependency},
	pages = {107636},
	title = {Position-aware self-attention based neural sequence labeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320304398},
	volume = {110},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320304398},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107636}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>Decision Support Systems</string>
		<key>keys</key>
		<string>XU2021113525,KAZMAIER2020113304,DUTTA2022113662,CHENG2022113864,LI2022113863,BISWAS2022113651,ROEDER2022113770,DEHGHAN2020113425,WANG2020113171,LIU2021113609,ZHANG2020113288,KUNDU2020113164,GOLDBERG2022113751,YANG2022113813,KUMAR2022113792,XU2020113162,XU2021113467,LI2022113755,ZHANG2022113765,ZHENG2020113369,WANG2021113465</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Expert Systems with Applications</string>
		<key>keys</key>
		<string>ALI2020113790,ULIAN2021115341,YOO2020112965,DU2021114791,REUBEN2022117027,BHOPALE2020113441,LIU2021115752,XUE2022116057,SEONG2021113988,LOPEZMONROY2020112909,KUMARASWAMY2022118433,FIOK2021115771,ZHOU2022116560,HOOSHYAR2022116670,QORIB2023118715,KWON2021114488,SHAO2022118221,WANG2022117317,JANG2021114042,RINALDI2021114320,DAI2023118841,KHALID2022115926,AKKASI2021115162,KAUR2023118997,THIRUMOORTHY2021115040,MOJRIAN2021114555,BI2022118352,ZHU2023118364,IWATSUKI2022115840,LI2022116600,GAO2021114191,LI2020112839,ZAMIRI2021114657,CALI2022118440,WANG2021114557,ELBOUSHAKI2020112829,AGUADO2022118103,MARTINEZHUERTAS2021115621,SALAHIAN2022119051,ALTINEL2022118606,MOIRANGTHEM2021113898,JIANG2021115537,MOHSIN2021113808,SARKAR2021115026,NGUYEN2022117096,LATHABAI2022118317,HAN2022116472,GUVEN2022116592,ZHANG2022115826,ELKASSAS2021113679,ZHANG2021115439,LI2022118336,ZHOU2020113361,FAHFOUH2020113517,JEONG2022118375,BENSASSI2021115375,RAHIMI2022116518,ALMUZAINI2022117384,CHRISTOPHE2021114831,LI2021114585,ARBANE2023118710,KIM2020113401,EFFROSYNIDIS2022117541,FANG2021114306,HUSSAIN2022118119,CAO2020113465,KIM2022117983,ZHOU2022116194,ZOTOVA2021114547,JEYARAJ2022115896,JOSHI2022116846,MA2023118695,GREGORIADES2021115546,LIU2022116741,HARALABOPOULOS2021114769,DARGAHINOBARI2021114303,VANDINTER2021115261,MEILIAN2020113427,WAHID2022116562,FALLAHNEJAD2022116433,DAU2020112871,CATELLI2022118290,GOMEZ2022118400,LEBENA2022117303,ALDUNATE2022118309,SRINIVASARAO2022116475,SKRJANC2022117881,CHEN2022116574,ZHANG2022116882,ZHANG2022116717,TERROSOSAENZ2020112892,WANG2022115887,ZHENG2021115030,TANG2021115070,LYKOUSAS2021114808,DHEERAJ2021115265,RANI2022118461,BELFORD2020113709,ZHANG2020113073,ERFANIAN2022116086,KIM2020113288,TANG2022118062,AMADORDOMINGUEZ2021115731,NUGUMANOVA2022117179,GOZUACIK2021115388,JOSHI2023118442,CAMPOS2022117510,ZIHAYAT2021114910,SHI2022116538,SAKSHI2023119028,ALTINELGIRGIN2021114599,SINGHCHAUHAN2020113673,ZHAO2022118335,RAHIMI2020113770,ANTONAKAKI2021114006,HACOHENKERNER2022117140,AYO2021114762,ALAMI2021114652,XIA2022118143,DUAN2020113540,CAO2022115977,DAHIR2021114909,KAUR2020113350,VIDANAGAMA2022117869,ELIGUZEL2022117433,JIA2022116405</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Information Sciences</string>
		<key>keys</key>
		<string>LI2020241,SHEN202063,KHAN202269,ZHANG2020306,BAI2022,DAU20201279,XIAO2021262,YANG2021185,ZHAO2021283,PRADHAN2021212,COSTA2021226,QIN202237,HOU2022215,CHAI20221029,WU2020100,ASGHARI2022184,CHEN2021343,ZHOU20221030,KONG202039,LEI202098,DONG2020203,WANG2021762,CHAUDHURI2022,ZHENG2022211,DAS2021279,PHAN2021243,NAKAMURA2021482,WANG2021136,FENG202279,LIU2020227,ETEMADI2022,LU202087,KHODABAKHSH20221,HUANG202018,TERRAGNI2020581,JUNIOR2021116,ZHONG2021178,LI20221023,LIANG202194,LIU2021129,HU2022239,HAN2020177,FROLOV2020595,LI2022186,HUYNHTHE2020112,SONG2020138,ZHOU2022809,BAEK2022235,ANNAMORADNEJAD2022144,GAO2022170,YANG202241,XIAO20211,LIU2022395,WAN2020243</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Journal of Informetrics</string>
		<key>keys</key>
		<string>BALLESTER2022101224,XIE2021101201,ZHOU2021101162,ALSUDAIS2021101139,HAJIBABAEI2022101275,XU2020101014,ZHANG2020101032,LEE2021101126,AMON2022101284,REHS2021101166,DOGAN2020101076,HUANG2021101145,WANG2021101214,EBADI2020101018,CHUNG2021101103,JUNG2022101320,JUNG2020101040,CHEN2022101281,HUANG2022101317,KIM2022101242,KIM2022101255,QIAN2020101047</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Knowledge-Based Systems</string>
		<key>keys</key>
		<string>ZHANG2021107135,YANG2021106687,BERNABEMORENO2020105236,PRADHAN2020105784,MOHSIN2022107711,WANG2020106433,XU2021106858,LENG2020105600,DU2021107247,DESSI2022109945,GAO2020105418,JEONG2021106659,PARK2020104825,PENG2022109933,PESSUTTO2020105339,DAI2022107659,CHEN2021107521,LIU2021106660,LI2021106846,ZHOU2020105695,AN2022107623,BEHPOUR2021106907,MARTIN2022109265,LIAO2022108665,WANDABWA2021107249,LI2021107163,PENTA2021107342,LIU2020106435,YANG2022108488,LI2021106827,QIN2021107160,QIAN2020105684,XU2021107225,LI2020105436,ELAKROUCHI2021106650,WAN2022109551,LIU2021106917,LIU2022108495,YE2022108699,ZHOU2020105458,QIN2020105750,GANGAVARAPU2020105321,KAUR2022108014,YERA2022109216,VENUGOPALAN2022108668,AYETIRAN2021106902,FACCHINETTI2022109266,ZHAO2022108550,GONZALEZSANTOS2021107113,ALI2020106438,ZHU2021106511,GUO2021107454,DECAMPOS2020105337,LIANG2022108050,XU2020106391,DONG2022108954,ZOU2022107927,LIU2020105918,DHELIM2020106227,CHEN2020105546,LOPEZ2021107455,JUNG2022110020,KUNDU2021106535,REN2021107093,HE2020106228,DELCARMENRODRIGUEZHERNANDEZ2021106740,ABEBE2020104817,WEN2020106344,ABUALIGAH2022108833,LI2021107359,LI2021106948,PONZA2020105051,DIGIROLAMO2021106563,CAO2020106114,ARAQUE2020105184,TOMER2022108108,CHEN2022110000,ORTEGABUENO2022107597,PRADHAN2020106181,DENHAM2020105114,YUE2020106206,ZHANG2022108006,XU2022107839,YANG2020105214,ZHU2022108741</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Pattern Recognition</string>
		<key>keys</key>
		<string>JIAN2021108100,TALAVERA2020107330,PATRO2021107586,ZHANG2022108217,XIE2020107205,DENITTO2020107318,WANG2022108230,GUO2022108334,CHEN2022108849,ZHENG2020107126,BOSE2021107784,KURBAN2022108621,JOO2020107514,HU2021107734,SELOSSE2020107315,JEON2022108592,REN2020107297,LI2021107824,WANG2020107479,WANG2022108512,TANG2022108787,LIU2020107053,ZHANG2022108661,WEI2021107636</string>
	</dict>
</array>
</plist>
}}
