%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-24 17:06:33 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{CHIEW20181,
	abstract = {Phishing was a threat in the cyber world a couple of decades ago and still is today. It has grown and evolved over the years as phishers are getting creative in planning and executing the attacks. Thus, there is a need for a review of the past and current phishing approaches. A systematic, comprehensive and easy-to-follow review of these approaches is presented here. The relevant mediums and vectors of these approaches are identified for each approach. The medium is the platform which the approaches reside and the vector is the means of propagation utilised by the phisher to deploy the attack. The paper focuses primarily on the detailed discussion of these approaches. The combination of these approaches that the phishers utilised in conducting their phishing attacks is also discussed. This review will give a better understanding of the characteristics of the existing phishing techniques which then acts as a stepping stone to the development of a holistic anti-phishing system. This review creates awareness of these phishing techniques and encourages the practice of phishing prevention among the readers. Furthermore, this review will gear the research direction through the types of phishing, while also allowing the identification of areas where the anti-phishing effort is lacking. This review will benefit not only the developers of anti-phishing techniques but the policy makers as well.},
	author = {Kang Leng Chiew and Kelvin Sheng Chek Yong and Choon Lin Tan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.050},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information security threats, Advanced phishing techniques, Anti-phishing, Attack vector, Taxonomy, Review},
	pages = {1-20},
	title = {A survey of phishing attacks: Their types, vectors and technical approaches},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418302070},
	volume = {106},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418302070},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.03.050}}

@article{CUI201794,
	abstract = {Individuals use Twitter for personal communication, whereas businesses, politicians and celebrities use Twitter for branding purposes. Distinguishing Personal from Branding Twitter accounts is important for Twitter analytics. Existing studies of Twitter account classification apply classical supervised learning, which requires intensive manual annotation for training. In this paper, we propose CDS (Collaborative Distant Supervision), a novel learning scheme for Twitter account classification that does not require intensive manual labelling. Twitter accounts are automatically labelled using heuristics for distant supervision learning. To achieve effective learning from heuristic labels, active learning is applied to identify and correct false positive labels, and semi-supervised learning is applied to further use false negatives missed by labelling heuristics for learning. Extensive experiments on Twitter data showed that CDS achieved high classification accuracy.},
	author = {Lishan Cui and Xiuzhen Zhang and A.K. Qin and Timos Sellis and Lifang Wu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.03.075},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Twitter, Classification, Distant supervision, Active learning, Semi-supervised learning},
	pages = {94-103},
	title = {CDS: Collaborative distant supervision for Twitter account classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741730235X},
	volume = {83},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741730235X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.03.075}}

@article{JOHN2017385,
	abstract = {Multi-document summarization is the process of extracting salient information from a set of source texts and present that information to the user in a condensed form. In this paper, we propose a multi-document summarization system which generates an extractive generic summary with maximum relevance and minimum redundancy by representing each sentence of the input document as a vector of words in Proper Noun, Noun, Verb and Adjective set. Five features, such as TF_ISF, Aggregate Cross Sentence Similarity, Title Similarity, Proper Noun and Sentence Length associated with the sentences, are extracted, and scores are assigned to sentences based on these features. Weights that can be assigned to different features may vary depending upon the nature of the document, and it is hard to discover the most appropriate weight for each feature, and this makes generation of a good summary a very tough task without human intelligence. Multi-document summarization problem is having large number of decision parameters and number of possible solutions from which most optimal summary is to be generated. Summary generated may not guarantee the essential quality and may be far from the ideal human generated summary. To address this issue, we propose a population-based multicriteria optimization method with multiple objective functions. Three objective functions are selected to determine an optimal summary, with maximum relevance, diversity, and novelty, from a global population of summaries by considering both the statistical and semantic aspects of the documents. Semantic aspects are considered by Latent Semantic Analysis (LSA) and Non Negative Matrix Factorization (NMF) techniques. Experiments have been performed on DUC 2002, DUC 2004 and DUC 2006 datasets using ROUGE tool kit. Experimental results show that our system outperforms the state of the art works in terms of Recall and Precision.},
	author = {Ansamma John and P.S. Premjith and M. Wilscy},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.05.075},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-document summarization, Multicriteria optimization, Latent semantic analysis, Non negative matrix factorization, DUC, ROUGE},
	pages = {385-397},
	title = {Extractive multi-document summarization using population-based multicriteria optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417304049},
	volume = {86},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417304049},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.05.075}}

@article{ALI2020113790,
	abstract = {The huge amount of research papers on the web makes finding a relevant manuscript a difficult task. In recent years many models were introduced to support researchers by providing personalized citation recommendations. Moreover, deep learning methods have been employed in this domain to improve the quality of the final recommendations. However, a thorough study that classifies citation recommendation models and examines their (a) strengths and weaknesses, (b) evaluation metrics used, (c) popular datasets, and challenges faced is missing. Therefore, with this survey, we present a new classification approach for deep learning models that provide citation recommendation. Our approach uses the following six criteria: data factors, data representation methods, methodologies, types of recommendations used, problems addressed, and personalization. Additionally, we present a comparative analysis of those models that use the same set of evaluation metrics and datasets. Moreover, we examine hot upcoming issues and solutions in light of explored literature. Also, the survey discusses and analyzes the evaluation metrics and datasets adopted by the explored models. Finally, we conclude our survey with trends and future directions to further assist research on that domain.},
	author = {Zafar Ali and Pavlos Kefalas and Khan Muhammad and Bahadar Ali and Muhammad Imran},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113790},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, Citation recommendation, Neural networks, Paper recommendation, Machine learning, Deep learning},
	pages = {113790},
	title = {Deep learning in citation recommendation models survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420306126},
	volume = {162},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420306126},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113790}}

@article{PORTUGAL2018205,
	abstract = {Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of recommender systems using machine learning algorithms often faces problems and raises questions that must be resolved. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies new research opportunities. The goals of this study are to (i) identify trends in the use or research of machine learning algorithms in recommender systems; (ii) identify open questions in the use or research of machine learning algorithms; and (iii) assist new researchers to position new research activity in this domain appropriately. The results of this study identify existing classes of recommender systems, characterize adopted machine learning approaches, discuss the use of big data technologies, identify types of machine learning algorithms and their application domains, and analyzes both main and alternative performance metrics.},
	author = {Ivens Portugal and Paulo Alencar and Donald Cowan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.12.020},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Systematic review of the literature, Recommender systems, Machine learning, Machine learning algorithms, Application domains, Performance metrics},
	pages = {205-227},
	title = {The use of machine learning algorithms in recommender systems: A systematic review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417308333},
	volume = {97},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417308333},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.12.020}}

@article{KWON2017386,
	abstract = {With an increasing attempt of finding latent semantics in a video dataset, trajectories have become key components since they intrinsically include concise characteristics of object movements. An approach to analyze a trajectory dataset has concentrated on semantic region retrieval, which extracts some regions in which have their own patterns of object movements. Semantic region retrieval has become an important topic since the semantic regions are useful for various applications, such as activity analysis. The previous literatures, however, have just revealed semantically relevant points, rather than actual regions, and have less consideration of temporal dependency of observations in a trajectory. In this paper, we propose a novel model for trajectory analysis and semantic region retrieval. We first extend the meaning of semantic regions that can cover actual regions. We build a model for the extended semantic regions based on a hierarchically linked infinite hidden Markov model, which can capture the temporal dependency between adjacent observations, and retrieve the semantic regions from a trajectory dataset. In addition, we propose a sticky extension to diminish redundant semantic regions that occur in a non-sticky model. The experimental results demonstrate that our models well extract semantic regions from a real trajectory dataset.},
	author = {Yongjin Kwon and Kyuchang Kang and Junho Jin and Jinyoung Moon and Jongyoul Park},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.02.026},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Trajectory analysis, Semantic regions, Nonparametric Bayesian models, Infinite hidden Markov models, Sticky extensions},
	pages = {386-395},
	title = {Hierarchically linked infinite hidden Markov model based trajectory analysis and semantic region retrieval in a trajectory dataset},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417301082},
	volume = {78},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417301082},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.02.026}}

@article{KARDAS2017343,
	abstract = {This paper introduces a Surveillance Video Analysis System, called SVAS, for surveillance domain, in which the semantic rules and the definition of event models can be learned or defined by the user for automatic detection and inference of complex video events. In the scope of SVAS, an event model method named Interval-Based Spatio-Temporal Model (IBSTM) is proposed. SVAS can learn action models and event models without any predefined threshold values and generates understandable and manageable IBSTM event models. Hybrid machine learning methods are proposed and used. A set of feature models named Threshold Model, which reflects the spatio-temporal motion analysis of an event, is kept as the first model. As the second model, Bag of Actions (BoA) model is used in order to reduce the search space in the detection phase. Markov Logic Network (MLN) model, which provides understandable and manageable logic predicates for users, is kept as the third model. SVAS has high performance event detection capability due to its interval-based hierarchical manner. It determines related candidate intervals for each main model of IBSTM and uses the related main model when needed rather than using all models as a whole. The main contribution of this study is to fill the semantic gap between humans and video computer systems such that, on the one hand it decreases human intervention through its learning capabilities, but on the other hand it also enables human intervention when necessary through its manageable event model method. The study achieves all of them in the most efficient way through its machine learning methods. The proposed system is applied to different event datasets from CAVIAR, BEHAVE and our synthetic datasets. The experimental results show that our approach improves the event recognition performance and precision as compared to the current state-of-the-art approaches.},
	author = {Karani Kardas and Nihan Kesim Cicekli},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.07.051},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Event detection, Markov logic networks, Video surveillance, Event model learning, Event inference},
	pages = {343-361},
	title = {SVAS: Surveillance Video Analysis System},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417305286},
	volume = {89},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417305286},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.07.051}}

@article{ULIAN2021115341,
	abstract = {News recommendations distinguishes from general content recommendations as it takes in consideration news freshness, sparsity, monotony and time. Recent works approach these features using hybrid Collaborative-Content-based Filtering methods, adapting clustering techniques to handle sparsity and monotony without considering the effects that different clustering methods may have over recommendation results. Such studies often evaluate the results of varying different parameters individually, ignoring possible interaction effects between them. They also base their results on metrics such as accuracy and recall that are sensitive to bias. To investigate the importance of clustering method selection to News Recommender System results we evaluated the effects of different traditional techniques in recommending news articles. We implemented an algorithm that used a hybrid Collaborative-Content-based Filtering method to incorporate user behavior, user interest, article popularity and time effect. The system uses an article selection method that built the recommendation set based on content features. With this algorithm, we examined the existence of interaction effects between the input parameters. We used a Gaussian regression process to explore the response surface while sequentially optimizing parameters. To avoid being misled by underlying biases we used Informedness, an accuracy metric that captures both positive and negative information from prediction results. Our results demonstrated that different clustering methods had a significant influence on the recommendation results. It was also found that a traditional hierarchical method outperformed optimization methods with important performance improvement. In addition, we demonstrated that parameters may interact with each other and that analyzing them separately may mislead interpretation.},
	author = {Douglas Zanatta Ulian and Jo{\~a}o Luiz Becker and Carla Bonato Marcolin and Eusebio Scornavacca},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115341},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, News recommender systems, Clustering, Collaborative filtering, Content filtering, Data mining},
	pages = {115341},
	title = {Exploring the effects of different Clustering Methods on a News Recommender System},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421007697},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421007697},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115341}}

@article{YOO2020112965,
	abstract = {In order to make computers understand human languages and to reason, human knowledge needs to be represented and stored in a form that can be processed by computers. Knowledge graphs have been developed for use as a form of the knowledge base for words and general relationships among words. However, they have two limitations. One is that the knowledge graph is limited in size and scope for most of the human languages. Another is that they are not able to deal with neologisms that form a part of the human common sense. Addressing these problems, we have developed and validated PolarisX which can automatically expand a knowledge graph, by crawling and analyzing the news sites and social media in real-time. We utilize and fine-tune the pre-trained multilingual BERT model for the construction of knowledge graphs without language dependencies. We extract new relationships using the BERT-based relation extraction model and integrate them into the knowledge graph. We verify the novelty and accuracy of PolarisX. It deals with neologisms and does not have language dependencies.},
	author = {SoYeop Yoo and OkRan Jeong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112965},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Auto expansion, Knowledge graph, Neologisms, Semantic analysis, Multilingual BERT model},
	pages = {112965},
	title = {Automating the expansion of a knowledge graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419306839},
	volume = {141},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419306839},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112965}}

@article{MORISE20191,
	abstract = {Ratings by users on various items such as products and services have become easily available on the Web. Also available in many cases, in addition to an overall rating for each item by each user, are multicriteria ratings from different viewpoints. Our previous study showed that multicriteria rating approaches performed better than single-criterion ones for both recommendation and rating aggregation. We have now formulated a Bayesian probabilistic model for multicriteria evaluation as an alternative to low-rank approximation. We evaluated the performance of this model, in which model capacity is controlled by integrating over all model parameters, and investigated whether it can be made to work more efficiently by using a Markov chain Monte Carlo method for both recommendation and rating aggregation. It performed better than low-rank approximation methods that obtain a maximum a posteriori estimate by fitting to the data.},
	author = {Hiroki Morise and Satoshi Oyama and Masahito Kurihara},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.04.044},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommendation, Multi-criteria rating, Collaborative filtering, Rating aggregation, Bayesian probabilistic models},
	pages = {1-8},
	title = {Bayesian probabilistic tensor factorization for recommendation and rating aggregation with multicriteria evaluation data},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930274X},
	volume = {131},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930274X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.04.044}}

@article{DU2021114791,
	abstract = {Predicting hospital readmission with effective machine learning techniques has attracted a great attention in recent years. The fundamental challenge of this task stems from characteristics of the data extracted from electronic health records (EHR), which are imbalanced class distributions. This challenge further leads to the failure of most existing models that only provide a partial understanding for the learning problem and result in a biased and inaccurate prediction. To address this challenge, we propose a new graph-based class-imbalance learning method by fully making use of the data from different classes. First, we conduct graph construction for learning the pattern discrimination from between-class and within-class data samples. Then we design an optimization framework to incorporate the constructed graphs to obtain a class-imbalance aware graph embedding and further alleviate performance degeneration. Finally, we design a neural network model as the classifier to conduct imbalanced classification, i.e., hospital readmission prediction. Comprehensive experiments on six real-world readmission datasets show that the proposed method outperforms state-of-the-art approaches in readmission prediction task.},
	author = {Guodong Du and Jia Zhang and Fenglong Ma and Min Zhao and Yaojin Lin and Shaozi Li},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114791},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hospital readmission, Graph embedding, Class-imbalance learning, Neural network model},
	pages = {114791},
	title = {Towards graph-based class-imbalance learning for hospital readmission},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002323},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002323},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114791}}

@article{REUBEN2022117027,
	abstract = {Retrieving information from an online search engine is the first and most important step in many data mining tasks, such as fake news detection. Most of the search engines currently available on the web, including all social media platforms, are black-boxes (i.e., opaque) supporting short keyword queries. In these settings, it is challenging to retrieve all posts and comments discussing a particular news item automatically and on a large scale. In this paper, we propose a method for generating short keyword queries given a prototype document. The proposed iterative query selection (IQS) algorithm interacts with the opaque search engine to iteratively improve the query, by maximizing the number of relevant results retrieved. Our evaluation of IQS was performed on the Twitter TREC Microblog 2012 and TREC-COVID 2019 datasets and demonstrated the algorithm's superior performance compared to state-of-the-art. In addition, we implemented IQS algorithm to automatically collect a large-scale dataset for fake news detection task of about 70K true and fake news items. The dataset, which we have made publicly available to the research community, includes over 22M accounts and 61M tweets. We demonstrate the usefulness of the dataset for fake news detection task achieving state-of-the-art performance.},
	author = {Maor Reuben and Aviad Elyashar and Rami Puzis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117027},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Query selection, Opaque search engine, Pseudo relevance feedback, Fake news},
	pages = {117027},
	title = {Iterative query selection for opaque search engines with pseudo relevance feedback},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422004432},
	volume = {201},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422004432},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117027}}

@article{PARADARAMI2017300,
	abstract = {In the context of recommendation systems, metadata information from reviews written for businesses has rarely been considered in traditional systems developed using content-based and collaborative filtering approaches. Collaborative filtering and content-based filtering are popular memory-based methods for recommending new products to the users but suffer from some limitations and fail to provide effective recommendations in many situations. In this paper, we present a deep learning neural network framework that utilizes reviews in addition to content-based features to generate model based predictions for the business-user combinations. We show that a set of content and collaborative features allows for the development of a neural network model with the goal of minimizing logloss and rating misclassification error using stochastic gradient descent optimization algorithm. We empirically show that the hybrid approach is a very promising solution when compared to standalone memory-based collaborative filtering method.},
	author = {Tulasi K. Paradarami and Nathaniel D. Bastian and Jennifer L. Wightman},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.04.046},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information retrieval, Artificial neural networks, Recommender systems, Supervised learning},
	pages = {300-313},
	title = {A hybrid recommender system using artificial neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417302968},
	volume = {83},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417302968},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.04.046}}

@article{BHOPALE2020113441,
	abstract = {This work explores the integrated power of swarm intelligence and advances in data mining techniques to solve the information retrieval (IR) problem of rapidly growing digital content on the World Wide Web. We propose a swarm optimized cluster based framework with frequent pattern mining techniques to retrieve user-specific knowledge from extensive document collections. In the pre-processing phase, we split the task into two sub-tasks. The first is to decompose the document collection into groups using a bio-inspired K-Flock clustering algorithm, while the second extracts frequent patterns from each cluster using a memory-efficient Recursive Elimination (RElim) algorithm. In the next phase, we implement a cosine similarity based probabilistic model to retrieve query-specific documents from clusters based on the matching scores between the closed frequent patterns of queries and clusters. The performance of a system is evaluated by conducting several experiments which are carried out on five well-known, diverse and variable size datasets viz- TREC 2014-15 CDS (Clinical Decision Support) datasets containing 733,138 records, OHSUMED dataset with 348,566 records from Medline database, NPL dataset with 11,429 records, LISA document collection of 6004 records, CACM (Collection of ACM) dataset of 3204 records. The results show that the proposed IR framework significantly outperforms the traditional sequential IR approach and other state-of-the-art IR approaches, both in terms of the quality of the returned documents and the time of execution.},
	author = {Amol P. Bhopale and Ashish Tiwari},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113441},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information retrieval, Swarm intelligence, Big data clustering, Frequent pattern mining, Unsupervised learning},
	pages = {113441},
	title = {Swarm optimized cluster based framework for information retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420302657},
	volume = {154},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420302657},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113441}}

@article{LIU2021115752,
	abstract = {In various recommender systems, ratings and reviews are the main information to show user preferences. However, recommendation models that only use ratings, such as collaborative filtering, are vulnerable to data sparsity. And models only using review information will also suffer from the sparsity of reviews. On one hand, most ratings and reviews are interrelated and complementary, reviews may explain why a user gives a high or low rating to an item. On the other hand, ratings and reviews are numerical and textual information, respectively, and they reflect the preference of the user from a coarse-grained level and a fine-grained level A user may comment positively about some aspects of an item, even he gives a very low score to this item. There are specific information among each of them because of their heterogeneity. Therefore, it is possible to learn more accurate representation of users and items by effectively integrating ratings and text reviews from different views, that is, shared-view and specific-view. In this paper, we propose a Shared-view and Specific-view Information extraction model for Recommendation (SSIR), which integrates the information from reviews and interaction matrix to predict ratings Our model has two key components, including shared-view information extraction and specific-view exploitation. From the perspective of shared-view, SSIR jointly minimizes the loss of confusion adversarial and rating prediction loss to extract the shared information from reviews and user--item interaction matrix. For the specific-view part, SSIR applies orthogonal constraints on shared-view and specific-view modules to extract the discriminative features from reviews and interaction data. We fuse the features extracted from these two views to predict the final ratings. In addition, we use auxiliary reviews to deal with the sparsity problem of reviews. Experimental results on eight datasets show the effectiveness and robustness of our method, which could adapt to the recommendation scenarios with fewer reviews and ratings.},
	author = {Huiting Liu and Jindou Zhao and Peipei Li and Peng Zhao and Xindong Wu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115752},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender system, Dual-view learning, Review analysis},
	pages = {115752},
	title = {Shared-view and specific-view information extraction for recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421011283},
	volume = {186},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421011283},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115752}}

@article{XUE2022116057,
	abstract = {Subway passenger flow prediction is of great significance in transportation planning and operation. Special events, as for vocal concerts and sports games, lead large-scaled passenger flow with few periodic trends. Therefore, predicting subway outbound flow during events is a challenging task. In recent years, social media has been used for socio-economic forecasting. Correlation analysis shows that the trend of social media volume can be used for passenger flow prediction under events occurrences. In this paper, besides traditional smart card data, we incorporate social media data into passenger flow prediction. The multivariate disturbance-based hybrid deep neural network (MDB-HDNN), which models the disturbances of the inbound flow from the nearby stations and the social media post trends, is proposed for subway passenger flow prediction during events. Experimental results on three real-world datasets demonstrate that the MDB-HDNN performs well under various settings and has better robustness. Our findings and results can provide decision support for schedule formulation and passenger flow guidance.},
	author = {Gang Xue and Shifeng Liu and Long Ren and Yicao Ma and Daqing Gong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116057},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social media, Subway passenger flow prediction, Attention, Spatiotemporal disturbances},
	pages = {116057},
	title = {Forecasting the subway passenger flow under event occurrences with multivariate disturbances},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421013981},
	volume = {188},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421013981},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116057}}

@article{SEONG2021113988,
	abstract = {With the development of machine learning technologies, predicting stock movements by analyzing news articles has been studied actively. Most of the existing studies utilize only the datasets of target companies, and some studies use datasets of the relevant companies in the Global Industry Classification Standard (GICS) sectors. However, we show that GICS has a limitation in finding relevance regarding stock prediction because heterogeneity exists in the GICS sectors. To solve this limitation, we suggest a methodology that reflects heterogeneity and searches for homogeneous groups of companies which have high relevance. Stock price movements are predicted using the K-means clustering and multiple kernel learning technique which integrates information from the target company and its homogeneous cluster. We experiment using three-year data from the Republic of Korea and compare the results of the proposed method with those of existing methods. The results show that the proposed method shows higher predictability than existing methods in the majority of cases. The results also imply that the necessity of cluster analysis depends on the heterogeneity in the sector, and it is essential to perform cluster analysis with a larger number of clusters as the heterogeneity increases.},
	author = {Nohyoon Seong and Kihwan Nam},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113988},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stock prediction, Data mining, Machine learning, Heterogeneity, Cluster analysis},
	pages = {113988},
	title = {Predicting stock movements based on financial news with segmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030765X},
	volume = {164},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030765X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113988}}

@article{LOPEZMONROY2020112909,
	abstract = {The Author Profiling (AP) task aims to predict demographic characteristics about the authors from documents (e.g., age, gender, native language). The research so far has focused only on forensic scenarios by performing post-analysis using all the available text evidence. This paper introduces the task of Early Author Profiling (EAP) in Twitter. The goal is to effectively recognize profiles using as few tweets as possible from the user history. The task is highly relevant to support social media analysis and different problems related to security and marketing, where prevention and anticipation is crucial. This work proposes a novel strategy that combines a state of the art representation for early text classification and specialized word-vectors for author profiling tasks. In this strategy we build prototypical features called Profile based Meta-Words, which allow us to model AP information at different levels of granularity. Our evaluation shows that the proposed methodology is well suited for profiling little text evidence (e.g., a handful of tweets) in early stages, but as more tweets become available other granularities better encode larger amounts of text in late stages. We evaluated the proposed ideas on gender and language variety identification for English and Spanish, and showed that the proposal outperforms state of the art methodologies.},
	author = {A. Pastor L{\'o}pez-Monroy and Fabio A. Gonz{\'a}lez and Thamar Solorio},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112909},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Early text classification, Author profiling, Social media analysis, Text mining},
	pages = {112909},
	title = {Early author profiling on Twitter using profile features with multi-resolution},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930627X},
	volume = {140},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930627X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112909}}

@article{MIRTALAIE2018267,
	abstract = {Sentiment knowledge extraction is a growing area of research in the literature. It helps in analyzing users' opinions about different entities or events, which can then be utilized by analysts for various purposes. Particularly, feature-based sentiment analysis is one of the challenging research areas that analyzes users' opinions on various features of a product or service. Of the three formats for the product reviews, our focus in this paper is limited to analyzing the pros/cons type. Due to the nature of pros/cons reviews, they are mostly concise and follow a different structure from other review types. Therefore, specialized techniques are needed to analyze these reviews and extract the customers' discussed product features along with their personal attitudes. In this paper, we propose the Pros/Cons Sentiment Analyzer (PCSA) framework that exploits dependency relations in extracting sentiment knowledge from pros/cons reviews. We also utilize two different lexicons to ascertain the polarity strength of the extracted features based on the customers' opinions. Several experiments are conducted to evaluate the performance of PCSA in its different phases.},
	author = {Monireh Alsadat Mirtalaie and Omar Khadeer Hussain and Elizabeth Chang and Farookh Khadeer Hussain},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.046},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Feature-based sentiment analysis, Pros/cons product reviews, Syntactic rules, Polarity strength, Product tree},
	pages = {267-288},
	title = {Extracting sentiment knowledge from pros/cons product reviews: Discovering features along with the polarity strength of their associated opinions},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304718},
	volume = {114},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304718},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.046}}

@article{KUMARASWAMY2022118433,
	abstract = {Insurance fraud is ranked second in the list of expensive crimes in the United States, with healthcare fraud being the second highest amongst all insurance fraud. Contrary to the popular belief, insurance fraud is not a victimless crime. The cost of crime is passed onto law-abiding citizens in the form of increased premiums or serious harm or danger to beneficiaries. To combat this kind of societal threat, there is an intense need for healthcare fraud detection systems to evolve. Some common roadblocks in implementing digital advancements (as seen in other domains) to healthcare are the complexity, heterogeneity of the data systems, and varied health program models across the United States. In other words, data are not stored in a centralized manner due to the sensitive domain nature, thus making it difficult to implement a robust real-world fraud-detection system. At the same time, in addition to the complexity of the varied systems involved, there is also the need to meet certain standards before a fraud actor can be prosecuted in a litigation setting. Thus, there is a human aspect to the fraud detection process flow in the real-world. In this article, a novel framework was outlined that converts diverse prescription claims (both fee-for-service and managed care) into a set of input variables/features suitable for implementation of an advanced statistical modeling fraud framework. This article thus aims to contribute to the existing literature by describing a process to transform prescription claims data to secondary features specific to provider fraud detection. The core idea was to focus on three main aspects of fraud (business heuristics on claims, provider-to-prescriber relations, and provider's client populations) to design the input features. A systematic method was proposed to extract features that have the potential to detect billing or behavioral outliers among pharmacy providers using information extracted from a secondary database (outpatient prescriptions). The application of a commonly used dimensionality reduction method, the Principal Component Analysis (PCA), was evaluated. PCA evaluates and reduces the extensive feature subspace to only those that captures the most variance in the data. To evaluate the features extracted from this framework, the application of the engineered features and the principal components to out-of-the-box logistic regression and Random Forest algorithms were considered to identify potential fraud. The engineered features when tested in different experimental settings with a logistic regression model had the highest area under the Receiver Operating Characteristic (ROC) curve of 0.76 and a weighted F score of 0.85 while a random forest model had the highest area under curve of 0.74 and a weighted F score of 0.88.},
	author = {Nishamathi Kumaraswamy and Mia K. Markey and Jamie C. Barner and Karen Rascati},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118433},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Medicaid, Fraud detection, Class imbalance, Machine learning, Statistical models},
	pages = {118433},
	title = {Feature engineering to detect fraud using healthcare claims data},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015330},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015330},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118433}}

@article{RUAS2019288,
	abstract = {Natural Language Understanding has seen an increasing number of publications in the last few years, especially after robust word embeddings models became prominent, when they proved themselves able to capture and represent semantic relationships from massive amounts of data. Nevertheless, traditional models often fall short in intrinsic issues of linguistics, such as polysemy and homonymy. Any expert system that makes use of natural language in its core, can be affected by a weak semantic representation of text, resulting in inaccurate outcomes based on poor decisions. To mitigate such issues, we propose a novel approach called Most Suitable Sense Annotation (MSSA), that disambiguates and annotates each word by its specific sense, considering the semantic effects of its context. Our approach brings three main contributions to the semantic representation scenario: (i) an unsupervised technique that disambiguates and annotates words by their senses, (ii) a multi-sense embeddings model that can be extended to any traditional word embeddings algorithm, and (iii) a recurrent methodology that allows our models to be re-used and their representations refined. We test our approach on six different benchmarks for the word similarity task, showing that our approach can produce state-of-the-art results and outperforms several more complex state-of-the-art systems.},
	author = {Terry Ruas and William Grosky and Akiko Aizawa},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.06.026},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-sense embeddings, Natural language processing, Word similarity, Synset},
	pages = {288-303},
	title = {Multi-sense embeddings through a word sense disambiguation process},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304269},
	volume = {136},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419304269},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.06.026}}

@article{FIOK2021115771,
	abstract = {Many institutions and companies find it valuable to know how people feel about their ventures; hence, scientific research in sentiment analysis has been intensely developed over time. Automated sentiment analysis can be considered as a machine learning (ML) prediction task, with classes representing human affective states. Due to the rapid development of ML and deep learning (DL), improvements in automatic sentiment analysis performance are achieved almost every year. Since 2013, Semantic Evaluation (SemEval) has hosted a worldwide community-acknowledged competition that allows for comparisons of recent innovations. The sentiment analysis tasks focus on assessing sentiment in Twitter posts authored by various publishers and addressing multiple subjects. Our study aimed to compare selected popular and recent natural language processing methods using a new data set of Twitter posts sent to a single Twitter account. For improved comparability of our experiments with SemEval, we adopted their metrics and also deployed our models on data published for SemEval-2017. In addition, we investigated if an unsupervised ML technique applied for the detection of topics in tweets can be leveraged to improve the predictive performance of a selected transformer model. We also demonstrated how a recent explainable artificial intelligence technique can be used in Twitter sentiment analysis to gain a deeper understanding of the models' predictions. Our results show that the most recent DL language modeling approach provides the highest quality; however, this quality comes at reduced model transparency.},
	author = {Krzysztof Fiok and Waldemar Karwowski and Edgar Gutierrez and Maciej Wilamowski},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115771},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Deep learning, Sentiment analysis, Machine learning, Explainability, Twitter},
	pages = {115771},
	title = {Analysis of sentiment in tweets addressed to a single domain-specific Twitter account: Comparison of model performance and explainability of predictions},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421011428},
	volume = {186},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421011428},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115771}}

@article{ZHOU2022116560,
	abstract = {As a variant problem of aspect-based sentiment analysis (ABSA), aspect category sentiment analysis (ACSA) aims to identify the aspect categories discussed in sentences and predict their sentiment polarities. However, most aspect-based sentiment analysis (ABSA) research focuses on predicting the sentiment polarities of given aspect categories or aspect terms explicitly discussed in sentences. In contrast, aspect categories are often discussed implicitly. Additionally, most of the research does not consider the relations between contextual words and aspect categories. This paper proposes a novel Semantic Relatedness-enhanced Graph Network (SRGN) model which integrates the semantic relatedness information through an Edge-gated Graph Convolutional Network (EGCN). We introduce an ontology-based approach and a distributional approach to calculate the semantic relatedness values between contextual words and aspect categories. EGCN with the capability to aggregate multi-channel edge features, is then applied to model the semantic relatedness values in a graphical structure. We also employ an aspect--context attention module to generate aspect-specific representations. The proposed SRGN is evaluated on five datasets constructed based on SemEval 2015, SemEval 2016 and MAMC-ACSA datasets. Experimental results indicate that our proposed model outperforms the baseline models in both accuracy and F1 score.},
	author = {Tao Zhou and Kris M.Y. Law},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116560},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect category sentiment analysis, Semantic relatedness, Edge-Gated Graph Convolutional Network, Aspect--context attention},
	pages = {116560},
	title = {Semantic Relatedness Enhanced Graph Network for aspect category sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000586},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000586},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116560}}

@article{HOOSHYAR2022116670,
	abstract = {Despite the multiple deep knowledge tracing (DKT) methods developed for intelligent tutoring systems and online learning environments, there exists only a few applications of such methods in educational computer games. One key challenge is that a player may deploy several interweaved and overlapped skills during gameplay, making the assessment task nontrivial. In this research, we present a generalizable DKT approach called GameDKT that integrates state-of-the-art machine learning with domain knowledge to model the learners' knowledge state during gameplay, in an attempt to monitor and trace their proficiency level for the different skills required for educational games. Our findings reveal that GameDKT approach could successfully predict the performance of players in the coming game task using the cross-validated CNN model with accuracy and AUC of roughly 85% and 0.913, respectively, thus outperforming the MLP baseline model by up to 14%. When the performance of players is forecasted for up to four game tasks in advance, results show that the CNN model can achieve more than 70% accuracy. Interestingly, this model seems to be better and faster at identifying local patterns and it could achieve a higher performance compared to RNN and LSTM in both one-step and multi-step prediction of learners' performance in game tasks.},
	author = {Danial Hooshyar and Yueh-Min Huang and Yeongwook Yang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116670},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Learner model, Deep knowledge tracing, Educational game, Prediction of player performance, Deep learning},
	pages = {116670},
	title = {GameDKT: Deep knowledge tracing in educational games},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422001555},
	volume = {196},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422001555},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116670}}

@article{HU2017277,
	abstract = {Bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning. However, Monte Carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. In this paper, we present the stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to implement and computationally efficient. We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC. Furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach.},
	author = {Wenbo Hu and Jun Zhu and Bo Zhang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2016.10.036},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Inference, Stochastic MCMC, Subgradient MCMC, Bayesian max-margin models, Approximate detailed balance},
	pages = {277-287},
	title = {Fast sampling methods for Bayesian max-margin models},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417416305760},
	volume = {69},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417416305760},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2016.10.036}}

@article{YOUSEFIAZAR201793,
	abstract = {We present methods of extractive query-oriented single-document summarization using a deep auto-encoder (AE) to compute a feature space from the term-frequency (tf) input. Our experiments explore both local and global vocabularies. We investigate the effect of adding small random noise to local tf as the input representation of AE, and propose an ensemble of such noisy AEs which we call the Ensemble Noisy Auto-Encoder (ENAE). ENAE is a stochastic version of an AE that adds noise to the input text and selects the top sentences from an ensemble of noisy runs. In each individual experiment of the ensemble, a different randomly generated noise is added to the input representation. This architecture changes the application of the AE from a deterministic feed-forward network to a stochastic runtime model. Experiments show that the AE using local vocabularies clearly provide a more discriminative feature space and improves the recall on average 11.2%. The ENAE can make further improvements, particularly in selecting informative sentences. To cover a wide range of topics and structures, we perform experiments on two different publicly available email corpora that are specifically designed for text summarization. We used ROUGE as a fully automatic metric in text summarization and we presented the average ROUGE-2 recall for all experiments.},
	author = {Mahmood Yousefi-Azar and Len Hamey},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2016.10.017},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deep Learning, Query-oriented Summarization, Extractive Summarization, Ensemble Noisy Auto-Encoder},
	pages = {93-105},
	title = {Text summarization using unsupervised deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417416305486},
	volume = {68},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417416305486},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2016.10.017}}

@article{CARVALHO2017374,
	abstract = {Twitter has become a major tool for spreading news, for dissemination of positions and ideas, and for the commenting and analysis of current world events. However, with more than 500 million tweets flowing per day, it is necessary to find efficient ways of collecting, storing, managing, mining and visualizing all this information. This is especially relevant if one considers that Twitter has no ways of indexing tweet contents, and that the only available categorization ``mechanism'' is the #hashtag, which is totally dependent of a user's will to use it. This paper presents an intelligent platform and framework, named MISNIS - Intelligent Mining of Public Social Networks' Influence in Society - that facilitates these issues and allows a non-technical user to easily mine a given topic from a very large tweet's corpus and obtain relevant contents and indicators such as user influence or sentiment analysis. When compared to other existent similar platforms, MISNIS is an expert system that includes specifically developed intelligent techniques that: (1) Circumvent the Twitter API restrictions that limit access to 1% of all flowing tweets. The platform has been able to collect more than 80% of all flowing portuguese language tweets in Portugal when online; (2) Intelligently retrieve most tweets related to a given topic even when the tweets do not contain the topic #hashtag or user indicated keywords. A 40% increase in the number of retrieved relevant tweets has been reported in real world case studies. The platform is currently focused on Portuguese language tweets posted in Portugal. However, most developed technologies are language independent (e.g. intelligent retrieval, sentiment analysis, etc.), and technically MISNIS can be easily expanded to cover other languages and locations.},
	author = {Joao P. Carvalho and Hugo Rosa and Gaspar Brogueira and Fernando Batista},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.08.001},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Twitter, Intelligent topic mining, Fuzzy fingerprints, Text analytics, Sentiment analysis},
	pages = {374-388},
	title = {MISNIS: An intelligent platform for twitter topic mining},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417305316},
	volume = {89},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417305316},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.08.001}}

@article{MATTHIES2018330,
	abstract = {Project-based learning is based on the idea of iteratively learning for future projects from the successes and failures of past projects. This paper proposes a semi-automated implementation approach for double-loop learning in project environments. A combined application of two complementary methods is suggested for this purpose: Latent Semantic Analysis (LSA) and Analytic Network Process (ANP). By this means, the approach addresses two problems of the project management practice. First, the information overload in project environments, whereby the LSA is used for the semi-automated extraction of lessons learned from large collections of textual project documentation. Second, the lack of procedures and methods for the practical implementation of available project knowledge, whereby the ANP is used for the systematic modeling of extracted lessons learned and their integration into the evaluation of project concepts and current project management routines. Thus, the proposed implementation approach improves the ability of project-based organizations to consequently learn from past failures or successes. From a practical perspective, evident shortcomings of existing computerized double-loop learning approaches are addressed. The proposed approach contributes to the project management practice not only by demonstrating a solution for the exploration of representative and potentially new lessons from multiple combined experience reports, but also by presenting a solution for the systematic assessment of such project-governing variables and their mutual relationships as part of the decision-making in new projects. From a theoretical perspective, specific research avenues for further development of the double-loop learning concept by means of expert and intelligent systems are provided.},
	author = {Benjamin Matthies and Andr{\'e} Coners},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.12.012},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Project management, Project knowledge, Double-loop learning, Latent semantic analysis, Analytic network process},
	pages = {330-346},
	title = {Double-loop learning in project environments: An implementation approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417308266},
	volume = {96},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417308266},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.12.012}}

@article{CEREZOCOSTAS201832,
	abstract = {Citizens are actively interacting with their surroundings, especially through social media. Not only do shared posts give important information about what is happening (from the users' perspective), but also the metadata linked to these posts offer relevant data, such as the GPS-location in Location-based Social Networks (LBSNs). In this paper we introduce a global analysis of the geo-tagged posts in social media which supports (i) the detection of unexpected behavior in the city and (ii) the analysis of the posts to infer what is happening. The former is obtained by applying density-based clustering techniques, whereas the latter is consequence of applying content aggregation techniques. We have applied our methodology to a dataset obtained from Instagram activity in New York City for seven months obtaining promising results. The developed algorithms require very low resources, being able to analyze millions of data-points in commodity hardware in less than one hour without applying complex parallelization techniques. Furthermore, the solution can be easily adapted to other geo-tagged data sources without extra effort.},
	author = {H{\'e}ctor Cerezo-Costas and Ana Fern{\'a}ndez-Vilas and Manuela Mart{\'\i}n-Vicente and Rebeca {P. D{\'\i}az-Redondo}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.11.019},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Data mining, Crowd detection, Density-based clustering, Content aggregation, Event detection},
	pages = {32-42},
	title = {Discovering geo-dependent stories by combining density-based clustering and thread-based aggregation techniques},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417307698},
	volume = {95},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417307698},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.11.019}}

@article{QORIB2023118715,
	abstract = {In 2019 there was an outbreak of coronavirus pandemic also known as COVID-19. Many scientists believe that the pandemic originated from Wuhan, China, before spreading to other parts of the globe. To reduce the spread of the disease, decision makers encouraged measures such as hand washing, face masking, and social distancing. In early 2021, some countries including the United States began administering COVID-19 vaccines. Vaccination brought a relief to the public; it also generated a lot of debates from anti-vaccine and pro-vaccine groups. The controversy and debate surrounding COVID-19 vaccine influenced the decision of several people in either to accept or reject vaccination. Because of data limitations, social media data, collected through live streaming public tweets using an Application Programming Interface (API) search, is considered a viable and reliable resource to study the opinion of the public on Covid-19 vaccine hesitancy. Thus, this study examines 3 sentiment computation methods (Azure Machine Learning, VADER, and TextBlob) to analyze COVID-19 vaccine hesitancy. Five learning algorithms (Random Forest, Logistics Regression, Decision Tree, LinearSVC, and Na{\"\i}ve Bayes) with different combination of three vectorization methods (Doc2Vec, CountVectorizer, and TF-IDF) were deployed. Vocabulary normalization was threefold; potter stemming, lemmatization, and potter stemming with lemmatization. For each vocabulary normalization strategy, we designed, developed, and evaluated 42 models. The study shows that Covid-19 vaccine hesitancy slowly decreases over time; suggesting that the public gradually feels warm and optimistic about COVID-19 vaccination. Moreover, combining potter stemming and lemmatization increased model performances. Finally, the result of our experiment shows that TextBlob+TF-IDF+LinearSVC has the best performance in classifying public sentiment into positive, neutral, or negative with an accuracy, precision, recall and F1 score of 0.96752, 0.96921, 0.92807 and 0.94702 respectively. It means that the best performance was achieved when using TextBlob sentiment score, with TF-IDF vectorization and LinearSVC classification model. We also found out that combining two vectorizations (CountVectorizer and TF-IDF) decreases model accuracy.},
	author = {Miftahul Qorib and Timothy Oladunni and Max Denis and Esther Ososanya and Paul Cotae},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118715},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Covid-19, Vaccine Hesitancy, Sentiment Analysis, Machine Learning, Twitter},
	pages = {118715},
	title = {Covid-19 vaccine hesitancy: Text mining, sentiment analysis and machine learning on COVID-19 vaccination Twitter dataset},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017407},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422017407},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118715}}

@article{PENSA201718,
	abstract = {During our digital social life, we share terabytes of information that can potentially reveal private facts and personality traits to unexpected strangers. Despite the research efforts aiming at providing efficient solutions for the anonymization of huge databases (including networked data), in online social networks the most powerful privacy protection ``weapons'' are the users themselves. However, most users are not aware of the risks derived by the indiscriminate disclosure of their personal data. Moreover, even when social networking platforms allow their participants to control the privacy level of every published item, adopting a correct privacy policy is often an annoying and frustrating task and many users prefer to adopt simple but extreme strategies such as ``visible-to-all'' (exposing themselves to the highest risk), or ``hidden-to-all'' (wasting the positive social and economic potential of social networking websites). In this paper we propose a theoretical framework to i) measure the privacy risk of the users and alert them whenever their privacy is compromised and ii) help the users customize semi-automatically their privacy settings by limiting the number of manual operations. By investigating the relationship between the privacy measure and privacy preferences of real Facebook users, we show the effectiveness of our framework.},
	author = {Ruggero G. Pensa and Gianpiero {Di Blasi}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.05.054},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Privacy measures, Online social networks, Active learning},
	pages = {18-31},
	title = {A privacy self-assessment framework for online social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417303767},
	volume = {86},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417303767},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.05.054}}

@article{DU2018190,
	abstract = {Currently, modeling and analyzing timed compatibility of Petri net based mediation-aided Web service composition by model checking are attracting increasing attention in the expert and intelligent systems community. However, existing methods cannot handle mediation-aided Web service composition involving complex mediation transitions, suffer from low efficiency owing to the larger size of time automata (TA) models, or are unable to automatically check temporal constraints whose activities do not include exchanged messages by observing TA. In this paper, we present a novel three-stage approach for analyzing timed compatibility of mediation-aided Web service composition via model checking. First, stage 1 treats each service in Petri net based mediation-aided Web service composition as a fragment. Second, stage 2 transforms fragments into a time automata net (TAN) based on structure transformation and interactive message transformation. Finally, stage 3 checks all types of temporal constraints. The main impact of our approach on expert and intelligent systems involves the following aspects: 1) mediation-aided service composition with complex mediation transitions can be dealt with; 2) compact TAN can be constructed for fragments in which the number of states and arcs is dramatically decreased and the verification time is extremely reduced compared with existing methods; and 3) temporal constraints whose activities have or do not have exchanged messages can be located in TAN. The main significance of our approach in the field of expert and intelligent systems is that it can greatly reduce the risk of making business decisions and the cost of handling temporal violations, and improves the innovation capability of enterprises.},
	author = {Yanhua Du and Benyuan Yang and Hesuan Hu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.06.005},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Mediation-aided service composition, Model checking, Timed compatibility, Temporal constraints, Petri nets},
	pages = {190-207},
	title = {Model checking of timed compatibility for mediation-aided web service composition: A three stage approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418303518},
	volume = {112},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418303518},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.06.005}}

@article{KWON2021114488,
	abstract = {In digital forensics, user profiling aims to predict characteristics of the user from digital evidence extracted from digital devices (e.g. smartphone, laptop, tablet). Previous researches showed promising results, but there are limitations to apply practical investigations. The researches so far have focused only on specific applications, devices, or operating systems by analyzing the order of execution or volatile data such as network traffic and online content. This paper introduces a user profiling method, named Entity Profiling with Binary Predicates (EPBP) model, which analyzes non-volatile data remained on digital devices. The proposed model defines that a user has two properties: tendency and impact, which indicate patterns of application usage. Based on the attributes, the EPBP model generates users' profiles and performs similarity analysis to differentiate between the users. We also present methods for clustering and anomaly detection through real case studies.},
	author = {Hongkyun Kwon and Sangjin Lee and Doowon Jeong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114488},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {User profiling, Digital forensics, Application usage, User similarity, Anomaly detection},
	pages = {114488},
	title = {User profiling via application usage pattern on digital devices for digital forensics},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420311349},
	volume = {168},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420311349},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114488}}

@article{GUPTA201949,
	abstract = {Summarization, is to reduce the size of the document while preserving the meaning, is one of the most researched areas among the Natural Language Processing (NLP) community. Summarization techniques, on the basis of whether the exact sentences are considered as they appear in the original text or new sentences are generated using natural language processing techniques, are categorized into extractive and abstractive techniques. Extractive summarization has been a very extensively researched topic and has reached to its maturity stage. Now the research has shifted towards the abstractive summarization. The complexities underlying with the natural language text makes abstractive summarization a difficult and a challenging task. This paper presents a comprehensive review of the various works performed in abstractive summarization field. For this purpose, we have selected the recent papers on this topic from Elsevier, ACM, IEEE, Springer, ACL Anthology, Cornell University Library and Google Scholar. The papers are categorized according to the type of abstractive technique used. The paper lists down the various challenges and discusses the future direction for research in this field. Along with these, we have identified the advantages and disadvantages of various methods used for abstractive summarization. We have also listed down the various tools which have been used or developed by researchers for abstractive summarization. The paper also discusses the evaluation techniques being used for assessing the abstractive summaries.},
	author = {Som Gupta and S. K Gupta},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.12.011},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Abstractive summarization, Concept finding, Semantic-Based summarization, Ontology-Based summarization, Deep learning},
	pages = {49-65},
	title = {Abstractive summarization: An overview of the state of the art},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418307735},
	volume = {121},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418307735},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.12.011}}

@article{YANG2018206,
	abstract = {After digitally designing components of vehicles, a design team creates a virtual manufacturing environment that resembles actual manufacturing facilities. During this digital pre-assembly process, a review team examines each component, and records its problems and requirements in part verification reports. Once these reports are delivered to specific design team responsible for each part, the design team can make appropriate adjustments to their designs. This digital pre-assembly process can evaluate and prevent flaws in design prior to actual manufacturing, improving production quality and reducing manufacturing cost. As these reports are written in free text form, they, however, are not fully utilized for understanding problems arising from the design process. This paper proposes a method of applying text mining techniques on verification reports to extract insights for quality improvement. In this paper, following three text mining approaches are proposed: (1) Extracting n-grams for text preprocessing and constructing domain ontology; (2) Extracting meaningful insights from text preprocessing; (3) Creating intuitive visual tools to understand the extracted insights. The proposed method is applied on approximately 140,000 reports, and is validated through the quality of the answers obtained for the questions posed by the domain experts. The proposed method successfully extracts useful information from the text database, and provides intuitive graphical interface, thereby satisfying the need of the domain experts. This paper proposes a systematic framework of transforming huge amount of raw text data into intuitive visualization. Through this framework, meaningful knowledge can be extracted, analyzed and shared to improve the quality of the products. Main contribution of our paper is that it proposes a framework for knowledge extraction from pre-assembly process. Not only does it systematically arrange the data, but it also combines various data sources and creates a knowledge system to improve efficiency of the design process.},
	author = {Jiwon Yang and Eunji Kim and Minhoe Hur and Sungzoon Cho and Myungbin Han and Iksang Seo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.09.002},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Knowledge extraction, Visualization, Digital design},
	pages = {206-215},
	title = {Knowledge extraction and visualization of digital design process},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417306036},
	volume = {92},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417306036},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.09.002}}

@article{HONG2019112813,
	abstract = {The cold-start problem is one of the critical challenges in personalized recommender systems. A lot of existing work has been studied to exploit a user-item rating matrix as well as additional information for users/items, e.g., user profiles, item contents, and social relationships among users. However, because existing work is primarily biased to the auxiliary information for users/items, it is difficult to identify various and reliable item neighbors that are relevant to cold-start items. To alleviate this limitation, we propose a new crowd-enabled framework, called CrowdStart, which is an integrated human-machine approach for new item recommendation. The main contributions of the CrowdStart framework are two-fold: (1) To find various and reliable item neighbors for new items, we design two-step crowdsourcing tasks that harness not only machine-only algorithms but also the knowledge of crowd workers (including a few experts and a large number of non-expert workers in a crowdsourcing platform). (2) We develop a novel hybrid model to exploit the user-item rating matrix, the content information about items, and the crowd-based item neighbors from human knowledge into new item recommendation. To evaluate the effectiveness of the CrowdStart framework, we conduct extensive experiments including both a user study and simulation tests. Through the empirical study, we found that the CrowdStart framework provides relevant, diverse, reliable, and explainable crowd-based neighbors for new items and the crowd-based neighbors are meaningful for improving the accuracy of new item recommendation. The datasets and detailed experimental results are available at https://goo.gl/1iXTUE.},
	author = {Dong-Gyun Hong and Yeon-Chang Lee and Jongwuk Lee and Sang-Wook Kim},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.07.030},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Collaborative filtering, New item recommendation, Crowdsourcing},
	pages = {112813},
	title = {CrowdStart: Warming up cold-start items using crowdsourcing},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419305093},
	volume = {138},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419305093},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.07.030}}

@article{NGUYEN201771,
	abstract = {Traditional summarization methods only use the internal information of a Web document while ignoring its social information such as tweets from Twitter, which can provide a perspective viewpoint for readers towards an event. This paper proposes a framework named SoRTESum to take the advantages of social information such as document content reflection to extract summary sentences and social messages. In order to do that, the summarization was formulated in two steps: scoring and ranking. In the scoring step, the score of a sentence or social message is computed by using intra-relation and inter-relation which integrate the support of local and social information in a mutual reinforcement form. To calculate these relations, 16 features are proposed. After scoring, the summarization is generated by selecting top m ranked sentences and social messages. SoRTESum was extensively evaluated on two datasets. Promising results show that: (i) SoRTESum obtains significant improvements of ROUGE-scores over state-of-the-art baselines and competitive results with the learning to rank approach trained by RankBoost and (ii) combining intra-relation and inter-relation benefits single-document summarization.},
	author = {Minh-Tien Nguyen and Minh-Le Nguyen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.01.023},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Data mining, Information retrieval, Document summarization, Social context summarization, RTE, Ranking, Unsupervised learning},
	pages = {71-84},
	title = {Intra-relation or inter-relation?: Exploiting social information for Web document summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417300325},
	volume = {76},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417300325},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.01.023}}

@article{ZHU20188,
	abstract = {With the increasing popularity of large scale Knowledge Graph (KG)s, many applications such as semantic analysis, search and question answering need to link entity mentions in texts to entities in KGs. Because of the polysemy problem in natural language, entity disambiguation is thus a key problem in current research. Existing disambiguation methods have considered entity prominence, context similarity and entity-entity relatedness to discriminate ambiguous entities, which are mainly working on document or paragraph level texts containing rich contextual information, and based on lexical matching for computing context similarity. When meeting short texts containing limited contextual information, such as web queries, questions and tweets, those conventional disambiguation methods are not good at handling single entity mention and measuring context similarity. In order to enhance the performance of disambiguation methods based on context similarity with such short texts, we propose SCSNED method for disambiguation based on semantic similarity between contextual words and informative words of entities in KGs. Specially, we exploit the effectiveness of both knowledge-based and corpus-based semantic similarity methods for entity disambiguation with SCSNED. Moreover, we propose a Category2Vec embedding model based on joint learning of word and category embedding, in order to compute word-category similarity for entity disambiguation. We show the effectiveness of these proposed methods with illustrative examples, and evaluate their effectiveness in a comparative experiment for entity disambiguation in real world web queries, questions and tweets. The experimental results have identified the effectiveness of different semantic similarity methods, and demonstrated the improvement of semantic similarity methods in SCSNED and Category2Vec over the conventional context similarity baseline. We further compare the proposed approaches with the state of the art entity disambiguation systems and show the performances of the proposed approaches are among the best performing systems. In addition, one important feature of the proposed approaches using semantic similarity, is the potential application on any existing KGs since they mainly use common features of entity descriptions and categories. Another contribution of the paper is an updated survey on background of entity disambiguation in KGs and semantic similarity methods.},
	author = {Ganggao Zhu and Carlos A. Iglesias},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.02.011},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Entity linking, Named entity disambiguation, Context similarity, Semantic similarity, Word embedding, Knowledge graph},
	pages = {8-24},
	title = {Exploiting semantic similarity for named entity disambiguation in knowledge graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418300897},
	volume = {101},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418300897},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.02.011}}

@article{SHAO2022118221,
	abstract = {The past decade has witnessed the rapid development of Artificial Intelligence (AI), especially the explosion of deep learning-related connectionist approaches. This study combines traditional literature review, bibliometric methods, and the Science of Science (SciSci) theory to scrutinize the development context of AI in the last decade on AMiner.44www.aminer.cn, AMiner is an academic mining system, which will be introduced in the following section. With the assistance of AMiner tools and datasets, this paper aims to describe a further explicit context and evolution of AI in the past decade from the development of connectionist approaches. Five aspects of the past decade are highlighted: self-learning and self-coding algorithms, Recurrent Neural Networks (RNN) algorithms, reinforcement learning, pre-trained models, and other typical deep learning algorithms, which represent the significant progress of this field. By combining these critical parts, we then summarize the current limitations and corresponding future of AI trends in the next decade and discuss some topics about the next generation of AI. Discoveries in this paper will benefit AI research in promoting understanding of the current critical stage and future trends of AI development and the AI industry in the dramatic ascendant for the academic research results transformation and its industrial layout.},
	author = {Zhou Shao and Ruoyan Zhao and Sha Yuan and Ming Ding and Yongli Wang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118221},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Artificial Intelligence, Frontier research, Future trend, Data analytics, Science of Science},
	pages = {118221},
	title = {Tracing the evolution of AI in the past decade and forecasting the emerging trends},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013732},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422013732},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118221}}

@article{WANG2022117317,
	abstract = {A large amount of information exists in many e-commerce and review websites as a valuable source for recommender systems. Recent solutions focus on exploring the correlation between sentiment and textual reviews in the review-based recommendation. However, these studies usually pay less attention to the differences of different users in sentimental expression styles or language usage habits when a user writes reviews. In this work, we argue that the individual reviewing behavior is closely related to personality, and sentimental expression is a manifestation of personality. Therefore, we propose a novel Persona-driven Sentimental Attentive Recommendation model (named PSAR) via personalized sentimental interactive representation learning for the review-based recommendation. The proposed model is devised to learn fragment-level and sequence-level personalized sentimental representation simultaneously from reviews. Besides, an attentive persona-driven interaction module is designed to capture word-level usage habits and sentence-level analogous tones. Comprehensive experimental results on four real-world datasets demonstrate that our model outperforms the state-of-the-art methods.},
	author = {Peipei Wang and Lin Li and Ru Wang and Xinhao Zheng and Jiaxi He and Guandong Xu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117317},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Review mining, Sentimental representation learning, Attention neural networks, Review-based recommendation},
	pages = {117317},
	title = {Learning persona-driven personalized sentimental representation for review-based recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422006789},
	volume = {203},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422006789},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117317}}

@article{JANG2021114042,
	abstract = {The role of text mining based on technological documents such as patents is important in the research field of technology intelligence for technology R&D planning. In addition, WordNet, an English-based lexical database, is widely used for pre-processing text data such as word lemmatization and synonym search. However, technological vocabulary information is complex and specific, and WordNet's ability to analyze technological information is limited in its reflecting technological features. Thus, to improve the text mining performance of technological information, this study proposes a methodology for designing a TechWord-based lexical database that is based on the lexical characteristics of technological words that are differentiated from general words. To do this, we define TechWord, a technology lexical information, and construct a TechSynset, a synonym set between TechWords. First, through dependency parsing between words, TechWord, a unit word that describes a technology, is structured and identifies nouns and verbs. The importance of connectivity is investigated by a network centrality index analysis based on the dependency relations of words. Subsequently, to search for synonyms suitable for the target technology domain, a TechSynset is constructed through synset information, with an additional analysis that calculates cosine similarity based on a word embedding vector. Applying the proposed methodology to the actual technology-related information analysis, we collect patent data on the technological fields of the automotive field, and present the results of the TechWord and TechSynset. This study improves technological information-based text mining by structuring the word-to-word link information in technological documents based on an automated process.},
	author = {Hyejin Jang and Yujin Jeong and Byungun Yoon},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114042},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Patent mining, Natural language processing, Text mining, Lexical analysis, WordNet},
	pages = {114042},
	title = {TechWord: Development of a technology lexical database for structuring textual technology information based on natural language processing},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420308101},
	volume = {164},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420308101},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114042}}

@article{RINALDI2021114320,
	abstract = {The amount of available multimedia data in different formats and from different sources increases everyday. From an information retrieval point of view, this high volume and heterogeneity of data involves several issues to be addressed related to information overload and lacks of well structured information. Even if modern information retrieval systems offer to the user manifold search options, it is still hard to find systems with optimal performances in the document seeking process starting from a given topic. In recent years, several frameworks have been proposed and developed to support this task based on different models and techniques. In this paper we propose a semantic approach to document classification using both textual and visual topic detection techniques based on deep neural networks and multimedia knowledge graph. A semantic multimedia knowledge base has been exploited and several experimental results show the effectiveness of our proposed approach.},
	author = {Antonio M. Rinaldi and Cristiano Russo and Cristian Tommasino},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114320},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multimedia topic detection, Document classification, Semantic analysis, Ontologies, Big data, Deep neural networks, Knowledge graph},
	pages = {114320},
	title = {A semantic approach for document classification using deep neural networks and multimedia knowledge graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310149},
	volume = {169},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420310149},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114320}}

@article{JAMES2017479,
	abstract = {Mechanisms for collecting unstructured feedback (i.e., text comments) from patients of healthcare providers have become commonplace, but analysis techniques to examine such feedback have not been frequently applied in this domain. To fill this gap, we apply a text mining methodology to a large set of textual feedback of physicians by their patients and relate the textual commentary to their numeric ratings. While perceptions of healthcare service quality in the form of numeric ratings are easy to aggregate, freeform textual commentary presents more challenges to extracting useful information. Our methodology explores aggregation of the textual commentary using a topic analysis procedure (i.e., latent Dirichlet allocation) and a sentiment tool (i.e., Diction). We then explore how the extracted topic areas and expressed sentiments relate to the physicians' quantitative ratings of service quality from both patients and other physicians. We analyze 23,537 numeric ratings plus textual feedback provided by patients of 3,712 physicians who have also been recommended by other physicians, and determine process quality satisfaction is an important driver of patient perceived quality, whereas clinical quality better reflects physician perceived quality. Our findings lead us to suggest that to maximize the usefulness of online reviews of physicians, potential patients should parse them for particular quality elements they wish to assess and interpret them within the scope of those quality elements.},
	author = {Tabitha L. James and Eduardo D. {Villacis Calderon} and Deborah F. Cook},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2016.11.004},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Service quality, Text mining, Healthcare},
	pages = {479-492},
	title = {Exploring patient perceptions of healthcare service quality through analysis of unstructured feedback},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417416306273},
	volume = {71},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417416306273},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2016.11.004}}

@article{LI2018107,
	abstract = {Being able to understand the implicit power structures and dynamics among members plays a crucial role for the management of the organization. This paper introduces a novel and comprehensive approach to analyzing organizational discourse data. The proposed approach provides a holistic view of the power structure implied by the communications. The paper contributes to the domain of text mining by integrating various text-mining techniques to demonstrating different aspects of a power structure within an organization. It also contributes to the domain of supply chain management by using the conventional communication discourse method as the guideline for the development of the tool. We applied the proposed approach to a seven-year collection of meeting minutes from a co-op and our findings were largely confirmed by members of the organization. We provide a roadmap of using the multi-aspect approach to analyzing organizational discourse data in supply networks.},
	author = {Jiexun Li and Zhaohui Wu and Bin Zhu and Kaiquan Xu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.11.009},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Supply networks, Organizational sense-making, Text mining, Social network analysis, Discourse analysis, Sentiment analysis},
	note = {Big Data Analytics for Business Intelligence},
	pages = {107-119},
	title = {Making sense of organization dynamics using text analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417307595},
	volume = {111},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417307595},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.11.009}}

@article{DAI2023118841,
	abstract = {The outbreak of COVID-19 brings almost the biggest explosions of scientific literature ever. Facing such volume literature, it is hard for researches to find desired citation when carrying out COVID-19 related research, especially for junior researchers. This paper presents a novel neural network based method, called citation relational BERT with heterogeneous deep graph convolutional network (CRB-HDGCN), for COVID-19 inline citation recommendation task. The CRB-HDGCN contains two main stages. The first stage is to enhance the representation learning of BERT model for COVID-19 inline citation recommendation task through CRB. To achieve the above goal, an augmented citation sentence corpus, which replaces the citation placeholder with the title of the cited papers, is used to lightly retrain BERT model. In addition, we extract three types of sentence pair according citation relation, and establish sentence prediction tasks to further fine-tune the BERT model. The second stage is to learn effective dense vector of nodes among COVID-19 bibliographic graph through HDGCN. The HDGCN contains four layers which are essentially all sub neural networks. The first layer is initial embedding layer which generates initial input vectors with fixed size through CRB and a multilayer perceptron. The second layer is a heterogeneous graph convolutional layer. In this layer, we expand traditional homogeneous graph convolutional network into heterogeneous by subtly adding heterogeneous nodes and relations. The third layer is a deep attention layer. This layer uses trainable project vectors to reweight the node importance simultaneously according to both node types and convolution layers, which further promotes the performance of learnt node vectors. The last decoder layer recovers the graph structure and let the whole network trainable. The recommendation is finally achieved by integrating the high performance heterogeneous vectors learnt from CRB-HDGCN with the query vectors. We conduct experiments on the CORD-19 and LitCovid datasets. The results show that compared with the second best method CO-Search, CRB-HDGCN improves MAP, MRR, P@100 and R@100 with 21.8%, 22.7%, 37.6% and 21.2% on CORD-19, and 29.1%, 25.9%, 15.3% and 11.3% on LitCovid, respectively.},
	author = {Tao Dai and Jie Zhao and Dehong Li and Shun Tian and Xiangmo Zhao and Shirui Pan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118841},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {COVID-19 citation recommendation, Deep graph convolutional network, Heterogeneous graph, Citation enhanced BERT, Text representation learning},
	pages = {118841},
	title = {Heterogeneous deep graph convolutional network with citation relational BERT for COVID-19 inline citation recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422018590},
	volume = {213},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422018590},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118841}}

@article{KHALID2022115926,
	abstract = {Massive Open Online Courses (MOOCs) are receiving attention from learners because MOOCs enable them to satisfy their learning needs through an open, participatory, and distributed way. With the increased interest from learners, the number of MOOCs available is increasing which has increased options for learners. This as a result has created the need for recommendation systems that help learners select suitable MOOCs. This literature review covers analysis of recommender systems (RSs) that have been implemented in MOOCs with the goal of providing insights on the trends reported in the academic literature on recommender systems in MOOCs. The review discusses different recommendation techniques, recommendation types and evaluation techniques that have been used and reported on. This review includes research work over eight years, i.e. from 1st January 2012 to 17th November 2020. After the filtering process, 67 papers were selected from journals and conferences from four academic databases (i.e., IEEE, ACM, Science Direct, and Springer). A framework is designed that classifies literature on the basis of both design and evaluation aspects of RS in MOOCs. This review concludes by highlighting gaps found in the literature.},
	author = {Asra Khalid and Karsten Lundqvist and Anne Yates},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115926},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Massive Open Online Courses (MOOCs), Recommender systems (RSs), Recommendation techniques, Evaluation techniques and classification framework},
	pages = {115926},
	title = {A literature review of implemented recommendation techniques used in Massive Open online Courses},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742101280X},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742101280X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115926}}

@article{AKKASI2021115162,
	abstract = {Word Sense Induction (WSI) concerns the automatic identification of the various senses of polysemous words. Any improvement in this process can directly affect the quality of the applications in which knowing the word's senses is important. For example, word sense disambiguation, information retrieval, and clustering of web search result in lexically ambiguous queries. In this paper, we propose a novel WSI model that makes use of automatically generated lexical substitutes for a target word to construct a graph and data preparation for the next steps. Following the data preparation step, we make use of Leader--Follower graph clustering to find the basic senses of the target word. The senses of the target word inside the remaining or new upcoming instances will be decided according to their contextual embedding's similarities with the basic sense. Besides, to make the number of found sense groups of a target word much closer to the reality, we apply post-processing at the end. The results of experiments on SemEval2010 dataset confirm that the proposed method outperforms all the state-of-the-art solutions in terms of both harmonic and geometric v-measure and f-score with a lower average number of sense groups.},
	author = {Abbas Akkasi and Jan Snajder},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115162},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Word sense induction, Natural language processing, Graph clustering, Clustering refinement, Lexical substitution},
	pages = {115162},
	title = {Word sense induction using leader-follower clustering of automatically generated lexical substitutes},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421006035},
	volume = {181},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421006035},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115162}}

@article{KAUR2023118997,
	abstract = {The assignment of codes to free-text clinical narratives have long been recognised to be beneficial for secondary uses such as funding, insurance claim processing and research. The current scenario of assigning clinical codes is a manual process which is very expensive, time-consuming and error prone. In recent years, many researchers have studied the use of Natural Language Processing (NLP), related machine learning and deep learning methods and techniques to resolve the problem of manual coding of clinical narratives and to assist human coders to assign clinical codes more accurately and efficiently. The main objective of this systematic literature review is to provide a comprehensive overview of automated clinical coding systems that utilise appropriate NLP, machine learning and deep learning methods and techniques to assign the International Classification of Diseases (ICD) codes to discharge summaries. We have followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and conducted a comprehensive search of publications from January, 2010 to December 2021 in four high quality academic databases: PubMed, ScienceDirect, Association for Computing Machinery (ACM) Digital Library, and the Association for Computational Linguistics (ACL) Anthology. We reviewed 6128 publications; 42 met the inclusion criteria. This review identified: 6 datasets having discharge summaries (2 publicly available, 4 acquired from hospitals); 14 NLP techniques along with some other data extraction processes, different feature extraction and embedding techniques. The review also shows that there is a significant increase in the use of deep learning models compared to machine learning. To measure the performance of classification methods, different evaluation metrics are used. Efforts are still required to improve ICD code prediction accuracy, availability of large-scale de-identified clinical corpora with the latest version of the classification system. This can be a platform to guide and share knowledge with the less experienced coders and researchers.},
	author = {Rajvir Kaur and Jeewani Anupama Ginige and Oliver Obst},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118997},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Computer assisted clinical coding, Clinical classification and coding, Discharge summaries, Natural Language Processing, Machine learning, Deep learning},
	pages = {118997},
	title = {AI-based ICD coding and classification approaches using discharge summaries: A systematic literature review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422020152},
	volume = {213},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422020152},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118997}}

@article{THIRUMOORTHY2021115040,
	abstract = {In this digital era, millions of Internet users are contributing vast amounts of data in the form of unstructured text documents. Organizing this material is a tedious task. The clustering of text document plays a vital role for organizing these unstructured text documents. In our paper, we make use of Hybrid Jaya Optimization algorithm (HJO) for text Document Clustering (DC), referred to as HJO-DC. We have used the Silhouette index as a metric to measure the quality of a solution. The proposed work is compared with partitioning techniques such as K-Means and K-Medoids and metaheuristic techniques such as Genetic algorithm, Cuckoo Search, Particle Swarm Optimizer, Firefly and Grey Wolf Optimizer. Remarkably, the proposed algorithm achieves the highest quality clustering in all benchmark examples.},
	author = {Karpagalingam Thirumoorthy and Karuppaiah Muneeswaran},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115040},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Document clustering, Jaya optimization, Cosine similarity, Crossover, Mutation},
	pages = {115040},
	title = {A hybrid approach for text document clustering using Jaya optimization algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421004814},
	volume = {178},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421004814},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115040}}

@article{MOJRIAN2021114555,
	abstract = {The explosive growth of textual data on the web and the problem of obtaining desired information through this enormous volume of data has led to a dramatic increase in demand for developing automatic text summarization systems. For this reason, this paper presents a novel multi-document text summarization approach, called MTSQIGA, which extracts salient sentences from source document collection to generate the summary. The proposed generic summarizer models extractive summarization as a binary optimization problem that applies a modified quantum-inspired genetic algorithm (QIGA) in its processing stage to find the best solution. Objective function of our approach plays an important role in optimizing linear combination of coverage, relevance, and redundancy factors which consists of six sentence scoring measures. To ensures the generation of a summary with predefined length limit, the presented QIGA employs a modified quantum measurement and a self-adaptive quantum rotation gate based on the quality and length of the summary. Evaluation of the proposed system was performed on DUC 2005 and 2007 benchmark datasets in terms of ROUGE standard measures. Comparison of MTSQIGA with existing state-of-the-art approaches for multi-document summarization shows superior performance of the proposed systems over other methods on both existing benchmark datasets. It also indicates promising efficiency of our proposed algorithm on applying quantum-inspired genetic algorithm to the text summarization tasks.},
	author = {Mohammad Mojrian and Seyed Abolghasem Mirroshandel},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114555},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Extractive summarization, Multi-document summarization, Quantum-inspired genetic algorithm, Objective function, Self-adaptive rotation gate, Quantum measurement},
	pages = {114555},
	title = {A novel extractive multi-document text summarization system using quantum-inspired genetic algorithm: MTSQIGA},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420311994},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420311994},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114555}}

@article{DOKUZ2017113,
	abstract = {Socially important locations are places that are frequently visited by social media users in their social media life. Discovering socially interesting, popular or important locations from a location based social network has recently become important for recommender systems, targeted advertisement applications, and urban planning, etc. However, discovering socially important locations from a social network is challenging due to the data size and variety, spatial and temporal dimensions of the datasets, the need for developing computationally efficient approaches, and the difficulty of modeling human behavior. In the literature, several studies are conducted for discovering socially important locations. However, majority of these studies focused on discovering locations without considering historical data of social media users. They focused on analysis of data of social groups without considering each user's preferences in these groups. In this study, we proposed a method and interest measures to discover socially important locations that consider historical user data and each user's (individual's) preferences. The proposed algorithm was compared with a na{\"\i}ve alternative using real-life Twitter dataset. The results showed that the proposed algorithm outperforms the na{\"\i}ve alternative.},
	author = {Ahmet Sakir Dokuz and Mete Celik},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.05.068},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Socially important locations mining, Spatial social media mining, Historical social media data analysis, Social media networking sites, Twitter},
	pages = {113-124},
	title = {Discovering socially important locations of social media users},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417303949},
	volume = {86},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417303949},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.05.068}}

@article{SAEED2019115,
	abstract = {With increasing popularity of social media, Twitter has become one of the leading platforms to report events in real-time. Detecting events from Twitter stream requires complex techniques. Event-related trending topics consist of a group of words which successfully detect and identify events. Event detection techniques must be scalable and robust, so that they can deal with the huge volume and noise associated with social media. Existing event detection methods mostly rely on burstiness, mainly the frequency of words and their co-occurrences. However, burstiness sometimes dominates other relevant details in the data which could be equally significant. Besides, the topological and temporal relationships in the data are often ignored. In this work, we propose a novel graph-based approach, called the Enhanced Heartbeat Graph (EHG), which detects events efficiently. EHG suppresses dominating topics in the subsequent data stream, after their first detection. Experimental results on three real-world datasets (i.e., Football Association Challenge Cup Final, Super Tuesday, and the US Election 2012) show superior performance of the proposed approach in comparison to the state-of-the-art techniques.},
	author = {Zafar Saeed and Rabeeh Ayaz Abbasi and Imran Razzak and Onaiza Maqbool and Abida Sadaf and Guandong Xu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.06.005},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Event detection, Twitter, Text stream, Emerging trends, Dynamic graph, Time series network, Big data},
	pages = {115-132},
	title = {Enhanced Heartbeat Graph for emerging event detection on Twitter using time series networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304051},
	volume = {136},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419304051},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.06.005}}

@article{BI2022118352,
	abstract = {With the prosperous development of unconventional oil and gas (UOG) began in the mid-1990 s, the proliferation of digital textual compliance reports from the UOG production life-cycle makes it imperative for experts to develop efficient ways of supporting emergency responses based on the textual based data sources. In this respect, we utilized the UOG compliance reports from the Pennsylvania Department of Environmental Protection from 2000 to 2019, then established an attentive neural-network framework to support on-site emergency responses. The advantages of attentive-based neural networks over the other mechanisms are that it not only generates powerful contextual vectors for follow-up tasks but also it allows us to observe the importance of violation factors with respect to different scenarios. The experimental results show that our model can extract valid representation from narrative texts in UOG violation compliance reports and achieve high performance in emergency response. At the same time, we obtained two intriguing practical implications: first, geographical and time characteristics are powerful indicators for supporting decision making in UOG on-site emergency responses; second, there is an urgent need for governments to implement different inspection strategies according to unique UOG sites rather than counties concerning specific geological features, which benefits from saving human labor and financial expenditures.},
	author = {Dan Bi and Ju-e Guo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118352},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Unconventional oil and gas, Violation analysis, Attentive neural network, Emergency response, Decision support system},
	pages = {118352},
	title = {Introducing attentive neural networks into unconventional oil and gas violation analysis and emergency response system},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014713},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014713},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118352}}

@article{ZHU2023118364,
	abstract = {Financial markets are based on the daily movements of thousands of tradable assets, such as stocks, resulting in billion-dollar trade volumes and affecting investors and companies around the globe. In this volatile and high-stakes environment, financial-service firms employ analysts to create compact market commentaries that serve as insightful summaries with key pieces of information. In this work, we attempt to automate this process by formally defining and algorithmically solving the Market Commentary Generation (MCG) problem. In addition to saving time and cost via automation, our approach makes a number of contributions that differentiate it from previous related work. These include the consideration of thousands of underlying time series, the ability to capture and encode significant market events that involve multiple financial entities, and the ability to deliver high quality commentary even in the presence of small and unlabeled historical datasets. Finally, our approach takes into account the strict compliance requirements of the finance domain, which prevent the use of black-box methods that can produce language that violates key rules and regulations. We compare our work against competitive baselines via an evaluation that includes both qualitative and quantitative experiments.},
	author = {Di Zhu and Theodoros Lappas and Thami Rachidi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118364},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {NLP, NLG, Text mining, Summarization, Financial markets},
	pages = {118364},
	title = {Commentary generation for financial markets},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014798},
	volume = {211},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014798},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118364}}

@article{IWATSUKI2022115840,
	abstract = {Formulaic expressions, such as `in this paper we propose', are helpful for authors of scholarly papers because they convey communicative functions; in the above, it is `showing the aim of this paper'. Thus, resources of formulaic expressions, such as a dictionary, that could be looked up easily would be useful. However, forms of formulaic expressions can often vary to a great extent. For example, `in this paper we propose', `in this study we propose' and `in this paper we propose a new method to' are all regarded as formulaic expressions. Such a diversity of spans and forms causes problems in both extraction and evaluation of formulaic expressions. In this paper, we propose a new approach that is robust to variation of spans and forms of formulaic expressions. Our approach regards a sentence as consisting of a formulaic part and non-formulaic part. Then, instead of trying to extract formulaic expressions from a whole corpus, by extracting them from each sentence, different forms can be dealt with at once. Based on this formulation, to avoid the diversity problem, we propose evaluating extraction methods by how much they convey specific communicative functions rather than by comparing extracted expressions to an existing lexicon. We also propose a new extraction method that utilises named entities and dependency structures to remove the non-formulaic part from a sentence. Experimental results show that the proposed extraction method achieved the best performance compared to other existing methods.},
	author = {Kenichi Iwatsuki and Florian Boudin and Akiko Aizawa},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115840},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Formulaic expressions, Multi-word expressions, Writing assistance, English for academic purposes},
	pages = {115840},
	title = {Extraction and evaluation of formulaic expressions used in scholarly papers},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742101201X},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742101201X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115840}}

@article{LI2022116600,
	abstract = {The explosive growth of text data has attracted many researchers to explore the efficient method to extract valuable hidden information. Many technologies, especially deep learning methods, have achieved great success in text analysis. However, the most powerful methods always require a considerable quantity of data for training, which may suffer from imbalanced data in some cases. In this paper, we propose a network-based Convolution Neural Network (NCNN) to mitigate the effect of imbalanced data. The proposed model first generates new synthetic samples for the imbalanced data based on the random walking of the network. Then an extra layer called Polar Layer is introduced to connect the output from the network model of the text to the classical CNN. Two electing strategies (n-NCNN and x-NCNN) are proposed to improve the performance of NCNN further. In the experimental section, the proposed model is applied to Reuters 21578 and WebKb. By comparing with six approaches, we prove the effectiveness of the proposed NCNN model on the imbalanced text data.},
	author = {Keping Li and Dongyang Yan and Yanyan Liu and Qiaozhen Zhu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116600},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Complex Network, CNN, Text Analysis, Imbalanced Data, Random Walk},
	pages = {116600},
	title = {A network-based feature extraction model for imbalanced text data},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742200094X},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742200094X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116600}}

@article{KAKISIM2019256,
	abstract = {Networked data is data composed of network objects and links. Network objects are characterized by high dimensional attributes and by links indicating the relationships among these objects. However, traditional feature selection and feature extraction methods consider only attribute information, thus ignoring link information. In the presented work, we propose a new unsupervised binary feature construction method (NetBFC) for networked data that reconstructs attributes for each object by exploiting link information. By exploring similar objects in the network and associating them, our method increases the similarities between objects with high probability of being in the same group. The proposed method enables local attribute enrichment and local attribute selection for each object by aggregating the attributes of similar objects in order to deal with the sparsity of networked data. In addition, this method applies an attribute elimination phase to eliminate irrelevant and redundant attributes which decrease the performance of clustering algorithms. Experimental results on real-world data sets indicate that NetBFC significantly achieves better performance when compared to baseline methods.},
	author = {Arzu Gorgulu Kakisim and Ibrahim Sogukpinar},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.12.030},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Feature construction, Feature extraction, Feature selection, Link reconstruction, Social media, Networked data},
	pages = {256-265},
	title = {Unsupervised binary feature construction method for networked data},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741830798X},
	volume = {121},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741830798X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.12.030}}

@article{GAO2021114191,
	abstract = {With the development of mobile Internet, microblog has become one of the most popular social platforms. The enormous user-generated microblogs have caused the problem of information overload, which makes users difficult to find the microblogs they actually need. Hence, how to provide users with accurate microblogs has become a hot and urgent issue. In this paper, we propose an approach of hybrid microblog recommendation, which is developed on a framework of deep neural network with a group of heterogeneous features as its input. Specifically, two new recommendation strategies are first constructed in terms of the extended user-interest tags and user interest topics, respectively. These two strategies additionally with the collaborative filtering are employed together to obtain the candidate microblogs for final recommendation. Then, we propose the heterogeneous features related to personal interests of users, interest in authors and microblog quality to describe the candidate microblogs. Finally, a deep neural network with multiple hidden layers is designed to predict and rank the microblogs. Extensive experiments conducted on the datasets of Sina Weibo and Twitter indicate that our proposed approach significantly outperforms the state-of-the-art methods. The code and the two datasets of this paper are publicly available at GitHub.},
	author = {Jiameng Gao and Chunxia Zhang and Yanyan Xu and Meiqiu Luo and Zhendong Niu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114191},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hybrid microblog recommendation, Deep neural network, Heterogeneous features, Extended user interest tags, Topic links},
	pages = {114191},
	title = {Hybrid microblog recommendation with heterogeneous features using deep neural network},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420309246},
	volume = {167},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420309246},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114191}}

@article{FRANCE2019456,
	abstract = {Marketing analytics is a diverse field, with both academic researchers and practitioners coming from a range of backgrounds including marketing, expert systems, statistics, and operations research. This paper provides an integrative review at the boundary of these areas. The aim is to give researchers in the intelligent and expert systems community the opportunity to gain a broad view of the marketing analytics area and provide a starting point for future interdisciplinary collaboration. The topics of visualization, segmentation, and class prediction are featured. Links between the disciplines are emphasized. For each of these topics, a historical overview is given, starting with initial work in the 1960s and carrying through to the present day. Recent innovations for modern, large, and complex ``big data'' sets are described. Practical implementation advice is given, along with a directory of open source R routines for implementing marketing analytics techniques.},
	author = {Stephen L. France and Sanjoy Ghose},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.11.002},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Analytics, Prediction, Marketing, Visualization, Segmentation, Data mining},
	pages = {456-475},
	title = {Marketing analytics: Methods, practice, implementation, and links to other fields},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741830722X},
	volume = {119},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741830722X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.11.002}}

@article{LI2020112839,
	abstract = {Circle structure of online brand communities allows companies to conduct cross-marketing activities by the influence of friends in different circles and build strong and lasting relationships with customers. However, existing works on the friend recommendation in social network do not consider establishing friendships between users in different circles, which has the problems of network sparsity, neither do they study the adaptive generation of appropriate link prediction algorithms for different circle features. In order to fill the gaps in previous works, the intelligent attention allocation link prediction algorithm is proposed to adaptively build attention allocation index (AAI) according to the sparseness of the network and predict the possible friendships between users in different circles. The AAI reflects the amount of attention allocated to the user pair by their common friend in the triadic closure structure, which is decided by the friend count of the common friend. Specifically, for the purpose of overcoming the problem of network sparsity, the AAIs of both the direct common friends and indirect ones are developed. Next, the decision tree (DT) method is constructed to adaptively select the suitable AAIs for the circle structure based on the density of common friends and the dispersion level of common friends' attention. In addition, for the sake of further improving the accuracy of the selected AAI, its complementary AAIs are identified with support vector machine model according to their similarity in value, direction, and ranking. Finally, the mutually complementary indices are combined into a composite one to comprehensively portray the attention distribution of common friends of users in different circles and predict their possible friendships for cross-marketing activities. Experimental results on Twitter and Google+ show that the model has highly reliable prediction performance.},
	author = {Shugang Li and Xuewei Song and Hanyu Lu and Linyi Zeng and Miaojing Shi and Fang Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112839},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Friend recommendation, Link prediction, AAI, Mutually complementary indices, Cross marketing},
	pages = {112839},
	title = {Friend recommendation for cross marketing in online brand community based on intelligent attention allocation link prediction algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930541X},
	volume = {139},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930541X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112839}}

@article{SCHOUTEN201968,
	abstract = {Many of today's businesses are driven by data, and while traditionally only quantitative data is considered, the role of textual data in our digital world is rapidly increasing. Text mining allows to extract and aggregate numerical data from textual documents, which in turn can be used to improve key decision processes. In this paper, we propose Heracles, a framework for developing and evaluating text mining algorithms, with a broad range of applications in industry. In contrast to other frameworks, Heracles supports both the development and evaluation stages of text mining algorithms. A practical use case shows the versatility and ease-of-use of the proposed framework in the domain of aspect-based sentiment analysis.},
	author = {Kim Schouten and Flavius Frasincar and Rommert Dekker and Mark Riezebos},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.03.005},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text mining, Algorithm evaluation, Research and development, Developers framework},
	pages = {68-84},
	title = {Heracles: A framework for developing and evaluating text mining algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419301587},
	volume = {127},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419301587},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.03.005}}

@article{ZAMIRI2021114657,
	abstract = {Image tag recommendation, aiming at assigning a set of relevant tags for images, is a useful way to help users organize images' content. Early methods in image tagging mainly demonstrated using low-level visual features. However, two visually similar photos may have different concepts (semantic gap). Although different multi-view tagging methods are proposed to learn the discriminative features, they usually do not consider the geographical correlation among images. Moreover, geographical-based image tagging models generally focused on the relevance criterion, i.e., how well the suggested tags describe image content. Diversity and redundancy should be controlled to guarantee the recommendation models' effectiveness and promote complementary information among tags. This paper proposes a robust multi-view image tagging method, termed MVDF-RSC, which considers the relevance, diversity, and redundancy criteria. Precisely, the proposed method consists of two phases: training and prediction. We propose a new robust optimization problem in the training phase to determine the similarity between data via the early fusion of multiple views of images and obtain clusters. In the prediction phase, relevant tags are recommended to each test data using a search-based method and a late fusion strategy. Comprehensive experiments on two geo-tagged image datasets demonstrate the proposed method's effectiveness over state-of-the-art alternatives.},
	author = {Mona Zamiri and Tahereh Bahraini and Hadi Sadoghi Yazdi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114657},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-view spectral clustering, Image annotation, Geo-tagged photos, Image tagging, Recommender systems, Geographical information},
	pages = {114657},
	title = {MVDF-RSC: Multi-view data fusion via robust spectral clustering for geo-tagged image tagging},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000981},
	volume = {173},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000981},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114657}}

@article{RANA2017273,
	abstract = {Opinion target extraction or aspect extraction is the most important subtask of the aspect-based sentiment analysis. This task focuses on the identification of the targets of user's opinions or sentiments from online reviews. In the recent years, syntactic patterns-based approaches have performed quite well and produced significant improvement in the aspect extraction task. However, these approaches are heavily dependent on the dependency parsers which produced syntactic relations following the grammatical rules and language constraints. In contemporary, users do not give much importance to these rules and constraints while expressing their opinions about particular product and neither reviewer websites restrict users to do so. This makes syntactic patterns-based approaches vulnerable. Therefore, in this paper, we are proposing a two-fold rules-based model (TF-RBM) which uses rules defined on the basis of sequential patterns mined from customer reviews. The first fold extracts aspects associated with domain independent opinions and the second fold extracts aspects associated with domain dependent opinions. We have also applied frequency- and similarity-based approaches to improve the aspect extraction accuracy of the proposed model. Our experimental evaluation has shown better results as compared with the state-of-the-art and most recent approaches.},
	author = {Toqir A. Rana and Yu-N Cheah},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.07.047},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect-based sentiment analysis, Opinion mining, Aspect extraction, Explicit aspects, Sequential pattern-based rules, Aspect pruning},
	pages = {273-285},
	title = {A two-fold rule-based model for aspect extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417305249},
	volume = {89},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417305249},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.07.047}}

@article{CALI2022118440,
	abstract = {E-commerce websites include large volume of online customer data regarding customer preferences. This study puts forward a novel Bayesian methodology to estimate the impact of product attributes on customer satisfaction by analyzing online data, so that product designers and market researchers are facilitated in their decision making processes. This method proves that valuable information on customer insights can be provided even if we have only data of overall customer satisfaction score and product attribute characteristics. The unknown data are acquired via statistical methods such that non-parametric density estimation is utilized to estimate distributions of satisfaction scores of product attributes. The impacts of product attributes on customer satisfaction are considered as weights of mixture kernel distributions and posterior distributions of weights are simulated with Markov Chain Monte Carlo method. The applicability of the proposed methodology is demonstrated by a case study, in which online data of mobile phone market are analyzed.},
	author = {Sedef {\c C}al and Adil Baykaso{\u g}lu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118440},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Attribute weighting, Customer satisfaction, Bayesian inference, Markov Chain Monte Carlo, Online ratings},
	pages = {118440},
	title = {A Bayesian based approach for analyzing customer's online sales data to identify weights of product attributes},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015378},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015378},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118440}}

@article{WANG2021114557,
	abstract = {Phrases are widely used in many text-based expert and intelligent systems. Phrase mining is a critical and preprocessing operation for these systems. With the increase of text data, errors in text corpus widely exist. Existing approaches focus on mining phrases on clean text corpus. However, neglecting to handle these errors may generate inaccurate results, which further leads to quality decline. To address the problem, we propose an error-tolerant phrase mining method, which not only conducts phrase mining in text corpus but also correct those phrases from errors. It could help to improve the performance of text-based expert and intelligent systems. To improve the performance and scalability, we propose several efficient and effective techniques to optimize the mining process. Experimental results show that our method achieves higher performance compared with state-of-the-art methods.},
	author = {Jiaying Wang and Jing Shan and Odafen Ehiaribho Santos and Jinling Bao},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114557},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Phrase mining, Error tolerant, Scalability},
	pages = {114557},
	title = {High quality error-tolerant phrase mining on text corpus},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742031201X},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742031201X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114557}}

@article{VERAS2019388,
	abstract = {In this paper, we address two research topics in Recommender Systems (RSs) which have been developed in parallel without a deeper integration: Cross-Domain RS (CDRS) and Context-Aware RS (CARS). CDRS have emerged to enhance the quality of recommendations in a target domain by leveraging sources of information in different domains. CDRS are especially useful to address cold-start, sparsity and diversity problems in target domains with scarce information. CARS, on its turn, have been proposed to consider contextual information for recommendations. Such systems are suitable when the users' interests change according to factors like time, location, among others. By combining these two approaches, better RSs can be developed, considering both the availability of useful data from multiple domains and the use of contextual information. In this paper, we formalize the combination of CDRS and CARS, which represents a more systematic integration of these approaches compared to previous work. Based on this formulation, we developed novel RSs techniques, named CD-CARS. To evaluate the developed CD-CARS techniques, we performed extensive experimentation through real datasets taking into account several scenarios. The recommendations were evaluated in terms of predictive and ranking performance, respectively achieving up to 62.6% and 45%, depending on the scenario, in comparison to traditional cross-domain collaborative filtering techniques. Therefore, the experimental results have shown that the integration of techniques developed in isolation can be useful in a variety of situations, in which recommendations can be improved by information gathered from different sources and can be refined by considering specific contextual information.},
	author = {Douglas V{\'e}ras and Ricardo Prud{\^e}ncio and Carlos Ferraz},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.06.020},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Cross-domain recommendation, Context-aware recommendation, Collaborative filtering recommendation, Cross-domain context-aware recommendation},
	pages = {388-409},
	title = {CD-CARS: Cross-domain context-aware recommender systems},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930421X},
	volume = {135},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930421X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.06.020}}

@article{ROMERO2019522,
	abstract = {Twitter has become instrumental as a means of spreading information, opinions or awareness about real-world events. The classification of event-related tweets is a challenging problem since tweets are noisy and sparse pieces of text that lack contextual information. Related work proposes contextual enrichment techniques using external sources (e.g. semantic web, external documents), often considering underlying assumptions about the target events. However, they lack guidelines for determining the textual features to enrich, the external sources to use, the properties to explore, and how to prevent the inclusion of unrelated information. In this paper, we propose a hybrid semantic enrichment framework for the classification of event-related tweets. We contribute to this field by leveraging different contextual enrichment strategies into a unifying framework targeted at a broad range of event types, where each enrichment technique has a role in the improvement of event classification. The framework also encompasses a solution to deal with the huge number of features that result from semantic enrichment, which combines a pruning method to select domain relevant semantic features and general-purpose feature selection techniques. We assessed the contribution of each framework component to event classification improvement using a broad experimental setting. Using seven events of distinct natures, we outperformed a word embeddings baseline in 93.6% of cases, and a textual baseline in 60.3% of cases. In most cases, we improved the recall, with no significant impact on the precision.},
	author = {Simone Romero and Karin Becker},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.10.028},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Event classification, Semantic web, DBPedia, Twitter, Discriminative features},
	pages = {522-538},
	title = {A framework for event classification in tweets based on hybrid semantic enrichment},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741830678X},
	volume = {118},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741830678X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.10.028}}

@article{ELBOUSHAKI2020112829,
	abstract = {Human gesture recognition has become a pillar of today's intelligent Human-Computer Interfaces as it typically provides more comfortable and ubiquitous interaction. Such expert system has a promising prospect in various applications, including smart houses, gaming, healthcare, and robotics. However, recognizing human gestures in videos is one of the most challenging topics in computer vision, because of some irrelevant environmental factors like complex background, occlusion, lighting conditions, and so on. With the recent development of deep learning, many researchers have addressed this problem by building single deep networks to learn spatiotemporal features from video data. However, the performance is still unsatisfactory due to the limitation that the single deep networks are incapable of handling these challenges simultaneously. Hence, the extracted features cannot efficiently capture both relevant shape information and detailed spatiotemporal variation of the gestures. One solution to overcome the aforementioned drawbacks is to fuse multiple features from different models learned on multiple vision cues. Aiming at this objective, we present in this paper an effective multi-dimensional feature learning approach, termed as MultiD-CNN, for human gesture recognition in RGB-D videos. The key to our design is to learn high-level gesture representations by taking advantages from Convolutional Residual Networks (ResNets) for training extremely deep models and Convolutional Long Short-Term Memory Networks (ConvLSTM) for dealing with time-series connections. More specifically, we first construct an architecture to simultaneously learn the spatiotemporal features from RGB and depth sequences through 3D ResNets which are then linked to a ConvLSTM to capture the temporal dependencies between them, and we show that they better combine appearance and motion information effectively. Second, to alleviate distractions from background and other variations, we propose a method that encodes the temporal information into a motion representation, while a two-stream architecture based on 2D-ResNets is then employed to extract deep features from this representation. Third, we investigate different fusion strategies at different levels for blending the classification results, and we show that integrating multiple ways of encoding the spatial and temporal information leads to a robust and stable spatiotemporal feature learning with better generalization capability. Finally, we perform different experiments to evaluate the performance of the investigated architectures on four kinds of challenging datasets, demonstrating that our approach is particularly impressive where it outperforms prior arts in both accuracy and efficiency. The obtained results affirm also the importance of embedding the proposed approach in other intelligent systems application areas.},
	author = {Abdessamad Elboushaki and Rachida Hannane and Karim Afdel and Lahcen Koutti},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112829},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Gesture recognition, Deep learning, Convolutional neural networks, Multimodal learning, Feature fusion, RGB-D video processing},
	pages = {112829},
	title = {MultiD-CNN: A multi-dimensional feature learning approach based on deep convolutional networks for gesture recognition in RGB-D image sequences},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419305317},
	volume = {139},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419305317},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112829}}

@article{GUO201921,
	abstract = {This paper presents a max-margin multi-scale convolutional factor analysis (MMCFA) model, which explores the strongly discriminative principle of max-margin learning to improve the classification performance of multi-scale convolutional factor analysis (CFA) model with application to image data classification. Compared with the traditional factor analysis (FA) model, the CFA model can maintain the spatial correlation among the image pixels in two-dimensional space and capture the structural information from images via convolution kernels. Moreover, to extract multi-level features, multi-scale convolution kernels are adopted to capture richer features at different scales of images. Since the unsupervised model may not offer discriminative factors for the classification task, it is expected to introduce the supervised information to the multi-scale CFA model when supervised information is available. To deal with it, a latent variable support vector machine (LVSVM) is linked to the factors learned from multi-scale CFA model, yielding max-margin discrimination, as the classification criterion in the feature space in our proposed model. The multi-scale CFA model and LVSVM learn parameters jointly in a united framework via the Gibbs inference. Experimental results on mixed national institute of standards and technology (MNIST) dataset, Fashion-MNIST dataset, SVHN dataset and measured synthetic aperture radar (SAR) images show that the learned convolution kernels and factors can describe data information well and the proposed model has excellent classification performance.},
	author = {Yuchen Guo and Lan Du and Jian Chen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.04.012},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Factor analysis (FA), Max-margin learning, Multi-scale convolution kernels, Image classification, Synthetic aperture radar (SAR) image},
	pages = {21-33},
	title = {Max-margin multi-scale convolutional factor analysis model with application to image classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930243X},
	volume = {133},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930243X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.04.012}}

@article{AGUADO2022118103,
	abstract = {This work presents a Case-Based Reasoning (CBR) module that integrates sentiment and stress analysis on text and keystroke dynamics data with context information of users interacting on Social Network Sites (SNSs). The context information used in this work is the history of positive or negative messages of the user, and the topics being discussed on the SNSs. The CBR module uses this data to generate useful feedback for users, providing them with warnings if it detects potential future negative repercussions caused by the interaction of the users in the system. We aim to help create a safer and more satisfactory experience for users on SNSs or in other social environments. In a set of experiments, we compare the effectiveness of the CBR module to the effectiveness of different affective state detection methods. We compare the capacity to detect cases of messages that would generate future problems or negative repercussions on the SNS. For this purpose, we use messages generated in a private SNS, called Pesedia. In the experiments in the laboratory, the CBR module managed to outperform the other proposed analyzers in almost every case. The CBR module was fine-tuned to explore its performance when populating the case base with different configurations.},
	author = {G. Aguado and V. Julian and A. Garcia-Fornes and A. Espinosa},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118103},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-agent system, Social networks, Sentiment analysis, Stress analysis, Case-based reasoning},
	pages = {118103},
	title = {A CBR for integrating sentiment and stress analysis for guiding users on social network sites},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012945},
	volume = {208},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422012945},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118103}}

@article{PATEL2019167,
	abstract = {Nowadays abundant amount of information is available on Internet which makes it difficult for the users to locate desired information. Automatic methods are needed to efficiently sieve and scavenge useful information from the Internet. Text summarization is identified and accepted as one of the solutions to find desired contents from one or more documents. The objective of proposed multi-document summarization is to gain good content coverage with information diversity. The proposed statistical feature based model utilizes the fuzzy model to deal with the imprecise and uncertainty of feature weight. Redundancy removal using cosine similarity is presented as enrichment to proposed work. The proposed approach is compared with DUC (Document Understanding Conference) participant systems and other summarization systems such as TexLexAn, ItemSum, Yago Summarizer, MSSF and PatSum using ROUGE measure on dataset DUC 2004. The experimental results show that our proposed work achieves a significant performance improvement over the other summarizers.},
	author = {Darshna Patel and Saurabh Shah and Hitesh Chhinkaniwala},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.05.045},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Feature extraction, Fuzzy logic, Text summarization, Sentence scoring, Cosine similarity, Rouge, Machine learning, Extrinsic evaluation},
	pages = {167-177},
	title = {Fuzzy logic based multi document summarization with improved sentence scoring and redundancy removal technique},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930380X},
	volume = {134},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930380X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.05.045}}

@article{LI2018103,
	abstract = {Text representations is a key task for many natural language processing applications such as document classification, ranking, sentimental analysis and so on. The goal of it is to numerically represent the unstructured text documents so that they can be computed mathematically. Most of the existing methods leverage the power of deep learning to produce a representation of text. However, these models do not consider about the problem that text itself is usually semantically ambiguous and reflects limited information. Due to this reason, it is necessary to seek help from external knowledge base to better understand text. In this paper, we propose a novel framework named Text Concept Vector which leverages both the neural network and the knowledge base to produce a high quality representation of text. Formally, a raw text is primarily conceptualized and represented by a set of concepts through a large taxonomy knowledge base. After that, a neural network is used to transform the conceptualized text into a vector form which encodes both the semantic information and the concept information of the original text. We test our framework on both the sentence level task and the document level task. The experimental results illustrate the effectiveness of our work.},
	author = {Yiming Li and Baogang Wei and Yonghuai Liu and Liang Yao and Hui Chen and Jifang Yu and Wenhao Zhu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.11.037},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text representation, Knowledge base, Representation learning, Network embedding},
	pages = {103-114},
	title = {Incorporating knowledge into neural network for text representation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417307935},
	volume = {96},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417307935},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.11.037}}

@article{MARTINEZHUERTAS2021115621,
	abstract = {Usually, computerized assessments of constructed responses use a predictive-centered approach instead of a validity-centered one. Here, we compared the convergent and discriminant validity of two computerized assessment methods designed to detect semantic topics in constructed responses: Inbuilt Rubric (IR) and Partial Contents Similarity (PCS). While both methods are distributional models of language and use the same Latent Semantic Analysis (LSA) prior knowledge, they produce different semantic representations. PCS evaluates constructed responses using non-meaningful semantic dimensions (this method is the standard LSA assessment of constructed responses), but IR endows original LSA semantic space coordinates with meaning. In the present study, 255 undergraduate and high school students were allocated one of three texts and were tasked to make a summary. A topic-detection task was conducted comparing IR and PCS methods. Evidence from convergent and discriminant validity was found in favor of the IR method for topic-detection in computerized constructed response assessments. In this line, the multicollinearity of PCS method was larger than the one of IR method, which means that the former is less capable of discriminating between related concepts or meanings. Moreover, the semantic representations of both methods were qualitatively different, that is, they evaluated different concepts or meanings. The implications of these automated assessment methods are also discussed. First, the meaningful coordinates of the Inbuilt Rubric method can accommodate expert rubrics for computerized assessments of constructed responses improving computer-assisted language learning. Second, they can provide high-quality computerized feedback accurately detecting topics in other educational constructed response assessments. Thus, it is concluded that: (1) IR method can represent different concepts and contents of a text, simultaneously mapping a considerable variability of contents in constructed responses; (2) IR method semantic representations have a qualitatively different meaning than the LSA ones and present a desirable multicollinearity that promotes the discriminant validity of the scores of distributional models of language; and (3) IR method can extend the performance and the applications of current LSA semantic representations by endowing the dimensions of the semantic space with semantic meanings.},
	author = {Jos{\'e} {\'A}. Mart{\'\i}nez-Huertas and Ricardo Olmos and Jos{\'e} A. Le{\'o}n},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115621},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Inbuilt rubric, Constructed responses, Summaries, Topic detection, Latent semantic analysis, Automated summary evaluation},
	pages = {115621},
	title = {Enhancing topic-detection in computerized assessments of constructed responses with distributional models of language},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421010150},
	volume = {185},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421010150},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115621}}

@article{SALAHIAN2022119051,
	abstract = {Nonnegative Matrix Factorization is a data analysis method to discover parts-based, linear representations of data. It has been successfully used in a great variety of applications. Deep Nonnegative Matrix Factorization (deep NMF) was recently established to cope with the extraction of hierarchical latent feature representation, and it has been demonstrated to achieve outstanding results in unsupervised representation learning. However, defining a suitable regularization for the deep models is a key challenge, and the existing Deep NMF approaches lack a well-suited regularization. In this paper, we propose the Deep Autoencoder-like NMF with Contrastive Regularization and Feature Relationship preservation (DANMF-CRFR) to address the above problem. Inspired by contrastive learning, this deep model is able to learn discriminative and instructive deep features while adequately enforcing the local and global structures of the data to its decoder and encoder components. Meanwhile, DANMF-CRFR also imposes feature correlations on the basis matrices during feature learning to improve part-based learning capabilities. Multiplicative updating rules and convergence guarantees are also provided. Extensive experimental results demonstrate the advantages of the proposed model. The source code for reproducing our results can be found at https://github.com/NavidSalahian/DANMF_CRFR.},
	author = {Navid Salahian and Fardin Akhlaghian Tab and Seyed Amjad Seyedi and Jovan Chavoshinejad},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.119051},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deep learning, Autoencoder structure, Nonnegative matrix factorization, Contrastive regularization, Data representation},
	pages = {119051},
	title = {Deep Autoencoder-like NMF with Contrastive Regularization and Feature Relationship Preservation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422020693},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422020693},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.119051}}

@article{ALTINEL2022118606,
	abstract = {Due to the huge size of the data accumulated on microblogging sites, recently, two fundamental questions have become very popular: 1) What percentage of this accumulated data has positive or negative sentiment polarity? 2) How is the distribution of this accumulated data on different topics? Inspired by these motivated necessities, this paper presents several different algorithms which are based on the Label Propagation Algorithm (LPA) in order to handle previously mentioned two fundamentals tasks: sentiment polarity detection task and topic-based text classification task. These algorithms are the Label Propagated- Relevance Frequency Classifier (LP-RFC) and LP-Abstract Frequency Classifier (LP-AFC). These algorithms can be defined as new semantic smoothing classifiers, which take advantage of the semantic connections among terms in the label propagation phase of the LPA. Additionally, another classifier, namely LP-ComRFC+AFC, was built. LP-ComRFC+AFC is actually a weighted summation classifier of the individual LP-RFC and LP-AFC. Furthermore, considering the shortage of labeled data in real-world scenarios, a semi-supervised version of LP-RFC and LP-AFC, namely ``Merging Unlabeled and Labeled Instances with Semantic Values of Terms'' (MULIS), was designed and implemented. For the experiments of the sentiment polarity detection task, three different datasets were use and for the experiments of topic-based text classification task, a self-collected tweet dataset was use. According to the experimental results, the suggested algorithms, and their composite form, LP-ComRFC+AFC, generated higher F1 scores than all of the baseline algorithms at nearly all of the training splits on the datasets.},
	author = {Ay{\c s}e Berna Altnel},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118606},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Label propagation algorithm, Social media analysis, Topic-based tweet classification, Sentiment polarity detection},
	pages = {118606},
	title = {Social media analysis by innovative hybrid algorithms with label propagation},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742201658X},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742201658X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118606}}

@article{FICCADENTI2019127,
	abstract = {This work presents a text mining context and its use for a deep analysis of the messages delivered by politicians. Specifically, we deal with an expert systems-based exploration of the rhetoric dynamics of a large collection of US Presidents' speeches, ranging from Washington to Trump. In particular, speeches are viewed as complex expert systems whose structures can be effectively analyzed through rank-size laws. The methodological contribution of the paper is twofold. First, we develop a text mining-based procedure for the construction of the dataset by using a web scraping routine on the Miller Center website -- the repository site collecting the speeches. Second, we explore the implicit structure of the discourse data by implementing a rank-size procedure over the individual speeches, being the words of each speech ranked in terms of their frequencies. The scientific significance of the proposed combination of text-mining and rank-size approaches can be found in its flexibility and generality, which let it be reproducible to a wide set of expert systems and text mining contexts. The usefulness of the proposed method and of the speeches analysis is demonstrated by the findings themselves. Indeed, in terms of impact, it is worth noting that interesting conclusions of social, political and linguistic nature on how 45 United States Presidents, from April 30, 1789 till February 28, 2017 delivered political messages can be carried out. Indeed, the proposed analysis shows some remarkable regularities, not only inside a given speech, but also among different speeches. Moreover, under a purely methodological perspective, the presented contribution suggests possible ways of generating a linguistic decision-making algorithm.},
	author = {Valerio Ficcadenti and Roy Cerqueti and Marcel Ausloos},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.12.049},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text mining, Natural Language Processing, Politics, Rank-size laws},
	pages = {127-142},
	title = {A joint text mining-rank size investigation of the rhetoric structures of the US Presidents' speeches},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418308170},
	volume = {123},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418308170},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.12.049}}

@article{MOIRANGTHEM2021113898,
	abstract = {Text classification, using deep learning techniques, has become a research challenge in natural language processing. Most of the existing deep learning models for text classification face difficulties when the length of the input text increases. Most models work well on shorter text inputs, however, their performance degrades with the increase in the input length. In this work, we introduce a model for text classification that can alleviate this problem. We present the hierarchical and lateral multiple timescales gated recurrent units (HL-MTGRU), in combination with pre-trained encoders to address the long text classification problem. HL-MTGRU can represent multiple temporal scale dependencies for the discrimination task. By combining the slow and fast units of the HL-MTGRU, our model effectively classifies long multi-sentence texts into the desired classes. We also show that the HL-MTGRU structure helps the model to prevent degradation of performance on longer text inputs. We demonstrate that the proposed network with the help of the latest pre-trained encoders for feature extraction outperforms the conventional models on various long text classification benchmark datasets.},
	author = {Dennis Singh Moirangthem and Minho Lee},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113898},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text classification, Multiple timescale, Temporal hierarchy, BERT, Pre-trained encoder},
	pages = {113898},
	title = {Hierarchical and lateral multiple timescales gated recurrent units with pre-trained encoder for long text classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030693X},
	volume = {165},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030693X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113898}}

@article{JIANG2021115537,
	abstract = {Stock market prediction has been a classical yet challenging problem, with the attention from both economists and computer scientists. With the purpose of building an effective prediction model, both linear and machine learning tools have been explored for the past couple of decades. Lately, deep learning models have been introduced as new frontiers for this topic and the rapid development is too fast to catch up. Hence, our motivation for this survey is to give a latest review of recent works on deep learning models for stock market prediction. We not only category the different data sources, various neural network structures, and common used evaluation metrics, but also the implementation and reproducibility. Our goal is to help the interested researchers to synchronize with the latest progress and also help them to easily reproduce the previous studies as baselines. Based on the summary, we also highlight some future research directions in this topic.},
	author = {Weiwei Jiang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115537},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stock market prediction, Deep learning, Machine learning, Feedforward neural network, Convolutional neural network, Recurrent neural network},
	pages = {115537},
	title = {Applications of deep learning in stock market prediction: Recent progress},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421009441},
	volume = {184},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421009441},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115537}}

@article{CHOI201927,
	abstract = {Among internet and smart device applications, Twitter has become a leading social media platform, disseminating online events occurring in the world on a real-time basis. Many studies have been conducted to identify valuable information on Twitter. Recently, Frequent Pattern Mining has been applied for topic detection on Twitter. In Frequent Pattern Mining, a topic is considered to be a group of words that appear simultaneously, however, the method only considers the frequency of words, and their utility for topic detection is not considered in the process of pattern generation. In this paper, we propose a method to detect emerging topics on Twitter based on High Utility Pattern Mining (HUPM), which takes frequency and utility into account at the same time. For a chunk of tweets by time-based windowing on the Twitter stream, we define the utility of words based on the growth rate in frequency and find groups of words with high frequency and high utility by HUPM. For post-processing to extract actual topic patterns from candidate topic patterns generated by HUPM, an efficient data structure called Topic-tree (TP-Tree) is also proposed. Experimental results demonstrated the effectiveness of the proposed method, which showed superior performance and shorter running time than other tested topic detection methods. In particular, the proposed method showed a 5% higher topic recall than the other compared methods for the three datasets used.},
	author = {Hyeok-Jun Choi and Cheong Hee Park},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.051},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Frequent pattern mining, High utility pattern mining, Topic detection, Twitter stream},
	pages = {27-36},
	title = {Emerging topic detection in twitter stream based on high utility pattern mining},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304767},
	volume = {115},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304767},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.051}}

@article{MOHSIN2021113808,
	abstract = {One of the areas most in need of improvement in the field of automated bug fixing, localization and triaging systems is that of an effective categorization, as this would bugs to reduce the time, cost and effort required to locate, assign and fix the bug. The existing approaches depend upon the textual similarity of the bug description and category in a given reported bug; accordingly, the challenges of unstructured bugs, technical terms, versatile ways of reporting the same bug, the diverse nature and sizes of datasets etc. are often overlooked. Consequently, this limits the classifier performance to a specific type of dataset, resulting in classification inefficiency. To this end, we propose a novel Self-Paced Bug Classifier (SPBC) that is capable of locating the target categories from the bug description of the historical data, maintained by multiple open-source software packages (Bugzilla, Mentis, Redmine). The proposed model introduces a self-paced back-traceable algorithm, controlled by a self-paced regularizer, which classifies textually independent bug descriptions with weighted data-independent tokens (the easy samples). Later on, the regularizer sets comparatively hard samples for textually dependent classification by capturing intra-class and inter-class discrimination features from bug descriptions, based on the weighted similarities of words; this is done with the help of a Key Feature Identification Matrix (KFIM), a Non-Independent and Identically Distributed (NIID) matrix. Easy-to-hard self-pace learning, integrated with textually dependent and independent classification, makes SPBC capable of simultaneously enhancing the effectiveness and robustness of intelligent systems through a substantial increase in precision (5--15% on average). The main advantage of SPBC is that it targets the spatial relationship between the data and the system, which makes it an apt learner of data and allows it to maintains sample insertion into the classifier at a controlled pace. Additionally, it maintains stability, which is not affected by the dataset's dimensionality and traits. As is evidenced by the experimental results on four different datasets from open-source projects, our model outperforms the baseline and state-of-the-art methods through a single-stroke solution with improved accuracy and stable performance (average 95% precision and 4% decrease in kappa); hence, it is significant for improving intelligent bug fixing and triaging systems.},
	author = {Hufsa Mohsin and Chongyang Shi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113808},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bug triaging, Defect localization, Self-paced learning, Bug report analysis, Bug classification},
	pages = {113808},
	title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420306230},
	volume = {167},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420306230},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113808}}

@article{ZANG2018250,
	abstract = {Object Bank (OB) is a high-level image representation encoding semantic and spacial information, and has superior performance in scene classification tasks. However, the dimensionality of OB feature is high, which demands massive computation. Existing dimensionality reduction methods for OB are incapable of achieving both high classification accuracy and substantial dimensionality reduction simultaneously. In order to solve this problem, we propose a threshold value filter pooling method to avoid noise accumulation in histogram-pooling and represent more useful information than max-pooling. We also propose a Matthew effect normalization method to highlight the useful information, and thus boost the performance of OB-based image scene classification. Finally, we apply these two methods in a dimensionality reduction framework to simplify OB representation and construct more proper descriptors, and thus achieve both dimensionality reduction and classification accuracy increase. We evaluated our framework on three real-world datasets, namely, event dataset UIUC-Sports, natural scene dataset LabelMe, and mixture dataset 15-Scenes. The classification results demonstrate that our framework not only obtains accuracies similar to or higher than the original OB representation, but also reduces the dimensionality significantly. The computational complexity analysis shows that it can reduce the time complexity of classification. Therefore, our framework can improve OB-based image scene classification through both computational complexity reduction and accuracy increase.},
	author = {Mujun Zang and Dunwei Wen and Tong Liu and Hailin Zou and Chanjuan Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.10.057},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Image classification, Object Bank, Dimensionality reduction, Pooling, Image feature},
	pages = {250-264},
	title = {A pooled Object Bank descriptor for image scene classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741730739X},
	volume = {94},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741730739X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.10.057}}

@article{YU2019365,
	abstract = {Collaborative filtering (CF) is one of the most successful recommendation techniques due to its simplicity and attractive accuracy. However, existing CF methods fail to interpret the reasons why they recommend a new item. In this paper, we propose a Contextual-boosted Deep Neural Collaborative filtering (CDNC) model for item recommendation, which simultaneously exploits both item introductions (textual features) and user ratings (collaborative features) to alleviate the cold-start problem and provide interpretable item recommendation. Specifically, we propose an interactive attention mechanism to learn the user representation, which makes use of the mutual information from both the user ratings and item introductions to supervise the representation learning of each other. With the learned attention weights, we can obtain the importance of each historical item among the historical list. Meanwhile, the attention model can assign different weights to the words in item introductions according to their importance. Therefore, CDNC can provide interpretations for the recommendations by assigning different attention weights to the historically interacted items and the words in the item introductions. On the other hand, we also learn the distributed representations of new-coming items with deep neural networks (i.e., LSTM), considering both rating and item introduction information. Finally, the user representation and the representations of new-coming item are concatenated to perform recommendation score prediction. Extensive experiments on four public benchmarks demonstrate the effectiveness of CDNC. In addition, CDNC has the advantage of interpreting the recommendations and providing user profiles for down-stream applications.},
	author = {Shuai Yu and Min Yang and Qiang Qu and Ying Shen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.06.051},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommendation systems, Interpretable recommendation, Interactive attention network, Collaborative filtering, Cold start problem},
	pages = {365-375},
	title = {Contextual-boosted deep neural collaborative filtering model for interpretable recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304518},
	volume = {136},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419304518},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.06.051}}

@article{SARKAR2021115026,
	abstract = {The significance of tourism in the globe today is enormous since it is a major source of income and jobs for a nation. Tourists are facing a range of difficulties as they select suitable tours, consisting of several itineraries in terms of their interests and distinct constraints. An itinerary consists of many Points of Interest (POIs) and a POI can further be splitted into several attractions which are named as POI within POI. For selecting the itinerary, the existing techniques use the characteristics of POIs. However, a POI consists of many attractions. Out of these, one dominating attraction's type is considered as POI type. This ignores the other type of attraction's present in that POI. It may cause improper selection of itineraries. Therefore, selection of itineraries by considering POI within POI is of great benefit. But, it is very challenging. For this task, we suggest an algorithm called PWP. It recommends multiple itineraries that are based on the interest of visitors, popularity of itineraries and the cost of itineraries. If a tourist wants to visit unknown areas, the PWP algorithm can be expanded further. We have taken the similar user's features to advise multiple itineraries using the Flickr dataset. The findings show that the proposed PWP algorithm out-performs the baseline algorithms in terms of real-life matrices and heuristic based metrics.},
	author = {Joy Lal Sarkar and Abhishek Majumder},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115026},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommendation system, Tourist, Cost, Itineraries, POIs},
	pages = {115026},
	title = {A new point-of-interest approach based on multi-itinerary recommendation engine},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100467X},
	volume = {181},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100467X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115026}}

@article{NGUYEN2022117096,
	abstract = {Nowadays, there has been a rapidly increasing number of scientific submissions in multiple research domains. A large number of journals have various acceptance rates, impact factors, and rankings in different publishers. It becomes time-consuming for many researchers to select the most suitable journal to submit their work with the highest acceptance rate. A paper submission recommendation system is more critical for the research community and publishers as it gives scientists another support to complete their submission conveniently. This paper investigates the submission recommendation system for two main research topics: computer science and applied mathematics. Unlike the previous works (Wang et al., 2018; Son et al., 2020) that extract TF--IDF and statistical features as well as utilize numerous machine learning algorithms (logistics regression and multiple perceptrons) for building the recommendation engine, we present an efficient paper submission recommendation algorithm by using different bidirectional transformer encoders and the Mixture of Transformer Encoders technique. We compare the performance between our methodology and other approaches by one dataset from Wang et al. (2018) with 14012 papers in computer science and another dataset collected by us with 223,782 articles in 178 Springer applied mathematics journals in terms of top K accuracy (K=1,3,5,10). The experimental results show that our proposed method extensively outperforms other state-of-the-art techniques with a significant margin in all top K accuracy for both two datasets. We publish all datasets collected and our implementation codes for further references.11https://github.com/BinhMisfit/PSRMTE.},
	author = {Dac Huu Nguyen and Son Thanh Huynh and Cuong Viet Dinh and Phong Tan Huynh and Binh Thanh Nguyen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117096},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommendation system, Deep learning, Transformer encoders},
	pages = {117096},
	title = {PSRMTE: Paper submission recommendation using mixtures of transformer},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422005024},
	volume = {202},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422005024},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117096}}

@article{LATHABAI2022118317,
	abstract = {The shift from `trust-based funding' to `performance-based funding' is one of the factors that has forced institutions to strive for continuous improvement of performance. Several studies have established the importance of collaboration in enhancing the performance of paired institutions. However, identification of suitable institutions for collaboration is sometimes difficult and therefore institutional collaboration recommendation systems can be vital. Currently, there are no well-developed institutional collaboration recommendation systems. In order to bridge this gap, we design a framework that recognizes the thematic strengths and core competencies of institutions, which can in turn be used for collaboration recommendations. The framework, based on NLP and network analysis techniques, is capable of determining the strengths of an institution in different thematic areas within a field and thereby determining the core competency and potential core competency areas of that institution. It makes use of recently proposed expertise indices such as x and x(g) indices for determination of core and potential core competency areas and can toss two kinds of recommendations: (i) for enhancement of strength of strong areas or core competency areas of an institution and (ii) for complementing the potentially strong areas or potential core competency areas of an institution. A major advantage of the system is that it can help to determine and improve the research portfolio of an institution within a field through suitable collaboration, which may lead to the overall improvement of the performance of the institution in that field. The framework is demonstrated by analyzing the performance of 195 Indian institutions in the field of `Computer Science'. Upon validation using standard metrics for novelty, coverage and diversity of recommendation systems, the framework is found to be of sufficient coverage and capable of tossing novel and diverse recommendations. The article thus presents an institutional collaboration recommendation system which can be used by institutions to identify potential collaborators.},
	author = {Hiran H. Lathabai and Abhirup Nandy and Vivek Kumar Singh},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118317},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Institutional collaboration, Recommendation system, NLP, Network analysis, Research expertise, Expertise indices},
	pages = {118317},
	title = {Institutional collaboration recommendation: An expertise-based framework using NLP and network analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014464},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014464},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118317}}

@article{PARK2019208,
	abstract = {A latent Dirichlet allocation (LDA) model is a machine learning technique to identify latent topics from text corpora within a Bayesian hierarchical framework. Current popular inferential methods to fit the LDA model are based on variational Bayesian inference, collapsed Gibbs sampling, or a combination of these. Because these methods assume a unimodal distribution over topics, however, they can suffer from large bias when text corpora consist of various clusters with different topic distributions. This paper proposes an inferential LDA method to efficiently obtain unbiased estimates under flexible modeling for heterogeneous text corpora with the method of partial collapse and the Dirichlet process mixtures. The method is illustrated using a simulation study and an application to a corpus of 1300 documents from neural information processing systems (NIPS) conference articles during the period of 2000--2002 and British Broadcasting Corporation (BBC) news articles during the period of 2004--2005.},
	author = {Hongju Park and Taeyoung Park and Yung-Seop Lee},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.04.028},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bayesian analysis, Latent Dirichlet allocation, Dirichlet process mixture, Partial collapse, Machine learning, Natural language processing},
	pages = {208-218},
	title = {Partially collapsed Gibbs sampling for latent Dirichlet allocation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419302581},
	volume = {131},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419302581},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.04.028}}

@article{DU2018157,
	abstract = {Hierarchical classification is an effective approach to categorization of large-scale text data. We introduce a relaxed strategy into the traditional hierarchical classification method to improve the system performance. During the process of hierarchy structure construction, our method delays node judgment of the uncertain category until it can be classified clearly. This approach effectively alleviates the `block' problem which transfers the classification error from the higher level to the lower level in the hierarchy structure. A new term weighting approach based on the Least Information Theory (LIT) is adopted for the hierarchy classification. It quantifies information in probability distribution changes and offers a new document representation model where the contribution of each term can be properly weighted. The experimental results show that the relaxation approach builds a more reasonable hierarchy and further improves classification performance. It also outperforms other classification methods such as SVM (Support Vector Machine) in terms of efficiency and the approach is more efficient for large-scale text classification tasks. Compared to the classic term weighting method TF*IDF, LIT-based methods achieves significant improvement on the classification performance.},
	author = {Yongping Du and Jingxuan Liu and Weimao Ke and Xuemei Gong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.02.003},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hierarchy classification, Relaxation strategy, Least Information Theory, Term weighting},
	pages = {157-164},
	title = {Hierarchy construction and text classification based on the relaxation strategy and least information model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418300733},
	volume = {100},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418300733},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.02.003}}

@article{LIU2019246,
	abstract = {Detecting key story elements such as protagonist, opponent, desire, turning points, battle, and victory, etc. is essential for various narrative work applications including content retrieval and content recommendation systems. The task of automatically identifying story elements is challenging because of its complexity and subjectiveness and currently, there are no available algorithms for this task. In this paper, we focus on identifying turning points in a story of a cartoon movie. The proposed methodology extends the novel two-clocks theory, originally validated on scripts of theatre plays, to video stories. The assumption behind the two-clocks theory is that the perception of time is different when some special event happens to a certain agent (e.g., time flows slower for a patient and quicker for a tourist). The story timeline is monitored with two clocks: an event clock, which measures the regular time flow of the story; and a weighted clock, which measures the timing of the story events. We have conducted an experiment on 28 episodes of a cartoon series and achieved promising results: 78.6% precision for turning points identification and 100% precision for key scene detection. The proposed approach is the first step towards development of intelligent systems for automated understanding of stories in narrative works such as cinema movies and even amateur videos uploaded to the Internet.},
	author = {Chang Liu and Mark Last and Armin Shmilovici},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.01.003},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Story's turning points, Story elements detection, Story understanding, Video analytics},
	pages = {246-255},
	title = {Identifying turning points in animated cartoons},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419300041},
	volume = {123},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419300041},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.01.003}}

@article{HAN2022116472,
	abstract = {Online communities, where lead users openly share their experiences and knowledge on product and technology in the form of a post, have become a fruitful source of innovation. While efforts have been made to provide a way of data-driven case-based reasoning (CBR), existing studies have limitations in reflecting lead users' characteristics into the CBR process. Current research has emphasized the retrieval and adaptation phase only, which retrieves, reuses, and revises cases. However, what is at the core of lead user characteristics is to find out the problem, and solve the problem by themselves before mass customers. This means CBR needs to focus on uncovering and defining problems and finding relevant solutions for designated problems. In response, this research suggests a novel approach for problem-oriented CBR approach to reflect lead user characteristics. First, this study defines problems, by extracting problem--solution sets related to the specific function using sentiment analysis. Second, this study improves case representation and case retrieval using subject-action-object (SAO) analysis and technology tree respectively. This study demonstrates the approach through a case of drone technology using lead user communities (diydrones.com), and our findings suggest that the approach can help firms broaden the knowledge of existing products to make an improvement.},
	author = {Mintak Han and Youngjung Geum},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116472},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Technology-intensive product innovation, Lead user, CBR, SAO analysis, Sentiment analysis},
	pages = {116472},
	title = {Problem-oriented CBR: Finding potential problems from lead user communities},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421017528},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421017528},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116472}}

@article{BAMAKAN2019200,
	abstract = {A social network as an essential communication platform facilitates the interactions of online users. Based on the interactions, users can influence or be affected by the opinions of others. The users being able to influence and shape the opinions of others are considered as opinion leaders. The problem of identifying opinion leaders is an important task due to its wide applications in reality, including product adoption for marketing and societal analytics. The problem has been attracting proliferating studies over the recent years. To overview and provide insights of the methodologies and enlighten the future study, we review the well-known techniques for opinion leader detection problems. These techniques are classified into descriptive approaches, statistical and stochastic methods, diffusion process based approaches, topological based methods, data mining and learning methods, and approaches based on hybrid content mining. The advantages and drawbacks of each method are systematically analyzed and compared, to provide deep understanding into the existing research challenges and the direction of future trends. The findings of this review would be useful for those researchers are interested in identifying opinion leaders and influencers in social networks and related fields.},
	author = {Seyed Mojtaba Hosseini Bamakan and Ildar Nurgaliev and Qiang Qu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.069},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Opinion leader, Social network analysis, Flow of influence, Influential users/nodes, Graph mining},
	pages = {200-222},
	title = {Opinion leader detection: A methodological review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304950},
	volume = {115},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304950},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.069}}

@article{KAUR2018397,
	abstract = {In view of the rise in security and privacy concern in social networks, there has been an inadvertent increase in research related to framing of appropriate measures to detect the security breaches in social networks. Cyber criminals are misusing social networking platforms for inappropriate and illegitimate purposes such as posting or sending of illegitimate content which a genuine user will rarely do. Hence, whenever a sensitive and unusual text is posted by a user, there is a need to authenticate whether it is posted by the legitimate owner of the account or some imposter who might have compromised the legitimate profile. The process of authentication called authorship verification helps to handle the same. In this paper, authorship verification has been performed using different textual features such as n-grams, Bag of words (BOW), stylometric and folksonomy features to examine the authorship of tweets posted by the users on the microblogging platform Twitter. Appropriate classification and statistical analysis techniques have been applied to compute different performance parameters. From the experimental analysis, an important observation found is that though char n-grams have an upper hand to other features, still other applicable measures such as word n-grams, BOW, stylometric and folksonomy features cannot be overlooked as each user maintained consistency in different set of features. Accordingly, different feature selection techniques have been used to rank and select best feature for each user. From the comparative analysis of various similarity and statistical based feature selection techniques it is observed that AHP weighted TOPSIS method surpassed others in terms of different performance parameters. Further computation as per ranked features helped to improve the result by achieving an overall average F-score value of 93.82%.},
	author = {Ravneet Kaur and Sarbjeet Singh and Harish Kumar},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.011},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Authorship verification, Compromised accounts, Online social networks, Natural language processing, AHP, TOPSIS, n-grams, Stylometry},
	pages = {397-414},
	title = {AuthCom: Authorship verification and compromised account detection in online social networks using AHP-TOPSIS embedded profiling based technique},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304275},
	volume = {113},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304275},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.011}}

@article{GUVEN2022116592,
	abstract = {In recent years, deep learning models have been used in the implementation of question answering systems. In this study, the performance of the question answering system was evaluated from the perspective of natural language processing using SQuAD, which was developed to measure the performance of deep learning language models. In line with the evaluations, in order to increase the performance, 3 natural language based methods, namely RNP, that can be used with pre-trained BERT language models have been proposed and they have increased the performance of the question answering system in which the pre-trained BERT models are used by 1.1% to 2.4%. As a result of the application of RNP methods with sentence selection, an increase in accuracy between 6.6% and 8.76% was achieved in answer detection. Since these methods don't require any training process, it has been shown that they can be used in question answering systems to increase the performance of any deep learning model.},
	author = {Zekeriya Anil Guven and Murat Osman Unalir},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116592},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, BERT, Text analysis, Question answering, SQuAD},
	pages = {116592},
	title = {Natural language based analysis of SQuAD: An analytical approach for BERT},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000884},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000884},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116592}}

@article{WANG2018163,
	abstract = {This paper proposes an approach KeyRank to extract proper keyphrases from a document in English. It first searches all keyphrase candidates from the document, and then ranks them for selecting top-N ones as final keyphrases. Existing studies show that extracting a complete keyphrase candidate set that includes semantic relations in context, and evaluating the effectiveness of each candidate are crucial to extract high quality keyphrases from documents. Based on that words do not repeatedly appear in an effective keyphrase in English, a novel keyphrase candidate search algorithm using sequential pattern mining with gap constraints (called KCSP) is proposed to extract keyphrase candidates for KeyRank. And then an effectiveness evaluation measure pattern frequency with entropy (called PF-H) is proposed for KeyRank to rank these keyphrase candidates. Our experimental results show that KeyRank has better performance. Its first component KCSP is much more efficient than a closely related approach SPMW, and its second component PF-H is an effective evaluation mechanism for ranking keyphrase candidates.11Our two-page extended abstract is published in AAAI 2017 (Wang, Sheng, & Wu, 2017).},
	author = {Qingren Wang and Victor S. Sheng and Xindong Wu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.12.031},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Keyphrase candidate search, Sequential pattern mining, Keyphrase candidate ranking, Entropy},
	pages = {163-176},
	title = {Document-specific keyphrase candidate search and ranking},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417308527},
	volume = {97},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417308527},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.12.031}}

@article{DO2019272,
	abstract = {The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the human emotional state. A current research focus for sentiment analysis is the improvement of granularity at aspect level, representing two distinct aims: aspect extraction and sentiment classification of product reviews and sentiment classification of target-dependent tweets. Deep learning approaches have emerged as a prospect for achieving these aims with their ability to capture both syntactic and semantic features of text without requirements for high-level feature engineering, as is the case in earlier methods. In this article, we aim to provide a comparative review of deep learning for aspect-based sentiment analysis to place different approaches in context.},
	author = {Hai Ha Do and PWC Prasad and Angelika Maag and Abeer Alsadoon},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.10.003},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	pages = {272-299},
	title = {Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306456},
	volume = {118},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418306456},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.10.003}}

@article{ZHANG2022115826,
	abstract = {Citation recommendation can help researchers quickly find supplementary or alternative references in massive academic resources. Current research on citation recommendation mainly focuses on the citing papers, resulting in the enormous cited papers are ignored, including the relations among cited papers and their citation context cited in citing papers. Moreover, cited paper's content is often denoted with its original title and abstract, which is hard to acquire and rarely considers different citation motivations. Furthermore, the most appropriate method for semantic representation of cited papers' relations and content is uncertain. Therefore, this paper studies citation recommendation from the perspective of semantic representation of cited papers' relations and content. Firstly, four forms of citation context are designed and extracted as cited papers' content considering citation motivations, as well as co-citation relationships are extracted as cited papers' relations. Secondly, 132 methods are designed for generating semantic vector of cited paper, including four network embedding methods, 16 methods by combining four text representation algorithms with four forms of citation content, and 112 fusion methods. Finally, similarity among cited papers is calculated for citation recommendation and a quantitative evaluation method based on link prediction is designed, to find the most appropriate form of citation content and the optimal method. The result shows that doc2vecC (Document to Vector through Corruption) with the form of CS&SS (Current Sentences and Surrounding Sentences) performs best, in which the AUC (Area Under Curve) and MAP (Macro Average Precision) reach 0.877 and 0.889 and have increased by 0.462 and 0.370 compared with the worst-performing method. This performance is slightly improved by parameters adjustment, and a case study is performed whose results have further proved the effectiveness of this method. In addition, among four forms of cited papers' content, CS&SS performs best in almost all methods. Furthermore, the fusion methods not always perform better than the single methods, where doc2vecC (CS&SS) performs better than the best fusion method GCN (Graph Convolutional Network). These results not only prove the effectiveness of citation recommendation from the perspective of cited paper, but also provide helpful and useful suggestions for method selection and citation content selection. The data and conclusions can be extended to other text mining-related tasks. Simultaneously, it is a preliminary research which needs to be further studied in other domains using emerging semantic representation methods.},
	author = {Jinzhu Zhang and Lipeng Zhu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115826},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Citation recommendation, Cited paper, Co-citation, Citation content, Semantic representation},
	pages = {115826},
	title = {Citation recommendation using semantic representation of cited papers' relations and content},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742101191X},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742101191X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115826}}

@article{ELKASSAS2021113679,
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.},
	author = {Wafaa S. El-Kassas and Cherif R. Salama and Ahmed A. Rafea and Hoda K. Mohamed},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113679},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Automatic text summarization, Text summarization approaches, Text summarization techniques, Text summarization evaluation},
	pages = {113679},
	title = {Automatic text summarization: A comprehensive survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	volume = {165},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420305030},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113679}}

@article{ROSTAMI2019231,
	abstract = {Agile team formation is an important requirement of software companies. Since the members of an agile team should be generalizing specialists (i.e. T-shaped experts), we need members who are specialist in a specific topic and have general knowledge in other topics of the team. Selecting such members results in an ideal team which is flexible, high-performing and low-cost. In this paper, we define the problem of agile team formation in which given a set of required skills of an agile team, the ideal output is a set of low-cost candidates who can collectively cover the required skills while they can effectively communicate with each other. We propose two retrieval models to address this problem and then we introduce three evaluation measures for assessment. These measures are coverage, communication and optimality. Our experiments on two test collections extracted from StackOverflow demonstrate the efficiency of our proposed models in comparison with several strong baselines.},
	author = {Peyman Rostami and Mahmood Neshati},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.10.015},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information retrieval, Expert finding, Agile methodologies, Team formation, T-shaped experts, StackOverflow},
	pages = {231-245},
	title = {T-shaped grouping: Expert finding models to agile software teams retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306572},
	volume = {118},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418306572},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.10.015}}

@article{ZHANG2021115439,
	abstract = {Recently, deep learning has dominated the recommender system, as it is able to effectively capture nonlinear and nontrivial user--item relationships, and perform complex nonlinear transformations. However, there are still some issues with respects to the existing methods. Firstly, they always treat user--item interactions independently, and may fail to cover more complex and hidden information that is inherently implicit in the local neighborhood surrounding an interaction sample. Secondly, by quantifying the dependence degree of user--item sequences, it demonstrates that both short-term and long-term dependent behavioral patterns co-exist. Unfortunately, typical deep learning methods might be problematic when coping with very long-term sequential dependencies. To address these issues, we propose a novel unified neural collaborative recommendation algorithm that capitalizes on memory networks for learning attention embedding from implicit interaction (NCRAE). Particularly, the attention is capable of learning the relative importance of different users and items from user--item interaction sequences, which provides a better solution for concentrating on inputs and helps to better memorize long-term sequential dependencies. Extensive experiments on three real-world datasets show significant improvements of our proposed NCRAE algorithm over the competitive methods. Empirical evidence shows that using memory networks for learning attention embeddings of users' implicit interaction yields better recommendation performance.},
	author = {Yihao Zhang and Xiaoyang Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115439},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Memory networks, Collaborative filtering, Attention embeddings, Behavioral patterns, Recommender systems},
	pages = {115439},
	title = {Learning attention embeddings based on memory networks for neural collaborative recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008538},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421008538},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115439}}

@article{LO2017282,
	abstract = {Social media data can be valuable in many ways. However, the vast amount of content shared and the linguistic variants of languages used on social media are making it very challenging for high-value topics to be identified. In this paper, we present an unsupervised multilingual approach for identifying highly relevant terms and topics from the mass of social media data. This approach combines term ranking, localised language analysis, unsupervised topic clustering and multilingual sentiment analysis to extract prominent topics through analysis of Twitter's tweets from a period of time. It is observed that each of the ranking methods tested has their strengths and weaknesses, and that our proposed `Joint' ranking method is able to take advantage of the strengths of the ranking methods. This `Joint' ranking method coupled with an unsupervised topic clustering model is shown to have the potential to discover topics of interest or concern to a local community. Practically, being able to do so may help decision makers to gauge the true opinions or concerns on the ground. Theoretically, the research is significant as it shows how an unsupervised online topic identification approach can be designed without much manual annotation effort, which may have great implications for future development of expert and intelligent systems.},
	author = {Siaw Ling Lo and Raymond Chiong and David Cornforth},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.03.029},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic identification, Multilingual analysis, Unsupervised learning, Social media},
	pages = {282-298},
	title = {An unsupervised multilingual approach for online social media topic identification},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417301847},
	volume = {81},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417301847},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.03.029}}

@article{JOSHI2019200,
	abstract = {In this paper, we propose SummCoder, a novel methodology for generic extractive text summarization of single documents. The approach generates a summary according to three sentence selection metrics formulated by us: sentence content relevance, sentence novelty, and sentence position relevance. The sentence content relevance is measured using a deep auto-encoder network, and the novelty metric is derived by exploiting the similarity among sentences represented as embeddings in a distributed semantic space. The sentence position relevance metric is a hand-designed feature, which assigns more weight to the first few sentences through a dynamic weight calculation function regulated by the document length. Furthermore, a sentence ranking and a selection technique are developed to generate the document summary by ranking the sentences according to the final score obtained through the fusion of the three sentences selection metrics. We also introduce a new summarization benchmark, Tor Illegal Documents Summarization (TIDSumm) dataset, especially to assist Law Enforcement Agencies (LEAs), that contains two sets of ground truth summaries, manually created, for 100 web documents extracted from onion websites in Tor (The Onion Router) network. Empirical results show that, on DUC 2002, on Blog Summarization, and on TIDSumm datasets, our text summarization approach obtains comparable or better performance than the state-of-the-art methods for different ROUGE metrics.},
	author = {Akanksha Joshi and E. Fidalgo and E. Alegre and Laura Fern{\'a}ndez-Robles},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.03.045},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Extractive text summarization, Auto-encoder, Deep learning, Sentence embedding, TOR darknet, Extractive summarization},
	pages = {200-215},
	title = {SummCoder: An unsupervised framework for extractive text summarization based on deep auto-encoders},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419302192},
	volume = {129},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419302192},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.03.045}}

@article{LI2022118336,
	abstract = {Session-based recommendation (SBR) is a practical task that predicts the next item based on an anonymous behavior sequence. Most of current methods employ graph neural network to model neighboring item transition information from global and local contexts (i.e, other and current sessions). However, they treat neighbors from other sessions equally without considering its items, which may have different contributions with the target item on varied aspects. In other words, they have not explored finer-granularity transition information in the global context, leading to sub-optimal performance. This paper fills this gap by proposing a novel method called Transition Information Enhanced Disentangled Graph Neural Network (TIE-DGNN) to capture finer-granular transition information between items and try to interpret the transition reason by modeling various factors of items. Specifically, we first propose a position-aware global graph to model neighboring item transition in the global context. Then, we slice item embeddings into blocks, each of which represents a factor, and use global-level disentangling layers to separately learn factor embeddings. Meanwhile, we train local-level item embeddings by using attention mechanisms to capture transition information from the current session. Further, inter-session and intra-session embeddings are generated by two types of item embeddings, respectively. Finally, we use contrastive learning techniques to enhance the robustness of two session embeddings. To this end, our model considers two levels of transition information. Especially in global context, it not only consider finer-granularity transition information between items but also take user intents at factor-level into account to interpret the key reason for the transition. Extensive experiments on three benchmark datasets demonstrate its superiority over state-of-the-art methods.},
	author = {Ansong Li and Jihua Zhu and Zhongyu Li and Haozhe Cheng},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118336},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Session-based recommendation, Graph neural networks, Disentangled representation learning, Contrastive learning.},
	pages = {118336},
	title = {Transition Information Enhanced Disentangled Graph Neural Networks for session-based recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014592},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014592},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118336}}

@article{ZHOU2020113361,
	abstract = {Heart failure (HF) is among the most costly diseases to our society, and the prevalence keeps on increasing these days. Early detection of HF plays a vital role in saving lives through adjusting lifestyles and drug interventions that can slow down disease progression or prevent HF. There are many cardiovascular risk factors associated with HF, and they often coexist. In this paper, we assess the predictive value of pathological factors for early HF detection through a social network based approach. We use electronic health records (collected from the project HeartCarer) and compute the similarity of risk factors. The similarity values are used to construct an unweighted and a weighted medical social network. The constructed medical social network is further divided into a HF high-risk group and HF low-risk group using a group division algorithm. Patients in the high-risk group will be suggested for early screening. To evaluate the prediction value of our method, we perform four experiments based on real world data. The results demonstrate the high effectiveness of our method on heart failure risk assessment, with the best accuracy close to 90%.},
	author = {Chunjie Zhou and Ali Li and Aihua Hou and Zhiwang Zhang and Zhenxing Zhang and Pengfei Dai and Fusheng Wang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113361},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Heart failure, Early warning, Social network, Risk factors, Medical big data},
	pages = {113361},
	title = {Modeling methodology for early warning of chronic heart failure based on real medical big data},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030186X},
	volume = {151},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030186X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113361}}

@article{FAHFOUH2020113517,
	abstract = {Opinion review is of great importance for both customers and organizations. Indeed, it helps customers in buying decisions and represents a valuable feedback for the companies, allowing them to improve their productions. However, numerous greedy companies resort to fake reviews in order to influence the customer and brighten the brand image, or to defame the one of their competitors. Various models are proposed in order to detect deceptive opinion reviews. Most of these models adopt traditional methods focusing on feature extraction and traditional classifiers. Unfortunately, these models do not capture the semantic aspect while ignoring the opinion's context. In order to tackle this issue, we propose a new approach based on Paragraph Vector Distributed Bag of Words (PV-DBOW) and the Denoising Autoencoder (DAE). The proposed customized model provides a strong representation which is based on a global representation of the opinions while preserving their semantics. Indeed, the embedding vectors capture the semantic meaning of all words in the context of each opinion. The generated review representations are fed into a fully connected neural network in order to detect deceptive opinion spam. The obtained results concerning the deception dataset show that our model is effective and outperforms the existing state-of-the-art methodologies.},
	author = {Anass Fahfouh and Jamal Riffi and Mohamed {Adnane Mahraz} and Ali Yahyaouy and Hamid Tairi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113517},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive opinion spam, Neural networks, Machine learning, Deep learning, Paragraph vector model, Denoising autoencoder model},
	pages = {113517},
	title = {PV-DAE: A hybrid model for deceptive opinion spam based on neural network architectures},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420303419},
	volume = {157},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420303419},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113517}}

@article{JEONG2022118375,
	abstract = {Social network services have become widely used, and hashtags, which are implicitly involved in delivering specific information, have shown to greatly improve user engagement. A number of prior studies have attempted to recommend appropriate hashtags for each social media user considering his/her posts by consequently extracting the important features from text and images. To develop this multi-dimensionality with hashtag recommendation, user demographic information also plays a significant role in the manner of personalized hashtag recommendation. Thus, this paper proposes the demographic hashtag recommendation (DemoHash) model to utilize users' demographic information extracted from their selfie images, in addition to textual and visual information. The experimental results with the datasets from Instagram show that our proposed model achieves a greater performance with F1-score, Precision, and Recall than the existing hashtag recommendation methods by average of 4.19%, 18.45%, and 3.91%, respectively. Our approach effectively combined the content-based as well as user-oriented modeling for personalized hashtag recommendation.},
	author = {Dahye Jeong and Soyoung Oh and Eunil Park},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118375},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hashtag recommendation, Multi-modal model, Demographic information},
	pages = {118375},
	title = {DemoHash: Hashtag recommendation based on user demographic information},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014877},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014877},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118375}}

@article{BENSASSI2021115375,
	abstract = {Context-aware recommender systems have received considerable attention from industry and academic areas. In this paper, we pay heed to the growing interest in integrating context-awareness and multi-criteria decision making in recommender systems, to deal with the most pressing challenges in music recommender systems, namely the diversity of the recommended playlist, the scalability of the system, and the cold start problem. This paper introduces a new multi-criteria recommendation approach, named MORec, which generates Top-N music recommendations by bootstrapping the system using beforehand collected data. We usher by gauging the relevance of contextual information from the relation between three elements: user, music genre, and the user's context. Then, we apply an aggregation technique to uncover the relationship between the context and the overall rating. Besides, we apply the K-means algorithm to generate a predictive model that comprises clusters of similar contexts defining the association between contextual dimensions and music genres. Carried out experiments emphasize very promising results of our approach in terms of clustering quality, compared to the Partitioning Around Medoids algorithm in terms of connectivity and stability. The comparison versus pioneering recommendation baselines underscored the effectiveness of MORec in terms of recommendation quality and usefulness.},
	author = {Imen {Ben Sassi} and Sadok {Ben Yahia} and Innar Liiv},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115375},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, User-based Study, Multi-criteria recommendation, Context aware recommender system (CARS), Clustering, Music online recommendation (MORec)},
	pages = {115375},
	title = {MORec: At the crossroads of context-aware and multi-criteria decision making for online music recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008010},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421008010},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115375}}

@article{RAHIMI2022116518,
	abstract = {A limitation of many probabilistic topic models such as Latent Dirichlet Allocation (LDA) is their inflexibility to use local contexts. As a result, these models cannot directly benefit from short-distance co-occurrences, which are more likely to be indicators of meaningful word relationships. Some models such as the Bigram Topic Model (BTM) consider local context by integrating language and topic models. However, due to taking the exact word order into account, such models suffer severely from sparseness. Some other models like Latent Dirichlet Co-Clustering (LDCC) try to solve the problem by adding another level of granularity assuming a document as a bag of segments, while ignoring the word order. In this paper, we introduce a new topic model which uses overlapping windows to encode local word relationships. In the proposed model, we assume a document is comprised of fixed-size overlapping windows, and formulate a new generative process accordingly. In the inference procedure, each word is sampled once in only a single window, while influencing the sampling of its other fellow co-occurring words in other windows. Word relationships are discovered in the document level, but the topic of each word is derived considering only its neighbor words in a window, to emphasize local word relationships. By using overlapping windows, without assuming an explicit dependency between adjacent words, we avoid ignoring the word order completely. The proposed model is straightforward, not severely prone to sparseness and as the experimental results show, produces more meaningful and more coherent topics compared to the three mentioned established models.},
	author = {Marziea Rahimi and Morteza Zahedi and Hoda Mashayekhi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116518},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Probabilistic topic model, Latent Dirichlet Allocation, Document clustering, Context window, Local co-occurrence, Word order},
	pages = {116518},
	title = {A probabilistic topic model based on short distance Co-occurrences},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000197},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000197},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116518}}

@article{LIANG2018322,
	abstract = {As the rapid development of Web 2.0 communities, social media service providers offer users a convenient way to share and create their own contents such as online comments, blogs, microblogs/tweets, etc. Understanding the latent emotions of such short texts from social media via the computational model is an important issue as such a model will help us to identify the social events and make better decisions (e.g., investment in stocking market). However, it is always very challenge to detect emotions from above user-generated contents due to the sparsity problem (e.g., a tweet is a short message). In this article, we propose an universal affective model (UAM) to classify readers' emotions over unlabeled short texts. Different from conventional text classification model, the UAM structurally consists of topic-level and term-level sub-models, and detects social emotions from the perspective of readers in social media. Through the evaluation on real-world data sets, the experimental results validate the effectiveness of the proposed model in terms of the effectiveness and accuracy.},
	author = {Weiming Liang and Haoran Xie and Yanghui Rao and Raymond Y.K. Lau and Fu Lee Wang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.027},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic model, Emotion classification, Biterm, Short text},
	pages = {322-333},
	title = {Universal affective model for Readers' emotion classification over short texts},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304445},
	volume = {114},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304445},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.027}}

@article{TERAN201863,
	abstract = {Online social networks (OSNs) are receiving great attention from the research community for different purposes, such as event detection, crisis management, and forecasting, among others. The increasing amount of research conducted with social networks opens the need for a classification methodology regarding trends in the field. This work does not cover all types of social networks; it focuses on the analysis of microblogs as a data source in the context of recommender systems (RSs). The main goal of this work is to provide authors with insights on the trends of academic literature reviews in the proposed context and to provide a comparison of different research approaches. The authors searched for up-to-date research papers related to RS methods using microblogs within a time period of five years, from 2012 to January 2018. Starting from 2012, a significant amount of research related to the subject field of RSs was conducted and identified by the authors of this work. After the filtering process, 39 papers were finally selected from journals and conferences in four different databases related to Internet technologies (i.e., IEEE, ACM, Science Direct, and Springer). A general classification presented in this work is then adopted and used to describe state-of-the-art social network recommendation approaches for microblogging. This work can be extended in the future to include novel methodologies and trends of RSs for microblogs.},
	author = {Luis Ter{\'a}n and Alvin Oti Mensah and Arianna Estorelli},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.006},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Classification, Recommender systems, Social networks, Microblogs},
	pages = {63-73},
	title = {A literature review for recommender systems techniques used in microblogs},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418301453},
	volume = {103},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418301453},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.03.006}}

@article{ALMUZAINI2022117384,
	abstract = {Every day the world produces an enormous amount of textual data. This unstructured text is of little use unless it is labeled using a combination of categories, keywords, tags. Humans can never annotate such massive data, and with a growing divide between the daily produced data and those annotated, the only alternative is to mechanize it. Automatic annotation process helps in saving resources in terms of time and cost. The process of multi-label annotation involves associating a document with multiple relevant labels. This paper proposes an unsupervised model to annotate corpus using multi-labels automatically. The model is based on multi-label topic modeling and genetic algorithm (GA). Topic modeling is a technique to extract the hidden topics from text, and the GA is used to find the optimal number of topics. We hyper-tuned the parameters of the topic modeling using two different training methods: variational Bayes and Gibbs sampling. The class imbalance in a corpus can affect the result of topic modeling, where the majority class dominates the minority class. We overcome this problem using the partitioning method. Though the proposed model was developed for the Arabic dataset, it is language neutral. We tested our model on three large Arabic corpora and three large English social media datasets. For the Arabic language, our work being the first work that tackles multi-label annotation, we needed a reference to compare our model. For the Arabic corpus, we compared the result of automatic annotation against humans using crowdsourcing (whose labeling was checked for quality). The analysis of the annotation shows an agreement among models (machine vs. human) of 79.30%. Moreover, for the English dataset, the results are quite competitive.},
	author = {Huda A. Almuzaini and Aqil M. Azmi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117384},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Arabic corpus, Topic modeling, Multi-label annotation, Genetic algorithm, Latent Dirichlet allocation, Crowdsourcing},
	pages = {117384},
	title = {An unsupervised annotation of Arabic texts using multi-label topic modeling and genetic algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422007266},
	volume = {203},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422007266},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117384}}

@article{GARCIAPABLOS2018127,
	abstract = {With the increase of online customer opinions in specialised websites and social networks, automatic systems to help organise and classify customer reviews by domain-specific aspect categories and sentiment polarity are more needed than ever. Supervised approaches for Aspect Based Sentiment Analysis achieve good results for the domain and language they are trained on, but manually labelling data to train supervised systems for all domains and languages is very costly and time consuming. In this work, we describe W2VLDA, an almost unsupervised system based on topic modelling that, combined with some other unsupervised methods and a minimal configuration step, performs aspect category classification, aspect-term and opinion-word separation and sentiment polarity classification for any given domain and language. We evaluate its domain aspect and sentiment classification performance in the multilingual SemEval 2016 task 5 (ABSA) dataset. We show competitive results for several domains (hotels, restaurants, electronic devices) and languages (English, Spanish, French and Dutch).},
	author = {Aitor Garc{\'\i}a-Pablos and Montse Cuadros and German Rigau},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.08.049},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Opinion mining, Aspect Based Sentiment Analysis, Almost unsupervised, Multilingual, Multidomain},
	pages = {127-137},
	title = {W2VLDA: Almost unsupervised system for Aspect Based Sentiment Analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417305961},
	volume = {91},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417305961},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.08.049}}

@article{CHRISTOPHE2021114831,
	abstract = {Identifying changes in the dynamics of a classification scheme is an important task to solve using textual data streams. Changes in the volume of documents classified into one category could be a sign of a new emerging structure, which therefore gives clues on the need to update the classification scheme. In this paper, we present a method based on forecasting techniques, change detection and time series monitoring in order to raise alerts as soon as a change occurs in the volume of a given category. We build features only based on the textual content that enable us to accurately predict the expected temporal evolution of such category. Then, we use statistical process control to determine if the current volume is too far away from the one we might expect. We test our method on the New York Times Annotated Corpus and on an industrial data set from Electricit{\'e} de France (EDF) and we observe that it raises alerts at the right time compared to other techniques from the literature.},
	author = {Cl{\'e}ment Christophe and Julien Velcin and Jairo Cugliari and Philippe Suignard and Manel Boumghar},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114831},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Change detection, Text streams, Forecasting},
	pages = {114831},
	title = {Change detection in textual classification with unexpected dynamics},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002724},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002724},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114831}}

@article{LI2021114585,
	abstract = {Currently, it is common to see untruthful opinions (also known as review spam, fraud or shilling attack) that resemble each other explicitly or implicitly across multiple business-to-customer websites or opinion sharing communities. Unfortunately, these fake recommendations can be fabricated by individual spammers or results of a manipulation campaign. Considering its severe harmfulness in influencing a product's reputation, grouped spam is more urgent to detect than individual fraud. Most state-of-the-art techniques of labeling grouped spam, e.g., Frequent Itemset Mining (FIM) or Latent Dirichlet Allocation (LDA), are completely unsupervised and incapable of making good use of officially recommended topics, such as appearance, speed and standby are three suggested aspects along a cell phone product in JD.com. In this paper, we introduce a novel approach based on aspect-oriented sentiment mining that can identify spam groups supported by nominated topics. Experiments show that our method is effective and outperforms several state-of-the-art solutions with statistical significance on two metrics, content duplication and burstiness of time.},
	author = {Jiandun Li and Pin Lv and Wei Xiao and Liu Yang and Pengpeng Zhang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114585},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Review spam, Opinion spam, Spam group, Nominated topic, Aspect-oriented sentiment},
	pages = {114585},
	title = {Exploring groups of opinion spam using sentiment analysis guided by nominated topics},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000269},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000269},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114585}}

@article{WEN201719,
	abstract = {Activity recognition has been a hot topic for decades, from the scientific research to the development of off-the-shelf commercial products. Since people perform the activities differently, to avoid overfitting, building a general model with activity data of various users is required before the deployment for personal use. However, annotating a large amount of activity data is expensive and time-consuming. In this paper, we build a general model for activity recognition with a limited amount of labelled data. We combine Latent Dirichlet Allocation (LDA) and AdaBoost to jointly train a general activity model with partially labelled data. After that, when AdaBoost is used for online prediction, we combine it with graphical models (such as HMM and CRF) to exploit the temporal information in human activities to smooth out the accidental misclassifications. Experiments with publicly available datasets show that we are able to obtain the accuracy of more than 90% with 1% labelled data.},
	author = {Jiahui Wen and Zhiying Wang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.01.002},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Activity recognition, General model, Co-training},
	pages = {19-28},
	title = {Learning general model for activity recognition with limited labelled data},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417300027},
	volume = {74},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417300027},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.01.002}}

@article{ARBANE2023118710,
	abstract = {Internet public social media and forums provide a convenient channel for people concerned about public health issues, such as COVID-19, to share and discuss information/misinformation with each other. In this paper, we propose a natural language processing (NLP) method based on Bidirectional Long Short-Term Memory (Bi-LSTM) technique to perform sentiment classification and uncover various issues related to COVID-19 public opinions. Bi-LSTM is an improved version of conventional LSTMs for generating the output from both left and right contexts at each time step. We experimented with real datasets extracted from Twitter and Reddit social media platforms, and our experimental results showed improved metrics compared with the conventional LSTM model as well as recent studies available in the literature. The proposed model can be used by official institutions to mitigate the effects of negative messages and to understand peoples' concerns during the pandemic. Furthermore, our findings shed light on the importance of using NLP techniques to analyze public opinion and to combat the spreading of misinformation and to guide health decision-making.},
	author = {Mohamed Arbane and Rachid Benlamri and Youcef Brik and Ayman Diyab Alahmar},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118710},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bi-LSTM, COVID-19, Deep learning, Natural language processing, Sentiment classification, Social media},
	pages = {118710},
	title = {Social media-based COVID-19 sentiment classification model using Bi-LSTM},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017353},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422017353},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118710}}

@article{KIM2020113401,
	abstract = {Blockchain has become one of the core technologies in Industry 4.0. To help decision-makers establish action plans based on blockchain, it is an urgent task to analyze trends in blockchain technology. However, most of existing studies on blockchain trend analysis are based on effort demanding full-text investigation or traditional bibliometric methods whose study scope is limited to a frequency-based statistical analysis. Therefore, in this paper, we propose a new topic modeling method called Word2vec-based Latent Semantic Analysis (W2V-LSA), which is based on Word2vec and Spherical k-means clustering to better capture and represent the context of a corpus. We then used W2V-LSA to perform an annual trend analysis of blockchain research by country and time for 231 abstracts of blockchain-related papers published over the past five years. The performance of the proposed algorithm was compared to Probabilistic LSA, one of the common topic modeling techniques. The experimental results confirmed the usefulness of W2V-LSA in terms of the accuracy and diversity of topics by quantitative and qualitative evaluation. The proposed method can be a competitive alternative for better topic modeling to provide direction for future research in technology trend analysis and it is applicable to various expert systems related to text mining.},
	author = {Suhyeon Kim and Haecheong Park and Junghye Lee},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113401},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Trend analysis, Topic modeling, Word2vec, Probabilistic latent semantic analysis, Blockchain},
	pages = {113401},
	title = {Word2vec-based latent semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology trend analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420302256},
	volume = {152},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420302256},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113401}}

@article{EFFROSYNIDIS2022117541,
	abstract = {This work creates and makes publicly available the most comprehensive dataset to date regarding climate change and human opinions via Twitter. It has the heftiest temporal coverage, spanning over 13 years, includes over 15 million tweets spatially distributed across the world, and provides the geolocation of most tweets. Seven dimensions of information are tied to each tweet, namely geolocation, user gender, climate change stance and sentiment, aggressiveness, deviations from historic temperature, and topic modeling, while accompanied by environmental disaster events information. These dimensions were produced by testing and evaluating a plethora of state-of-the-art machine learning algorithms and methods, both supervised and unsupervised, including BERT, RNN, LSTM, CNN, SVM, Naive Bayes, VADER, Textblob, Flair, and LDA.},
	author = {Dimitrios Effrosynidis and Alexandros I. Karasakalidis and Georgios Sylaios and Avi Arampatzis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117541},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Climate change, Machine learning, Sentiment analysis, Topic modeling, Twitter},
	pages = {117541},
	title = {The climate change Twitter dataset},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422008624},
	volume = {204},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422008624},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117541}}

@article{BASTANI2019256,
	abstract = {The Consumer Financial Protection Bureau (CFPB), created by congress in 2011, receives and processes consumer complaints pertaining to various financial services. Every complaint narrative provides insight into problems that consumers are experiencing. With increasing number of the CFPB complaint narratives, manual review of these documents by human experts is not feasible. This requires an intelligent system to analyze narratives automatically and provide insightful knowledge to the experts. In this paper, we propose an intelligent approach based on latent Dirichlet allocation (LDA) to analyze the CFPB consumer complaints. The proposed approach aims to extract latent topics in the CFPB complaint narratives, and explores their associated trends over time. The time trends will then be used to evaluate the effectiveness of the CFPB regulations and expectations on financial institutions in creating a consumer oriented culture. The technology-human partnership between the proposed approach and the CFPB experts could certainly improve consumer experience by providing more efficient and effective investigations of consumer complaint narratives.},
	author = {Kaveh Bastani and Hamed Namavari and Jeffrey Shaffer},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.03.001},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Analytics, Latent Dirichlet allocation, Topic modeling, CFPB, Decision support system, Consumer complaint narratives},
	pages = {256-271},
	title = {Latent Dirichlet allocation (LDA) for topic modeling of the CFPB consumer complaints},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741930154X},
	volume = {127},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741930154X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.03.001}}

@article{TAVAKOLI2018186,
	abstract = {Mobile application (app) websites such as Google Play and AppStore allow users to review their downloaded apps. Such reviews can be useful for app users, as they may help users make an informed decision; such reviews can also be potentially useful for app developers, if they contain valuable information concerning user needs and requirements. However, in order to unleash the value of app reviews for mobile app development, intelligent mining tools that can help discern relevant reviews from irrelevant ones must be provided. This paper surveys the state of the art in the development of such tools and techniques behind them. To gain insight into the maturity of the current support mining tools, the paper will also find out what app development information these tools have discovered and what challenges they are facing. The results of this survey can inform the development of more effective and intelligent app review mining techniques and tools.},
	author = {Mohammadali Tavakoli and Liping Zhao and Atefeh Heydari and Goran Nenadi{\'c}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.05.037},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Mobile application review, App review, App review mining, app development, Intelligent app review mining tools, Intelligent app review mining techniques},
	pages = {186-199},
	title = {Extracting useful software development information from mobile application reviews: A survey of intelligent mining techniques and tools},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418303361},
	volume = {113},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418303361},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.05.037}}

@article{FANG2021114306,
	abstract = {The purpose of this paper is to develop a technology-based model for identifying various criteria in a decision-making situation. We used topic modeling to discover critical criteria and their corresponding weights in the Analytic Hierarchy Process (AHP). Approximately 100,000 hotel reviews and 100,000 restaurant reviews were scraped from TripAdvisor.com for criteria determination. Next, an AHP model with criteria and 12 hotels/restaurants as alternatives were compared and ranked. The results compared favorably with more than 1000 reviews of these hotels/restaurants in TripAdvisor.com, thus validating the methodology.},
	author = {Jin Fang and Fariborz Y. Partovi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114306},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Analytic hierarchy process, Topic model, Latent Dirichlet allocation, Hotel selection, Restaurant selection, Group decision},
	pages = {114306},
	title = {Criteria determination of analytic hierarchy process using a topic model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310046},
	volume = {169},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420310046},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114306}}

@article{HUSSAIN2022118119,
	abstract = {Aspect-based sentiment analysis (ABSA) has gained a rising concentration recently. It aims to provide a set of aspect terms and sentiments from a piece of text. Educational Data Mining (EDM) is now an essential tool for analysing pedagogical data. In academic institutions, student feedback is an influential gauge to measure the quality of the teaching--learning process. It helps higher education institutions to reconsider and improve their policies for student recruitment and retention. This paper proposed a situation awareness multi-layer topic modelling and enhanced hybrid machine learning approach for evaluating students' textual feedback data in academic institutions. The proposed Aspect2Labels (A2L) approach is divided our system into three layers. To preserve semantic information, we extracted general aspects terms in the first layer known as high-level aspects. We pulled low-level aspects terms associated with high-level aspect terms in the second layer and the third layer used for sentiment orientation. We used zero-shot learning, LDA, and different variants of LDA for the aspect extraction process. We performed annotation on unlabelled students' comments using our proposed A2L approach, and we obtained 91.3% accuracy in this process. We developed and tested novel algorithms for aspect terms mapping to label each aspect term to corresponding feedback. Different machine learning algorithms have been used to classify sentiments according to extracted aspects. We have also proposed and used Variable Global Feature Selection Scheme (VGFSS) and Variable Stopwords Filtering (VSF) to improve the performance of classifiers. We have managed to get 97% and 93% accuracy on the test dataset using Support Vector Machine (SVM) and Artificial Neural Networks (ANN), respectively. We highly suggest that our novel approach of aspect-oriented sentiment analysis could provide adequate understanding to analyse students' feedback.},
	author = {Shabir Hussain and Muhammad Ayoub and Ghulam Jilani and Yang Yu and Akmal Khan and Junaid Abdul Wahid and Muhammad Farhan Ali Butt and Guangqin Yang and Dietmar P.F. Moller and Hou Weiyan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118119},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect-based sentiment analysis, Machine learning applications in education, Aspect extraction, Topic modelling, Opinion mining, Situational awareness},
	pages = {118119},
	title = {Aspect2Labels: A novelistic decision support system for higher educational institutions by using multi-layer topic modelling approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013045},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422013045},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118119}}

@article{CAO2020113465,
	abstract = {Electronic commerce has become a popular shopping mode. To enhance their reputations, attract more customers, and finally obtain more benefits, dishonest sellers often recruit buyers or robots to post a large number of deceptive reviews to mislead users. According to the interpretability of learning results, existing methods for detecting deceptive reviews can be mainly divided into explicit feature-based mining ones and neural network-based implicit feature mining ones. The nature of these works is accurate text classification based on coarse-grained features (e.g., topic, sentence, and document) or fine-grained features (e.g., word). To take full merits of existing approaches, this paper proposes a new framework that explores a method to combine the coarse-grained features and the fine-grained features. In this framework, the coarse-grained implicit semantic features of the topic distribution are learned by the concatenation of a Latent Dirichlet Allocation (LDA) topic model and a 2-layered neural network. The fine-grained implicit semantic features from the word vectors representation of the reviews are parallelly learned by a deep learning framework. Finally, these two granular features are combined and adopted to train a Support Vector Machine (SVM) classifier for detecting whether a review is deceptive or not. To verify the effectiveness and performance of this framework, we derive three models by specifying three popular deep learning models, such as TextCNN, long short-term memory (LSTM), and Bi-directional LSTM (BiLSTM) to learn the fine-grained features. Experimental results on a mixed-domain dataset and balanced/unbalanced in-domain datasets show that all the combination models are superior to the corresponding baseline models considering single features.},
	author = {Ning Cao and Shujuan Ji and Dickson K.W. Chiu and Mingxiang He and Xiaohong Sun},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113465},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive reviews detection, LDA topic model, Deep learning, Coarse-grained features, Fine-grained features},
	pages = {113465},
	title = {A deceptive review detection framework: Combination of coarse and fine-grained features},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030289X},
	volume = {156},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030289X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113465}}

@article{BELFORD2018159,
	abstract = {Topic models can provide us with an insight into the underlying latent structure of a large corpus of documents. A range of methods have been proposed in the literature, including probabilistic topic models and techniques based on matrix factorization. However, in both cases, standard implementations rely on stochastic elements in their initialization phase, which can potentially lead to different results being generated on the same corpus when using the same parameter values. This corresponds to the concept of ``instability'' which has previously been studied in the context of k-means clustering. In many applications of topic modeling, this problem of instability is not considered and topic models are treated as being definitive, even though the results may change considerably if the initialization process is altered. In this paper we demonstrate the inherent instability of popular topic modeling approaches, using a number of new measures to assess stability. To address this issue in the context of matrix factorization for topic modeling, we propose the use of ensemble learning strategies. Based on experiments performed on annotated text corpora, we show that a K-Fold ensemble strategy, combining both ensembles and structured initialization, can significantly reduce instability, while simultaneously yielding more accurate topic models.},
	author = {Mark Belford and Brian {Mac Namee} and Derek Greene},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.08.047},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic modeling, Topic stability, LDA, NMF},
	pages = {159-169},
	title = {Stability of topic modeling via matrix factorization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417305948},
	volume = {91},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417305948},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.08.047}}

@article{GROKLUMANN2019171,
	abstract = {We examine the long-term relationship between signals derived from nine years of unstructured social media microblog text data and financial market developments in five major economic regions. Employing statistical language modeling techniques we construct directional sentiment metrics and link these to aggregate stock index returns. To address the noise in finance-related Twitter messages we identify expert users whose tweets predominantly focus on finance topics. We document that expert users are the main drivers behind an interdependence between Twitter sentiment and financial markets. The direct prediction value of expert sentiment metrics for stock index returns, however, is found to be elusive and short-lived. Yet, we detect significant predictive gains over benchmark models in times of negative market returns. In consequence, the relation between expert sentiment metrics and stock indices is sufficient to devise hypothetically profitable cross-sectional as well as time series momentum investment strategies for futures based on Twitter signals that survive basic transaction cost assumptions. In this context, our results show that expert sentiment signals can yield higher risk-adjusted returns than classical price-based signals.},
	author = {Axel Gro{\ss}-Klu{\ss}mann and Stephan K{\"o}nig and Markus Ebner},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.06.027},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text mining in finance, Social media data, Sentiment extraction, Trend-Following, Alternative data},
	pages = {171-186},
	title = {Buzzwords build momentum: Global financial Twitter sentiment and the aggregate stock market},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304270},
	volume = {136},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419304270},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.06.027}}

@article{KIM2022117983,
	abstract = {Given the exponential growth of patent documents, automatic patent summarization methods to facilitate the patent analysis process are in strong demand. Recently, the development of natural language processing (NLP), text-mining, and deep learning has greatly improved the performance of text summarization models for general documents. However, existing models cannot be successfully applied to patent documents, because patent documents describing an inventive technology and using domain-specific words have many differences from general documents. To address this challenge, we propose in this study a multi-patent summarization approach based on deep learning to generate an abstractive summarization considering the characteristics of a patent. Single patent summarization and multi-patent summarization were performed through a patent-specific feature extraction process, a summarization model based on generative adversarial network (GAN), and an inference process using topic modeling. The proposed model was verified by applying it to a patent in the drone technology field. In consequence, the proposed model performed better than existing deep learning summarization models. The proposed approach enables high-quality information summary for a large number of patent documents, which can be used by R&D researchers and decision-makers. In addition, it can provide a guideline for deep learning research using patent data.},
	author = {Sunhye Kim and Byungun Yoon},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117983},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Patent summarization, Generative adversarial network (GAN), Patent analysis, Natural language processing (NLP), Text mining},
	pages = {117983},
	title = {Multi-document summarization for patent documents based on generative adversarial network},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012118},
	volume = {207},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422012118},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117983}}

@article{ZHOU2022116194,
	abstract = {Projection-based methods for generating high-quality Cross-Lingual Embeddings (CLEs) have shown state-of-the-art performance in many multilingual applications. Supervised methods that rely on character-level information or unsupervised methods that need only monolingual information are both popular and have their pros and cons. However, there are still problems in terms of the quality of monolingual word embedding spaces and the generation of the seed dictionaries. In this work, we aim to generate effective CLEs with auxiliary Topic Models. We utilize both monolingual and bilingual topic models in the procedure of generating monolingual embedding spaces and seed dictionaries for projection. We present a comprehensive evaluation of our proposed model through the means of bilingual lexicon extraction, cross-lingual semantic word similarity and cross-lingual document classification tasks. We show that our proposed model outperforms existing supervised and unsupervised CLE models built on basic monolingual embedding spaces and seed dictionaries. It also exceeds CLE models generated from representative monolingual topical word embeddings.},
	author = {Dong Zhou and Xiaoya Peng and Lin Li and Jun-mei Han},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116194},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Cross-lingual embeddings, Topical models, Word embedding models, Projection-based methods, Seed dictionaries},
	pages = {116194},
	title = {Cross-lingual embeddings with auxiliary topic models},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421015116},
	volume = {190},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421015116},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116194}}

@article{ZOTOVA2021114547,
	abstract = {Popular social media networks provide the perfect environment to study the opinions and attitudes expressed by users. While interactions in social media such as Twitter occur in many natural languages, research on stance detection (the position or attitude expressed with respect to a specific topic) within the Natural Language Processing field has largely been done for English. Although some efforts have recently been made to develop annotated data in other languages, there is a telling lack of resources to facilitate multilingual and crosslingual research on stance detection. This is partially due to the fact that manually annotating a corpus of social media texts is a difficult, slow and costly process. Furthermore, as stance is a highly domain- and topic-specific phenomenon, the need for annotated data is specially demanding. As a result, most of the manually labeled resources are hindered by their relatively small size and skewed class distribution. This paper presents a method to obtain multilingual datasets for stance detection in Twitter. Instead of manually annotating on a per tweet basis, we leverage user-based information to semi-automatically label large amounts of tweets. Empirical monolingual and cross-lingual experimentation and qualitative analysis show that our method helps to overcome the aforementioned difficulties to build large, balanced and multilingual labeled corpora. We believe that our method can be easily adapted to easily generate labeled social media data for other Natural Language Processing tasks and domains.},
	author = {Elena Zotova and Rodrigo Agerri and German Rigau},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114547},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stance detection, Multilingualism, Text categorization, Fake news, Deep learning},
	pages = {114547},
	title = {Semi-automatic generation of multilingual datasets for stance detection in Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742031191X},
	volume = {170},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742031191X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114547}}

@article{JEYARAJ2022115896,
	abstract = {With overwhelming volumes of official emails being exchanged in enterprises every day, emails have become vital information storehouses. Automatic generation of FAQs from email systems helps in identifying important information and could serve potential applications such as chatbots and intelligent email answering. While there exist studies in the literature focusing on automatic FAQ generation and automated email answering, there are few studies that apply recently developed deep learning techniques to fetch FAQs from emails. This paper proposes a novel framework named F-Gen, which is an expert system that generates potential FAQs from emails utilizing state-of-the-art methodologies. The key characteristics of this study are as follows: 1. Designing F-Gen with various subsystems that interoperate together for the FAQ generation 2. Identifying the parameters that determine a valid FAQ. The three subsystems of F-Gen are: (a) query classifier subsystem (QC subsystem) for email texts, (b) FAQ group generator subsystem (FGG subsystem) for generating FAQ groups from email queries. And (c) FAQ generator subsystem (FG subsystem) for conversion of email query clusters into FAQs. Experiments on the email dataset that practically reflect the above-mentioned problem resulted in FAQs with a ROUGE-1 F-Score of 74.10% when compared with the ground truth.},
	author = {Shiney Jeyaraj and Raghuveera T.},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115896},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Expert system, Deep learning applications, Email text mining, Information retrieval, FAQ generation},
	pages = {115896},
	title = {A deep learning based end-to-end system (F-Gen) for automated email FAQ generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421012525},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421012525},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115896}}

@article{XU2018106,
	abstract = {Traditional topic modeling has been widely studied and popularly employed in expert systems and information systems. However, traditional topic models cannot discover structural relations among topics, thus losing the chance to explore the data more deeply. Hierarchical topic modeling has the capability of learning topics, as well as discovering the hierarchical topic structure from text data. But purely unsupervised models tend to generate weak topic hierarchies. To solve this problem, we propose a novel knowledge-based hierarchical topic model (KHTM), which can incorporate prior knowledge into topic hierarchy building. A key novelty of this model is that it can mine prior knowledge automatically from the topic hierarchies of multiple domains corpora. In this paper, the knowledge is represented as the word pairs which satisfy the requirement of frequent co-occurrence, and knowledge is organized in form of hierarchical structure. We also propose an iterative learning algorithm. For evaluation, we crawled two new multi-domain datasets and conducted comprehensive experiments. The experimental results show that our algorithm and model can generate more coherent topics, and more reasonable hierarchical structure.},
	author = {Yueshen Xu and Jianwei Yin and Jianbin Huang and Yuyu Yin},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.008},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Hierarchical topic modeling, Text mining, Knowledge mining, Non-parametric Bayesian learning, Gibbs sampling},
	pages = {106-117},
	title = {Hierarchical topic modeling with automatic knowledge mining},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418301477},
	volume = {103},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418301477},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.03.008}}

@article{MIRONCZUK201836,
	abstract = {The aim of this study is to provide an overview the state-of-the-art elements of text classification. For this purpose, we first select and investigate the primary and recent studies and objectives in this field. Next, we examine the state-of-the-art elements of text classification. In the following steps, we qualitatively and quantitatively analyse the related works. Herein, we describe six baseline elements of text classification including data collection, data analysis for labelling, feature construction and weighing, feature selection and projection, training of a classification model, and solution evaluation. This study will help readers acquire the necessary information about these elements and their associated techniques. Thus, we believe that this study will assist other researchers and professionals to propose new studies in the field of text classification.},
	author = {Marcin Micha{\l} Miro{\'n}czuk and Jaros{\l}aw Protasiewicz},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.058},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text classification, Document classification, Text classification overview, Document classification overview},
	pages = {36-54},
	title = {A recent overview of the state-of-the-art elements of text classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741830215X},
	volume = {106},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741830215X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.03.058}}

@article{JOSHI2022116846,
	abstract = {In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we formulate a novel strategy to arrange them in their order of importance. A graph-based strategy is applied to find the significant keywords and related sentence rankings in the document. We also formulate a sentence novelty measure based on bigrams, trigrams, and sentence embeddings to eliminate redundant sentences from the summary. The ranks of all the sentences -- computed for each feature -- are finally fused to get the final score for each sentence in the document. We evaluate our approach on publicly available summarization datasets --- CNN/DailyMail and DUC 2002. Experimental results show that our approach outperforms other existing state-of-the-art summarization methods.},
	author = {Akanksha Joshi and Eduardo Fidalgo and Enrique Alegre and Rocio Alaiz-Rodriguez},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116846},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text summarization, Extractive, Topic, Embeddings, Keywords},
	pages = {116846},
	title = {RankSum---An unsupervised extractive text summarization based on rank fusion},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422002998},
	volume = {200},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422002998},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116846}}

@article{MA2023118695,
	abstract = {Mental models play a crucial role in explaining and driving human innovation activities. To help researchers clarify the changes of mental models in various innovation situations, an exploration of its topic dynamic evolution changes is urgently needed. However, most existing works have discussed the topic-semantic distributions of collected documents along the overall timeline, which ignores the semantic details of fusion and evolution between topics in continuous time. This paper discovers and reveals the multi-level information evolving of topics, by integrating latent Dirichlet allocation (LDA) and Word2vec harmoniously to generate the topic evolution maps of the corpus from global to local perspectives. Which include topic distribution trends and their dynamic evolution under the overall time series, as well as the merging and splitting of semantic information between topics in the adjacent time span. These reveal the correlation between topics and the full life cycle of a topic emerging, developing, maturing, and fading. Then, the integrated method was used to perform an analysis of topic evolution with 3984 abstracts of mental model-related papers published between 1980 and 2020. Finally, the performance of the proposed method was compared to that of three traditional topic evolution generated methods based on the standard evaluation metrics. The experimental results demonstrated that our method outperforms other methods both in terms of the content and strength of topic evolution. The proposed method could mine the latent evolution information more clearly and comprehensively from a vast number of papers and is also suited to the various applications of expert systems related to information mining works.},
	author = {Jian Ma and Lei Wang and Yuan-Rong Zhang and Wei Yuan and Wei Guo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118695},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Mental models, Topic evolution, Word2vec, Semantic correlation},
	pages = {118695},
	title = {An integrated latent Dirichlet allocation and Word2vec method for generating the topic evolution of mental models from global to local},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017250},
	volume = {212},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422017250},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118695}}

@article{GREGORIADES2021115546,
	abstract = {This paper presents a machine learning approach involving tourists' electronic word of mouth (eWOM) to support destination marketing campaigns. This approach enhances optimisation of a critical aspect of marketing campaigns, that is, the communication of the right content to the right consumers. The proposed method further considers aggregate cultural and economic-related information of the tourists' country of origin with topic modelling and Decision Tree (DT) models. Each DT addresses different dimensions of culture and purchasing power and the way these dimensions are associated with the topics discussed in eWOM, thus revealing patterns relating tourists' experiences with potential explanations for their dissatisfaction/satisfaction. The method is implemented in a case study in the context of tourism in Cyprus focusing on two hotel groups (2/3 and 4/5 stars) to account for their differences. Patterns emerged from the extraction of rules from DTs illuminate combinations of variables associated with tourist experience (negative or positive) for each of the two hotel categories and verify the asymmetric relationship between service performance and satisfaction. The approach can be used by management during marketing campaigns to design messages to better address the desires and needs of tourists from different cultural and economic backgrounds, as these emerge from the data analysis.},
	author = {Andreas Gregoriades and Maria Pampaka and Herodotos Herodotou and Evripides Christodoulou},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115546},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic modelling, Cultural and economic distance, Decision trees, Shapley additive explanation, Tourists' reviews},
	pages = {115546},
	title = {Supporting digital content marketing and messaging through topic modelling and decision trees},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421009532},
	volume = {184},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421009532},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115546}}

@article{LIU2022116741,
	abstract = {The focused crawler based on semantic analysis is a research hotspot in the field of information retrieval. The domain ontology is generally applied to construct the topic model of the focused crawler. In order to overcome the limitations of builders' knowledge reserve and subjective consciousness in the process of constructing artificially ontology, a semi-automatic construction method of domain ontology based on ontology learning technology combining the latent Dirichlet allocation and the Apriori algorithm is proposed in this article. When evaluating the relevance between a hyperlink and a specific topic, the joint evaluation method considering both the web text and the link structure is usually used. However, the traditional weighted sum method is difficult to reasonably determine the optimal weights of these evaluating indicators. To solve this problem, a multi-objective optimization model for link evaluation and a subsequent multi-objective ant colony optimization algorithm (MOACO) are proposed. In the MOACO, a method of the nearest farthest candidate solution (NFCS) is combined with the fast non-dominated sorting to select a set of Pareto-optimal hyperlinks and guide the crawlers' search directions. The experimental results of the focused crawling on the domain knowledge of typhoon disasters and rainstorm disasters prove that the ability of the proposed focused crawlers to retrieve topic-relevant webpages.},
	author = {Jingfa Liu and Yi Dong and Zhaoxia Liu and Duanbing Chen},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116741},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Focused crawler, Multi-objective ant colony optimization, Ontology, Ontology learning},
	pages = {116741},
	title = {Applying ontology learning and multi-objective ant colony optimization method for focused crawling to meteorological disasters domain knowledge},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742200210X},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742200210X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116741}}

@article{HARALABOPOULOS2021114769,
	abstract = {Text has traditionally been used to train automated classifiers for a multitude of purposes, such as: classification, topic modelling and sentiment analysis. State-of-the-art LSTM classifier require a large number of training examples to avoid biases and successfully generalise. Labelled data greatly improves classification results, but not all modern datasets include large numbers of labelled examples. Labelling is a complex task that can be expensive, time-consuming, and potentially introduces biases. Data augmentation methods create synthetic data based on existing labelled examples, with the goal of improving classification results. These methods have been successfully used in image classification tasks and recent research has extended them to text classification. We propose a method that uses sentence permutations to augment an initial dataset, while retaining key statistical properties of the dataset. We evaluate our method with eight different datasets and a baseline Deep Learning process. This permutation method significantly improves classification accuracy by an average of 4.1%. We also propose two more text augmentations that reverse the classification of each augmented example, antonym and negation. We test these two augmentations in three eligible datasets, and the results suggest an -averaged, across all datasets-improvement in classification accuracy of 0.35% for antonym and 0.4% for negation, when compared to our proposed permutation augmentation.},
	author = {Giannis Haralabopoulos and Mercedes Torres Torres and Ioannis Anagnostopoulos and Derek McAuley},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114769},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text, Augmentation, Multilabel, Multiclass, LSTM},
	pages = {114769},
	title = {Text data augmentations: Permutation, antonyms and negation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002104},
	volume = {177},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002104},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114769}}

@article{DONG2018210,
	abstract = {In electronic commerce, online reviews play very important roles in customers' purchasing decisions. Unfortunately, malicious sellers often hire buyers to fabricate fake reviews to improve their reputation. In order to detect deceptive reviews and mine the topics and sentiments from the reviews, in this paper, we propose an unsupervised topic-sentiment joint probabilistic model (UTSJ) based on Latent Dirichlet Allocation (LDA) model. This model first employs Gibbs sampling algorithm to approximate parameters of maximum likelihood function offline and obtain topic-sentiment joint probabilistic distribution vector for each review. Secondly, a Random Forest classifier and a SVM (Support Vector Machine) classifier are trained offline, respectively. Experimental results on real-life datasets show that our proposed model is better than baseline models such as n-grams, character n-grams in token, POS (part-of-speech), LDA, and JST (Joint Sentiment/Topic). Moreover, our UTSJ model outperforms or performs similarly to benchmark models in detecting deceptive reviews over balanced dataset and unbalanced dataset in different domains. Particularly, our UTSJ model is good at dealing with real-life unbalanced big data, which makes it very suitable for being applied in e-commerce environment.},
	author = {Lu-yu Dong and Shu-juan Ji and Chun-jin Zhang and Qi Zhang and DicksonK.W. Chiu and Li-qing Qiu and Da Li},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.005},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive review detection, Topic-sentiment joint probabilistic model, Latent dirichlet allocation, Gibbs sampling},
	pages = {210-223},
	title = {An unsupervised topic-sentiment joint probabilistic model for detecting deceptive reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304184},
	volume = {114},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304184},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.005}}

@article{PAVLINEK201783,
	abstract = {Supervised text classification methods are efficient when they can learn with reasonably sized labeled sets. On the other hand, when only a small set of labeled documents is available, semi-supervised methods become more appropriate. These methods are based on comparing distributions between labeled and unlabeled instances, therefore it is important to focus on the representation and its discrimination abilities. In this paper we present the ST LDA method for text classification in a semi-supervised manner with representations based on topic models. The proposed method comprises a semi-supervised text classification algorithm based on self-training and a model, which determines parameter settings for any new document collection. Self-training is used to enlarge the small initial labeled set with the help of information from unlabeled data. We investigate how topic-based representation affects prediction accuracy by performing NBMN and SVM classification algorithms on an enlarged labeled set and then compare the results with the same method on a typical TF-IDF representation. We also compare ST LDA with supervised classification methods and other well-known semi-supervised methods. Experiments were conducted on 11 very small initial labeled sets sampled from six publicly available document collections. The results show that our ST LDA method, when used in combination with NBMN, performed significantly better in terms of classification accuracy than other comparable methods and variations. In this manner, the ST LDA method proved to be a competitive classification method for different text collections when only a small set of labeled instances is available. As such, the proposed ST LDA method may well help to improve text classification tasks, which are essential in many advanced expert and intelligent systems, especially in the case of a scarcity of labeled texts.},
	author = {Miha Pavlinek and Vili Podgorelec},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.03.020},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Classification, Topic modeling, LDA, Semi-supervised learning, Self-training},
	pages = {83-93},
	title = {Text classification method based on self-training and LDA topic models},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417301665},
	volume = {80},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417301665},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.03.020}}

@article{DARGAHINOBARI2021114303,
	abstract = {Telegram is a new Instant Messaging application providing key features for both public and private messaging. Telegram is similar to group broadcast or micro-blogging platforms, while on the other hand, it has features of ordinary Instant Messaging applications such as WhatsApp. In this paper, investigating a real dataset crawled from Telegram, we provide several observations which can explain the information flow, business model of content providers, and social sensing aspects of Telegram. The crawled dataset which is manually labeled by six persons contains two months of public messages of selected Telegram channels. Moreover, we introduce the viral messages in instant messaging services and propose formal definition of these messages as well as deeply analyzing their characteristics and features. Detection of virality characteristics of messages in Telegram can be beneficial for both end-users and digital marketers. Consequently, we propose statistical and word embedding approaches to detect viral messages and their sentiment and message category.Our experiments indicate that the word embedding approach can significantly outperform other baseline models.},
	author = {Arash {Dargahi Nobari} and Malikeh Haj Khan Mirzaye Sarraf and Mahmood Neshati and Farnaz {Erfanian Daneshvar}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114303},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Telegram, Instant messaging, Sentiment analysis, Social sensing, Viral message},
	pages = {114303},
	title = {Characteristics of viral messages on Telegram; The world's largest hybrid public and private messenger},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310010},
	volume = {168},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420310010},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114303}}

@article{VANDINTER2021115261,
	abstract = {The systematic literature review (SLR) process includes several steps to collect secondary data and analyze it to answer research questions. In this context, the document retrieval and primary study selection steps are heavily intertwined and known for their repetitiveness, high human workload, and difficulty identifying all relevant literature. This study aims to reduce human workload and error of the document retrieval and primary study selection processes using a decision support system (DSS). An open-source DSS is proposed that supports the document retrieval step, dataset preprocessing, and citation classification. The DSS is domain-independent, as it has proven to carefully select an article's relevance based solely on the title and abstract. These features can be consistently retrieved from scientific database APIs. Additionally, the DSS is designed to run in the cloud without any required programming knowledge for reviewers. A Multi-Channel CNN architecture is implemented to support the citation screening process. With the provided DSS, reviewers can fill in their search strategy and manually label only a subset of the citations. The remaining unlabeled citations are automatically classified and sorted based on probability. It was shown that for four out of five review datasets, the DSS's use achieved significant workload savings of at least 10%. The cross-validation results show that the system provides consistent results up to 88.3% of work saved during citation screening. In two cases, our model yielded a better performance over the benchmark review datasets. As such, the proposed approach can assist the development of systematic literature reviews independent of the domain. The proposed DSS is effective and can substantially decrease the document retrieval and citation screening steps' workload and error rate.},
	author = {Raymon {van Dinter} and Cagatay Catal and Bedir Tekinerdogan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115261},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Systematic literature review (SLR), Citation screening, Document retrieval, Decision support, Automation, Deep learning, Convolutional neural network, Natural language processing},
	pages = {115261},
	title = {A decision support system for automating document retrieval and citation screening},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100693X},
	volume = {182},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100693X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115261}}

@article{LEE2018121,
	abstract = {In the past, most historical research has been manually carried out by exploring historical facts reading between the lines of documents. Nowadays, historical big data has become electronically available and advances in machine learning techniques allow us to analyze the vast amount of historical data. From a historical perspective, making inferences about political stances of historical figures is important for grasping historical rivalries and power structures of an era. Thus, in this paper, we propose an approach to the systematic inference of power mechanisms based on a human network constructed from historical data. In this network, humans are linked according to the degree of kinship using genealogy records, and identified by political stances on agendas recorded in the annals of a dynasty as a political force. And then, a machine learning algorithm, semi-supervised learning, classifies humans who cannot identify political stances as political forces that reflect the links of the networks. The data consist of the genealogy of the Andong Gwon clan, a record of family relations of 10,243 people from the 10th to 15th century Korea, and the Annals of the Joseon Dynasty, a historical volume that describes historical facts of the Joseon Dynasty for 472 years and is composed of 1894 fascicles and 888 books. From the data, we construct a human network based on a historically meaningful period (1443--1488), and classify people into two political forces using the proposed method. We suggest that this machine learning approach to historical study could be utilized as a potent reference tool devoid of the subjectivism of human experts in the field of history.},
	author = {Dong-gi Lee and Sangkuk Lee and Myungjun Kim and Hyunjung Shin},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.059},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Machine learning, Semi-supervised learning, Historical big data, Genealogy},
	pages = {121-131},
	title = {Historical inference based on semi-supervised learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418302161},
	volume = {106},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418302161},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.03.059}}

@article{BOUGIATIOTIS201886,
	abstract = {In this paper, we examine the ability of low-level multimodal features to extract movie similarity, in the context of a content-based movie recommendation approach. In particular, we demonstrate the extraction of multimodal representation models of movies, based on textual information from subtitles, as well as cues from the audio and visual channels. With regards to the textual domain, we emphasize our research in topic modeling of movies based on their subtitles, in order to extract topics that discriminate between movies. Regarding the visual domain, we focus on the extraction of semantically useful features that model camera movements, colors and faces, while for the audio domain we adopt simple classification aggregates based on pretrained models. The three domains are combined with static metadata (e.g. directors, actors) to prove that the content-based movie similarity procedure can be enhanced with low-level multimodal information. In order to demonstrate the proposed content representation approach, we have built a small dataset of 160 widely known movies. We assert movie similarities, as propagated by the individual modalities and fusion models, in the form of recommendation rankings. Extensive experimentation proves that all three low-level modalities (text, audio and visual) boost the performance of a content-based recommendation system, compared to the typical metadata-based content representation, by more than 50% relative increase. To our knowledge, this is the first approach that utilizes a wide range of features from all involved modalities, in order to enhance the performance of the content similarity estimation, compared to the metadata-based approaches.},
	author = {Konstantinos Bougiatiotis and Theodoros Giannakopoulos},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.11.050},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Content-based movie recommendation, Topic modeling, Movie audio-Visual analysis, Multimodal fusion, Information retrieval},
	pages = {86-102},
	title = {Enhanced movie content similarity based on textual, auditory and visual information},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417308059},
	volume = {96},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417308059},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.11.050}}

@article{MEILIAN2020113427,
	abstract = {Learning the low-dimensional vector representation of networks can effectively reduce the complexity of various network analysis tasks, such as link prediction, clustering and classification. However, most of the existing network representation learning (NRL) methods are aimed at homogeneous or static networks, while the real-world networks are usually heterogeneous and tend to change dynamically over time, therefore providing an intelligent insight into the evolution of heterogeneous networks is more practical and significant. Based on this consideration, we focus on the dynamic representation learning problem for heterogeneous information networks, and propose a random walk based Dynamic Representation Learning method for Heterogeneous Information Networks (HIN_DRL), which can learn the representation of network nodes at different timestamps. Specifically, we improve the first step of the existing random walk based NRL methods, which generally include two steps: constructing node sequences through random walk process, and then learning node representations by throwing the node sequences into a homogeneous or heterogeneous Skip-Gram model. In order to construct optimized node sequences for evolving heterogeneous networks, we propose a method for automatically extracting and extending meta-paths, and propose a new method for generating node sequences via dynamic random walk based on meta-path and timestamp information of networks. We also propose two strategies for adjusting the quantity and length of node sequences during each random walk process, which makes it more effective to construct the node sequences for heterogeneous information networks at a specific timestamp, thus improving the effect of dynamic representation learning. Extensive experimental results show that compared with the state-of-art algorithms, HIN_DRL achieves better results in Macro-F1, Micro-F1 and NMI for multi-label node classification, multi-class node classification and node clustering on several real-world network datasets. Furthermore, case studies of visualization and dynamic on Microsoft Academic dataset demonstrate that HIN_DRL can learn network representation dynamically and more effectively.},
	author = {LU Meilian and YE Danna},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113427},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Dynamic representation learning, Heterogeneous information networks, Meta path, Dynamic random walk},
	pages = {113427},
	title = {HIN_DRL: A random walk based dynamic network representation learning method for heterogeneous information networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420302517},
	volume = {158},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420302517},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113427}}

@article{WAHID2022116562,
	abstract = {The abundant use of social media impacts every aspect of life, including crisis management. Disaster management needs real-time data to be used in machine learning and deep learning models to aid their decision making. Mostly the data that is newly generated from social media is unstructured and unlabeled. Current text classification models based on supervised deep learning models heavily rely on human-labeled data that very small size and imbalanced in the context of disasters, ultimately affecting the generalization of models. In this study, we propose Topic2labels (T2L) framework which provides an automated way of labeling the data through LDA (latent dirichlet allocation) topic modeling approach and utilize Bert (the bidirectional encoder representation from transformer) embeddings for construction of feature vector to be employed to classify the data contextually. Our framework consists of three layers. In the first layer, we adopt LDA to generate the topics from the data, and develop a new algorithm to rank the topics, and map the highest ranked dominant topic into label to annotate the data. In the second layer, we transform the labeled text into feature representation through Bert embeddings and in the third layer we leveraged deep learning models as classifiers to classify the textual data into multiple categories. Experimental results on crisis-related datasets show that our framework performs better in terms of classification performance and yields improvement as compared to other baseline approaches.},
	author = {Junaid Abdul Wahid and Lei Shi and Yufei Gao and Bei Yang and Lin Wei and Yongcai Tao and Shabir Hussain and Muhammad Ayoub and Imam Yagoub},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116562},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social media, Natural language processing, Neural network, Topic modeling, Annotation, Classification, Transformer, Crisis response},
	pages = {116562},
	title = {Topic2Labels: A framework to annotate and classify the social media data through LDA topics and deep learning models for crisis response},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000604},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000604},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116562}}

@article{FALLAHNEJAD2022116433,
	abstract = {The growing popularity of community question answering websites can be seen by the growing number of users. Many methods are proposed to identify talented users in these communities, but many of them suffer from vocabulary mismatches. The solution to this problem can be found in translation approaches. The present paper proposes two translation methods for extracting more relevant translations. The proposed methods rely on the attention mechanism. The methods use multi-label classifiers that take each question as input and predict the skills related to the question. Using the attention mechanism, the model is able to focus on specific parts of the given input and predict the correct labels. The ultimate goal of these networks is to predict skills related to questions. Using word attention scores, we can find out how relevant a single word is to a particular skill. As a result of these attention scores, we obtain more relevant translations for each skill. We then use these translations to bridge the lexical gap and improve expert retrieval results. Extensive experiments on two large sub-collections of the StackOverflow dataset demonstrate that the proposed methods outperform the best baseline method by up to 14.11/% MAP improvement.},
	author = {Zohreh Fallahnejad and Hamid Beigy},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116433},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Expert finding, Semantic matching, Translation models, StackOverflow},
	pages = {116433},
	title = {Attention-based skill translation models for expert finding},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421017206},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421017206},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116433}}

@article{DAU2020112871,
	abstract = {The main goal of Aspect-Based Opinion Mining is to extract product's aspects and the associated user opinions from the user text review. Although this serves as vital source information for enhancing rating prediction performance, few studies have attempted to fully utilize it for better accuracy of recommendation systems. Most of these studies typically assign equal weights to all aspects in the opinion mining process, however, in practices; users tend to give different priority on different aspects of the product when reaching overall ratings. In addition, most of the existing methods typically rely on handcrafted, rule-based or double propagation methods in the opinion mining process which are known to be time-consuming and often inclined to errors. This could affect the reliability and performance of the recommender systems (RS). Therefore, in this paper, we propose a weighted Aspect-based Opinion mining using Deep learning method for Recommender system (AODR) that can extract product's aspects and the underlying weighted user opinions from the review text using a deep learning method and then fuse them into extended collaborative filtering (CF) technique for improving the RS. The proposed method is basically comprised of two components: (1) Aspect-based opinion mining module which aims to extract the product aspects from the review text to generate aspect rating matrix. (2) Recommendation generation component that uses tensor factorization (TF) technique to compute weighted aspect ratings and finally infer the overall rating prediction. We evaluate the proposed model in terms of both aspect extraction and recommendation performance. Experiment results on different datasets show that our AODR model achieves better results compared to the baselines.},
	author = {Aminu Da'u and Naomie Salim and Idris Rabiu and Akram Osman},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112871},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect-based opinion mining, Convolutional neural network, Deep learning, Collaborative filtering, Recommender system, Rating prediction},
	pages = {112871},
	title = {Weighted aspect-based opinion mining using deep learning for recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419305810},
	volume = {140},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419305810},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112871}}

@article{CATELLI2022118290,
	abstract = {Today, reviews are the advertising medium par excellence through which companies are able to influence customers' spending decisions. Although the initial purpose of reviews was to provide companies with a feedback tool to improve products and services based on customer needs, they soon became a way to climb the sales rankings, often illegally. In fact, deceptive and fake reviews have managed to evade the often non-existent means of validation of online platforms, proliferating a new business. To combat this phenomenon, several classification methods have been developed to train automated tools in the arduous task of distinguishing between genuine and misleading reviews, the most recent based on machine and deep learning techniques. This paper proposes a multi-label classification methodology based on the Google BERT neural language model to build a deceptive review detector aided by its sentiment awareness: improved modeling of the link between sentiment polarity and deceptiveness during the fine-tuning phase by exploiting the Binary Cross Entropy with Logits loss function adds to the advantages provided by pre-trained contextual models, which are able to capture word polysemy through word embeddings and benefit from pre-training on huge corpora. Tests were performed on the Deceptive Opinion Spam Corpus and Yelp New York City datasets, providing a quantitative and qualitative analysis of the results which, when compared with the state of the art available in the literature, showed an encouraging increase in performance.},
	author = {Rosario Catelli and Hamido Fujita and Giuseppe {De Pietro} and Massimo Esposito},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118290},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive reviews, Sentiment, BERT, Multi-label, Deep learning, Neural language model},
	pages = {118290},
	title = {Deceptive reviews and sentiment polarity: Effective link by exploiting BERT},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014269},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014269},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118290}}

@article{WU2019285,
	abstract = {Sentiment lexicon plays an important role in sentiment analysis system. In most existing sentiment lexica, each sentiment word or phrase is given a sentiment label or score. However, a sentiment word may express different sentiment orientations describing different targets. It's beneficial but challenging to incorporate knowledge of opinion targets into sentiment lexicon. In this paper we propose an automatic approach to construct a target-specific sentiment lexicon, in which each term is an opinion pair consisting of an opinion target and an opinion word. The approach solves two principle problems in construction process, namely, opinion target extraction and opinion pair sentiment classification. An unsupervised algorithm is proposed to extract opinion pairs in high quality. Both semantic feature and syntactic feature are incorporated in the algorithm, to extract opinion pairs containing correct opinion targets. A group of opinion pairs are generated and a framework is proposed to classify their sentiment polarities. Knowledge of available resources including general-purpose sentiment lexicon and thesaurus, and context knowledge including syntactic relations and sentiment information in sentences, are extracted and integrated in a unified framework to calculate sentiment scores of opinion pairs. Experimental results on product reviews datasets in different domains prove the effectiveness of our method in target-specific sentiment lexicon construction, which can improve performances of opinion target extraction and opinion pair sentiment classification. In addition, our lexicon also achieves better performance in target-level sentiment classification compared with several general-purpose sentiment lexicons.},
	author = {Sixing Wu and Fangzhao Wu and Yue Chang and Chuhan Wu and Yongfeng Huang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.09.024},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Opinion mining, Sentiment lexicon, Sentiment analysis, Opinion target extraction},
	pages = {285-298},
	title = {Automatic construction of target-specific sentiment lexicon},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306018},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418306018},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.09.024}}

@article{GOMEZ2022118400,
	abstract = {The recent pandemic has changed the way we see education. During recent years, Massive Open Online Course (MOOC) providers, such as Coursera or edX, are reporting millions of new users signing up on their platforms. Though online review systems are standard among many verticals, no standardized or fully decentralized review systems exist in the MOOC ecosystem. In this vein, we believe that there is an opportunity to leverage available open MOOC reviews in order to build simpler and more transparent reviewing systems, allowing users to really identify the best courses out there. Specifically, in our research we analyze 2.4 million reviews (which is the largest MOOC reviews dataset used until now) from five different platforms in order to determine the following: (1) if the numeric ratings provide discriminant information to learners, (2) if NLP-driven sentiment analysis on textual reviews could provide valuable information to learners, (3) if we can leverage NLP-driven topic finding techniques to infer themes that could be important for learners, and (4) if we can use these models to effectively characterize MOOCs based on the open reviews. Results show that numeric ratings are clearly biased (63% of them are 5-star ratings), and the topic modeling reveals some interesting topics related with course advertisements, the real applicability, or the difficulty of the different courses.},
	author = {Manuel J. Gomez and Mario Calder{\'o}n and Victor S{\'a}nchez and F{\'e}lix J. Garc{\'\i}a Clemente and Jos{\'e} A. Ruip{\'e}rez-Valiente},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118400},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Massive Open Online Courses, Natural language processing, Sentiment analysis, Recommendation systems, Online education},
	pages = {118400},
	title = {Large scale analysis of open MOOC reviews to support learners' course selection},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015081},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015081},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118400}}

@article{TU201720,
	abstract = {Nowadays, due to the rapid growth of digital technologies, huge volumes of image data are created and shared on social media sites. User-provided tags attached to each social image are widely recognized as a bridge to fill the semantic gap between low-level image features and high-level concepts. Hence, a combination of images along with their corresponding tags is useful for intelligent retrieval systems, those are designed to gain high-level understanding from images and facilitate semantic search. However, user-provided tags in practice are usually incomplete and noisy, which may degrade the retrieval performance. To tackle this problem, we present a novel retrieval framework that automatically associates the visual content with textual tags and enables effective image search. To this end, we first propose a probabilistic topic model learned on social images to discover latent topics from the co-occurrence of tags and image features. Moreover, our topic model is built by exploiting the expert knowledge about the correlation between tags with visual contents and the relationship among image features that is formulated in terms of spatial location and color distribution. The discovered topics then help to predict missing tags of an unseen image as well as the ones partially labeled in the database. These predicted tags can greatly facilitate the reliable measure of semantic similarity between the query and database images. Therefore, we further present a scoring scheme to estimate the similarity by fusing textual tags and visual representation. Extensive experiments conducted on three benchmark datasets show that our topic model provides the accurate annotation against the noise and incompleteness of tags. Using our generalized scoring scheme, which is particularly advantageous to many types of queries, the proposed approach also outperforms state-of-the-art approaches in terms of retrieval accuracy.},
	author = {Nguyen Anh Tu and Kifayat Ullah Khan and Young-Koo Lee},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.01.055},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Image retrieval, Image annotation, Social image tagging, Topic modeling, Probabilistic graphical model},
	pages = {20-33},
	title = {Featured correspondence topic model for semantic search on social image collections},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417300684},
	volume = {77},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417300684},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.01.055}}

@article{LEBENA2022117303,
	abstract = {In this work, we cope with the classification of Electronic Health Records (EHR) in Spanish according to the International Classification of Diseases (ICD). We employ Topic Models representing each document as a probabilistic distribution over topics, offering a low-dimensional representation of documents. The trend is to turn to an embedding text representation, but these approaches require large amounts of textual data. We found Topic Models as a suitable alternative approach to deal with the few resources available for Spanish clinical text mining. Besides, they are interpretable and aid the explainability in artificial intelligence (XAI). We explored two different methods, known as Latent Dirichlet Allocation (LDA) and Partially Labelled Latent Dirichlet Allocation (PLDA), the supervised approach of the former. We assessed the results attained in Spanish with an analogous task in English as a reference. Evaluation methods were applied directly to the representation, with metrics to determine topic coherence and the relationship between topics and ICD labels. We learned that PLDA was able to discover topics associated with the ICD. This finding means that this representation itself can reveal ICD codes previous to classification. Also, this representation was used as predictive features to feed a conventional classifier to show their competence in a downstream task. We conclude that in a context with a lack of big data availability, PLDA emerges as a versatile candidate, able to offer a competitive representation of EHRs. While other works are primarily concerned with supervised categorization and do not pay attention to the representation, LDA and PLDA offer an interpretable approach that can be associated with ICDs. Moreover, compared with those that employ LDA, we demonstrate how its' supervised version, PLDA, can be more intuitive as it shows a closer relation with the ICDs.},
	author = {Nuria Lebe{\~n}a and Alberto Blanco and Alicia P{\'e}rez and Arantza Casillas},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117303},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-label classification, Document classification, Electronic Health Records, ICD classification, Topic models, Partially labelled dirichlet allocation},
	pages = {117303},
	title = {Preliminary exploration of topic modelling representations for Electronic Health Records coding according to the International Classification of Diseases in Spanish},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422006662},
	volume = {204},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422006662},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117303}}

@article{ALDUNATE2022118309,
	abstract = {It is of utmost importance for marketing academics and service industry practitioners to understand the factors that influence customer satisfaction. This study proposes a novel framework to analyze open-ended survey data and extract drivers of customer satisfaction. This is done automatically via deep learning models for natural language processing. According to 11 drivers acknowledged by the marketing literature to determine customer experience, the data is cast into a multi-label classification problem. This expert system not only supports the automatic analysis of new data but also ranks the drivers according to their importance to various service industries and provides important insights into their applications. Experiments carried out using 25,943 customer survey responses related to 39 service companies in 13 different economic sectors show that the drivers can be identified accurately.},
	author = {{\'A}ngeles Aldunate and Sebasti{\'a}n Maldonado and Carla Vairetti and Guillermo Armelini},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118309},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Analytics, Customer satisfaction, Customer feedback, Natural language processing, Deep learning, BERT},
	pages = {118309},
	title = {Understanding customer satisfaction via deep learning and natural language processing},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014397},
	volume = {209},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014397},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118309}}

@article{SRINIVASARAO2022116475,
	abstract = {Email messaging is the most common way of providing effective communication between internauts. Consequently, the total sent and received emails count will be increased. But, the internaut can't remember all such emails. Even though email thread identification approaches give satisfactory benefits to the internauts, but they may fail to alert them for a cause to identify the sentiments behind an email thread. To address, this issue Probabilistic Latent Semantic Analysis clustering algorithm has been used in this paper to identify the email sentiment thread sequence. The sentiment and the thread sequence within the emails have been discovered as clustering sentiment polarity and temporal categories with the help of PLSA clusters. At the initial stage, we used three feature extraction methods, latent semantic analysis (LDA), bag of words (BoW), TF-IDF and SentiWordNet (SWN) lexicon for generating sentiment features of email. Next, Probabilistic Latent Semantic Analysis algorithm is used to form email clusters based on sentiment features. Thus, it helps to identify thread sentiment and sequence of sentiment threads. Email threads give a mechanism by which any user will be able to find out the sequence in the thread on the basis of sentiment analysis of email related to a specific set of communication during a specific time period. Various parameters evaluation measures have been considered in this work to evaluate the proposed model such as accuracy, precision, recall and F-measure, and the proposed algorithm is compared with other standard algorithms. Furthermore, a statistical test has also been performed.},
	author = {Ulligaddala Srinivasarao and Aakanksha Sharaff},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116475},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Sentiment email clusters, Probabilistic latent semantic analysis, Topic modeling, SWN lexicon, Sentiment sequence of threads},
	pages = {116475},
	title = {Email thread sentiment sequence identification using PLSA clustering algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421017553},
	volume = {193},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421017553},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116475}}

@article{ZHENG2018244,
	abstract = {We present a tag-aware dynamic music recommendation framework that achieves personalized and accurate music recommendations to users. The proposed framework leverages the available semantic labels (in terms of tags) of music tracks to complement a highly sparse user-item interaction matrix, which effectively addresses the data sparsity issue faced by most music recommendation systems. Music tracks are more accurately represented by aggregating the latent factors derived from both the tag space and the user interaction information. The proposed framework further employs a Gaussian state-space model to capture the evolving nature of users' preferences over time, which helps achieve time-sensitive recommendation of music. A variational approximation is developed to achieve fast inference and learning of model parameters. Experiments conducted using actual music data and comparison with state-of-the-art competitive recommendation algorithms help demonstrate the effectiveness of the proposed framework.},
	author = {Ervine Zheng and Gustavo Yukio Kondo and Stephen Zilora and Qi Yu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.04.014},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Music recommendation, Matrix factorization, Temporal dynamics, Tag aware},
	pages = {244-251},
	title = {Tag-aware dynamic music recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418302446},
	volume = {106},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418302446},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.04.014}}

@article{CHEN2018516,
	abstract = {Event detection over microblogs has attracted great research interest due to its wide application in crisis management and decision making etc. In natural disasters, complex events are reported in real time on social media sites, but these reports are invisible to crisis coordinators. Detecting these crisis events helps watchers to make right decisions rapidly, reducing injuries, deaths and economic loss. In sporting activities, detecting events helps audiences make better and more timely game viewing plans. However, existing event detection techniques are not effective at handling complex social events that evolve over time. In this paper, we propose an event detection method that takes advantage of retweeting behavior for handling the events evolution. Specifically, we first propose a topic model called RL-LDA to capture the social media information over hashtag, location, textual and retweeting behavior. Using RL-LDA, a complex event can be well handled by exploring the correlation between retweeting behavior and the event. Then to maintain the RL-LDA in a dynamic environment, we propose a dynamic update algorithm, which incrementally updates events over real time streams. Experiments over real-world datasets show that RL-LDA detects the temporal evolution of complex events effectively and efficiently.},
	author = {Xi Chen and Xiangmin Zhou and Timos Sellis and Xue Li},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.08.022},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social media, Event detection, Retweeting behavior},
	pages = {516-523},
	title = {Social event detection with retweeting behavior correlation},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741830530X},
	volume = {114},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741830530X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.08.022}}

@article{SKRJANC2022117881,
	abstract = {In this paper, we present an evolving data-based approach to automatically cluster Twitter users according to their behavior. The clustering method is based on the Gaussian probability density distribution combined with a Takagi--Sugeno fuzzy consequent part of order zero (eGauss0). This means that this method can be used as a classifier that is actually a mapping from the feature space to the class label space. The eGauss method is very flexible, is computed recursively, and the most important thing is that it starts learning ``from scratch''. The structure adapts to the new data using adding and merging mechanisms. The most important feature of the evolving method is that it can process data from thousands of Twitter profiles in real time, which can be characterized as a Big Data problem. The final clusters yield classes of Twitter profiles, which are represented as different activity levels of each profile. In this way, we could classify each member as ordinary, very active, influential and unusual user. The proposed method was also tested on the Iris and Breast Cancer Wisconsin datasets and compared with other methods. In both cases, the proposed method achieves high classification rates and shows competitive results.},
	author = {Igor {\v S}krjanc and Goran Andonovski and Jos{\'e} Antonio Iglesias and Mar{\'\i}a Paz Sesmero and Araceli Sanchis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117881},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Evolving clustering, Twitter data analysis, Online method, Gaussian probability},
	pages = {117881},
	title = {Evolving Gaussian on-line clustering in social network analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422011320},
	volume = {207},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422011320},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117881}}

@article{CHEN2022116574,
	abstract = {Recommending the appropriate APIs from a large volume of Open APIs to application developers both accurately and efficiently has become a challenging problem. Established work usually takes only one feature of Open APIs into account, which decreases the accuracy of recommendations. In order to overcome this problem, we propose an ensemble-based approach to Open APIs recommendation with a multiple feature model, which integrates both machine learning and deep learning and synthesizes a set of multiple features to accurately make the recommendation. This approach employs One-hot, Word2vec similarity and Matrix Factorization techniques to obtain the multiple features information respectively, then concatenates the features to obtain a Multivariate Information Feature (MIF) matrix, and leverages an optimized Gradient Boosting Decision Tree (GBDT) and Gated Recurrent Unit (GRU) for feature selection. GBDT is good at processing dense numerical features, while GRU is good at processing sparse categorical features, Finally, the results are synthesized to obtain a recommendation result. We have compared our approach with other four Open APIs recommendation approaches on the Programmable Web, and verified the effectiveness of our approach in precision, recall, F1-measure.},
	author = {Junwu Chen and Ye Wang and Qiao Huang and Bo Jiang and Pengxiang Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116574},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Open APIs, APIs recommendation, Neural networks, Machine learning, Ensemble model},
	pages = {116574},
	title = {Open APIs recommendation with an ensemble-based multi-feature model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000719},
	volume = {196},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000719},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116574}}

@article{ZHANG2022116882,
	abstract = {The World Health Organization (WHO) declared on 11th March 2020 the spread of the coronavirus disease 2019 (COVID-19) a pandemic. The traditional infectious disease surveillance had failed to alert public health authorities to intervene in time and mitigate and control the COVID-19 before it became a pandemic. Compared with traditional public health surveillance, harnessing the rich data from social media, including Twitter, has been considered a useful tool and can overcome the limitations of the traditional surveillance system. This paper proposes an intelligent COVID-19 early warning system using Twitter data with novel machine learning methods. We use the natural language processing (NLP) pre-training technique, i.e., fine-tuning BERT as a Twitter classification method. Moreover, we implement a COVID-19 forecasting model through a Twitter-based linear regression model to detect early signs of the COVID-19 outbreak. Furthermore, we develop an expert system, an early warning web application based on the proposed methods. The experimental results suggest that it is feasible to use Twitter data to provide COVID-19 surveillance and prediction in the US to support health departments' decision-making.},
	author = {Yiming Zhang and Ke Chen and Ying Weng and Zhuo Chen and Juntao Zhang and Richard Hubbard},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116882},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {COVID-19 surveillance, Early warning system, Text classification, BERT, Epidemic intelligence},
	pages = {116882},
	title = {An intelligent early warning system of analyzing Twitter data using machine learning on COVID-19 surveillance in the US},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422003268},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422003268},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116882}}

@article{KORFIATIS2019472,
	abstract = {Service quality is a multi-dimensional construct which is not accurately measured by aspects deriving from numerical ratings and their associated weights. Extant literature in the expert and intelligent systems examines this issue by relying mainly on such constrained information sets. In this study, we utilize online reviews to show the information gains from the consideration of factors identified from topic modeling of unstructured data which provide a flexible extension to numerical scores to understand customer satisfaction and subsequently service quality. When numerical and textual features are combined, the explained variation in overall satisfaction improves significantly. We further present how such information can be of value for firms for corporate strategy decision-making when incorporated in an expert system that acts as a tool to perform market analysis and assess their competitive performance. We apply our methodology on airline passengers' online reviews using Structural Topic Models (STM), a recent probabilistic extension to Latent Dirichlet Allocation (LDA) that allows the incorporation of document level covariates. This innovation allows us to capture dominant drivers of satisfaction along with their dynamics and interdependencies. Results unveil the orthogonality of the low-cost aspect of airline competition when all other service quality dimensions are considered, thus explaining the success of low-cost carriers in the airline market.},
	author = {Nikolaos Korfiatis and Panagiotis Stamolampros and Panos Kourouthanassis and Vasileios Sagiadinos},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.09.037},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Electronic WOM, Unstructured data, Service quality, Correspondence analysis, Structural topic model},
	pages = {472-486},
	title = {Measuring service quality from unstructured data: A topic modeling application on airline passengers' online reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306146},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418306146},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.09.037}}

@article{DOSSANTOS201834,
	abstract = {The quality of any text mining technique is highly dependent on the features that are used to represent the document collection. A classical form of document representation is the vector space model (VSM), according to which the documents are represented as vectors of weights that correspond to the features of the documents. The bag-of-words model is the most popular VSM approach due to its simplicity and general applicability, but this model does not include term dependency and has a high dimensionality. In the literature, several models for document representation have been proposed in order to capture the dependency of terms. Among them, the topic model representation is one of the most interesting approaches - since it describes the collection of documents in a way that reveals their internal structure and the interrelationships therein, and also provides a dimensionality reduction. However, even for topic models, the efficient extraction of information concerning the relations among terms for document representation is still a major research challenge. In order to address this issue, we proposed thelatent association rule cluster based model (LARCM). The LARCM is a non-probabilistic topic model that makes use of association rule clustering to build a document representation with low dimensionality in such a way that each feature (i.e., topic) is comprised of information concerning relations among the terms. We evaluated the interpretability of the topics obtained by using our proposed model against the ones provided by the traditional latent dirichlet allocation (LDA) model and the LDA model using a document representation that includes correlated terms (i.e., bag-of-related-words). The experimental results indicated that the LARCM provides topics with better interpretability than the LDA models. Additionally, we used the topics obtained by the LARCM in two different applications: text classification and page recommendation. With respect to text classification, the topics were used to improve document collection representation. Concerning page recommendation, topics were used as contextual information in context-aware recommender systems. Results have shown that the topics provided by the LARCM can be used to improve both applications.},
	author = {Fabiano Fernandes {dos Santos} and Marcos Aur{\'e}lio Domingues and Camila Vaccari Sundermann and Veronica Oliveira {de Carvalho} and Maria Fernanda Moura and Solange Oliveira Rezende},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.06.021},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Document representation, Topic model, Association rules, Clustering, Text classification, Context-aware recommender systems},
	pages = {34-60},
	title = {Latent association rule cluster based model to extract topics for classification and recommendation applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418303671},
	volume = {112},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418303671},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.06.021}}

@article{ZHANG2022116717,
	abstract = {Explainable rating predication becomes challenging with the largely growing number of information and items. Of particular interest is to capture users' preferences for various items by using textual reviews to achieve accurate and interpretable recommendations. In this paper, we report an aspect-aware explainable neural attentional recommender model for rating predication (AENAR) and this model enables intelligent predication and recommendation by capturing the varying aspect attentions that users pay to different items. The experimental results based on six public datasets reveals that the designed model consistently outperforms five existing state-of-the-art alternatives. Furthermore, the designed attention network allows to highlight the context-aware information in textual reviews that unambiguously suggest users' aspect-level preference for their desired items, improving the interpretability of the rating prediction.},
	author = {Tianwei Zhang and Chuanhou Sun and Zhiyong Cheng and Xiangjun Dong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116717},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Recommender systems, Neural networks, Attention mechanism, Deep learning},
	pages = {116717},
	title = {AENAR: An aspect-aware explainable neural attentional recommender model for rating predication},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422001920},
	volume = {198},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422001920},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116717}}

@article{KORENCIC2018357,
	abstract = {There is a rising need for automated analysis of news text, and topic models have proven to be useful tools for this task. However, as the quality of the topics induced by topic models greatly varies, much research effort has been devoted to their automated evaluation. Recent research has focused on topic coherence as a measure of a topic's quality. Existing topic coherence measures work by considering the semantic similarity of topic words. This makes them unfit to detect the coherence of transient topics with semantically unrelated topic words, which abound in news media texts. In this paper, we introduce the notion of document-based topic coherence and propose novel topic coherence measures that estimate topic coherence based on topic documents rather than topic words. We evaluate the proposed measures on two datasets containing topics manually labeled for document-based coherence, on which the proposed measures outperform a strong baseline as well as word-based coherence measures. We also demonstrate the usefulness of document-based coherence measures for automated topic discovery from news media texts.},
	author = {Damir Koren{\v c}i{\'c} and Strahil Ristov and Jan {\v S}najder},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.063},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic models, Topic coherence, Topic model evaluation, Text analysis, News text, Exploratory analysis},
	pages = {357-373},
	title = {Document-based topic coherence measures for news media text},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304883},
	volume = {114},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304883},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.063}}

@article{ADLOUNI2019432,
	abstract = {In this paper we face the problem of Community Question Answering for the Arabic language. In this setting, a member of the community posts an initial query expressed in Natural Language. Other participants post their own interventions: answers, comments, additional questions, etc. contributing to building a rather tangled thread of nodes containing Natural Language short texts. The task consists in answering the initial query using the thread as the space of possible answers. The task can be approached as a classification, a regression or a ranking problem. In our case we select the set of possible candidates, we assign a relevance score to each candidate and we rank them accordingly. We propose a bunch of unsupervised models and show that a model based on Latent Semantic Indexing approach outperforms state of the art models for this task. We also use transfer learning to power the embeddings layer of various deep learning models and prove that the pairwise approaches outperform their pointwise counterparts. All the proposed models have been evaluated on Semeval 2017 Task 3 Subtask D: Arabic Community Question Answering and achieve state of the art or near performance.},
	author = {Yassine El Adlouni and Horacio Rodr{\'\i}guez and Mohammed Meknassi and Said Ouatik {El Alaoui} and Noureddine En-nahnahi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.07.024},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Community question answering, Information retrieval, Arabic natural language processing, Learning to rank, Deep learning},
	pages = {432-442},
	title = {A multi-approach to community question answering},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419305032},
	volume = {137},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419305032},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.07.024}}

@article{TERROSOSAENZ2020112892,
	abstract = {Nowadays, cities are dynamic ecosystems where urban changes occur at a very fast pace. Hence, social sensing has become a powerful tool to uncover the actual land-use of a metropolis. However, current solutions for land-use discovery based on user-generated data usually rely on an information retrieval mechanism applied on a textual corpus. This causes ad-hoc place labelling with limited semantic meaning. In this line, the present work introduces a novel data-driven methodology that extends existing solutions by means of a classifier based on a pre-defined hierarchy of land categories. Two types of social networks --text-based and venue-based platforms-- are utilized to train the classifier, which is then applied to infer the use of the land based on text data in areas where venue data are not available. The approach has been evaluated by using large datasets comprising two large cities, showing an accuracy above 90% in predicting the land-use categories.},
	author = {Fernando Terroso-Saenz and Andr{\'e}s Mu{\~n}oz},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.112892},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Urban computing, Volunteer Geographic Information (VGI), Land usage, Supervised classification},
	pages = {112892},
	title = {Land use discovery based on Volunteer Geographic Information classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419306086},
	volume = {140},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419306086},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.112892}}

@article{XIE2019178,
	abstract = {With the wide adoption of service-oriented computing and cloud computing, service-based systems (SBSs), a kind of software systems that can offer certain functionalities by leveraging one or more Web services, become increasingly popular. A challenging issue in SBS development is to find suitable services from a variety of available (semantics different) services. Towards this issue, we propose a new service recommendation approach that can integrate diverse information of SBSs and their component services. In this research, SBSs, services, their respective attributes (e.g. content and categories) and SBS-service composition relations are modeled as a heterogeneous information network (HIN); and several semantic similarities between SBSs are measured on a set of meta-paths in the HIN. Particularly, a word embedding technique is used to learn word vectors from the content of SBSs and services, which contribute to better functional similarities between SBSs. Afterwards, the combinational weights of different similarities are optimized using a Bayesian personalized ranking algorithm. Services are finally recommended based on collaborative filtering. We identify two recommendation scenarios with different SBS requirements. By conducting a series of experiments on a real-world dataset crawled from the ProgrammableWeb, we validate the effectiveness of our approach and find out the optimal combinations of SBS similarities for those two scenarios.},
	author = {Fang Xie and Jian Wang and Ruibin Xiong and Neng Zhang and Yutao Ma and Keqing He},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.01.025},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Service recommendation, Service-based system, Heterogeneous information network, Word embedding, Collaborative filtering},
	pages = {178-194},
	title = {An integrated service recommendation approach for service-based system development},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419300260},
	volume = {123},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419300260},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.01.025}}

@article{WANG2022115887,
	abstract = {Sentiment mining has been a helpful mechanism that targets to understand the market feedback on certain commodities by utilizing user comments. In general, the process of yielding each comment is essentially associated with his/her criteria for rating (i.e., the degree of harshness) , which makes users provide biased comments. For instance, for a tolerant user, although the user is extremely dissatisfied with the product, harshness still makes her yield a neutral comment which cannot indicate the product quality. Existing work straightforwardly removes the comments of harsh users and those of tolerant ones, which is not the best strategy. To this end, we propose a harshness-aware sentiment analysis framework for product review. First, we depict the process of providing comments from users as a probabilistic graphical model in which the harshness is incorporated. Second, we employ a Bayesian-based inference for sentiment mining. Extensive experimental evaluations have shown that the results of the proposed method are more consistent with the expert evaluations than those of the state-of-the-art methods, and even outperform the method which infers the final evaluations with the ground truth of comments without considering users' harshness.},
	author = {Xun Wang and Ting Zhou and Xiaoyang Wang and Yili Fang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115887},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Sentiment mining, Bayesian inference, Probabilistic graphical model},
	pages = {115887},
	title = {Harshness-aware sentiment mining framework for product review},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421012458},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421012458},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115887}}

@article{MA2019346,
	abstract = {In this paper, we will propose a novel approach based on graph analysis which will use community structure detection algorithm to detect topics in the keywords graph of micro-blogging data. Furthermore, considering the specificity of the Sina microblogging, we propose novel keywords filtering model and graph generation algorithm to meet the dual requirements of topic detection and community detection. We validate our approach on a big natural disaster dataset from Sina micro-blog, in which about 103 micro-blogging posts with about 104 distinct feature tags. The experimental results definitely revealed the relationship between the keywords and the natural disaster topics. Our methodology is a scalable method which can adapt to the changes in the amount of data. Especially, we can get abundant information about natural disasters in the topic detection and help the government guide the rescue of disasters.},
	author = {Tinghuai Ma and YuWei Zhao and Honghao Zhou and Yuan Tian and Abdullah Al-Dhelaan and Mznah Al-Rodhaan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.08.010},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic detection, Community detection, Natural disaster, Sina microblogging, Graph analysis},
	pages = {346-355},
	title = {Natural disaster topic extraction in Sina microblogging based on graph analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418305189},
	volume = {115},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418305189},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.08.010}}

@article{ZHENG2021115030,
	abstract = {Recently, the explosive increase in social media data enables manufacturers to collect product defect information promptly. Extant literature gathers defect information like defective components or defect symptoms without distinguishing defect-related (DR) texts from defect-unrelated (DUR) texts and thus makes defects discussed by few texts buried in enormous DUR texts. Moreover, existing studies do not consider the defect severity which is valuable and important for manufacturers to make remedial decisions. To bridge these research gaps, we propose a novel approach that integrates the probabilistic graphic model named Product Defect Identification and Analysis Model (PDIAM) with Failure Mode and Effect Analysis (FMEA) to derive product defect information from social media data. Comparing to extant studies, PDIAM identifies DR texts and then extracts defect information from these texts. And PDIAM provides more defect information than previous researches. Besides, we further analyze defect severity with the combination of FMEA and PDIAM which alleviates the inherent subjectivity brought by expert evaluation in the traditional FMEA. A case study in the automobile industry proves the predominant performance of our approach and great potential in defect management.},
	author = {Lu Zheng and Zhen He and Shuguang He},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115030},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Product defect discovery, Social media data, Probabilistic graphic model, FMEA, Text analysis},
	pages = {115030},
	title = {An integrated probabilistic graphic model and FMEA approach to identify product defects from social media data},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421004711},
	volume = {178},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421004711},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115030}}

@article{TANG2021115070,
	abstract = {The rapid adoption of services-related technologies, such as cloud computing, has lead to the explosive growth of web services. Automated service classification that groups web services by similar functionality is a widely used technique to facilitate the management and discovery of web services within a large-scale repository. The existing service classification approaches primarily focus on learning the isolated representations of service features but ignored their internal semantic correlations. To address the aforementioned issue, we propose a novel deep neural network with the Co-Attentive Representation Learning (CARL-Net) mechanism for effectively classifying services by learning interdependent characteristics of service without feature engineering. Specifically, we propose a service data augmentation mechanism by extracting informative words from the service description using information gain theory. Such a mechanism can learn a correlation matrix among embedded augmented data and description, thereby obtaining their interdependent semantic correlation representations for service classification. We evaluate the effectiveness of CARL-Net by comprehensive experiments based on a real-world dataset collected from ProgrammableWeb, which includes 10,943 web services. Compared with seven web service classification baselines based on CNN, LSTM, Recurrent-CNN, C-LSTM, BLSTM, ServeNet and ServeNet-BERT, the CARL-Net can achieve an improvement of 5.66%--172.21% in the F-measure of web service classification.},
	author = {Bin Tang and Meng Yan and Neng Zhang and Ling Xu and Xiaohong Zhang and Haijun Ren},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115070},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Service computing, Web service classification, Co-attentive representation},
	pages = {115070},
	title = {Co-attentive representation learning for web services classification},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100511X},
	volume = {180},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100511X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115070}}

@article{LYKOUSAS2021114808,
	abstract = {Social networks are evolving to engage their users more by providing them with more functionalities. One of the most attracting ones is streaming. Users may broadcast part of their daily lives to thousands of others world-wide and interact with them in real-time. Unfortunately, this feature is reportedly exploited for grooming. In this work, we provide the first in-depth analysis of this problem for social live streaming services. More precisely, using a dataset that we collected, we identify predatory behaviours and grooming on chats that bypassed the moderation mechanisms of the LiveMe, the service under investigation. Beyond the traditional text approaches, we also investigate the relevance of emojis in this context, as well as the user interactions through the gift mechanisms of LiveMe. Finally, our analysis indicates the possibility of grooming towards minors, showing the extent of the problem in such platforms.},
	author = {Nikolaos Lykousas and Constantinos Patsakis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114808},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Online grooming, Social networks, LDA, Text analysis, Emoji},
	pages = {114808},
	title = {Large-scale analysis of grooming in modern social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002499},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002499},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114808}}

@article{DHEERAJ2021115265,
	abstract = {Mining the emotions in the text related to mental health-care oriented is a challenging aspect, especially dealing with a long-text sequence of data. The extraction of emotions depends upon the various psychological depression factors like negative and ambiguity. Identifying these factors is the most perplexing task for every psychiatrist to treat their patients. Our study includes the deep learning (DL) models with global vector representations (GloVe) embeddings to capture the text sequence of data. We proposed a model multi-head attention with bidirectional long short-term memory and convolutional neural network (MHA-BCNN) is a pre-eminent mechanism that outperforms better than past research works for capturing the negative text-based emotions. In this paper, by using DL extracted the various negative mental-health emotions like addiction, anxiety, depression, insomnia, stress, and obsessive cleaning disorder (OCD). By using the GloVe embeddings and handled the ambiguity factors like multiple emotion words in a certain sequence. As we proposed a vigorous appliance in our research to capture and hoard the long-term dependencies. We extracted the questions related to mental health issues were posted by the patients in an online mental healthcare-oriented platform. We efficaciously handled both negative and ambiguity factors at the document level. Our suggested exemplary MHA-BCNN surmounts various aspects from preceding research works and ensued preeminent performance. Experimental results show that our proposed framework MHA-BCNN outperformed than the erstwhile research works.},
	author = {Kodati Dheeraj and Tene Ramakrishnudu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115265},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Emotion analysis, Mental-health-care, GloVe-embeddings, Deep learning, MHA-BCNN},
	pages = {115265},
	title = {Negative emotions detection on online mental-health related patients texts using the deep learning with MHA-BCNN model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421006977},
	volume = {182},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421006977},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115265}}

@article{RANI2022118461,
	abstract = {Disaster event detection aims to identify events like terrorist attacks, fire incidents, stampede incidents, building collapse, etc., reported in the online news articles or social media. Place of occurrence of disaster event is a significant feature associated with events for location-sensitive disaster event detection. Efficient feature selection and their augmentation with location information can contribute towards the evolution of traditional approaches and their adoption for location-sensitive disaster event detection leading to improvement in the overall process as a whole. Since the evaluation of event detection techniques deliberates various intrinsic and extrinsic performance metrics, the decision-making for the selection of feature sets is treated as a Multiple-Criteria Decision Making (MCDM) problem. This paper proposes a framework, GeoClust, that is based on feature engineering of traditional textual features in order to enhance their capability for improved location-sensitive disaster event detection. The framework augments context-free and context-based textual feature sets with feature sets of place of occurrence of the events and evaluates their performance using unsupervised machine learning algorithms for various performance metrics. Finally, the best feature set is selected using AHP-TOPSIS technique of MCDM in order to tune the system for automatic and efficient location-sensitive disaster event detection in real-time. Extensive set of experiments have been performed in order to evaluate the framework on a dataset of online news articles reporting disaster events about terrorist attacks, fire incidents, stampede incidents, building collapse and maoist attacks happened at different locations in India. The results show that the location-augmented feature sets significantly improve performance of location-sensitive disaster event detection as compared with traditional feature sets. The results also demonstrate that the context-based feature sets with location-augmentation are ranked higher than the context-free feature sets in MCDM analysis.},
	author = {Monika Rani and Sakshi Kaushal},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118461},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Location-sensitive disaster event detection, Feature engineering, Multiple-criteria decision making (MCDM), AHP-TOPSIS, Context-free and context-based feature sets},
	pages = {118461},
	title = {GeoClust: Feature engineering based framework for location-sensitive disaster event detection using AHP-TOPSIS},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015548},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015548},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118461}}

@article{ALSALEMI2018531,
	abstract = {Boosting algorithms have been proved effective for multi-label learning. As ensemble learning algorithms, boosting algorithms build classifiers by composing a set of weak hypotheses. The high computational cost of boosting algorithms in learning from large volumes of data such as text categorization datasets is a real challenge. Most boosting algorithms, such as AdaBoost.MH, iteratively examine all training features to generate the weak hypotheses, which increases the learning time. RFBoost was introduced to manage this problem based on a rank-and-filter strategy in which it first ranks the training features and then, in each learning iteration, filters and uses only a subset of the highest-ranked features to construct the weak hypotheses. This step ensures accelerated learning time for RFBoost compared to AdaBoost.MH, as the weak hypotheses produced in each iteration are reduced to a very small number. As feature ranking is the core idea of RFBoost, this paper presents and investigates seven feature ranking methods (information gain, chi-square, GSS-coefficient, mutual information, odds ratio, F1 score, and accuracy) in order to improve RFBoost's performance. Moreover, an accelerated version of RFBoost, called RFBoost1, is also introduced. Rather than filtering a subset of the highest-ranked features, FBoost1 selects only one feature, based on its weight, to build a new weak hypothesis. Experimental results on four benchmark datasets for multi-label text categorization) Reuters-21578, 20-Newsgroups, OHSUMED, and TMC2007(demonstrate that among the methods evaluated for feature ranking, mutual information yields the best performance for RFBoost. In addition, the results prove that RFBoost statistically outperforms both RFBoost1 and AdaBoost.MH on all datasets. Finally, RFBoost1 proved more efficient than AdaBoost.MH, making it a better alternative for addressing classification problems in real-life applications and expert systems.},
	author = {Bassam Al-Salemi and Masri Ayob and Shahrul Azman Mohd Noah},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.07.024},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {RFBoost, Boosting, Multi-label learning, Text categorization, Feature ranking},
	pages = {531-543},
	title = {Feature ranking for enhancing boosting-based multi-label text categorization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418304408},
	volume = {113},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418304408},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.07.024}}

@article{BELFORD2020113709,
	abstract = {Topic modeling is a popular unsupervised technique that is used to discover the latent thematic structure in text corpora. The evaluation of topic models typically involves measuring the semantic coherence of the terms describing each topic, where a single value is used to summarize the quality of an overall model. However, this can create difficulties when one seeks to interpret the strengths and weaknesses of a given topic model. With this in mind, we propose a new ensemble topic modeling approach that incorporates both stability information, in the form of term co-associations, and semantic similarity information, as derived from a word embedding constructed on a background corpus. Our evaluations show that this approach can simultaneously yield higher quality models when considering the produced topic descriptors and document-topic assignments, while also facilitating the comparison and evaluation of solutions through the visualization of the discovered topical structure, the ordering of the topic descriptors, and the ranking of term pairs which appear in topic descriptors.},
	author = {Mark Belford and Derek Greene},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113709},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic modeling, Ensemble learning, Evaluation, Word embeddings, Interpretation},
	pages = {113709},
	title = {Ensemble topic modeling using weighted term co-associations},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305339},
	volume = {161},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420305339},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113709}}

@article{ZHANG2020113073,
	abstract = {Openness to experience, one of the essential individual characteristics, is of great theoretical and practical value in psychological and behavioral domains. Although typical machine learning methods can be utilized to extract individuals' openness to experience from the large-scale textual data like the unprecedented massive user generated contents (UGCs), they are often regarded as ``black boxes'' because they are unable to provide knowledge about the influential factors of openness to experience. This is of no help for us to investigate why a particular level of openness to experience is predicted for an individual. In addition, high dimensionality and sparseness of textual data impairs the performance of the typical machine learning method in extracting individuals' characteristics. In this study, we propose an interpretable data-driven mixture method for qualified modeling and predicting individuals' openness to experience. The proposed method extends the latent Dirichlet allocation (LDA) to overcome the problem of high dimensionality and sparseness in modeling the textual data, and can effectively extract two influential variables, namely, the topic preference and the expressed emotional intensity, to make an accurate prediction and to help us fully understand individuals' openness to experience lurking in the textual data. Experimental results indicate the effectiveness of the proposed method in drawing individuals' openness to experience, and also validate the predictive ability of topic preference and expressed emotional intensity which are indicated in psychological literature to be influential factors of openness to experience.},
	author = {Yishi Zhang and Haiying Wei and Yaxuan Ran and Yang Deng and Dan Liu},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.113073},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Openness to experience, Interpretability, Topic modeling, Maximum-A-Posteriori estimation, Data-driven},
	pages = {113073},
	title = {Drawing openness to experience from user generated contents: An interpretable data-driven topic modeling approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419307900},
	volume = {144},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419307900},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.113073}}

@article{ERFANIAN2022116086,
	abstract = {With the huge expansion of user generated content on social networks, event detection has emerged as a major challenge and source of knowledge discovery. This knowledge is employed in different applications such as recommender systems, crisis management systems, and decision support systems. Dynamicity, overlapping, and evolutionary behavior are the most important issues in event detection. This paper proposes a novel evolutionary model for event detection to capture the dynamism and evolving behavior of events. The proposed method uses a matrix decomposition technique and a Dirichlet Process to detect events and handle their dynamicity. This model consists of two components, namely preliminary event detection and event evolvement tracking. The former component extracts preliminary events from the available data using the matrix decomposition method. Then, subsequent data is employed into a Non-Parametric Bayesian Network, namely Dirichlet Process Mixture Model to evolve the preliminary events. During the evolvement process, data may migrate between extracted events or new events may be discovered. The experimental results and comparisons with several recently developed approaches show the superiority of the proposed approach, and its ability to capture the evolutionary behavior of events over time.},
	author = {P.M.A. Yashar Erfanian and Bagher Rahimpour Cami and Hamid Hassanpour},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116086},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Event detection, Event evolution, Topic modeling, Social network analysis, Incremental clustering},
	pages = {116086},
	title = {An evolutionary event detection model using the Matrix Decomposition Oriented Dirichlet Process},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421014226},
	volume = {189},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421014226},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116086}}

@article{KIM2020113288,
	abstract = {Due to its simplicity and intuitive interpretability, spherical k-means is often used for clustering a large number of documents. However, there exist a number of drawbacks that need to be addressed for much effective document clustering. Without well-dispersed initial points, spherical k-means fails to converge quickly, which is critical for clustering a large number of documents. Furthermore, its dense centroid vectors needlessly incorporate the impact of infrequent and less-informative words, thereby distorting the distance calculation between the document vectors. In this paper, we propose practical improvements on spherical k-means to overcome these issues during document clustering. Our proposed initialization method not only guarantees dispersed initial points, but is also up to 1000 times faster than previously well-known initialization method such as k-means++. Furthermore, we enforce sparsity on the centroid vectors by using a data-driven threshold that is capable of dynamically adjusting its value depending on the clusters. Additionally, we propose an unsupervised cluster labeling method that effectively extracts meaningful keywords to describe each cluster. We have tested our improvements on seven different text datasets that include both new and publicly available datasets. Based on our experiments on these datasets, we have found that our proposed improvements successfully overcome the drawbacks of spherical k-means in significantly reduced computation time. Furthermore, we have qualitatively verified the performance of the proposed cluster labeling method by extracting descriptive keywords of the clusters from these datasets.},
	author = {Hyunjoong Kim and Han Kyul Kim and Sungzoon Cho},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113288},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Spherical k-means, Document clustering, k-means initialization, Sparse vector projection, Clustering labeling},
	pages = {113288},
	title = {Improving spherical k-means for document clustering: Fast initialization, sparse centroid projection, and efficient cluster labeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420301135},
	volume = {150},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420301135},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113288}}

@article{PHAM2019328,
	abstract = {Recently, heterogeneous network representation learning has attracted a lot of attentions due to its potential applications. Our works in this paper are concentrated on how to leverage the output of network representation learning by combining with the topic similarity between nodes in content-based heterogeneous information network (CHIN). These unique challenges come from the shortage of topic similarity evaluation between text-based nodes which limit the accuracy of the similarity search as well other network mining tasks. Moreover, the massive sizes of current real-world network also raises challenges for traditional standalone-based heterogeneous network analysis models. Different from previous network representation learning models, such as: Node2Vec or Metapath2Vec, our proposed W-MethPath2Vec model uses the topic-driven meta-path-based random walk mechanism for generating heterogeneous neighborhood of nodes as the learning features. Then, these learning nodes' features are used to train the learning model which is used for solving various heterogeneous network mining tasks such as: node similarity search, clustering, classification, link prediction, etc. The W-MethPath2Vec model enables the simultaneous modeling of structural and topic correlations between nodes in heterogeneous networks. Moreover, the W-MethPath2Vec model is implemented in the Apache Spark-based distributed framework which enables the capability of handling large-scaled networks. We tested our W-MethPath2Vec model with the previous state-of-the-art approaches in the real-world datasets to demonstrate the effectiveness of our proposed model.},
	author = {Phu Pham and Phuc Do},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.01.015},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Heterogeneous information network, Representation learning, Topic similarity, Large-scaled network, Apache Spark},
	pages = {328-344},
	title = {W-MetaPath2Vec: The topic-driven meta-path-based model for large-scaled content-based heterogeneous information network representation learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419300156},
	volume = {123},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419300156},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.01.015}}

@article{TANG2022118062,
	abstract = {With rapid development of socio-economics, the task of discovering functional zones becomes critical to better understand the interactions between social activities and spatial locations. In this paper, we propose a framework to discover the real functional zones from the biased and extremely sparse Point of Interests (POIs). To cope with the bias and sparsity of POIs, the unbiased inner influences between spatial locations and human activities are introduced to learn a balanced and dense latent region representation. In addition, a spatial location based clustering method is also included to enrich the spatial information for latent region representation and enhance the region functionality consistency for the fine-grained region segmentation. Moreover, to properly annotate the various and fine-grained region functionalities, we estimate the functionality of the regions and rank them by the differences between the normalized POI distributions to reduce the inconsistency caused by the fine-grained segmentation. Thus, our whole framework is able to properly address the biased categories in sparse POI data and explore the true functional zones with a fine-grained level. To validate the proposed framework, a case study is evaluated by using very large real-world users GPS and POIs data from city of Raleigh. The results demonstrate that the proposed framework can better identify functional zones than the benchmarks, and, therefore, enhance understanding of urban structures with a finer granularity under practical conditions.},
	author = {Wen Tang and Alireza Chakeri and Hamid Krim},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118062},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Functional zones discovering, Latent region representation learning, Sparse and bias POIs, GPS data, Conditional random field clustering, Function annotation},
	pages = {118062},
	title = {Discovering urban functional zones from biased and sparse points of interests and sparse human activities},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012672},
	volume = {207},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422012672},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118062}}

@article{AMADORDOMINGUEZ2021115731,
	abstract = {Hyper-personalization policies entail a considerable improvement regarding previous personalization approaches. However, they present several issues that need to be addressed, such as minimal explainability and privacy invasion. A hierarchical Multi-Agent System (MAS) is presented in this work to provide a solution to these concerns. The system is formulated as a hybrid approach, where some of the agents work autonomously, while the user input triggers the remaining. At the autonomous level, a set of Virtual Identities (VIs) representing different user profiles interact with Black-Box Hyper-Personalization Online Systems (BBHOS), gathering a set of targeted responses. Associative patterns and profile aggregations can then be inferred from the analysis of these responses. In the user-triggered level, the real user is virtualized as an identity that represents their features. The virtual identity serves as an intermediary between the personalization system and the real user. This virtualization hinders the personalization service from extracting sensitive contextual information about the real user, protecting their privacy. The results obtained by the user identity on its interaction with the personalization service are then analyzed, adjusting the content of the response to fit the user's requests instead of their features. A use case on the functioning of the analysis of search engines is presented to illustrate the complete behavior of the proposed architecture.},
	author = {Elvira Amador-Dom{\'\i}nguez and Emilio Serrano and Daniel Manrique},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115731},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Multi-agent system, Virtual identities, Personalization},
	pages = {115731},
	title = {A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421011118},
	volume = {186},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421011118},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115731}}

@article{NUGUMANOVA2022117179,
	abstract = {This work describes automatic term extraction approach based on the combination of the probabilistic topic modelling (PTM) and non-negative matrix factorization (NMF). Topic modeling algorithms including NMF-based ones do not require expensive and time-consuming manual annotations for domain terms, but only a corpus of domain documents. The topics emerge from the corpus documents without any supervision as sets of most probable words. This work is aimed to investigate how fully and precisely these most probable words from topics can reflect domain terminology. We run a series of experiments on the novel, qualitatively annotated dataset ACTER that was first used in the TermEval 2020 Shared Task. We compare five different NMF algorithms and four different NMF initializations when changing the number of topics extracted from documents and the number of most probable words extracted from topics in order to determine optimal combinations for best performance of term extraction. Finally, we compare the obtained optimal combinations of NMF with the competitive methods in TermEval 2020 and prove that our approach is second only to two much more sophisticated, domain-dependent supervised methods.},
	author = {Aliya Nugumanova and Darkhan Akhmed-Zaki and Madina Mansurova and Yerzhan Baiburin and Almasbek Maulit},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117179},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Automatic term extraction, Probabilistic topic modeling, NMF, Unsupervised term extraction, ACTER dataset, TermEval shared task},
	pages = {117179},
	title = {NMF-based approach to automatic term extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422005668},
	volume = {199},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422005668},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117179}}

@article{JORGEBOTANA201971,
	abstract = {One insufficiently grounded criticism made against Latent Semantic Analysis is that it is impossible to semantically interpret its dimensions. This is not true, as several studies have transformed the latent semantic space to interpret them, by means of some methods. One of them is the Inbuilt-Rubric method. Rather than grouping concepts around dimensions, as in Exploratory Factor Analysis based rotation methods, the Inbuilt-Rubric is a method that perform an ``a priori'' imposition of concepts onto the latent semantic space. It uses a confirmatory strategy. This study seeks to propose solutions for two limitations found in the current Inbuilt-Rubric methodology: one solution is inspired by Bifactor Models and the management of common variance of the concepts involved; and the other one is based in randomizing the sequence to perform the process. Both methods outperform the current Inbuilt-Rubric version in relevant content detection. The reported improvements can be incorporated into expert systems that use Latent Semantic Analysis and Inbuilt-Rubric in relevant content detection or text classification tasks.},
	author = {Guillermo Jorge-Botana and Ricardo Olmos and Jos{\'e} Mar{\'\i}a Luz{\'o}n},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2019.04.055},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Latent semantic analysis, Bifactor model, Distributional semantics, Inbuilt-Rubric method, Rotation, Text assessment},
	pages = {71-80},
	title = {Could LSA become a ``Bifactor'' model? Towards a model with general and group factors},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419302854},
	volume = {131},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417419302854},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2019.04.055}}

@article{GOZUACIK2021115388,
	abstract = {Social media platforms are considered one of the most effective intermediaries for companies to interact with consumers. Social media-based decision support systems for the marketing domain are highly developed, but product development and innovation-oriented studies remain limited. This study offers a novel approach which utilises opinion retrieval theme along with sentiment analysis to support the decision-making process for product analysis and development. To achieve this aim, we propose an end-to-end social media-based opinion retrieval system and utilise machine learning and natural language processing techniques. Google Glass is chosen as a use-case as this product was unable to achieve its commercial targets despite its superior technological offerings. We design a multi-task deep neural network architecture for the training of sentiment prediction and opinion detection tasks. We first divide the tweets containing certain useful opinions and suggestions into two categories based on their sentiment labels. The negative tweets are analysed to identify product-related concerns, whereas the positive and neutral tweets are used to extract innovative ideas and identify new use cases for product development. We visualise and interpret the clusters of keywords extracted from each sentiment label group. Apart from methodological contributions, this study offers practical contributions for the next generations of smart glasses.},
	author = {Necip Gozuacik and C. Okan Sakar and Sercan Ozcan},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115388},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deep learning, Feedback retrieval, Natural language processing, Opinion mining, Sentiment analysis, Text analytics},
	pages = {115388},
	title = {Social media-based opinion retrieval for product analysis using multi-task deep neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008125},
	volume = {183},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421008125},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115388}}

@article{JOSHI2023118442,
	abstract = {In this paper, we propose DeepSumm, a novel method based on topic modeling and word embeddings for the extractive summarization of single documents. Recent summarization methods based on sequence networks fail to capture the long range semantics of the document which are encapsulated in the topic vectors of the document. In DeepSumm, our aim is to utilize the latent information in the document estimated via topic vectors and sequence networks to improve the quality and accuracy of the summarized text. Each sentence is encoded through two different recurrent neural networks based on probabilistic topic distributions and word embeddings, and then a sequence to sequence network is applied to each sentence encoding. The outputs of the encoder and the decoder in the sequence to sequence networks are combined after weighting using an attention mechanism and converted into a score through a multi-layer perceptron network. We refer to the score obtained through the topic model as Sentence Topic Score (STS) and to the score generated through word embeddings as Sentence Content Score (SCS). In addition, we propose Sentence Novelty Score (SNS) and Sentence Position Score (SPS) and perform a weighted fusion of the four scores for each sentence in the document to compute a Final Sentence Score (FSS). The proposed DeepSumm framework was evaluated on the standard DUC 2002 benchmark and CNN/DailyMail datasets. Experimentally, it was demonstrated that our method captures both the global and the local semantic information of the document and essentially outperforms existing state-of-the-art approaches for extractive text summarization with ROUGE-1, ROUGE-2, and ROUGE-L scores of 53.2, 28.7 and 49.2 on DUC 2002 and 43.3, 19.0 and 38.9 on CNN/DailyMail dataset.},
	author = {Akanksha Joshi and Eduardo Fidalgo and Enrique Alegre and Laura Fern{\'a}ndez-Robles},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118442},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Text summarization, Extractive, Seq2seq, Attention networks, Topic models},
	pages = {118442},
	title = {DeepSumm: Exploiting topic models and sequence to sequence networks for extractive text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422015391},
	volume = {211},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422015391},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118442}}

@article{CAMPOS2022117510,
	abstract = {Massive Open Online Courses (MOOCs) have been widely disseminated due to the arrival of Web 2.0. However, the growth of MOOCs brings some difficulties for students in choosing suitable courses in these ecosystems. In recent years, some recommendation systems emerged to solve this problem but remain limited since they do not identify the student's prior knowledge broadly or the student's goals. To overcome this limitation, this work proposes the Fragmented Recommendation for MOOCs Ecosystems (FReME), a recommendation system to suggest parts of courses from multiple providers (i.e., Khan Academy, Udemy, and edX). FReME is based on the student profile and on the MOOCs ecosystems perspective to balance the ecological environment and strengthen interactions. Moreover, we differ from the current recommendation systems since our method identifies and reduces the students' knowledge gap optimizing the learning process. Experimental results conducted with a dataset integrating 3 MOOCs providers and 19 students demonstrated that the implemented techniques are more consistent than other approaches. Finally, it was verified through precision, utility, novelty, and confidence that our recommendations are 62,24% accurate, 68.89% useful, 72.81% reliable, and present new content in 99.12% of cases. These results validate that FReME supports students in reducing their knowledge gap.},
	author = {Rodrigo Campos and Rodrigo Pereira dos Santos and Jonice Oliveira},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117510},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Online education systems, Content-based recommendation, Topic modeling, Non-negative matrix factorization, Unsupervised machine learning},
	pages = {117510},
	title = {Providing recommendations for communities of learners in MOOCs ecosystems},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422008375},
	volume = {205},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422008375},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117510}}

@article{ZIHAYAT2021114910,
	abstract = {A patent gives the owner of an invention the exclusive rights to make, use and sell their invention. Before a new patent application is filed, patent lawyers are required to engage in Prior Art Search to determine the likelihood that an invention is novel, valid or to make sense of the domain. To perform this search, existing platforms utilize keywords and Boolean Logic, which disregards the syntax and semantics of natural language and thus, making the search extremely difficult. Consequently, studies regarding semantics using neural embeddings exist, but these only consider a narrow number of unidirectional words. In this study, we propose an end-to-end framework to consider bidirectional semantics, syntax and the thematic nature of natural language for prior art search. The proposed framework goes beyond keywords as input queries and takes a patent as the input. The contributions of this paper is twofold; adapting pre-trained embedding models (e.g., BERT) to address the semantics and syntax of language, followed by the second component, which exploits topic modeling to build a diversified answer that covers all themes across domains of the input patent. We evaluate the performance of the proposed framework on the CLEF-IP 2011 benchmark dataset and a real-world dataset obtained from Google patent repository and show that the proposed framework outperforms existing methods and returns meaningful results for a given patent.},
	author = {Morteza Zihayat and Rochelle Etwaroo},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114910},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Question answering, Prior art search, Pre-trained embeddings, Topic modeling, Search diversification, Sensemaking},
	pages = {114910},
	title = {A non-factoid question answering system for prior art search},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421003511},
	volume = {177},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421003511},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114910}}

@article{SHI2022116538,
	abstract = {Public concern detection provides potential guidance to the authorities for crisis management before or during a pandemic outbreak. Detecting people's concerns and attention from online social media platforms has been widely acknowledged as an effective approach to relieve public panic and prevent a social crisis. However, detecting concerns in time from massive volumes of information in social media turns out to be a big challenge, especially when sufficient manually labelled data is in the absence during public health emergencies, e.g., COVID-19. In this paper, we propose a novel end-to-end deep learning model to identify people's concerns and the corresponding relations based on Graph Convolutional Networks and Bi-directional Long Short Term Memory integrated with Concern Graphs. Except for the sequential features from BERT embeddings, the regional features of tweets can be extracted by the Concern Graph module, which not only benefits the concern detection but also enables our model to be high noise-tolerant. Thus, our model can address the issue of insufficient manually labelled data. We conduct extensive experiments to evaluate the proposed model by using both manually labelled tweets and automatically labelled tweets. The experimental results show that our model can outperform the state-of-the-art models on real-world datasets.},
	author = {Jingli Shi and Weihua Li and Sira Yongchareon and Yi Yang and Quan Bai},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.116538},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Concern detection, COVID-19, Auto concern extraction, Concern graph, Graph Convolutional Network},
	pages = {116538},
	title = {Graph-based joint pandemic concern and relation extraction on Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422000379},
	volume = {195},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422000379},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.116538}}

@article{SAKSHI2023119028,
	abstract = {Context
Although recognition works on mathematical expressions have been explored for four decades, the current literature and trends are varied and frequently influenced by distinct emerging methods and technology. This situation instigates the necessity of an organized review to provide heedful insight into research trends and patterns currently prevailing in the domain of mathematical expression recognition (MER).
Objective
To identify and associate (semantic mapping) the leading research zones, core research areas, and research trends steering in the MER domain. Identifying prominent recognition models based on extracted research areas. To develop the development chart from extracted research trends for directing the future works in this direction.
Method
A manual and automatic search has been performed across the reputed digital libraries for corpus formation. The formulated corpus is used for topic modeling, and Latent Dirichlet Allocation is deployed for information modeling for achieving defined objectives.
Result
The corpus of 325 research papers published from 1967 to 2021 has been processed using LDA. The five major research areas and ten research trends are identified. Leading research area is ``Segmentation and Classification Procedures'', and the trend with the highest related publications is ``Contextual and Graph-based recognition''. ``Attention and Deep Networks'' has emerged as the newborn trend, and the identified newborn, young, and matured trends impetrate more exploration from the MER research community.},
	author = {Sakshi and Vinay Kukreja},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.119028},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Mathematical expressions, Research trends, Pattern recognition, Latent dirichlet allocation, Topic modelling},
	pages = {119028},
	title = {Recent trends in mathematical expressions recognition: An LDA-based analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422020462},
	volume = {213},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422020462},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.119028}}

@article{SHAMS2017136,
	abstract = {Aspect extraction is one of the fundamental steps in analyzing the characteristics of opinions, feelings and emotions expressed in textual data provided for a certain topic. Current aspect extraction techniques are mostly based on topic models; however, employing only topic models causes incoherent aspects to be generated. Therefore, this paper aims to discover more precise aspects by incorporating co-occurrence relations as prior domain knowledge into the Latent Dirichlet Allocation (LDA) topic model. In the proposed method, first, the preliminary aspects are generated based on LDA. Then, in an iterative manner, the prior knowledge is extracted automatically from co-occurrence relations and similar aspects of relevant topics. Finally, the extracted knowledge is incorporated into the LDA model. The iterations improve the quality of the extracted aspects. The competence of the proposed ELDA for the aspect extraction task is evaluated through experiments on two datasets in the English and Persian languages. The experimental results indicate that ELDA not only outperforms the state-of-the-art alternatives in terms of topic coherence and precision, but also has no particular dependency on the written language and can be applied to all languages with reasonable accuracy. Thus, ELDA can impact natural language processing applications, particularly in languages with limited linguistic resources.},
	author = {Mohammadreza Shams and Ahmad Baraani-Dastjerdi},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2017.02.038},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect extraction, Topic modeling, Sentiment analysis, Latent Dirichlet Allocation (LDA), Co-occurrence relations},
	pages = {136-146},
	title = {Enriched LDA (ELDA): Combination of latent Dirichlet allocation with word co-occurrence analysis for aspect extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S095741741730129X},
	volume = {80},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741741730129X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.02.038}}

@article{ALTINELGIRGIN2021114599,
	abstract = {Inspired by the importance of social media, a Social Network Opinion Leaders (SNOL) system has been proposed in this paper. The purpose of this system is to identify topic-based opinion leaders of social media. In order to accomplish this goal, several steps have been taken, such as data collection, data processing, data analysis, data classification, ranking of topic-based opinion leaders, and evaluation. The SNOL system has two main parts. In the first part, collected tweets are classified by semantic kernels for topic-based analysis. In the second part, leadership scores are given to each user in the network according to topic modeling and user modeling results. Leadership scores are then calculated with the formula generated and opinion leaders are determined for each category. Experiments are performed on data gathered from Twitter including 17,234,924 tweets from 38,727 users. The evaluation of opinion leader detection is a difficult job since there is no standard method for identifying opinion leaders. Therefore, the evaluation of the results of this study has been done using two different methods, retweet count and spread score, to prove that the suggested methodology outperforms the PageRank algorithm. The results have also been evaluated considering the user-topic sentiment correlation of the retrieved lists. Furthermore, SNOL has been compared against some opinion leader detection methods previously presented in the literature. The experimental results show that SNOL generates remarkably higher performance than the PageRank algorithm and other existing algorithms in the literature for nearly all topics and all selected top N opinion leaders.},
	author = {Berna {Altnel Girgin}},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114599},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social network analysis, Opinion leader detection, Flow of influence, Sentiment polarity score, PageRank algorithm, Semantic kernels},
	pages = {114599},
	title = {Ranking influencers of social networks by semantic kernels and sentiment information},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000403},
	volume = {171},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000403},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114599}}

@article{SINGHCHAUHAN2020113673,
	abstract = {Social networking sites have a wealth of user-generated unstructured text for fine-grained sentiment analysis regarding the changing dynamics in the marketplace. In aspect-level sentiment analysis, aspect term extraction (ATE) task identifies the targets of user opinions in the sentence. In the last few years, deep learning approaches significantly improved the performance of aspect extraction. However, the performance of recent models relies on the accuracy of dependency parser and part-of-speech (POS) tagger, which degrades the performance of the system if the sentence doesn't follow the language constraints and the text contains a variety of multi-word aspect-terms. Furthermore, lack of domain and contextual information is again an issue to extract domain-specific, most relevant aspect terms. The existing approaches are not capable of capturing long term dependencies for noun phrases, which in turn fails to extract some valid aspect terms. Therefore, this paper proposes a two-step mixed unsupervised model by combining linguistic patterns with deep learning techniques to improve the ATE task. The first step uses rules-based methods to extract the single word and multi-word aspects, which further prune domain-specific relevant aspects using fine-tuned word embedding. In the second step, the extracted aspects in the first step are used as label data to train the attention-based deep learning model for aspect-term extraction. The experimental evaluation on the SemEval-16 dataset validates our approach as compared to the most recent and baseline techniques.},
	author = {Ganpat {Singh Chauhan} and Yogesh {Kumar Meena} and Dinesh Gopalani and Ravi Nahta},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113673},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Aspect extraction, Aspect-level sentiment analysis, Attention model, Deep learning, LSTM, Unsupervised learning},
	pages = {113673},
	title = {A two-step hybrid unsupervised model with attention mechanism for aspect extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420304978},
	volume = {161},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420304978},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113673}}

@article{ZHAO2022118335,
	abstract = {Scientific paper summarization aims at generating a short and concise digest while preserving important information of the original document. Currently, scientific paper summarization faces two main challenges. First, inter-sentence relations are hard to learn, especially in the case of long-form scientific papers. Second, structural information of the well-structured scientific papers has not been fully exploited. To overcome the above two challenges, we propose a novel Heterogeneous Tree structure-based extractive Summarization (HetTreSum) model, where each document is modeled as a tree structure to learn inter-sentence relations and structural information of the original document is incorporated, enabling the tree structure to have a global perspective of the whole document. Then an iterative updating strategy is presented to interactively refine nodes of the tree structure for better contextualized representations, which can further enhance summarization performance. Experimental results on PubMed and arXiv datasets show that our proposed HetTreeSum model achieves significantly advanced performance compared with various scientific paper summarization models.},
	author = {Jintao Zhao and Libin Yang and Xiaoyan Cai},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118335},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Scientific paper summarization, Heterogeneous tree structure, Inter-sentence relations, Structural information, Iterative updating strategy},
	pages = {118335},
	title = {HetTreeSum: A Heterogeneous Tree Structure-based Extractive Summarization Model for Scientific Papers},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422014580},
	volume = {210},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422014580},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118335}}

@article{RAHIMI2020113770,
	abstract = {A human is capable of understanding and classifying a text but a computer can understand the underlying semantics of a text when texts are represented in a way comprehensible by computers. The text representation is a fundamental stage in natural language processing (NLP). One of the main drawbacks of existing text representation approaches is that they only utilize one aspect or view of a text e.g. They only consider texts by their words while the topic information can be extracted from text as well. The term-document and document-topic matrix are two views of a text and contain complementary information. We use the strength of both views to extract a richer representation. In this paper, we propose three different text representation methods with the help of these two matrices and tensor factorization to utilize the power of both views. The proposed approach (Tens-Embedding) was applied in the tasks of text classification, sentence-level and document-level sentiment analysis and text clustering wherein the conducted experiments on 20newsgroups, R52, R8, MR and IMDB datasets indicated the superiority of the proposed method in comparison with other document embedding techniques.},
	author = {Zahra Rahimi and Mohammad Mehdi Homayounpour},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113770},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Text classification, Text representation, Document embeddings, Tensor factorization, Topic modeling},
	pages = {113770},
	title = {Tens-embedding: A Tensor-based document embedding method},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420305947},
	volume = {162},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420305947},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113770}}

@article{ZHENG2018168,
	abstract = {Social image annotation, which aims at inferring a set of semantic concepts for a social image, is an effective and straightforward way to facilitate social image search. Conventional approaches mainly demonstrated on adopting the visual features and tags, without considering other types of metadata. How to enhance the accuracy of social image annotation by fully exploiting multi-modal features is still an opening and challenging problem. In this paper, we propose an improved Multi-Modal Data Fusion based Latent Dirichlet Allocation (LDA) topic model (MMDF-LDA) to annotate social images via fusing visual content, user-supplied tags, user comments, and geographic information. When MMDF-LDA samples annotations for one data modality, all the other data modalities are exploited. In MMDF-LDA, geographical topics are generated from GPS locations of social images, and annotations have different probability to be used in different geographical regions. A social image is divided into several patches in advance, and then MMDF-LDA assigns annotations for the patches of social images by estimating the probability of annotation-patch assignment. Through experiments in social image annotation and retrieval on several datasets, we demonstrate the effectiveness of the proposed MMDF-LDA model in comparison with state-of-the-art methods.},
	author = {Liu Zheng and Zhang Caiming and Chen Caixian},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.03.014},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social image, Multi-modal data fusion, LDA model, Semantic annotation, Geographical topic},
	pages = {168-184},
	title = {MMDF-LDA: An improved Multi-Modal Latent Dirichlet Allocation model for social image annotation},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418301544},
	volume = {104},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418301544},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.03.014}}

@article{ANTONAKAKI2021114006,
	abstract = {Twitter is the third most popular worldwide Online Social Network (OSN) after Facebook and Instagram. Compared to other OSNs, it has a simple data model and a straightforward data access API. This makes it ideal for social network studies attempting to analyze the patterns of online behavior, the structure of the social graph, the sentiment towards various entities and the nature of malicious attacks in a vivid network with hundreds of millions of users. Indeed, Twitter has been established as a major research platform, utilized in more than ten thousands research articles over the last ten years. Although there are excellent review and comparison studies for most of the research that utilizes Twitter, there are limited efforts to map this research terrain as a whole. Here we present an effort to map the current research topics in Twitter focusing on three major areas: the structure and properties of the social graph, sentiment analysis and threats such as spam, bots, fake news and hate speech. We also present Twitter's basic data model and best practices for sampling and data access. This survey also lays the ground of computational techniques used in these areas such as Graph Sampling, Natural Language Processing and Machine Learning. Along with existing reviews and comparison studies, we also discuss the key findings and the state of the art in these methods. Overall, we hope that this survey will help researchers create a clear conceptual model of Twitter and act as a guide to expand further the topics presented.},
	author = {Despoina Antonakaki and Paraskevi Fragopoulou and Sotiris Ioannidis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.114006},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social networks, Twitter, Survey, Social graph, Sentiment analysis, Spam, Bots, Fake news, Hate speech},
	pages = {114006},
	title = {A survey of Twitter research: Data model, graph structure, sentiment analysis and attacks},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030779X},
	volume = {164},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030779X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.114006}}

@article{HACOHENKERNER2022117140,
	abstract = {Author profiling from text documents has become a popular task in latest years, in natural language applications. Author profiling is important for various domains such as advertising, marketing, forensics, and security. This survey focuses on profiling age and gender, the two features, which are probably the most researched profile attributes. In this paper, we present an overview of representative studies and datasets of the field (including those organized by PAN) with several significant leaps. Due to the increasing use of deep learning (DL) methods in recent years, we have also reviewed several DL systems that profile authors' age and gender. Most age and gender datasets contain blog posts or Twitter messages written in English, Spanish or Arabic. There are also several relevant datasets written in Dutch, Italian, Portuguese, Turkish, and Russian. There is no consistency and no uniformity in the datasets concerning to the number and types of their documents, the division into training, dev, and test sets, the types of the applied preprocessing methods, and the quality measures used to evaluate the classification results. A prominent interesting finding is that the best age accuracy results are not as high as we might have expected taking into account relatively simple types of classification especially by gender (only 2 categories) when a large number of teams have competed over the years. Another interesting finding that repeats itself in various classification tasks is that classical ML methods are still better than DL methods for age and gender classification tasks. Most classical systems used word unigrams and bigrams and character 3--4-5-grams. Several systems also used various types of stylistic features. While many earlier systems did not apply preprocessing methods, most recent systems applied several preprocessing methods, e.g., lowercase conversion and replacement of various strings (e.g., URLs, LF characters, and User Mentions). We also suggest several potential future issues in age and gender profiling research.},
	author = {Yaakov HaCohen-Kerner},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117140},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Age classification, Author profiling, Deep learning, Gender classification, Supervised machine learning, Text classification},
	pages = {117140},
	title = {Survey on profiling age and gender of text authors},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742200536X},
	volume = {199},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742200536X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117140}}

@article{AYO2021114762,
	abstract = {The key challenges for automatic hate-speech classification in Twitter are the lack of generic architecture, imprecision, threshold settings and fragmentation issues. Most studies used binary classifiers for hate speech classification, but these classifiers cannot really capture other emotions that may overlap between positive or negative class. Hence, a probabilistic clustering model for hate speech classification in twitter was developed to tackle problems with hate speech classification. A metadata extractor was used to collect tweets containing hate speech keywords and a crowd-sourced experts was employed to label the collected hate tweets into two categories: hate speech and non-hate speech. Features representation was done with Term Frequency- Inverse Document Frequency (TF-IDF) model and enhanced with topics inferred by a Bayes classifier. A rule-based clustering method was used to automatically classify real-time tweets into the correct topic clusters. Fuzzy logic was then used for hate speech classification using semantic fuzzy rules and a score computation module. From the evaluation results, it was observed that the developed model performed better in hate speech detection with F1-sore of 0.9256 using a 5-fold cross validation. Similarly, the developed model for hate speech classification performed better with F1-score of 91.5 compared to related models. The developed model also indicates a more perfect test having an AUC of 0.9645, when compared to similar methods. The Paired Sample t-Test validated the efficiency of the developed model for hate speech classification.},
	author = {Femi Emmanuel Ayo and Olusegun Folorunso and Friday Thomas Ibharalu and Idowu Ademola Osinuga and Adebayo Abayomi-Alli},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114762},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Twitter, Hate speech, Fuzzy logic, Combinatorial algorithm, Bayesian function, Sentiment analysis},
	pages = {114762},
	title = {A probabilistic clustering model for hate speech classification in twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421002037},
	volume = {173},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421002037},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114762}}

@article{ZHU2019430,
	abstract = {When users express their stances towards a topic in social media, they might elaborate their viewpoints or reasoning. Oftentimes, viewpoints expressed by different users exhibit a hierarchical structure. Therefore, detecting this kind of hierarchical viewpoints offers a better insight to understand the public opinion. In this paper, we propose a novel Bayesian model for hierarchical viewpoint discovery from tweets. Driven by the motivation that a viewpoint expressed in a tweet can be regarded as a path from the root to a leaf of a hierarchical viewpoint tree, the assignment of the relevant viewpoint topics is assumed to follow two nested Chinese restaurant processes. Moreover, opinions in text are often expressed in un-semantically decomposable multi-terms or phrases, such as `economic recession'. Hence, a hierarchical Pitman--Yor process is employed as a prior for modelling the generation of phrases with arbitrary length. Experimental results on two Twitter corpora demonstrate the effectiveness of the proposed Bayesian model for hierarchical viewpoint discovery.},
	author = {Lixing Zhu and Yulan He and Deyu Zhou},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2018.09.028},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Natural language processing, Opinion mining, Bayesian modelling},
	pages = {430-438},
	title = {Hierarchical viewpoint discovery from tweets using Bayesian modelling},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306055},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417418306055},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2018.09.028}}

@article{ALAMI2021114652,
	abstract = {Humans must easily handle the vast amounts of data being generated by the revolution of information technology. Thus, Automatic Text summarization has been applied to various domains in order to find the most relevant information and make critical decisions quickly. In the context of Arabic, text summarization techniques suffer from several problems. First, most existing methods do not consider the context or domain to which the document belongs. Second, the majority of the existing approaches are based on the traditional bag-of-words representation, which involves high dimensional and sparse data, and makes it difficult to capture relevant information. Third, research in Arabic Text summarization is fairly small and only recently compared to that on Anglo-Saxon and other languages due to the shortage of Arabic corpora, resources, and automatic processing tools. In this paper, we try to overcome these limitations by proposing a new approach using documents clustering, topic modeling, and unsupervised neural networks in order to build an efficient document representation model. First, a new document clustering technique using Extreme learning machine is performed on large text collection. Second, topic modeling is applied to documents collection in order to identify topics present in each cluster. Third, each document is represented in a topic space by a matrix where rows represent the document sentences and columns represent the cluster topics. The generated matrix is then trained using several unsupervised neural networks and ensemble learning algorithms in order to build an abstract representation of the document in the concept space. Important sentences are ranked and extracted according to a graph model with a redundancy elimination component. The proposed approach is evaluated on Essex Arabic Summaries Corpus and compared against other Arabic text summarization approaches using ROUGE measure. Experimental results showed that the models trained on topic representation learn better representations and improve significantly the summarization performance. In particular, ensemble learning models demonstrated an important improvement on Rouge recall and promising results on F-measure.},
	author = {Nabil Alami and Mohammed Meknassi and Noureddine En-nahnahi and Yassine {El Adlouni} and Ouafae Ammor},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114652},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Arabic text summarization, Natural language processing, Deep learning, Neural networks, Clustering, Topic modeling},
	pages = {114652},
	title = {Unsupervised neural networks for automatic Arabic text summarization using document clustering and topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000932},
	volume = {172},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421000932},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114652}}

@article{XIA2022118143,
	abstract = {The Fintech mobile payment platform is expanding rapidly; this expansion, in turn, creates numerous risks. There is an urgent need to better understand these risks and to spur more secure payment behavior. This research aims to develop knowledge graphs of the mobile payment platform based on deep learning for risk analysis and policy inferences. We identify entities from collected policy documents, extract the relationships among the entities, and draw a risk knowledge graph on mobile payments. The use of unsupervised semi-automatic knowledge acquisition, we argue, can reduce the risk of mobile payment caused by a lack of knowledge. A significant benefit of this method is that risk knowledge can be acquired without supervision. Unlike other models, the absence of manual labeling allows for the relation extraction of triples to be unsupervised, while the previous triplet extraction was supervised. Compared with other unsupervised models, the precision of our model is improved, and the recall is the same as that of previous unsupervised shutdown extraction.Unsupervised relationship extraction can extract text relationships quickly and on a large scale, saving human resources for labeling.This method offers a potential solution to a fundamental problem; the content and quantity of policy documents exceed organizations' and individuals' ability to understand them. Our approach suggests the viability of developing a national policy risk knowledge graph to help mobile payment platforms understand national policies and reduce platforms' operational risks while allowing users to quickly learn the risks of mobile payments and minimize the impact of those risks.},
	author = {Huosong Xia and Yuan Wang and Jeffrey Gauthier and Justin Zuopeng Zhang},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.118143},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Fintech, Mobile payment, Deep learning, Knowledge graph},
	pages = {118143},
	title = {Knowledge graph of mobile payment platforms based on deep learning: Risk analysis and policy implications},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422013264},
	volume = {208},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422013264},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118143}}

@article{DUAN2020113540,
	abstract = {Classification of investors' sentiments in stock message boards has attracted a great deal of attention. Since the messages are usually short, we propose a semi-supervised learning method to make full use of the features in both train and test messages. The generative emotion model takes message, emotion and words into consideration simultaneously. Based on the facts that words are of different ability in discriminating sentiments, they are categorized into three classes in the model with different emotion strength. Training the generative model can transform the messages into emotion vectors which finally feeds to a sentiment classifier. The experiment results show that the proposed model and learning method are efficient for modeling sentiment in short text, and by properly selecting the amount of train data and the percent of test samples, we can achieve higher classification accuracy than traditional ones. The results indicate that the generative model is effective for short message sentiment classification, and provides a significant approach for the implementation of semi-supervised learning which is a typical expert and intelligent information processing method.},
	author = {Jiangjiao Duan and Banghui Luo and Jianping Zeng},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113540},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Sentiment analysis, Generative model, Semi-supervised learning, Stock message board},
	pages = {113540},
	title = {Semi-supervised learning with generative model for sentiment classification of stock messages},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742030364X},
	volume = {158},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742030364X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113540}}

@article{CAO2022115977,
	abstract = {The increasing online reviews play an essential role in the e-commerce platform, which profoundly affects the purchase decisions of consumers. However, rampant dishonest sellers manipulate other buyers or robots to post deceptive reviews for profit. Recently, the detection of deceptive reviews has attracted general research attention, which mainly comprises two directions, traditional methods based on statistics and intelligent methods based on neural networks. These methods use a single feature or multiple features for classifier design. To make full use of different features for better feature representation of detecting deceptive reviews, this paper proposes a new feature fusion strategy and verifies its performance by comparing it with other feature fusion strategies. First, we utilize three independent models for feature extraction: the TextCNN, the Bidirectional Gated Recurrent Unit (GRU), and the Self-Attention are used to learn local semantic features, temporal semantic features, and weighted semantic features of reviews, respectively. Secondly, after obtaining different feature representations from the fully connected layers of these three models, we concatenate them together to form the final documental representation. Finally, we use a full connection layer and the sigmoid function to further learn and complete deceptive review detection. Experiments on three balanced and unbalanced in-domain small datasets (hotel, restaurant, doctor) and mixed-domain datasets show that our model is superior to baselines. Experiments on large-scale data with various imbalanced proportions verify the effectiveness of our method. We also analyze the results of different datasets from the perspective of part of speech to improve the model's interpretability.},
	author = {Ning Cao and Shujuan Ji and Dickson K.W. Chiu and Maoguo Gong},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.115977},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Deceptive reviews detection, Separated training, Convolutional neural network, Recurrent neural network, Self attention},
	pages = {115977},
	title = {A deceptive reviews detection model: Separated training of multi-feature learning and classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421013270},
	volume = {187},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421013270},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.115977}}

@article{DAHIR2021114909,
	abstract = {Query Expansion (QE) approaches that involve the reformulation of queries by adding new terms to the initial user query, are intended to ameliorate the vocabulary mismatch between the query keywords and the documents' in Information Retrieval Systems (IRS). One big issue in QE is the selection of the right candidate terms for expansion. For this purpose Linked Data can be used, as a valuable resource, for providing additional expansion features such as the values of sub- and super classes of resources. The underlying research question is whether interlinked data and vocabulary items provide features which can be taken into account for query expansion. In this paper, we introduced a new QE approach that aimed at improving IRS by using the well-known distribution based method Bose-Einstein statistics (Bo1) as well as Linked Data from the knowledge base DBpedia using different numbers of expansion terms. We evaluated the effectiveness of each method individually as well as their combinations using two Text REtrieval Conference (TREC) test collections. Our approach has lead to significant improvement in terms of precision, recall, Mean Average Precision (MAP) at rank 10, and normalized Discounted Cumulative Gain (nDCG) at different ranks compared to Pseudo Relevance Feedback (PRF) that we used as a baseline. The results show that the inclusion of semantic annotations clearly improves the retrieval performance over the baseline method.},
	author = {Sarah Dahir and Abderrahim {El Qadi} and Hamid Bennis},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.114909},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Information retrieval, Query expansion, DBpedia spotlight, Term distribution, Language model},
	pages = {114909},
	title = {Query expansion based on term distribution and DBpedia features},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742100350X},
	volume = {176},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742100350X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.114909}}

@article{KAUR2020113350,
	abstract = {Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24%, 95.64%, 95.87% precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.},
	author = {Sawinder Kaur and Parteek Kumar and Ponnurangam Kumaraguru},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2020.113350},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Clickbait, News, Classifier, Features, Social media},
	pages = {113350},
	title = {Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420301755},
	volume = {151},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417420301755},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2020.113350}}

@article{VIDANAGAMA2022117869,
	abstract = {Majority of customers and manufacturers who tend to purchase and trade via e-commerce websites primarily rely on reviews before making purchasing decisions and product improvements. Deceptive reviewers consider this opportunity to write fake reviews to mislead customers and manufacturers. This calls for the necessity of identifying fake reviews before making them available for decision making. Accordingly, this research focuses on a fake review detection method that incorporates review-related features including linguistic features, Part-of-Speech (POS) features, and sentiment analysis features. A domain feature ontology is used in the feature-level sentiment analysis and all the review-related features are extracted and integrated into the ontology. The fake review detection is enhanced through a rule-based classifier by inferencing the ontology. Due to the lack of a labeled dataset for model training, the Mahalanobis distance method was used to detect outliers from an unlabeled dataset where the outliers were selected as fake reviews for model training. The performance measures of the rule-based classifier were improved by integrating linguistic features, POS features, and sentiment analysis features, in spite of considering them separately.},
	author = {D.U. Vidanagama and A.T.P. Silva and A.S. Karunananda},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117869},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Domain ontology, Rule-based classifier, Outliers, Feature-level sentiment analysis, Review-related features},
	pages = {117869},
	title = {Ontology based sentiment analysis for fake review detection},
	url = {https://www.sciencedirect.com/science/article/pii/S095741742201123X},
	volume = {206},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741742201123X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117869}}

@article{ELIGUZEL2022117433,
	abstract = {Due to the rapid incline in the number of documents along with social media usage, text categorization has become an important concept. There are tasks required to be fulfilled during the text categorization, such as extracting useful data from different perspectives, reducing the high feature space dimension, and improving effectiveness. In order to accomplish these tasks, feature selection, and feature extraction gain importance. This paper investigates how to solve feature selection and extraction problems. Also, this study aims to decide which topics are the focus of a document. Moreover, the Twitter data-set is utilized as a document and an Uncapacitated P-Median Problem (UPMP) is applied to make clustering. In this study, UPMP is used on Twitter data collection for the first time to collect clustered tweets. Therefore, a novel hybrid genetic bat algorithm (HGBA) is proposed to solve the UPMP for our case. The proposed novel approach is applied to analyze the Twitter data-set of the Nepal earthquake. The first part of the analysis includes the data pre-processing stage. The Latent Dirichlet Allocation (LDA) method is applied to the pre-processed text. After that, a similarity (distance) matrix is generated by utilizing the Jensen Shannon Divergence (JSD) model. The study's main goal is to use Twitter to assess the needs of victims during and after a disaster. To evaluate the applicability of the proposed approach, experiments are conducted on the OR-Library data-set. The results demonstrate that the proposed approach successfully extracts topics and categorizes text.},
	author = {Nazmiye Elig{\"u}zel and Cihan {\c C}etinkaya and T{\"u}rkay Dereli},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2022.117433},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Bat algorithm, Feature extraction, Feature selection, Genetic algorithm, Uncapacitated P-median problem, Text categorization},
	pages = {117433},
	title = {A novel approach for text categorization by applying hybrid genetic bat algorithm through feature extraction and feature selection methods},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422007709},
	volume = {202},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417422007709},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.117433}}

@article{JIA2022116405,
	abstract = {Social network analysis is a fundamental problem inherent in various applications, which can be handled mainly based on a graph model given in advance. However, it is generally ignored in most existing studies that the social networks may only contain users and no users' connections are explicitly available. Then the connections between users play a considerable role in building the graph, and it is necessary to calculate users' similarities. Traditional methods of user similarity calculation are extensively based on the topics of text content because they can effectively reflect users' interests. Nevertheless, these methods mostly ignore the importance of time series in the texts, where the texts with time series can reveal the activity trends of users in social networks. In this work, we explore a new problem of building a social network graph over text with time series. Our basic idea is that social media users are more similar if they have similar text semantics in similar time sequences. To obtain the semantics of text with time series, we extract topic words of each user from the corresponding text with our proposed Time-Biterm Topic Model (T-BTM), which improves the BTM model by taking the time-topic distribution into account. On this basis, we further propose a novel time series-based graph model with text, called Text with Time series for Graph (TT-Graph) model, which explicitly considers the user similarity and time series similarity. With the TT-Graph model, we propose novel methods for topic detection, community detection, and link prediction in social network analysis. Extensive experiments demonstrate that topic detection, community detection and link prediction can be effectively conducted on the TT-Graph model, and the credibility of our model can be proved.},
	author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
	date-added = {2022-10-23 18:25:44 +0200},
	date-modified = {2022-10-23 18:25:44 +0200},
	doi = {https://doi.org/10.1016/j.eswa.2021.116405},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Social network graph, Text semantics, Time series, User similarity},
	pages = {116405},
	title = {TT-graph: A new model for building social network graphs from texts with time series},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421016948},
	volume = {192},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417421016948},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2021.116405}}

@article{REN2021107093,
	abstract = {Generating long macro reports from a piece of breaking news is quite a challenging task. Essentially, this task is a long text generation problem from short text. Apparently, the difficulty of this task lies in the logic inference of human beings. To address this issue, this paper proposes a novel hybrid deep generative neural model which first learns the outline of the input news and then generates macro financial reports from the learnt outline. In the outline generation component, we generate the outline text using the framework of Pointer-Generator network with attention mechanism. In the target report generation component, we generate the macro financial reports by the revised VAE model. To train our end-to-end model, we have collected the experimental dataset containing over one hundred thousand pairs of news-report data. Extensive experiments are then evaluated on this dataset. The proposed model achieves the SOTA performance against both the baseline models and the state-of-the-art models with respect to evaluation criteria BLEU, ROUGE and human scores. Although the readability of the generated reports by our approach is better than that of the rest models, it remains an open problem which needs further efforts in the future.},
	author = {Yunpeng Ren and Wenxin Hu and Ziao Wang and Xiaofeng Zhang and Yiyuan Wang and Xuan Wang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107093},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Financial data mining, Text generation, Natural language generation},
	pages = {107093},
	title = {A hybrid deep generative neural model for financial report generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003567},
	volume = {227},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121003567},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107093}}

@article{HE2020106228,
	abstract = {The classical high-order rating distance model which aims to minimize not only (i) the difference between the estimated and real ratings of the same user--item pair (i.e., the first-order rating distance), but also (ii) the difference between the estimated and real rating difference of the same user across different items (i.e., the second-order rating distance), and use stochastic gradient descent to solve this convex optimization problem in recommender systems, has good performance in prediction accuracy. However, when the manually set parameter for the second-order rating difference is fixed, this model will not converge as the size of dataset increasing, and its performance on efficiency is slow compared with the matrix factorization method. Aiming at improving such model's adaptability and efficiency, we propose an improved high-order rating distance model with omitting rules based on slack variable, in which the static parameter used to balance the first-order rating distance and the second-order rating distance is replaced by a data-scale sensitive function. We choose Newton method to solve the convex recommendation optimization problem defined in this paper instead of stochastic gradient descent. Our model not only achieves the adaptability by eliminating several static parameters for module balancing, reduces the computation complexity, but also accelerates the optimization function convergence speed. We provide solid theoretical support and conduct comprehensive experiments on four real-world datasets. Experimental results show the proposed model has good performance in terms of prediction accuracy and efficiency.},
	author = {Yifan He and Haitao Zou and Hualong Yu and Shang Zheng and Shang Gao},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106228},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Slack variable, High-order rating distance, Newton method},
	pages = {106228},
	title = {Adaptive and efficient high-order rating distance optimization model with slack variable},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304366},
	volume = {205},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304366},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106228}}

@article{NGUYEN2018313,
	abstract = {Cancer is a worldwide problem and one of the leading causes of death. Increasing prevalence of cancer, particularly in developing countries, demands better understandings of the effectiveness and adverse consequences of different cancer treatment regimes in real patient populations. Current understandings of cancer treatment toxicities are often derived from either ``clean'' patient cohorts or coarse population statistics. Thus, it is difficult to get up-to-date and local assessments of treatment toxicities for specific cancer centers. To address these problems, we propose a novel and efficient method for discovering toxicity progression patterns in the form of temporal association rules (TARs). A temporal association rule is defined as a rule where the diagnosis codes in the right hand side (e.g., a combination of toxicities/complications) are temporally occurred after the diagnosis codes in the left hand side (e.g., a particular type of cancer treatment). Our method develops a lattice structure to efficiently discover TARs. More specifically, the lattice structure is first constructed to store all frequent diagnosis codes in the dataset. It is then traversed using the paternity relations among nodes to generate TARs. Our extensive experiments show the effectiveness of the proposed method in discovering major toxicity patterns in comparison with the temporal comorbidity analysis. In addition, our method significantly outperforms existing methods for mining TARs in terms of runtime.},
	author = {Dang Nguyen and Wei Luo and Dinh Phung and Svetha Venkatesh},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.07.031},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Cancer treatment, Toxicity, Pairwise association analysis, Data mining, Temporal association rules},
	pages = {313-328},
	title = {LTARM: A novel temporal association rule mining method to understand toxicities in a routine cancer treatment},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118303824},
	volume = {161},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118303824},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.07.031}}

@article{DELCARMENRODRIGUEZHERNANDEZ2021106740,
	abstract = {In the Artificial Intelligence (AI) field, and particularly within the area of Machine Learning (ML), recommender systems have attracted significant research attention. These systems attempt to alleviate the increasing information overload that users can experience in the current Big Data era, by providing personalized recommendations of items that they may find relevant. Besides, given the importance of mobile computing, these systems have evolved to consider also the dynamic context of the mobile users (location, time, weather conditions, etc.) to offer them more appropriate suggestions and information while on the move. In this paper, we provide an extensive survey of recent advances towards intelligent mobile Context-Aware Recommender Systems (mobile CARS) from an information management perspective, with an emphasis on mobile computing and AI techniques, along with an analysis of existing research gaps and future research directions. We focus on approaches that go beyond just considering the location of the user and exploit also other context information. In this study, we have identified that deep learning approaches are promising artificial intelligence models for mobile CARS. Additionally, in a near future, we expect a higher prominence of push-based recommendation solutions where at least part of the recommendation engine could be executed in the mobile devices, which could share data and tasks in a distributed way.},
	author = {Mar{\'\i}a {del Carmen Rodr{\'\i}guez-Hern{\'a}ndez} and Sergio Ilarri},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106740},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Context-Aware Recommender Systems, Mobile computing, Context-aware computing, Personalization, Information management},
	pages = {106740},
	title = {AI-based mobile context-aware recommender systems from an information management perspective: Progress and directions},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121000034},
	volume = {215},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121000034},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106740}}

@article{ABEBE2020104817,
	abstract = {Various methods have been put forward to perform automatic social-based event detection and description. Yet, most of them do not capture the semantic meaning embedded in online social media data, which are usually highly heterogeneous and unstructured, and do not identify event relationships (e.g., car accident temporally occurs after storm, and geographically occurs near soccer match). To address this problem, we introduce a generic Social-based Event Detection, Description, and Linkage framework titled SEDDaL, taking as input: a collection of social media objects from heterogeneous sources (e.g., Flickr, YouTube, and Twitter), and producing as output a collection of semantically meaningful events interconnected with spatial, temporal, and semantic relationships. The latter are required as the building blocks for event-based Collective Knowledge (CK) organization, where CK underlines the combination of all known data, information, and metadata concerning a given concept or event. SEDDaL consists of four main modules for: i) describing social media objects in a generic Metadata Representation Space Model (MRSM) consisting of three composite dimensions: temporal, spatial, and semantic, ii) evaluating the similarity between social media objects' descriptions following MRSM, iii) detecting events from similar social media objects using an adapted unsupervised learning algorithm, where events are represented as clusters of objects in MRSM, and iv) identifying directional, metric, and topological relationships between events following MRSM's dimensions. We believe this is the first study to provide a generic model for describing semantic-aware events and their relationships extracted from social metadata on the Web. Experimental results confirm the quality and potential of our approach.},
	author = {Minale A. Abebe and Joe Tekli and Fekade Getahun and Richard Chbeir and Gilbert Tekli},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.06.025},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social media, Metadata, Semantics, Similarity evaluation, Event detection, Event relationships, Collective knowledge},
	pages = {104817},
	title = {Generic metadata representation framework for social-based event detection, description, and linkage},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302928},
	volume = {188},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302928},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.06.025}}

@article{ZHANG201849,
	abstract = {In the one-class collaborative filtering (OCCF) scenario, the elements of the user-item rating matrix consist each take one of only two values: either ``like'' or unknown. Previous methods for solving the OCCF problem can be roughly categorized into content-based methods, pointwise methods, and pairwise methods. A fundamental assumption of these approaches is that all missing values in a rating matrix can be treated as ``dislike''. However, this assumption may not hold because the missing values are not always negative. Sometimes users do not give positive feedback on an item simply because they are not familiar with it rather than because they dislike it. In addition, content-based methods usually require textual information on the items. In many cases, however, sufficient textual information is not available; therefore, content-based methods are not applicable. Moreover, a user's preference for items usually drifts over time, but the previous methods cannot capture the temporal dynamics of this drift. In this paper, we propose to modify the latent Dirichlet allocation (LDA) model to address the above-mentioned problems. Our method uses only observed rating data to predict users' interests and effectively avoids the issue of data skew. Furthermore, to address the issue that users' preferences for items usually drift over time, we assign a different weight to each rating according to its timestamp when using Gibbs sampling to estimate the parameters of the LDA model. In this way, the temporal dynamics of the user preferences can be captured. We report experiments conducted to evaluate our model. The results show that the proposed model outperforms state-of-the-art approaches for the OCCF problem.},
	author = {Haijun Zhang and Xiaoming Zhang and Zhiyong Tian and Zhenping Li and Jianye Yu and Feng Li},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.036},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {One-class collaborative filtering, Latent dirichlet allocation, Temporal dynamics},
	pages = {49-56},
	title = {Incorporating temporal dynamics into LDA for one-class collaborative filtering},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300996},
	volume = {150},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300996},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.036}}

@article{WEN2020106344,
	abstract = {Traditional recommender systems encounter several challenges such as data sparsity and unexplained recommendation. To address these challenges, many works propose to exploit semantic information from review data. However, these methods have two major limitations in terms of the way to model textual features and capture textual interaction. For textual modeling, they simply concatenate all the reviews of a user/item into a single review. However, feature extraction at word/phrase level can violate the meaning of the original reviews. As for textual interaction, they defer the interactions to the prediction layer, making them fail to capture complex correlations between users and items. To address those limitations, we propose a novel Hierarchical Text Interaction model (HTI) for rating prediction. In HTI, we propose to model low-level word semantics and high-level review representations hierarchically. The hierarchy allows us to exploit textual features at different granularities. To further capture complex user--item interactions, we propose to exploit semantic correlations between each user--item pair at different hierarchies. At word level, we propose an attention mechanism specialized to each user--item pair, and capture the important words for representing each review. At review level, we mutually propagate textual features between the user and item, and capture the informative reviews. The aggregated review representations are integrated into a collaborative filtering framework for rating prediction. Experiments on five real-world datasets demonstrate that HTI outperforms state-of-the-art models by a large margin. Further case studies provide a deep insight into HTI's ability to capture semantic correlations at different levels of granularities for rating prediction.},
	author = {Jiahui Wen and Jingwei Ma and Hongkui Tu and Mingyang Zhong and Guangda Zhang and Wei Yin and Jian Fang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106344},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Hierarchical neural network, Interactive networks, Review texts, Rating prediction},
	pages = {106344},
	title = {Hierarchical text interaction for rating prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305013},
	volume = {206},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305013},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106344}}

@article{ABUALIGAH2022108833,
	abstract = {Text document clustering is to divide textual contents into clusters or groups. It received wide attention due to the vast amount of daily data from the Web. In the last decade, Meta-Heuristic (MH) techniques have been adopted to solve clustering problems. Motivated by that, the authors introduce a reliable version of the newly developed MH algorithm called Arithmetic Optimization Algorithm (AOA). Math arithmetic operators inspire the AOA: multiplication, subtraction adding, and division. The AOA showed good performance in several global problems; nonetheless, it suffers from entrapment in local optima in complicated and high dimensional problems. Therefore, this paper proposes an improved version of AOA for the text document clustering problem. The Improved AOA (IAOA) introduces an integration between Opposition-based learning (OBL) and Levy flight distribution (LFD) with AOA to tackle the limitations of the traditional AOA. The IAOA is examined with different UCI datasets for the text clustering problems and assessed with a set of CEC2019 benchmark functions as a global optimization algorithm with extensive comparison to existing optimization algorithms. Overall, experimental results show the superiority of the proposed IAOA compared to several optimization algorithms. Moreover, the proposed IAOA is compared with twenty-one state-of-the-art methods using thirty-one benchmark text datasets, and the results proved the superiority of the proposed IAOA.},
	author = {Laith Abualigah and Khaled H. Almotairi and Mohammed A.A. Al-qaness and Ahmed A. Ewees and Dalia Yousri and Mohamed Abd Elaziz and Mohammad H. Nadimi-Shahraki},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108833},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Arithmetic Optimization Algorithm (AOA), Opposition-based learning, Levy flight, Text clustering, CEC2019 problems},
	pages = {108833},
	title = {Efficient text document clustering approach using multi-search Arithmetic Optimization Algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003975},
	volume = {248},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003975},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108833}}

@article{QIN2018342,
	abstract = {In social network analysis, community detection is a basic step to understand the structure and function of networks. Some conventional community detection methods may have limited performance because they merely focus on the networks' topological structure. Besides topology, content information is another significant aspect of social networks. Although some state-of-the-art methods started to combine these two aspects of information for the sake of the improvement of community partitioning, they often assume that topology and content carry similar information. In fact, for some examples of social networks, the hidden characteristics of content may unexpectedly mismatch with topology. To better cope with such situations, we introduce a novel community detection method under the framework of non-negative matrix factorization (NMF). Our proposed method integrates topology as well as content of networks and has an adaptive parameter (with two variations) to effectively control the contribution of content with respect to the identified mismatch degree. Based on the disjoint community partition result, we also introduce an additional overlapping community discovery algorithm, so that our new method can meet the application requirements of both disjoint and overlapping community detection. The case study using real social networks shows that our new method can simultaneously obtain the community structures and their corresponding semantic description, which is helpful to understand the semantics of communities. Related performance evaluations on both artificial and real networks further indicate that our method outperforms some state-of-the-art methods while exhibiting more robust behavior when the mismatch between topology and content is observed.},
	author = {Meng Qin and Di Jin and Kai Lei and Bogdan Gabrys and Katarzyna Musial-Gabrys},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.07.037},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social network analysis, Community detection, Semantic description, Non-negative matrix factorization, Robustness},
	pages = {342-356},
	title = {Adaptive community detection incorporating topology and content in social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118303885},
	volume = {161},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118303885},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.07.037}}

@article{LI2021107359,
	abstract = {Key elements refer to the elements that play a crucial role in disseminating information in social networks. The influence discovery of key elements can guide a series of works, such as public opinion control, user recommendation, and marketing promotion. Recently, there have been many studies on the influence of elements, but at present, many methods focus on either the influence discovery of key elements of different types or the dynamic influence discovery of a certain type of element alone, and rarely consider the combination of the two. Therefore, this study proposes a key element discovery algorithm based on a ternary association graph and representation learning, which can detect the influence of paths, users, and user groups. Additionally, the changes of different types of key elements can be analyzed according to the influence of elements in each stage of topic communication.},
	author = {Qian Li and Meiling Li and Xu Shi and Bin Wu and Yunpeng Xiao},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107359},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social networks, Hotspot topic, Key elements influence, Representation learning, Ternary association graph},
	pages = {107359},
	title = {A key elements influence discovery scheme based on ternary association graph and representation learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121006213},
	volume = {229},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121006213},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107359}}

@article{LI2021106948,
	abstract = {Online recommender systems generally suffer from severe data sparsity problems, and this are particularly prevalent in newly launched systems that do not have sufficient amounts of data. Cross-domain recommendations can provide us with some new ideas for assisting with user recommendations in sparse target domains by transferring knowledge from a source domain with rich data. In this paper, a deep sparse autoencoder prediction model based on adversarial learning for cross-domain recommendations (DSAP-AL) is proposed to improve the accuracy of rating predictions in similar cross-domain recommender systems. Specifically, joint matrix factorization and adversarial network learning models are adopted to integrate and align user and item latent factor spaces in a unified pattern. Then, a deep sparse autoencoder is represented and modeled by transferring the latent factors and interlayer weights. Furthermore, a domain factor adaptation algorithm is proposed to capture robust user and item factors, and the learned regularization constraints are added to the objective function, thereby alleviating the data sparsity issue. Experimental results on four real-world datasets demonstrate that, even without overlapping entities (users or items) in the source and target domains, the proposed DSAP-AL method achieves competitive performance relative to other state-of-the-art individual and cross domain approaches. Moreover, the DSAP-AL model is not only effective for scenarios with sparse data but also robust for noise-containing recommendations.},
	author = {Yakun Li and Jiadong Ren and Jiaomin Liu and Yixin Chang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106948},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Data sparsity, Sparse autoencoder, Adversarial learning, Recommender systems, Matrix factorization},
	pages = {106948},
	title = {Deep sparse autoencoder prediction model based on adversarial learning for cross-domain recommendations},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121002112},
	volume = {220},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121002112},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106948}}

@article{PONZA2020105051,
	abstract = {Many text mining tasks, such as clustering, classification, retrieval, and named entity linking, benefit from a measure of relatedness between entities in a knowledge graph. We present a thorough study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph. To facilitate this study, we introduce a new dataset with human judgments of entity relatedness. No clear dominance is seen between measures based on textual similarity and graph proximity. Some of the better measures involve expensive global graph computations. We propose a new, space-efficient, computationally lightweight, two-stage framework for relatedness computation. In the first stage, a small weighted subgraph is dynamically grown around the two query entities; in the second stage, relatedness is derived based on computations on this subgraph. Our system shows better agreement with human judgment than existing proposals both on the new dataset and on an established one. Our framework also shows improvements with respect to the state-of-the-art on three different extrinsic evaluations in the domains of ranking entity pairs, entity linking, and synonym extraction.},
	author = {Marco Ponza and Paolo Ferragina and Soumen Chakrabarti},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105051},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Entity relatedness, Wikipedia, Knowledge graph},
	pages = {105051},
	title = {On Computing Entity Relatedness in Wikipedia, with Applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119304447},
	volume = {188},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119304447},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105051}}

@article{DIGIROLAMO2021106563,
	abstract = {Current Online Social Networks represent a means for the continuous generation and distribution of information, which is slightly changed when moving from a user to another during the traversing of the network. Such an amount of information can overcome the capacity of a single user to manage it, so it would be useful to reduce it so that the user is able to have a summary of the information flowing the network. To this aim, it is of crucial importance to detect events within such an information stream, composing of the most representative words containing in each information instance, representing the event described by the set of tweet categorized together. There is a vast literature on off-line event detection on data-sets acquired from online social networks, but a similar solid set of approaches is missing if the detection has to be done on-line, which is demanding by the current applications. The driving idea described in this paper is to realize on-line clustering of tweets by leveraging on evolutionary game theory and the replicator dynamics, which have been used with success in many classification problems and/or multiobjective optimizations. We have adapted and enhanced a evolutionary clustering from the literature to meet the needs of on-line tweet clustering. Such a solution has been implemented according to the Kappa architectural model and assessed against state-of-the art approaches showing higher values of topic and keyword recall on two realistic data-sets.},
	author = {Rocco {Di Girolamo} and Christian Esposito and Vincenzo Moscato and Giancarlo Sperl{\'\i}},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106563},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Event detection, Tweet categorization, Online social networks, Online clustering, Game theory, Dempster--Shafer theory},
	pages = {106563},
	title = {Evolutionary game theoretical on-line event detection over tweet streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120306924},
	volume = {211},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120306924},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106563}}

@article{CAO2020106114,
	abstract = {Due to the short attention span and instant gratification phenomenon, micro-videos are growing exponentially while gaining more and more concerns. Yet the sheer number of micro-videos leads to severe information overload issues, making it difficult for users to identify their desired micro-videos. The hashtag, mainly utilized in the domain of the microblog or the image, is the indicator or the core idea of the target content and can be applied to various information retrieval scenarios (e.g., search, browse, and categorization). So far, however, little attention has been paid to perform the hashtag recommendation for micro-videos via harnessing multiple modalities. In this article, we devise a neural network-based solution, LOGO (short for ``muLti-mOdal-based hashtaG recOmmendation''), to recommend hashtags for micro-videos by utilizing multiple modalities. The proposed LOGO approach first represents each modality as the combination of sequential units in it, weighted by the attention mechanism. In this way, the sequential and attentive features are captured simultaneously. After that, the LOGO integrates the representations of all modalities via a multi-view representation learning framework, which projects the representations into a common space under the restriction of the modality similarity. Ultimately, the LOGO feed the projections of three modalities in the common space and the embeddings of hashtags into a customized neural collaborative filtering framework to perform the hashtag recommendation. Extensive experiments on the scope of both overall performance comparison and micro-level analyses have well-justified the effectiveness and rationality of our proposed approach.},
	author = {Da Cao and Lianhai Miao and Huigui Rong and Zheng Qin and Liqiang Nie},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106114},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Micro-videos, Hashtag recommendation, Multiple modalities, Deep neural network},
	pages = {106114},
	title = {Hashtag our stories: Hashtag recommendation for micro-videos via harnessing multiple modalities},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120303798},
	volume = {203},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120303798},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106114}}

@article{KEIKHA201847,
	abstract = {Social network analysis provides meaningful information about behavior of network members that can be used for diverse applications such as classification, link prediction. However, network analysis is computationally expensive because of feature learning for different applications. In recent years, many researches have focused on feature learning methods in social networks. Network embedding represents the network in a lower dimensional representation space with the same properties which presents a compressed representation of the network. In this paper, we introduce a novel algorithm named ``CARE'' for network embedding that can be used for different types of networks including weighted, directed and complex. Current methods try to preserve local neighborhood information of nodes, whereas the proposed method utilizes local neighborhood and community information of network nodes to cover both local and global structure of social networks. CARE builds customized paths, which are consisted of local and global structure of network nodes, as a basis for network embedding and uses the Skip-gram model to learn representation vector of nodes. Subsequently, stochastic gradient descent is applied to optimize our objective function and learn the final representation of nodes. Our method can be scalable when new nodes are appended to network without information loss. Parallelize generation of customized random walks is also used for speeding up CARE. We evaluate the performance of CARE on multi label classification and link prediction tasks. Experimental results on various networks indicate that the proposed method outperforms others in both Micro and Macro-f1 measures for different size of training data.},
	author = {Mohammad Mehdi Keikha and Maseud Rahgozar and Masoud Asadpour},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.028},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Representation learning, Network embedding, Community detection, Skip-gram model, Link prediction},
	pages = {47-54},
	title = {Community aware random walk for network embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300911},
	volume = {148},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300911},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.028}}

@article{ZHANG2018143,
	abstract = {Recently, a large number of geo-tagged landmark images have been uploaded through various social media services. Usually, these geo-tagged images are annotated by users with GPS and tags related to the landmarks where they are taken. Landmark tagging aims to automatically annotate an image with the tags to describe the landmark where the image is taken. It has been observed that the images and tags show strong correlation with the geographical locations. The widely used assumption by many existing tagging methods is that images are independently and identically distributed is not effective to capture the geographical correlation. In this paper, we study the novel problem of utilizing the geographical correlation among images and landmarks for better tagging landmark images. In particular, we propose an unsupervised feature learning approach to learn the geographically discriminative features across geographical locations, by integrating latent space learning and geographically structural analysis (LSGSA) into a joint model. A latent space learning model is proposed to effectively fuse the heterogeneous features of visual content and tags. Meanwhile, the geographical structure analysis and group sparsity are applied to learn the geographically discriminative features. Then, a geo-guided sparse reconstruction method is proposed to tag images by utilizing the discriminative information of features, in which the landmark-specific tags are boosted by a weighting method. Experiments on the real-world datasets demonstrate the superiority of our approach.},
	author = {Xiaoming Zhang and Zhonghua Zhao and Haijun Zhang and Senzhang Wang and Zhoujun Li},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.03.005},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Image tagging, Feature learning, Landmark image, Landmark features},
	pages = {143-154},
	title = {Unsupervised geographically discriminative feature learning for landmark tagging},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118301229},
	volume = {149},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118301229},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.03.005}}

@article{ARAQUE2020105184,
	abstract = {Moral rhetoric plays a fundamental role in how we perceive and interpret the information we receive, greatly influencing our decision-making process. Especially when it comes to controversial social and political issues, our opinions and attitudes are hardly ever based on evidence alone. The Moral Foundations Dictionary (MFD) was developed to operationalize moral values in the text. In this study, we present MoralStrength, a lexicon of approximately 1,000 lemmas, obtained as an extension of the Moral Foundations Dictionary, based on WordNet synsets. Moreover, for each lemma it provides with a crowdsourced numeric assessment of Moral Valence, indicating the strength with which a lemma is expressing the specific value. We evaluated the predictive potentials of this moral lexicon, defining three utilization approaches of increased complexity, ranging from lemmas' statistical properties to a deep learning approach of word embeddings based on semantic similarity. Logistic regression models trained on the features extracted from MoralStrength, significantly outperformed the current state-of-the-art, reaching an F1-score of 87.6% over the previous 62.4% (p-value <0.01), and an average F1-Score of 86.25% over six different datasets. Such findings pave the way for further research, allowing for an in-depth understanding of moral narratives in text for a wide range of social issues.},
	author = {Oscar Araque and Lorenzo Gatti and Kyriaki Kalimeri},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105184},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Moral foundations, Moral values, Lexicon, Twitter data, Natural language processing, Machine learning},
	pages = {105184},
	title = {MoralStrength: Exploiting a moral lexicon and embedding similarity for moral foundations prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511930526X},
	volume = {191},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511930526X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105184}}

@article{TOMER2022108108,
	abstract = {A novel text summarization framework referred to as Skip-Though Vector and Bi-encoder Based Automatic Text Summarization (STV--BEATS) is proposed in this paper. STV--BEATS utilizes --- (a) skip-though vector to generate sentence-based embedding; and (b) Long Short-Term Memory (LSTM) based deep autoencoder to reduce dimensions of skip thought vectors. STV--BEATS works in the conjunction of extractive and abstractive summarization models to enhance the overall quality of the results. For each sentence, relevance and novelty metrics are calculated on the intermediate representation of the deep autoencoder to generate the final sentence score. The highly scored sentences are selected to generate an extractive summary. On the other hand, the abstractive part is composed of two encoders and a decoder which works as --- (a) the first GRU-based bi-directional encoder and decoder work as basic sequence-to-sequence model on the extractive summary; and (b) the second GRU-based unidirectional encoder is used for fine encoding. Extensive computer experiments are conducted to determine the effectiveness of the STV--BEATS. Three standard benchmark datasets, namely, CNN/Daily Mail, DUC-2004, and DUC-2002 are used during experiments. Further, recall-oriented understudy for gisting evaluation (ROUGE) is used for validation of the STV--BEATS. Result reveals that the proposed STV--BEATS is capable of effective text summarization and achieves substantially better results over the state-of-the-art models.},
	author = {Minakshi Tomer and Manoj Kumar},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108108},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Abstractive text summarization, auto encoder, extractive text summarization, primary encoder, secondary encoder, skip thought vector},
	pages = {108108},
	title = {STV-BEATS: Skip Thought Vector and Bi-Encoder based Automatic Text Summarizer},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011680},
	volume = {240},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011680},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108108}}

@article{CHEN2022110000,
	abstract = {As more and more conversation-oriented streaming videos become available, streaming platforms have gradually taken the place of traditional media for people to access information. Still, conversation-oriented streaming videos are often lengthy and people are reluctant to view the whole video. In this research, we investigated highlight extraction on conversation-oriented streaming videos. Previous highlight extraction methods analyzed visual features of videos and are therefore unable to deal with conservation-oriented streaming videos whose highlights are related to streamer discourses and viewer responses. For this reason, the proposed highlight extraction method called COHETS does not evaluate visual features but rather simultaneously examines textual streams of streamer discourses and viewer messages to extract meaningful highlights. Experiments based on real world streaming data demonstrate that streamer discourses and viewer responses via their feedback messages are useful for extracting highlights of conversation-oriented streaming videos. Also, our designed position enrichment and message attention techniques effectively distill the embeddings of the two textual streams and lead to extraction results that are superior to those of state-of-the-art deep learning-based highlight extraction methods and extraction-based text summarization methods.},
	author = {Chien Chin Chen and Liang-Wei Lo and Sheng-Jie Lin},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.110000},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Supervised learning, Natural language processing, Video highlight extraction},
	pages = {110000},
	title = {COHETS: A highlight extraction method using textual streams of streaming videos},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122010930},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122010930},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.110000}}

@article{ORTEGABUENO2022107597,
	abstract = {Making machines understand language and reasoning on it has been one of the most challenging problems addressed by Artificial Intelligent researchers. This challenge increases when figurative language is used for communicating complex meanings, intentions, emotions and attitudes in creative and funny ways. In fact, sentiment analysis approaches struggle when facing irony, satire and other figurative languages, particularly those where the explanation of a prediction might arguably be as necessary as the prediction itself. This paper describes a new model MvAttLSTM based on deep learning for irony and satire detection in tweets written in distinct Spanish variants. The proposed model is based on an attentive-LSTM informed with three additional views learned from distinct perspectives. We investigate two strategies to pass these views into MvAttLSTM. We perform an extensive evaluation on three corpora, one for irony detection and two for satire detection. Moreover, in order to study the robustness of our proposed model, we investigate its performance on humor recognition. Experiments confirm that the proposed views help our model to improve its performance. Moreover, they show that affective information benefits our model to detect irony and satire. In particular, a first analysis of the results highlights the discriminating power of emotional features obtained from SenticNet and SEL lexicon. Overall, our system achieves the state-of-the-art performance in irony and satire detection in Spanish variants and competitive results in humor recognition.},
	author = {Reynier Ortega-Bueno and Paolo Rosso and Jos{\'e} E. {Medina Pagola}},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107597},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Irony and satire, Attention mechanism, Linguistic features, Contextualized pre-trained embedding, Fusing representation, Spanish variants, Figurative language},
	pages = {107597},
	title = {Multi-view informed attention-based model for Irony and Satire detection in Spanish variants},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121008595},
	volume = {235},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121008595},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107597}}

@article{VILLEGAS2018173,
	abstract = {Context-aware recommender systems leverage the value of recommendations by exploiting context information that affects user preferences and situations, with the goal of recommending items that are really relevant to changing user needs. Despite the importance of context-awareness in the recommender systems realm, researchers and practitioners lack guides that help them understand the state of the art and how to exploit context information to smarten up recommender systems. This paper presents the results of a comprehensive systematic literature review we conducted to survey context-aware recommenders and their mechanisms to exploit context information. The main contribution of this paper is a framework that characterizes context-aware recommendation processes in terms of: i) the recommendation techniques used at every stage of the process, ii) the techniques used to incorporate context, and iii) the stages of the process where context is integrated into the system. This systematic literature review provides a clear understanding about the integration of context into recommender systems, including context types more frequently used in the different application domains and validation mechanisms---explained in terms of the used datasets, properties, metrics, and evaluation protocols. The paper concludes with a set of research opportunities in this field.},
	author = {Norha M. Villegas and Cristian S{\'a}nchez and Javier D{\'\i}az-Cely and Gabriel Tamura},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.11.003},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Context-aware recommender systems, Pre-filtering, Post-filtering, Context modeling, Recommender systems evaluation},
	pages = {173-200},
	title = {Characterizing context-aware recommender systems: A systematic literature review},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117305075},
	volume = {140},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117305075},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.11.003}}

@article{HUANG201731,
	abstract = {This paper develops a principled approach to accurately identify interesting places in a city through social sensing applications. Social sensing has emerged as a new application paradigm, where a crowd of social sources (humans or devices on their behalf) collectively contribute a large amount of observations about the physical world. This paper studies an interesting place finding problem, in which the goal is to correctly identify the interesting places in a city. Important challenges exist in solving this problem: (i) the interestingness of a place is not only related to the number of users who visit it, but also depends upon the travel experience of the visiting users; (ii) the user's social connections could directly affect their visiting behavior and the interestingness judgment of a given place. In this paper, we develop a new Social-aware Interesting Place Finding Plus (SIPF+) approach that addresses the above challenges by explicitly incorporating both the user's travel experience and social relationship into a rigorous analytical framework. The SIPF+ scheme can find interesting places not typically identified by traditional travel websites (e.g., TripAdvisor, Expedia). We compare our solution with state-of-the-art baselines using two real-world datasets collected from location-based social network services and verified the effectiveness of our approach.},
	author = {Chao Huang and Dong Wang and Brian Mann},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.02.006},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Interesting place finding, Social dependency, Social sensing, Crowdsourcing, Expectation maximization},
	pages = {31-40},
	title = {Towards social-aware interesting place finding in social sensing applications},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511730059X},
	volume = {123},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511730059X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.02.006}}

@article{LIAO20179,
	abstract = {Opinion object-attribute extraction is one of the fundamental tasks of fine-grained sentiment analysis. It is accomplished by identifying opinion aspect entities (including object entities and attribute entities) and then aligning object entities to attribute entities. Recent studies on knowledge graphs have shown that by adding the embeddings of semantic structures between opinion aspect entities, structure-based learning models can achieve better performance in link-prediction than traditional methods. The studies, however, focused only on learning semantic structures between aspect entities, did not take language expression features into account. In this paper, we propose the Fusion RElation Embedded Representation Learning (FREERL) framework, by which, one can fuse semantic structures and language expression features such as statistical co-occurrence or dependency syntax, into the embeddings of object entities and attribute entities. The obtained embeddings are then used to align object-attribute pairs and to predict new pairs in a zero-shot scenario. Experimental results on the datasets of COAE2014 and COAE2015 show that the best results in our framework achieve 12.1% and 32.1% improvements over the baselines, respectively.},
	author = {Jian Liao and Suge Wang and Deyu Li and Xiaoli Li},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.07.015},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Fusion learning, Structure-based embedding, Language expression feature, Entity representation},
	pages = {9-17},
	title = {FREERL: Fusion relation embedded representation learning framework for aspect extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117303301},
	volume = {135},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117303301},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.07.015}}

@article{PRADHAN2020106181,
	abstract = {Manually selecting appropriate scholarly venues is becoming a tedious and time-consuming task for researchers due to many reasons that include relevance, scientific impact, and research visibility. Sometimes, high-quality papers get rejected due to mismatch between the area of the paper and the scope of the journal. Recommending appropriate academic venues can, therefore, enable researchers to identify and take part in relevant conferences and publish in journals that matter the most. A researcher may certainly know of a few leading venues for her specific field of interest. However, a venue recommendation system becomes particularly helpful when exploring a new domain or when more options are needed. Due to high dimensionality and sparsity of text data, and complex semantics of the natural language, journal identification presents difficult challenges. We propose a novel and unified architecture that contains a Bi-directional LSTM (Bi-LSTM) and a Hierarchical Attention Network (HAN) to address the above problems. We call the proposed architecture modularized Hierarchical Attention-based Scholarly Venue Recommender system (HASVRec), which only requires the abstract, title, keywords, field of study, and author of a new paper along with its past publication record to recommend scholarly venues. Experiments on the DBLP-Citation-Network V11 dataset exhibit that our proposed approach outperforms several state-of-the-art methods in terms of accuracy, F1, nDCG, MRR, average venue quality, and stability.},
	author = {Tribikram Pradhan and Abhinav Gupta and Sukomal Pal},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106181},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender system, Bidirectional LSTM, Attentive pooling, Deep learning, Hierarchical Attention Network (HAN)},
	pages = {106181},
	title = {HASVRec: A modularized Hierarchical Attention-based Scholarly Venue Recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304135},
	volume = {204},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304135},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106181}}

@article{ORTEGAMENDOZA2018169,
	abstract = {The Author Profiling (AP) task aims to predict specific profile characteristics of authors by analyzing their written documents. Nowadays, its relevance has been highlighted thanks to several applications in computer forensics, security and marketing. Most previous contributions in AP have been devoted to determine a suitable set of features to model the writing profile of authors. However, in social media this task is challenging due to the informal communication. In this regard, we present a novel approach, which considers that terms located in phrases exposing personal information have a special value for discriminating the author's profile. The aim of this research work is to emphasize the value of such personal phrases by means of two new proposals: a feature selection method and term weighting scheme, both based on a novel measure called Personal Expression Intensity (PEI) which scores the quantity of personal information revealed by a term. For evaluating the latter ideas, we show experimental results in age and gender prediction of media users on six different collections. Average improvements of 7.34% and 5.76% for age and gender classification were obtained when comparing to the best result from state-of-the-art, indicating that personal phrases play a key role for the AP task by means of selecting and weighting terms.},
	author = {Rosa Mar{\'\i}a Ortega-Mendoza and A. Pastor L{\'o}pez-Monroy and Anilu Franco-Arcega and Manuel Montes-y-G{\'o}mez},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.01.014},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Author profiling, Feature selection, Term weighting, Personal information, PEI},
	pages = {169-181},
	title = {Emphasizing personal information for Author Profiling: New approaches for term selection and weighting},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300224},
	volume = {145},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300224},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.01.014}}

@article{TELLEZ2018110,
	abstract = {A great variety of text tasks such as topic or spam identification, user profiling, and sentiment analysis can be posed as a supervised learning problem and tackled using a text classifier. A text classifier consists of several subprocesses, some of them are general enough to be applied to any supervised learning problem, whereas others are specifically designed to tackle a particular task using complex and computational expensive processes such as lemmatization, syntactic analysis, etc. Contrary to traditional approaches, we propose a minimalist and multi-propose text-classifier able to tackle tasks independently of domain and language. We named our approach TC. Our approach is composed of several easy-to-implement text transformations, text representations, and a supervised learning algorithm. These pieces produce a competitive classifier in several challenging domains such as informally written text. We provide a detailed description of TC along with an extensive experimental comparison with relevant state-of-the-art methods, i.e., TC was compared on 30 different datasets. Regarding accuracy, TC obtained the best performance in 20 datasets while achieves competitive results in the remaining ones. The compared datasets include several problems like topic and polarity classification, spam detection, user profiling and authorship attribution. Furthermore, our approach allows the usage of the technology even without an in-depth knowledge of machine learning and natural language processing.},
	author = {Eric S. Tellez and Daniela Moctezuma and Sabino Miranda-Jim{\'e}nez and Mario Graff},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.03.003},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Text classification, Hyperparameter optimization, Text modelling},
	pages = {110-123},
	title = {An automated text categorization framework based on hyperparameter optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118301217},
	volume = {149},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118301217},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.03.003}}

@article{BING201838,
	abstract = {In the procedure of Web search, a user first comes up with an information need and a query is issued with the need as guidance. After that, some URLs are clicked and other queries may be issued if those URLs do not meet his need well. We advocate that Web search is governed by a unified hidden space, and each involved element such as query and URL has its inborn position, i.e., projected as a vector, in this space. Each of above actions in the search procedure, i.e. issuing queries or clicking URLs, is an interaction result of those elements in the space. In this paper, we aim at uncovering such a unified hidden space of Web search that uniformly captures the hidden semantics of search queries, URLs and other involved elements in Web search. We learn the semantic space with search session data, because a search session can be regarded as an instantiation of users' information need on a particular semantic topic and it keeps the interaction information of queries and URLs. We use a set of session graphs to represent search sessions, and the space learning task is cast as a vector learning problem for the graph vertices by maximizing the log-likelihood of a training session data set. Specifically, we developed the well-known Word2vec to perform the learning procedure. Experiments on the query log data of a commercial search engine are conducted to examine the efficacy of learnt vectors, and the results show that our framework is helpful for different finer tasks in Web search.},
	author = {Lidong Bing and Zheng-Yu Niu and Piji Li and Wai Lam and Haifeng Wang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.037},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Web search, Query representation, Embedding space, Session analysis},
	pages = {38-48},
	title = {Learning a unified embedding space of web search from large-scale query log},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511830100X},
	volume = {150},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511830100X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.037}}

@article{WANG201885,
	abstract = {With the rapid development of information technology, scientific social network, i.e. SSN, have become the fastest and most convenient way for researchers to communicate with each other. However, a lot of published articles are shared via SSN every day which make it difficult for researchers to find highly valuable articles. To solve above information overload problem, a novel hybrid approach integrating with social information, i.e., HAR-SI, is proposed for the article recommendation in SSN. Unlike the traditional CBF and CF recommendation approaches, the social tag information and social friend information, which have been proved playing a significant role on recommendation in many domains, are effectively integrated into these two approaches separately to improve the accuracy in SSN. Prediction results made by the improved CBF and CF separately are combined with a hybrid method. In order to verify the effectiveness of the proposed HAR-SI, a real-life dataset from CiteULike was employed. Experimental results show that the proposed approach provides higher quality recommendations than the baseline methods, thus providing a more effective manner to recommend articles in SSN.},
	author = {Gang Wang and XiRan He and Carolyne Isigi Ishuga},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.024},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Article recommendation, Scientific social network, Hybrid approach, Content-based filtering, Collaborative filtering},
	pages = {85-99},
	title = {HAR-SI: A novel hybrid article recommendation approach integrating with social information in scientific social network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300820},
	volume = {148},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300820},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.024}}

@article{DENHAM2020105114,
	abstract = {The rise in the Internet of Things (IoT) and other sensor networks has created many vertically-distributed and high-velocity data streams that require specialized algorithms for true distributed data mining. This paper proposes a novel Hierarchical Distributed Stream Miner (HDSM) that learns relationships between the features of separate data streams with minimal data transmission to central locations. Experimental evaluation demonstrates significant improvements in classification accuracy over previously proposed distributed stream-mining approaches while minimizing data transmission and computational costs. HDSM's potential for dynamically trading off accuracy with computational resource costs is also demonstrated.},
	author = {Benjamin Denham and Russel Pears and M. Asif Naeem},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105114},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Distributed data stream mining, Vertically-distributed data, Online classification},
	pages = {105114},
	title = {HDSM: A distributed data mining approach to classifying vertically distributed data streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119304836},
	volume = {189},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119304836},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105114}}

@article{BENABDERRAHMANE201895,
	abstract = {The recent proliferation of social networks as a main source of information and interaction has led to a huge expansion of automatic e-recruitment systems and by consequence the multiplication of web channels (job boards) that are dedicated to job offers disseminating. In a strategic and economic context where cost control is fundamental, it has become necessary to identify the relevant job board for a given new job offer has become necessary. The purpose of this work is to present the recent results that we have obtained on a new job board recommendation system that is a decision-making tool intended to guide recruiters while they are posting a job on the Internet. Firstly, the Doc2Vec embedded representation is used to analyse the textual content of the job offers, then the job applicant clickstreams history on various job boards are stored in a large learning database, and then represented as time series. Secondly, a deep neural network architecture is used to predict future values of the clicks on the job boards. Third, and in parallel, dimensionality reduction techniques are used to transform the clicks numerical time series into temporal symbolic sequences. Forecasting algorithms are then used to predict future symbols for each sequence. Finally, a list of top ranked job boards are kept by maximizing the clickstreams forecasting in both representations. Our experiments are tested on a real dataset, coming from a job-posting database of an industrial partner. The promising results have shown that using deep learning, the recommendation system outperforms standard multivariate models.},
	author = {Sidahmed Benabderrahmane and Nedra Mellouli and Myriam Lamolle},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.03.025},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender system, Time series, Deep learning, Symbolic sequences, Big data, E-recruitment},
	pages = {95-113},
	title = {On the predictive analysis of behavioral massive job data using embedded clustering and deep recurrent neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118301576},
	volume = {151},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118301576},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.03.025}}

@article{LEE201770,
	abstract = {Contextual information is helpful in building systems that can meet users' needs more efficiently and practically. Human activity provides a special kind of contextual information that can be combined with the perceived environmental data to determine appropriate service actions. In this study, we develop a smartphone-based mobile system that includes two core modules for recognizing human activities and then making music streaming recommendation accordingly. Machine learning methods with feature selection techniques are used to perform activity recognition from smartphone signals, and collaborative filtering methods are adopted for music recommendation. A series of experiments are conducted to evaluate the performance of our activity-aware framework. Moreover, we implement a mobile music streaming recommendation system on a smartphone-cloud platform to demonstrate that the proposed approach is practical and applicable to real-world applications.},
	author = {Wei-Po Lee and Chun-Ting Chen and Jhih-Yuan Huang and Jhen-Yi Liang},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.06.002},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Activity recognition, Context-awareness, Mobile music recommendation, Feature extraction, Classification, Smartphone},
	pages = {70-82},
	title = {A smartphone-based activity-aware system for music streaming recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117302757},
	volume = {131},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117302757},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.06.002}}

@article{YUE2020106206,
	abstract = {Observed rating data in Web2.0 applications concerns user attributes and rating scores, which explicitly reflects users' overall evaluation on events, products and various informative items. However, the unobservable user preference is critical for personalized services, precise marketing, accurate advertising, etc. In this paper, by adopting Bayesian network (BN) with a latent variable as the knowledge framework to describe user preference using the latent variable, we propose user preference Bayesian network (UPBN) to represent dependence relations among the latent and observed variables. By incorporating the classic expectation maximization (EM) algorithm and scoring & search idea for learning a BN, we focus on UPBN construction from rating data, i.e., the learning of probability parameters and graphical structure. To make UPBN fit the rating data, we first give the constraints of structure and parameters in terms of inherence dependencies among user preference, latent variable and characteristics of EM. Consequently, we present a parallel and constraint induced algorithm for UPBN construction based on EM, structural EM (SEM) and Bayesian information criterion. To deal with the large amount of iterations of probability computations and guarantee the efficiency of model construction, we implement our algorithms upon Spark for the massive intermediate results and large scale rating datasets. Experimental results show the expressiveness of UPBN for preference modeling and the efficiency of model construction, and also demonstrate that UPBN outperforms some state-of-the-art models for user preference estimation and rating prediction.},
	author = {Kun Yue and Xinran Wu and Liang Duan and Shaojie Qiao and Hao Wu},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106206},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Rating data, User preference, Latent variable, Bayesian network, Expectation maximization, Spark},
	pages = {106206},
	title = {A parallel and constraint induced approach to modeling user preference from rating data},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304251},
	volume = {204},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304251},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106206}}

@article{WANG2017125,
	abstract = {Recommender systems based on locations and tags have received a great deal of interest over the last few years. Whereas, recent advances do not transcend limits of recommendation algorithms that solely use geographical information or textual information. In this paper, we propose a novel location and tag aware recommendation framework that uses ratings, locations and tags to generate recommendation. In this framework, all users are partitioned into several clusters by a newly designed Memetic Algorithm (MA) based clustering method. Normal users are recommended items obtained by applying Latent Dirichlet Allocation (LDA) to users within each cluster. For cold-start users, each cluster is viewed as a new user. Each cluster is recommended a list of items by applying LDA to all clusters. The recommendation list to the querying cluster is recommended to all cold-start users in this cluster. Extensive experiments on real-world datasets demonstrate that compared with state-of-the-art location and tag aware recommendation algorithms, the proposed algorithm has better performance on making recommendations and alleviating cold-start problem.},
	author = {Shanfeng Wang and Maoguo Gong and Haoliang Li and Junwei Yang and Yue Wu},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.05.030},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Memetic algorithm, Clustering, Latent Dirichlet Allocation, Location-based recommendation},
	pages = {125-134},
	title = {Memetic algorithm based location and topic aware recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117302836},
	volume = {131},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117302836},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.05.030}}

@article{PANG2019104786,
	abstract = {With the rapid development of the Internet, the data generated from application platforms such as online shopping, e-education, and digital entertainment has exhibited dramatical growth, which has caused serious information overload to Internet users. The traditional recommendation approaches are crucial for Internet users to extract valuable information from various information. However, there exist some problems such as sparse data, cold start, and over-reliance on manual extracted feature and so on. To address the above problems, this paper proposes a novel recommender with Attention-based Convolutional Neural Network and Factorization Machines (ACNN-FM), which achieves the recommendation with comments. Firstly, from the perspective of local to overall, this paper proposes a word-level attention mechanism and a phrase-level attention mechanism to increase the ability to remember the importance and the order of historical vocabulary (phrase) in the process of text processing of convolutional neural networks. Secondly, it constructs a model to automatically extract hidden features of users and items from comments in the form of natural language. Finally, we utilize factorization machines to analyze the association between the hidden features of users and items, and implement the recommendation based on the association. Extensive experiments are conducted for demonstrating that ACNN-FM method outperforms state-of-the-art NARR method, and ACNN-FM has the highest data utilization among NARR, DeepCoNN, BCF and NMF methods, thus the recommendation performance is significantly improved in large-scale data environment.},
	author = {Guangyao Pang and Xiaoming Wang and Fei Hao and Jiehang Xie and Xinyan Wang and Yaguang Lin and Xueyang Qin},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.05.029},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommendation, Convolutional neural network, Attention mechanism, Factorization machines, Machine learning},
	pages = {104786},
	title = {ACNN-FM: A novel recommender with attention-based convolutional neural network and factorization machines},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302412},
	volume = {181},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302412},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.05.029}}

@article{ZHANG2022108006,
	abstract = {Recently, text-to-Image (T2I) generation has been well developed by improving synthesis authenticity, text-consistency and generation diversity. However, large amount of pairwise image--text data required restricts generalization of synthesis models only to its pre-trained language. In this paper, a cross-lingual pre-training method is proposed to adapt target low-resource language to pre-trained generative models. As far as we known, this is the first time that arbitrary input languages could access T2I generation. This joint encoding scheme fulfills both universal and visual semantic alignment. With any prepared GAN-based T2I framework, pre-trained source encoder model could be easily fine-tuned to construct target encoder model and hence entirely enable transfer of T2I synthesis ability between languages. After that, a semantic-level alignment independent of source T2I structure is established to guarantee optimal text consistency and detail generation. Different from monolingual T2I methods that apply discriminator to enhance generation quality, we use an adversarial training scheme that optimizes the sentence-level alignment along with the word-level alignment with a self-attention mechanism. Considering of training for low-resource languages lack of parallel texts in practice, target input embedding is designed available for zero-shot learning. Experimental results prove robustness of the proposed cross-lingual T2I pre-training on multiple downstream generative models and target languages applied.},
	author = {Han Zhang and Suyi Yang and Hongqing Zhu},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108006},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Cross-lingual pre-training, Text-to-image synthesis, Universal contextual word vector space, Semantic alignment, Joint adversarial training},
	pages = {108006},
	title = {CJE-TIG: Zero-shot cross-lingual text-to-image generation by Corpora-based Joint Encoding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011138},
	volume = {239},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011138},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108006}}

@article{XU2022107839,
	abstract = {Quantifying and predicting the long-term impact of both scientific papers and individual authors have important implications for many academic policy decisions, from identifying emerging trends to assessing the merits of proposals for potential funding. This paper presents SI-HDGNN, a novel heterogeneous dynamical graph neural network that explicitly models a heterogeneous, weighted, directed and attributed academic graph, enabling a prediction of the cumulative scientific impact of papers and authors by a specifically designed aggregation method. Unlike the existing feature-based or homogeneous approaches, SI-HDGNN addresses the problem by capturing the temporal--structural characteristics of the papers and authors as well as their complex interactions and long-term dependencies. Extensive experiments conducted on three large-scale multidisciplinary academic datasets demonstrate its superior performance in predicting the long-term scientific impact of both scientific papers and authors.},
	author = {Xovee Xu and Ting Zhong and Ce Li and Goce Trajcevski and Fan Zhou},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107839},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Scientific impact prediction, Heterogeneous information network, Graph neural network, Information diffusion, Science of science},
	pages = {107839},
	title = {Heterogeneous dynamical academic network for learning scientific impact propagation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121010285},
	volume = {238},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121010285},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107839}}

@article{YANG2020105214,
	abstract = {Last decade has witnessed a drastically increasing development of smart devices, while related mobile applications have emerged significantly in people's daily life. As such, understanding the pattern of mobile application usage and related online behavior is of great importance for a variety of purposes, such as application engineering, resource optimization, and marketing. Existing research of online usage discovery includes surveys from end-users, application provider-related analysis, and usage log mining. These works, however, suffer from some limitations, such as lacking of user socio-economics background, insufficient coverage and sample bias, etc. A novel and comprehensive application-usage profiling algorithm, termed as TAG, is proposed in this study to investigate online behavior. The proposed algorithm consists of three major steps: (i) T-step: representing usage data as a Term Frequency-Inverse Document Frequency based matrix; (ii) A-step: applying Alternating Least Squares factorization technique to reduce data sparseness and dimension; and last (iii) G-step: utilizing a smoothed Gaussian Mixture Model for clustering purpose. The performance of the proposed TAG algorithm is evaluated, taking a national dataset generated from 31,280 devices and 30,155 applications over 30 months as an example. Experimental results demonstrate that the proposed algorithm outperforms existing methods via forming accurate usage groups from school-level online behavior. As such, the superior clustering outcome demonstrates the flexibility and applicability of the proposed work for understanding online pattern using complex application usage data. Resultant knowledge can in turn be used to inform decision making and improve application development.},
	author = {Jie Yang and Jun Ma and Sarah K. Howard},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105214},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {User online behavior, Mobile applications, Alternating least squares, Smooth Gaussian mixture model},
	pages = {105214},
	title = {Usage profiling from mobile applications: A case study of online activity for Australian primary schools},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119305441},
	volume = {191},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119305441},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105214}}

@article{CHI201788,
	abstract = {Serial crimes pose a great threat to public security. Linking crimes committed by the same offender can assist the detecting of serial crimes and is of great importance in maintaining public security. Currently, most crime analysts still link serial crimes empirically especially in China and desire quantitative tools to help them. This paper presents a decision support system for crime linkage based on various, including behavioral, features of criminal cases. Its underlying technique is pairwise classification based on similarity, which is interpretable and easy to tune. We design feature similarity algorithms to calculate the pairwise similarities and build up a classifier to determine whether a case pair should belong to a series. A comprehensive case study of a real-world robbery dataset demonstrates its promising performance even with the default setting. This system has been deployed in a public security bureau of China and running for more than one year with positive feedback from users. The use of this system would provide individual officers with strong support in crimes investigation then allow law enforcement agency to save resources, since the system not only can link serial crimes automatically based on a classification model learned from historical crime data, but also has flexibility in training data update and domain experts interaction, including adjusting the key components like similarity matrices and decision thresholds to reach a good tradeoff between caseload and number of true linked pairs.},
	author = {Hong Chi and Zhihong Lin and Huidong Jin and Baoguang Xu and Mingliang Qi},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.02.017},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Serial crimes, Crime linkage, Pairwise case linkage, Robbery, Hierarchical similarity},
	pages = {88-101},
	title = {A decision support system for detecting serial crimes},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117300837},
	volume = {123},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117300837},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.02.017}}

@article{ZHANG2018236,
	abstract = {Traditional stock market prediction approaches commonly utilize the historical price-related data of the stocks to forecast their future trends. As the Web information grows, recently some works try to explore financial news to improve the prediction. Effective indicators, e.g., the events related to the stocks and the people's sentiments toward the market and stocks, have been proved to play important roles in the stocks' volatility, and are extracted to feed into the prediction models for improving the prediction accuracy. However, a major limitation of previous methods is that the indicators are obtained from only a single source whose reliability might be low, or from several data sources but their interactions and correlations among the multi-sourced data are largely ignored. In this work, we extract the events from Web news and the users' sentiments from social media, and investigate their joint impacts on the stock price movements via a coupled matrix and tensor factorization framework. Specifically, a tensor is firstly constructed to fuse heterogeneous data and capture the intrinsic relations among the events and the investors' sentiments. Due to the sparsity of the tensor, two auxiliary matrices, the stock quantitative feature matrix and the stock correlation matrix, are constructed and incorporated to assist the tensor decomposition. The intuition behind is that stocks that are highly correlated with each other tend to be affected by the same event. Thus, instead of conducting each stock prediction task separately and independently, we predict multiple correlated stocks simultaneously through their commonalities, which are enabled via sharing the collaboratively factorized low rank matrices between matrices and the tensor. Evaluations on the China A-share stock data and the HK stock data in the year 2015 demonstrate the effectiveness of the proposed model.},
	author = {Xi Zhang and Yunjia Zhang and Senzhang Wang and Yuntao Yao and Binxing Fang and Philip S. Yu},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.12.025},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social media, Stock correlation, Tensor factorization, Stock prediction},
	pages = {236-247},
	title = {Improving stock market prediction via heterogeneous information fusion},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117306032},
	volume = {143},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117306032},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.12.025}}

@article{ZHU2022108741,
	abstract = {Bug localization, which aims to locate buggy source code files for given bug reports, is a crucial yet challenging software-mining task. Despite remarkable success, the state of the art falls short in handling (1) bug reports with diverse characteristics and (2) programs with wildly different behaviors. In response, this paper proposes a graph-based neural model BLoco for automated bug localization. To be specific, our proposed model decomposes bug reports into several bug clues to capture bug-related information from various perspectives for highly diverse bug reports. To understand the program in depth, we first design a code hierarchical network structure, Code-NoN, based on basic blocks to represent source code files. Correspondingly, a multilayer graph neural network is tailored to capture program behaviors from the Code-NoN structure of each source code file. Finally, BLoco further incorporates a bi-affine classifier to comprehensively predict the relationship between the bug reports and source files. Extensive experiments on five large-scale real-world projects demonstrate that the proposed model significantly outperforms existing techniques.},
	author = {Ziye Zhu and Hanghang Tong and Yu Wang and Yun Li},
	date-added = {2022-10-23 18:24:17 +0200},
	date-modified = {2022-10-23 18:24:17 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108741},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Software mining, Bug localization, Bug report, Program behavior, Hierarchical network, Network of networks},
	pages = {108741},
	title = {Enhancing bug localization with bug report decomposition and code hierarchical network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003483},
	volume = {248},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003483},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108741}}

@article{WEI201741,
	abstract = {Document embedding is a technology that captures informative representations from high-dimensional observations by some structure-preserving maps over corpus and has been intensively explored in machine learning. Recently, some manifold-inspired embedding methods become a hot topic, mainly due to their ability in capturing discriminative embedding. However, the existing methods capture the embeddings based on the geometrical information of nearest neighbors without considering the intrinsic documents-generating structure on a subspace, thus leads to a limitation to uncover intrinsic semantic information. In this paper, we propose a semi-supervised local-invariant method, called Discriminative Locally Document Embedding (Disc-LDE), aiming to build a smooth affine map for document embedding by preserving documents-generating structure on a subspace. Disc-LDE models the documents-generating structure as a pseudo-document by a generative probabilistic model of subspace, where the subspace is acquired by a transductive learning of multi-agent random walk on neighborhood graph, and regularizes the training of Auto-Encoders (AEs) to jointly recover the input document and its pseudo-document. Under a general regularized function learning framework, the regularized training can impact the parameterized encoder network become smooth to variations along the documents-generating structure of the local field on manifold. The experimental results on three widely-used corpora demonstrate Disc-LDE could efficient capture the intrinsic semantic structure to improve the clustering and classification performance to the state-of-the-arts methods.},
	author = {Chao Wei and Senlin Luo and Jia Guo and Zhouting Wu and Limin Pan},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.01.012},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Document embedding, Smooth affine map, Generative probabilistic model, Multi-agent random walk, Regularized auto-encoders},
	pages = {41-57},
	title = {Discriminative locally document embedding: Learning a smooth affine map by approximation of the probabilistic generative structure of subspace},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117300242},
	volume = {121},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117300242},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.01.012}}

@article{ALI201927,
	abstract = {Social networks play a key role in providing a new approach to collecting information regarding mobility and transportation services. To study this information, sentiment analysis can make decent observations to support intelligent transportation systems (ITSs) in examining traffic control and management systems. However, sentiment analysis faces technical challenges: extracting meaningful information from social network platforms, and the transformation of extracted data into valuable information. In addition, accurate topic modeling and document representation are other challenging tasks in sentiment analysis. We propose an ontology and latent Dirichlet allocation (OLDA)-based topic modeling and word embedding approach for sentiment classification. The proposed system retrieves transportation content from social networks, removes irrelevant content to extract meaningful information, and generates topics and features from extracted data using OLDA. It also represents documents using word embedding techniques, and then employs lexicon-based approaches to enhance the accuracy of the word embedding model. The proposed ontology and the intelligent model are developed using Web Ontology Language and Java, respectively. Machine learning classifiers are used to evaluate the proposed word embedding system. The method achieves accuracy of 93%, which shows that the proposed approach is effective for sentiment classification.},
	author = {Farman Ali and Daehan Kwak and Pervez Khan and Shaker El-Sappagh and Amjad Ali and Sana Ullah and Kye Hyun Kim and Kyung-Sup Kwak},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.02.033},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social network analysis, Sentiment analysis, Topic modeling, Mobility users, Word embedding},
	pages = {27-42},
	title = {Transportation sentiment analysis using word embedding and ontology-based topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119300942},
	volume = {174},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119300942},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.02.033}}

@article{ZHANG2021107135,
	abstract = {Incorporating extra attributes of customer reviews, such as user and product information, to align text representations to each attribute has improved the sentiment polarity classification performance. Existing works only treated such attributes separately thus ignored the interactive information between these attributes. In this paper, we proposed an interactive attributes attention model that considered all attributes to be relevant and investigated the interactive relationships in and across separate features to improve the sentiment classification performance for customer reviews. In addition to the local text encoder, three more interactive attribute encoders, including user--product, user--text, and product--text encoders, are applied to extract implicit information to align attribute features to text representations with a bilinear interaction instead of self-attention. To better integrate different information, a multiloss objective function is used to further improve the performance. The comparative experiments on the IMDB, Yelp, and Amazon datasets show that the proposed model achieves significant improvements in the effects of the bilinear interactions in and across attributes and local text features.},
	author = {You Zhang and Jin Wang and Xuejie Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107135},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Text classification, Sentence representation, Bilinear attention},
	pages = {107135},
	title = {Personalized sentiment classification of customer reviews via an interactive attributes attention model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003981},
	volume = {226},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121003981},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107135}}

@article{YANG2021106687,
	abstract = {Review-based recommendation algorithms can alleviate the data sparsity issue in collaborative filtering by combining user ratings and reviews in model learning. However, most existing methods simplify the feature extraction process from reviews by assuming that different granularities of information (e.g., word, review, and feature) are equally important, which cannot optimally leverage the most important information and thus achieves suboptimal recommendation accuracy. Besides, many existing works directly regard text features as users or items representations, which may not be enough to make precise representations due to the large amount of redundant information in reviews. To tackle the two problems mentioned above, we propose a deep learning-based method named Hierarchical Attention Network Oriented Towards Crowd Intelligence (HANCI). First, HANCI replaces the commonly-used topic models or CNN text processor with an RNN text processor in review feature extraction, which can fully exploit the advantages of the sequential dependencies of reviews by using the whole hidden layers of the bidirectional LSTM as outputs. Second, HANCI weighs the importance of features guided by crowd intelligence to more accurately represent each user on each item, and vice versa. Third, HANCI utilizes a hierarchical attention network based on multi-level review text analysis to extract more precise user preferences and item latent features, so that HANCI can explore the importance of words, the usefulness of reviews and the importance of features to achieve more accurate recommendation. Extensive experiments on three public datasets show that HANCI outperforms the state-of-the-art review-based recommendation algorithms in accuracy and meanwhile provides insightful explanations.},
	author = {Chao Yang and Weixin Zhou and Zhiyu Wang and Bin Jiang and Dongsheng Li and Huawei Shen},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106687},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Crowd intelligence, Explainable recommendation, Hierarchical attention, Review representation, Recommender system},
	pages = {106687},
	title = {Accurate and Explainable Recommendation via Hierarchical Attention Network Oriented Towards Crowd Intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120308169},
	volume = {213},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120308169},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106687}}

@article{BERNABEMORENO2020105236,
	abstract = {The latest development in cognitive technologies are helping us understand emotions and sentiments with unprecedented precision. Polarity detection is the key enabler to sentiment analysis and typically relies on experimental dictionaries, where terms are assigned polarity scores, yet lacking contextual information and based on human inputs and conventions. In this article, we present a novel approach to automatically extract a polarity dictionary from a particular domain, the stock market, without human intervention and addressing the scaling and thresholding problem. Our approach tracks the price changes of particular stocks over time, using it as a guiding polarity value. The magnitude of the price variation for a particular stock is then attributed to the financial news about this stock in corresponding period of time and that is what we use as our working corpus. On top of that, we derive the so-called binned corpus and apply the well-known TF--IDF information retrieval techniques to compute the TF--IDF value for each term. These values are then disseminated within the neighbourhood of each term based on the embeddings-enabled cosine distance. After introducing the problem and providing the background information, we thoroughly describe our method and all the components required to implement the system. Last but not least, we assign the terms to fuzzy linguistic labels and provide a volatility metric indicating how reliable our scores are depending on their distribution of occurrences in the corpus. To show how our approach works, we implement it for the Euro Stoxx 50 from January 2018 to March 2019 and discuss the results compared with typical approaches, pointing out potential improvements for further research work.},
	author = {J. Bernab{\'e}-Moreno and A. Tejeda-Lorente and J. Herce-Zelaya and C. Porcel and E. Herrera-Viedma},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105236},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Polarity extraction, Word embeddings, Information retrieval, Contextual bias, Fuzzy polarity},
	pages = {105236},
	title = {A context-aware embeddings supported method to extract a fuzzy sentiment polarity dictionary},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119305556},
	volume = {190},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119305556},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105236}}

@article{PRADHAN2020105784,
	abstract = {In academia, researchers collaborate with their peers to improve the quality of research and thereby enhance academic profiles. However, information overload in big scholarly data poses a challenge in identifying potential researchers for fruitful collaboration. In this article, we introduce a multi-level fusion-based model for collaborator recommendation, DRACoR (Deep learning and Random walk based Academic Collaborator Recommender). DRACoR fuses deep learning and biased random walk model to provide the recommendation for potential collaborators that share similar research interests at the peer level. We run a topic model on abstracts and Doc2Vec on titles on year-wise publications to capture the dynamic research interests of researchers. Author--author cosine similarity is computed from the feature vectors extracted from abstracts and titles and is then used to weigh edges in the author--author graph (AAG). We also aggregate various meta-path features with profile-aware features to bias the random walk behavior. Finally, we employ a random walk with restart(RWR) to recommend top N collaborators where the edge weights are used to bias the random walker's behavior. Extensive experiments on DBLP and hep-th datasets demonstrate the effectiveness of our proposed DRACoR model against various state-of-the-art methods in terms of precision, recall, F1-score, MRR, and nDCG.},
	author = {Tribikram Pradhan and Sukomal Pal},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105784},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Collaborator recommendation, Meta-path analysis, Random walk with restart (RWR), Topic modeling, Rank-based fusion, LSTM model},
	pages = {105784},
	title = {A multi-level fusion based decision support system for academic collaborator recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301817},
	volume = {197},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301817},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105784}}

@article{TANG2019426,
	abstract = {With the fast development of online social platforms, social emotion detection, focusing on predicting readers' emotions evoked by news articles, has been intensively investigated. Considering emotions as latent variables, various probabilistic graphical models have been proposed for emotion detection. However, the bag-of-words assumption prohibits those models from capturing the inter-relations between sentences in a document. Moreover, existing models can only detect emotions at either the document-level or the sentence-level. In this paper, we propose an effective Bayesian model, called hidden Topic--Emotion Transition model, by assuming that words in the same sentence share the same emotion and topic and modeling the emotions and topics in successive sentences as a Markov chain. By doing so, not only the document-level emotion but also the sentence-level emotion can be detected simultaneously. Experimental results on the two public corpora show that the proposed model outperforms state-of-the-art approaches on both document-level and sentence-level emotion detection.},
	author = {Donglei Tang and Zhikai Zhang and Yulan He and Chao Lin and Deyu Zhou},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.11.014},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social emotion detection, Sentiment analysis, Topic model, Hidden topic--emotion transition model},
	pages = {426-435},
	title = {Hidden topic--emotion transition model for multi-level social emotion detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118305586},
	volume = {164},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118305586},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.11.014}}

@article{ALP2019944,
	abstract = {Prevalent usage of social media attracted companies and researchers to analyze its dynamics and effects on user behavior. One of the most intriguing aspects of social networks is to identify influencers who are experts on a specific topic. With the identification of these users within the network, many applications can be built for user recommendation, information diffusion modeling, viral marketing, user modeling and many more. In this paper, we aim to identify topic-based experts using a large dataset collected from Twitter. Our proposed approach has three phases: (1) identification of topics on social media posts (more specifically, tweets), (2) user modeling, based on a group of user specific features, and (3) Influence Factorization to identify topical influencers. The main advantage of the proposed method is to identify future influencers as well as current ones on Twitter. Moreover, it is an easy to implement algorithm using Spark MLlib, which can be easily extended to include other user specific features, and compare with other methodologies. The effectiveness of the proposed method is tested on a large dataset that contains tweets of 180K user over 70 day period. The experimental results show that our proposed method identifies influencers successfully when used with a hybrid user specific feature that contains follower count and authenticity information, and is a highly scalable and extensible algorithm.},
	author = {Zeynep Zengin Alp and {\c S}ule G{\"u}nd{\"u}z {\"O}{\u g}{\"u}d{\"u}c{\"u}},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.10.020},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Data mining, Influence analysis, Social media analysis, Matrix Factorization, Collaborative filtering, Influence prediction, Influence maximization},
	pages = {944-954},
	title = {Influence Factorization for identifying authorities in Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118305069},
	volume = {163},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118305069},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.10.020}}

@article{HE201711,
	abstract = {Due to the popularity of social networks, such as microblogs and Twitter, a vast amount of short text data is created every day. Much recent research in short text becomes increasingly significant, such as topic inference for short text. Biterm topic model (BTM) benefits from the word co-occurrence patterns of the corpus, which makes it perform better than conventional topic models in uncovering latent semantic relevance for short text. However, BTM resorts to Gibbs sampling to infer topics, which is very time consuming, especially for large-scale datasets or when the number of topics is extremely large. It requires O(K) operations per sample for K topics, where K denotes the number of topics in the corpus. In this paper, we propose an acceleration algorithm of BTM, FastBTM, using an efficient sampling method for BTM, which converges much faster than BTM without degrading topic quality. FastBTM is based on Metropolis-Hastings and alias method, both of which have been widely adopted in Latent Dirichlet Allocation (LDA) model and achieved outstanding speedup. Our FastBTM can effectively reduce the sampling complexity of biterm topic model from O(K) to O(1) amortized time. We carry out a number of experiments on three datasets including two short text datasets, Tweets2011 Collection dataset and Yahoo! Answers dataset, and one long document dataset, Enron dataset. Our experimental results show that when the number of topics K increases, the gap in running time speed between FastBTM and BTM gets especially larger. In addition, our FastBTM is effective for both short text datasets and long document datasets.},
	author = {Xingwei He and Hua Xu and Jia Li and Liu He and Linlin Yu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.06.005},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {BTM, Topic model, Alias method, Metropolis-Hastings, Acceleration algorithm},
	pages = {11-20},
	title = {FastBTM: Reducing the sampling time for biterm topic model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117302782},
	volume = {132},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117302782},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.06.005}}

@article{MOHSIN2022107711,
	abstract = {Effective bug classification and assignment to relevant developers improves the efficiency of software management. However, textually dependent approaches produce inconsistent results on varying datasets, while approaches that depend upon multi-source data can produce dataset conflicts and inaccuracy. Accordingly, we introduce a model based on Self-Paced Association augmentation and Node embedding (SPAN), which uses an effective combination of textually dependent and independent bug categorization to produce consistent results, followed by a bug assignment mechanism to prevent conflicts. To this end, we present a novel unified classifier and assignment model that exploits the connections between nodes in the Software Bug Report Network (SBRNet) to identify the target features. The model is capable of accurately categorizing bugs in a self-paced manner with association augmentation. Finally, we present an approach that assigns the most appropriate developer for bug resolution through SBRNet node information embedding. Our deep two-step self-paced solution is capable of categorizing software bugs with improved accuracy, while still utilizing fewer features. Results reveal that our model is more effective (up to 98% classification accuracy and 96% for bug assignment) when compared to its counterparts.},
	author = {Hufsa Mohsin and Chongyang Shi and Shufeng Hao and He Jiang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107711},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Bug classification, Bug triage, Association augmentation, Graph embedding, Bug assignment, Bug report analysis},
	pages = {107711},
	title = {SPAN: A self-paced association augmentation and node embedding-based model for software bug classification and assignment},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121009606},
	volume = {236},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121009606},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107711}}

@article{WANG2020106433,
	abstract = {Social voting is an emerging new feature in online social platforms, through which users can express their attitudes and opinions towards various interested subjects. Since both social relations and textual content decide the votes propagation, the diverse sources present opportunities and challenges for recommender systems. In this paper, we jointly consider these two factors for the online voting recommendation. First, we conduct feature learning on the vote content. Note that the vote questions are usually short and contain informal expressions, existing text mining methods cannot handle it well. We propose a novel topic-enhanced word embedding (TEWE) method, which learns the word vectors by considering both token-level semantics and document-level mixture topics. Second, we propose two Joint Topic-Semantic-aware Social Matrix Factorization (JTS-MF) models, which fuse social relations and textual content for the vote recommendation. Specifically, JTS-MF1 directly identifies the interaction strength to calculate the similarity among users and votes, while JTS-MF2 aims to preserve inter-user and inter-vote similarities during matrix factorization. Extensive experimental results on real online voting dataset show the effectiveness of our approaches against several state-of-the-art baselines. JTS-MF1 and JTS-MF2 models surpass the matrix factorization based method, with 25.4% and 57.1% improvements in the top-1 recall, and 59.12% and 25.1% improvements in the top-10 recall.},
	author = {Jia Wang and Hongwei Wang and Miao Zhao and Jiannong Cao and Zhuo Li and Minyi Guo},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106433},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Online voting, Recommender systems, Topic-enhanced word embedding, Matrix factorization},
	pages = {106433},
	title = {Joint Topic-Semantic-aware Social Matrix Factorization for online voting recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305621},
	volume = {210},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305621},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106433}}

@article{YAN2018149,
	abstract = {This paper studies response selection for human-computer conversation systems. Existing retrieval-based human-computer conversation systems are intended to reply to user utterances based on existing utterance-response pairs. However, collecting sufficient utterance-response pairs is intractable in practical situations, especially for many specific domains. We introduce DocChat a novel information retrieval approach for human-computer conversation systems that can use unstructured documents rather than semi-structured utterance-response pairs, to react to user utterances. The key of DocChat is a learning to rank model with features designed at various levels of granularity which is proposed to quantify the relevance between utterances and responses directly. We conduct comprehensive experiments on both sentence selection and real human-computer conversation scenarios. Empirical studies of sentence selection datasets shows reasonable improvements and the strong adaptability of our model. We compare DocChat with Xiaoice, a famous open domain chitchat engine in China. Side-by-side evaluation shows that DocChat is a good complement for human-computer conversation systems using utterance-response pairs as the primary source of responses. Furthermore, we release a large scale open-domain dataset for sentence selection which contains 304,413 query-sentence pairs.},
	author = {Zhao Yan and Nan Duan and Junwei Bao and Peng Chen and Ming Zhou and Zhoujun Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.11.033},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Human-computer conversation system, Human-robot interaction, Text matching, Natural language processing, Artificial intelligence},
	pages = {149-159},
	title = {Response selection from unstructured documents for human-computer conversation systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117305646},
	volume = {142},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117305646},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.11.033}}

@article{XU2021106858,
	abstract = {In this paper, we study abstractive summarization for product reviews in the recommender systems, which aims to generate condensed text for online reviews. The summary generation is not only relevant with the content of the review itself but should be fully aware of the intrinsic features of the corresponding user and product, i.e., personalization, which are helpful to identify the saliency information in the reviews. Therefore, we propose a Rating-boosted Abstractive Review Summarization with personalized generation (RARS). In our approach, we first propose a neural review-level attention model to effectively learn user preference embedding and product characteristic embedding from their history reviews. Then, we design a personalized decoder to generate the personalized summary, which utilizes the representations of the user and the product to calculate saliency scores for words in the input review to guide the summary generation process. In addition, the rating information can explicitly indicate the sentiment opinion, hence we jointly optimize the summary generation and rating prediction through a multi-task framework, where the two tasks inherently share user preference embedding and product characteristics embedding. Extensive experiments on four datasets show that our model can effectively improve the performance of both review summarization and rating prediction.},
	author = {Hongyan Xu and Hongtao Liu and Wang Zhang and Pengfei Jiao and Wenjun Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106858},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Summary generation, Rating prediction, Recommender system},
	pages = {106858},
	title = {Rating-boosted abstractive review summarization with neural personalized generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001210},
	volume = {218},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001210},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106858}}

@article{KHODABAKHSH20181,
	abstract = {The wide adoption of social networking and microblogging platforms by a large number of users across the globe has provided a rich source of unstructured information for understanding users' behaviors, interests and opinions at both micro and macro levels. An active area in this space is the detection of important real-world events from user-generated social content. The works in this area identify instances of events that impact a large number of users. However, a more nuanced form of an event, known as life event, is also of high importance, which in contrast to real-world events, does not impact a large number of users and is limited to at most a few people. For this reason, life events, such as marriage, travel, and career change, among others, are more difficult to detect for several reasons: i) they are specific to a given user and do not have a wider reaching reflection; ii) they are often not reported directly and need to be inferred from the content posted by individual users; and iii) many users do not report their life events on social platforms, making the problem highly class-imbalanced. In this paper, we propose a semantic approach based on word embedding techniques to model life events. We then use word mover's distance to measure the similarity of a given tweet to different types of life events, which are used as input features for a multi-class classifier. Furthermore, we show that when a sequence of tweets that have appeared before and after a given tweet of interest (temporal stacking) are considered, the performance of the life event detection task improves significantly.},
	author = {Maryam Khodabakhsh and Mohsen Kahani and Ebrahim Bagheri and Zeinab Noorian},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.021},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social web, Social networks, Event detection, Personal life events},
	pages = {1-16},
	title = {Detecting life events from twitter based on temporal semantic features},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511830073X},
	volume = {148},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511830073X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.021}}

@article{LENG2020105600,
	abstract = {Deep neural network needs large amount of data for training, to obtain more data, many simple data augmentation algorithms have been proposed. In this paper, we propose a LDA-based data augmentation algorithm to extend the training set. The proposed LDA-based data augmentation algorithm uses the topic model LDA to detect the key audio words in the recordings, and further to detect the key audio events and non-key audio events for each recording; with the detected key-audio-event segments, for each acoustic scene class, the probability distribution of key-audio-event's occurrence numbers, the probability distribution of key-audio-event's locations under each occurrence number and the probability distribution of key-audio-event's durations under each occurrence number is counted, and then the new recordings are generated according to these probability distributions. Experiments are done on the public TUT acoustic scenes 2016 dataset, and the experimental results show that compared with the other simple data augmentation algorithms, the proposed LDA-based data augmentation algorithm is more stable and effective, it can get better generalization ability for different kinds of neural network on different datasets.},
	author = {Yan Leng and Weiwei Zhao and Chan Lin and Chengli Sun and Rongyan Wang and Qi Yuan and Dengwang Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105600},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Acoustic scene classification, Topic model, LDA, Key audio event, Non-key audio event},
	pages = {105600},
	title = {LDA-based data augmentation algorithm for acoustic scene classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120300733},
	volume = {195},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120300733},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105600}}

@article{HOANG2019104788,
	abstract = {Treatment directly affects patient health status. In recent years, the fast development electronic medical records (EMRs) has provided valuable resources for solving healthcare issues, especially learning and recommending treatments. However, most of the related studies are limited in exploiting various patient information, handling varying-length treatment records, capturing different kinds of treatment patterns and interpreting the recommendation mechanism. This research proposes three methods, one for learning and two for recommending treatments, to overcome the above drawbacks. All methods adopt a mixed-variate restrict Boltzmann machine to represent different kinds of patient records. The treatment learning method captures significant changes in prescription indication to split varying-length records flexibly and organizes sequences of prescription drugs into regimen trees to reveal many more different kinds of treatment patterns. The two treatment recommendation methods illustrate different ideas that can improve the treatment recommendation mechanism by combining the treatments derived from patient groups and neighbor patients. Our experimental evaluation was conducted on three acute disease cohorts extracted from the MIMIC III database. The obtained results show that the proposed methods are able to provide different kinds of treatment patterns and yield competitive efficacy with better interpretability as compared to relevant studies.},
	author = {Khanh Hung Hoang and Tu Bao Ho},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.05.031},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Healthcare mining, Treatment patterns, Treatment recommendation, Electronic medical records, Treatment regimen},
	pages = {104788},
	title = {Learning and recommending treatments using electronic medical records},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302436},
	volume = {181},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302436},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.05.031}}

@article{DU2021107247,
	abstract = {The prediction of event propagation has received extensive attention from the knowledge discovery community for applications such as social network analysis. The data describing these phenomena are multidimensional asynchronous event data that affect each other and show complex dynamic patterns in the continuous-time domain. The study of these dynamic processes and the mining of their potential correlations provide a foundation for the application of event propagation forecasting. However, conventional forecasting methods often make strong assumptions about the generative processes of the event data that may or may not reflect the reality, and the strong parametric assumptions also restrict the expressive power of the respective processes. Therefore, it is difficult to capture both the temporal and spatial effects of past event sequences. Most of the existing methods capture the intensity function of the Hawkes processes conditioned only on the historical events while ignoring the spatial information and the influences among different events. In this work, we propose the Astrologer, a graph neural Hawkes process that can capture the propagation of events from historical events on graph. The underlying idea of Astrologer is to incorporate the conditional intensity function of the Hawkes processes with a graph convolutional recurrent neural network. Using both synthetic and real-world datasets, we show that, Astrologer can learn the dynamics of event propagation without knowing the actual parametric forms. Astrologer can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.},
	author = {Haizhou Du and Yan Zhou and Yunpu Ma and Shiwei Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107247},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Hawkes process, Graph neural network, Forecasting, Spatio-temporal events},
	pages = {107247},
	title = {Astrologer: Exploiting graph neural Hawkes process for event propagation prediction with spatio-temporal characteristics},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121005098},
	volume = {228},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121005098},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107247}}

@article{DESSI2022109945,
	abstract = {Science communication has a number of bottlenecks that include the rising number of published research papers and its non-machine-accessible and document-based paradigm, which makes the exploration, reading, and reuse of research outcomes rather inefficient. Recently, Knowledge Graphs (KG), i.e., semantic interlinked networks of entities, have been proposed as a new core technology to describe and curate scholarly information with the goal to make it machine readable and understandable. However, the main drawback of the use of such a technology is that researchers are asked to manually annotate their research papers and add their contributions within the KGs. To address this problem, in this paper we propose SCICERO, a novel KG generation approach that takes in input text from research articles and generates a KG of research entities. SCICERO uses Natural Language Processing techniques to parse the content of scientific papers to discover entities and relationships, exploits state-of-the-art Deep Learning Transformer models to make sense and validate extracted information, and uses Semantic Web best practices to formally represent the extracted entities and relationships, making the written content of research papers machine-actionable. SCICERO has been tested on a dataset of 6.7M papers about Computer Science generating a KG of about 10M entities. It has been evaluated on a manually generated gold standard of 3,600 triples that cover three Computer Science subdomains (Information Retrieval, Natural Language Processing, and Machine Learning) obtaining remarkable results.},
	author = {Danilo Dess{\'\i} and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109945},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Knowledge graph, Scholarly domain, Scientific facts, Artificial intelligence},
	pages = {109945},
	title = {SCICERO: A deep learning and NLP approach for generating scientific knowledge graphs in the computer science domain},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
	volume = {258},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122010383},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109945}}

@article{GAO2020105418,
	abstract = {Network embedding aims to map vertices in a complex network into a continuous low-dimensional vector space. Meanwhile, the original network structure and inherent properties must be preserved. Most of the existing methods merely focus on preserving local structural features of vertices, whereas they largely ignore the community patterns and rich attribute information. For example, the title of papers in an academic citation network could imply their research directions, which are potentially valuable in seeking more meaningful representations of these papers. In this paper, we propose a Community-oriented Attributed Network Embedding (COANE) framework, which can smoothly incorporate the community information and text contents of vertices into network embedding. We design a margin-based random walk procedure on the network coupled with flexible margins among communities, which limit the scope of random walks. Inspired by the analogy between vertex sequences and documents, the statistical topic model is adopted to extract community features in the network. Furthermore, COANE integrates textual semantics into representations through the topic model while preserving their structural correlations. Experiments on real-world networks indicate that our proposed method outperforms six state-of-the-art network embedding approaches on network visualization, vertex classification and link prediction.},
	author = {Yuan Gao and Maoguo Gong and Yu Xie and Hua Zhong},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105418},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Representation learning, Attributed network embedding (ANE), Community detection, Topic model},
	pages = {105418},
	title = {Community-oriented attributed network embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306513},
	volume = {193},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306513},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105418}}

@article{JEONG2021106659,
	abstract = {In the real world, group recommendation that recommends items to a set of users (i.e., a group) is a challenging problem since it is very difficult to ensure the satisfaction of all group members with different preferences. Many existing group recommendation systems commonly use aggregation methods, which are insufficient to model group behavior where preference of a user as an individual is often changed when he/she is a member of a group. Some recent methods attempt to reflect this dynamic group behavior. However, they still have limitations in capturing complex relationships between items and group members. Moreover, previous approaches do not fully utilize useful context information available, and only use rating data. Reflecting context information together with ratings helps resolve a well-known data sparsity problem in group recommendation. In this paper, we propose a novel group recommendation framework called Dynamic Group behavior modeling that utilizes Context information for group recommendation(DGC). In DGC, we newly develop dynamic group behavior modeling that enables summarization of complex patterns in group decision making processes. To apply context information, firstly, we extract relevant context information from a heterogeneous information network (HIN) that contains rich information between various entities. Then, context information is properly applied to a group recommendation model by using semi-supervised learning that is composed of a supervised loss for label prediction and an unsupervised loss for context prediction. Experimental results show that our method provides significant performance improvement over other existing methods.},
	author = {Hyun Ji Jeong and Kwang Hee Lee and Myoung Ho Kim},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106659},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	pages = {106659},
	title = {DGC: Dynamic group behavior modeling that utilizes context information for group recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307887},
	volume = {213},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120307887},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106659}}

@article{PARK2020104825,
	abstract = {An essential challenge in aspect term sentiment classification using deep learning is modeling a tailor-made sentence representation towards given aspect terms to enhance the classification performance. To seek a solution to this, we have two main research questions: (1) Which factors are vital for a sentiment classifier? (2) How will these factors interact with dataset characteristics? Regarding the first question, harmonious combination of location attention and content attention may be crucial to alleviate semantic mismatch problem between aspect terms and opinion words. However, location attention does not reflect the fact that critical opinion words usually come left or right of corresponding aspect terms, as implied in the target-dependent method although not well elucidated before. Besides, content attention needs to be sophisticated to combine multiple attention outcomes nonlinearly and consider the entire context to address complicated sentences. We merge all these significant factors for the first time, and design two models differing a little in the implementation of a few factors. Concerning the second question, we suggest a new multifaceted view on the dataset beyond the current tendency to be somewhat indifferent to the dataset in pursuit of a universal best performer. We then observe the interaction between factors of model architecture and dimensions of dataset characteristics. Experimental results show that our models achieve state-of-the-art or comparable performances and that there exist some useful relationships such as superior performance of bi-directional LSTM over one-directional LSTM for sentences containing multiple aspects and vice versa for sentences containing only one aspect.},
	author = {Hyun-jung Park and Minchae Song and Kyung-Shik Shin},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.06.033},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Aspect-based sentiment analysis, Sentiment classification, Deep learning, LSTM, GRU, Attention},
	pages = {104825},
	title = {Deep learning models and datasets for aspect term sentiment classification: Implementing holistic recurrent attention on target-dependent memories},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119303004},
	volume = {187},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119303004},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.06.033}}

@article{PENG2022109933,
	abstract = {In Community Question Answering (CQA) websites, expert finding aims to seek relevant experts for answering questions. The core of expert finding is to match candidate experts and target questions precisely. Most existing methods usually learn a single feature vector for the expert from the historically answered questions, and then match the target question, which would lose fine-grained and low-level semantic matching information. In this paper, instead of matching with a unified expert embedding, we propose an expert finding method with a multi-grained hierarchical matching framework, named EFHM. Specifically, we design a word-level and question-level match encoder to learn the fine-grained semantic matching between each historical answered question and target question, and then propose an expert-level match encoder to learn an overall expert feature for matching the target question. Through the hierarchical matching mechanism, our model has the potential to capture the comprehensive relevance between candidate experts and target questions. Experimental results on six real-world CQA datasets demonstrate that the proposed method could achieve better performance than existing state-of-the-art methods.},
	author = {Qiyao Peng and Wenjun Wang and Hongtao Liu and Yinghui Wang and Hongyan Xu and Minglai Shao},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109933},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Expert finding, Hierarchical matching, Personalized, Community question answering},
	pages = {109933},
	title = {Towards comprehensive expert finding with a hierarchical matching network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122010267},
	volume = {257},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122010267},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109933}}

@article{CHEN20171,
	abstract = {The popularity of online forums provides a good opportunity to learn user interests which can be used in many business scenarios, such as product or news recommendation. There exist many approaches to infer forum topics and users' interests. Among them, Author-Topic (AT) like models are most popular. But a thread in online forum is composed of a root post and some response posts which may be relevant or irrelevant to the root post. So the assumption of AT that response posts are generated from user's interest topics is not comprehensive. In this paper, we distinguish user's serious and unserious interest topics and argue that the topic of a relevant response post is jointly determined by its author's serious interest topics and the topics of its root post, while the topic of irrelevant response post is only determined by its author's unserious interest topics. Based on these assumptions, we propose Forum-LDA to model the generative process of root post, relevant and irrelevant response posts jointly. Therefore, our model can not only learn more coherent topics and serious interests, but also identify unserious users who publish many irrelevant posts. Extensive experiments on real forum dataset demonstrate the advantages of our model in tasks such as user interest and unserious user discovery.},
	author = {Chaotao Chen and Jiangtao Ren},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.04.006},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {User interest, Topic model, Forum content analysis},
	pages = {1-7},
	title = {Forum latent Dirichlet allocation for user interest discovery},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117301727},
	volume = {126},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117301727},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.04.006}}

@article{VARSHNEY201766,
	abstract = {In past few years, social networking has significantly contributed to online presence of users. These social networks are hosts to a number of viral phenomena. This has fetched a lot of attention from various researchers and marketers all over the world. Major portion of the studies done in the field of information diffusion through social networks has focused on the problem of influence maximization. These methods demand the diffusion probabilities associated with the links in the social networks to be provided as inputs. However, the problem of computing these diffusion probabilities has not been as widely explored as the problem of influence maximization. In this paper, we tackle the problem of predicting the probabilities of diffusion of a message through the links of a social network. This paper presents a Bayesian network based approach for solving the aforesaid problem. In addition to the features related to the social network, this machine learning based Bayesian framework utilizes user interests and content similarity modeled using the latent topic information. We evaluate the proposed method using the data obtained from the well-known social network platform - Twitter.},
	author = {Devesh Varshney and Sandeep Kumar and Vineet Gupta},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.07.003},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social network analysis, Information diffusion, Diffusion network, Bayesian network modeling, Diffusion probability},
	pages = {66-76},
	title = {Predicting information diffusion probabilities in social networks: A Bayesian networks based approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117303180},
	volume = {133},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117303180},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.07.003}}

@article{ZHANG2019194,
	abstract = {Document representation plays an important role in the fields of text mining, natural language processing, and information retrieval. Traditional approaches to document representation may suffer from the disregard of the correlations or order of words in a document, due to unrealistic assumption of word independence or exchangeability. Recently, long--short-term memory (LSTM) based recurrent neural networks have been shown effective in preserving local contextual sequential patterns of words in a document, but using the LSTM model alone may not be adequate to capture global topical semantics for learning document representation. In this work, we propose a new topic-enhanced LSTM model to deal with the document representation problem. We first employ an attention-based LSTM model to generate hidden representation of word sequence in a given document. Then, we introduce a latent topic modeling layer with similarity constraint on the local hidden representation, and build a tree-structured LSTM on top of the topic layer for generating semantic representation of the document. We evaluate our model in typical text mining applications, i.e., document classification, topic detection, information retrieval, and document clustering. Experimental results on real-world datasets show the benefit of our innovations over state-of-the-art baseline methods.},
	author = {Wenyue Zhang and Yang Li and Suge Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.03.007},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Document representation, Deep learning, Long--short term memory, Topic modeling},
	pages = {194-204},
	title = {Learning document representation via topic-enhanced LSTM model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119301182},
	volume = {174},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119301182},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.03.007}}

@article{PESSUTTO2020105339,
	abstract = {In the last few years, there has been growing interest in aspect-based sentiment analysis, which deals with extracting, clustering, and rating the overall opinion about the features of the entity being evaluated. Techniques for aspect extraction can produce an undesirably large number of aspects --- with many of those relating to the same product feature. Hence, aspect clustering becomes necessary. Current solutions for aspect clustering are monolingual, but in many practical situations, reviews for a given entity are available in several languages, calling for multilingual integration. In this article, we address the novel task of multilingual aspect clustering, which aims at grouping semantically related aspects extracted from reviews written in several languages. Our method is unsupervised and relies on the contextual information of the aspects, which is represented by word embeddings. This representation allied with a suitable similarity measure allows clustering related aspects. Our experiments on two datasets with five languages each showed that our unsupervised clustering technique achieves results that outperform monolingual baselines adapted to work with multilingual data. We also show the benefits of the multilingual approach compared to using languages in isolation.},
	author = {Lucas Rafael Costella Pessutto and Danny Suarez Vargas and Viviane P. Moreira},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105339},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Aspect-based sentiment analysis, Multilingual aspect clustering, Unsupervised learning, Word embeddings},
	pages = {105339},
	title = {Multilingual aspect clustering for sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306070},
	volume = {192},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306070},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105339}}

@article{DAI2022107659,
	abstract = {Text classification is an important and classical problem in natural language processing. Recently, Graph Neural Networks (GNNs) have been widely applied in text classification and achieved outstanding performance. Despite the success of GNNs on text classification, existing methods are still limited in two main aspects. On the one hand, transductive methods cannot easily adapt to new documents. Since transductive methods incorporate all documents into their text graph, they need to reconstruct the whole graph and retrain their system from scratch when new documents come. However, this is not applicable to real-world situations. On the other hand, many state-of-the-art algorithms ignore the quality of text graphs, which may lead to sub-optimal performance. To address these problems, we propose a Graph Fusion Network (GFN), which can overcome these limitations and boost text classification performance. In detail, in the graph construction stage, we build homogeneous text graphs with word nodes, which makes the learning system capable of making inference on new documents without rebuilding the whole text graph. Then, we propose to transform external knowledge into structural information and integrate different views of text graphs to capture more structural information. In the graph reasoning stage, we divide the process into three steps: graph learning, graph convolution, and graph fusion. In the graph learning step, we adopt a graph learning layer to further adapt text graphs. In the graph fusion step, we design a multi-head fusion module to integrate different opinions. Experimental results on five benchmarks demonstrate the superiority of our proposed method.},
	author = {Yong Dai and Linjun Shou and Ming Gong and Xiaolin Xia and Zhao Kang and Zenglin Xu and Daxin Jiang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107659},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Graph Neural Networks, Text classification, External knowledge, Graph fusion},
	pages = {107659},
	title = {Graph Fusion Network for Text Classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121009217},
	volume = {236},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121009217},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107659}}

@article{HU2017105,
	abstract = {Event detection aims to discover news documents that report on the same event and arrange them under the same group. With the explosive growth of online news, there is a need for event detection to facilitate better navigation for users in news spaces. Existing works usually represent documents based on TF-IDF scheme and use a clustering algorithm for event detection. However, traditional TF-IDF vector representation suffers problems of high dimension and sparse semantics. In addition, with more news documents coming, IDF need to be incrementally updated. In this paper, we present a novel document representation method based on word embeddings, which reduces the dimension and alleviates the sparse semantics compared to TF-IDF, and thus improves the efficiency and accuracy. Based on the document representation, we propose an adaptive online clustering method for online news event detection, which improves both the precision and recall by using time slicing and event merging respectively. The resulted events are further improved by an adaptive post-processing step which can automatically detect noisy events and further process them. Experiments on standard and real-world datasets show that our proposed adaptive online event detection method significantly improves the performance of event detection in terms of both efficiency and accuracy compared to state-of-the-art methods.},
	author = {Linmei Hu and Bin Zhang and Lei Hou and Juanzi Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.09.039},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Word embedding, Adaptive online clustering, Event detection},
	pages = {105-112},
	title = {Adaptive online event detection in news streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117304550},
	volume = {138},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117304550},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.09.039}}

@article{CHEN2021107521,
	abstract = {Travel recommendation is very critical to helping users quickly find products or services that they are interested in. The key to travel recommender systems is learning user shopping intentions, which are expressed through various supervision signals, such as the clicked products and their titles. Existing travel recommendation methods commonly infer user intentions from click behaviors on travel products. However, remarkable keywords in the product title, such as departure, destination, travel time, hotel, and transportation are paid less attention. To this end, we hypothesize that modeling click sequences and product keywords in title jointly would result in a more holistic representation of a product and towards more accurate recommendations. Thus, we propose a TRKG (short for Travel Recommendation with Keywords Generation) model, which fulfills the travel recommendation and keywords generation tasks simultaneously. To generate explainable outputs, unlike most previous approaches that regard the product title as a hidden feature vector, TRKG regards keywords in the product title as an additional supervision signal. Meanwhile, TRKG integrates the long-term and short-term user preferences in the travel recommendation component and the keywords generation component. To evaluate the proposed model, we constructed datasets from a large tourism e-commerce website in China. Extensive experiments demonstrate that the proposed method yields significant improvements over state-of-the-art methods.},
	author = {Lei Chen and Jie Cao and Guixiang Zhu and Youquan Wang and Weichao Liang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107521},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommendation system, Travel recommendation, Keywords generation, Deep learning, Multi-task learning},
	pages = {107521},
	title = {A multi-task learning approach for improving travel recommendation with keywords generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121007838},
	volume = {233},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121007838},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107521}}

@article{WANG201768,
	abstract = {Named entity disambiguation (NED) refers to the task of mapping entity mentions in running texts to the correct entries in a specific knowledge base (e.g., Wikipedia). Although there has been a lot of work on NED for long and formal texts like Wikipedia and news, the task is not well studied for questions in community question answering (CQA). The challenges of the task include little context for mentions in questions, lack of ground truth for learning, and language gaps between CQA and knowledge bases. To overcome these problems, we propose a topic modelling approach to NED for questions. Our model performs learning in an unsupervised manner, but can take advantage of weak supervision signals estimated from the metadata of CQA and knowledge bases. The signals can enrich the context of mentions in questions, and bridge the language gaps between CQA and knowledge bases. Besides these advantages, our model simulates people's behavior in CQA and thus is intuitively interpretable. We conduct experiments on both Chinese and English CQA data. The experimental results show that our method can significantly outperform state-of-the-art methods when we apply them to questions in CQA.},
	author = {Fang Wang and Wei Wu and Zhoujun Li and Ming Zhou},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.03.017},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Named entity disambiguation, Topic model, Unsupervised learning, Community question answering},
	pages = {68-77},
	title = {Named entity disambiguation for questions in community question answering},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117301442},
	volume = {126},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117301442},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.03.017}}

@article{LIU2021106660,
	abstract = {Matrix factorization-based collaborative filtering, learning user and item latent features, has been one of the powerful recommendation techniques. Due to its simply modeling of user--item interactions by inner product of two vectors as a linear model, its efficiency needs an improvement. Neural Network-based matrix factorization has been proposed to deal with this issues. Usually, these methods are proposed on clean data, but in real applications, there are possibly unexpected noises and outliers, due to many subjective or objective reasons. The noisy instances would disturb the learning of normal instances and thus cause adverse affect as the model would also be easily over-fitted. Thus, we propose an enhanced neural matrix factorization model by introducing a self-paced learning (SPL) schema, which can automatically distinguish noisy instances and learn the model mostly based on clean instances. The main contribution of our model is that we design a bounded SPL learning schema with a parameter to control how many instances will be finally induced in the model learning. Thus, different from traditional SPL that gradually selects instances until all are selected, the bounded SPL mechanism tries to learn the model mainly on clean data and exclude noisy instances. The effectiveness of proposed method on collaborative filtering is demonstrated by extensive experiments on three widely used datasets.},
	author = {Zhen Liu and Xiaodong Feng and Yecheng Wang and Wenbo Zuo},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106660},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommendation, Deep learning, Noisy and outlier corruption, Instance weighting, Self-paced learning},
	pages = {106660},
	title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307899},
	volume = {213},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120307899},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106660}}

@article{LI2021106846,
	abstract = {Automatic keyphrase extraction algorithms aim to identify words and phrases that contain the core information in documents. As online scholarly resources have become widespread in recent years, better keyphrase extraction techniques are required to improve search efficiency. We present two features, keyphrase semantic diversity and keyphrase coverage, to overcome limitations of existing methods for unsupervised keyphrase extraction. Keyphrase semantic diversity is the degree of semantic variety in the extraction result, which is introduced to avoid extracting synonym phrases that contain the same high-score candidate. Keyphrase coverage refers to candidates' representativeness of other words in documents. We propose an unsupervised keyphrase extraction method called TripleRank, which evaluates three features: word position (a sensitive feature for academic documents) and two innovative features mentioned above. The architecture of TripleRank includes three sub-models that score the three features and a summing model. Though involving multiple models, there is no typical iteration process in TripleRank; hence, the computational cost is relatively low. TripleRank has led the experiment results on four academic datasets compared to four state-of-the-art baseline models, which confirmed the influence of keyphrase semantic diversity and keyphrase coverage and proved the efficiency of our method.},
	author = {Tuohang Li and Liang Hu and Hongtu Li and Chengyu Sun and Shuai Li and Ling Chi},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106846},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Keyphrase extraction, Keyphrase semantic diversity, Keyphrase coverage, Unsupervised approach},
	pages = {106846},
	title = {TripleRank: An unsupervised keyphrase extraction algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512100109X},
	volume = {219},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512100109X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106846}}

@article{ZHOU2020105695,
	abstract = {Due to the prevalence of social media sites, users are allowed to conveniently share their ideas and activities anytime and anywhere. Therefore, these sites hold substantial real-world event related data. Different from traditional social event detection methods which mainly focus on single-media, multi-modal social event detection aims at discovering events in vast heterogeneous data such as texts, images and video clips. These data denote real-world events from multiple dimensions simultaneously so that they can provide comprehensive and complementary understanding of social event. In recent years, multi-modal social event detection has attracted intensive attentions. This paper concentrates on conducting a comprehensive survey of extant works. Two current attempts in this field are firstly reviewed: event feature learning and event inference. Particularly, event feature learning is a pre-requisite because of its ability on translating social media data into computer-friendly numerical form. Event inference aims at deciding whether a sample belongs to a social event. Then, several public datasets in the community are introduced and the comparison results are also provided. At the end of this paper, a general discussion of the insights is delivered to promote the development of multi-modal social event detection.},
	author = {Han Zhou and Hongpeng Yin and Hengyi Zheng and Yanxia Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105695},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multi-modality, Social event detection, Feature learning, Event inference},
	pages = {105695},
	title = {A survey on multi-modal social event detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301271},
	volume = {195},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301271},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105695}}

@article{LI2018203,
	abstract = {Online travel has developed dramatically during the past three years in China. This results in a large amount of unstructured data like tourism reviews from which it is hard to extract useful knowledge. In this paper, a DWWP system consisting of domain-specific new words detection (DW) and word propagation (WP) is presented. DW deals with the negligence of user-invented new words and converted sentiment words by means of AMI (Assembled Mutual Information). Inspired by social networks, the new method WP incorporates manually calibrated sentiment scores, semantic and statistical similarity information, which improves the quality of sentiment lexicon in comparison with existing data-driven methods. Experimental results show that DWWP improves seventeen percentage points compared with graph propagation and four percentage points compared with label propagation in terms of accuracy on Dataset I and Dataset II, respectively.},
	author = {Wei Li and Kun Guo and Yong Shi and Luyao Zhu and Yuanchun Zheng},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.004},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Online travel, DWWP, Chinese new words detection, Sentiment lexicon, Word propagation},
	pages = {203-214},
	title = {DWWP: Domain-specific new words detection and word propagation system for sentiment analysis in the tourism domain},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300558},
	volume = {146},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300558},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.004}}

@article{WU201866,
	abstract = {Aspect term extraction (ATE) and opinion target extraction (OTE) are two important tasks in fine-grained sentiment analysis field. Existing approaches to ATE and OTE are mainly based on rules or machine learning methods. Rule-based methods are usually unsupervised, but they can't make use of high level features. Although supervised learning approaches usually outperform the rule-based ones, they need a large number of labeled samples to train their models, which are expensive and time-consuming to annotate. In this paper, we propose a hybrid unsupervised method which can combine rules and machine learning methods to address ATE and OTE tasks. First, we use chunk-level linguistic rules to extract nominal phrase chunks and regard them as candidate opinion targets and aspects. Then we propose to filter irrelevant candidates based on domain correlation. Finally, we use these texts with extracted chunks as pseudo labeled data to train a deep gated recurrent unit (GRU) network for aspect term extraction and opinion target extraction. The experiments on benchmark datasets validate the effectiveness of our approach in extracting opinion targets and aspects with minimal manual annotation.},
	author = {Chuhan Wu and Fangzhao Wu and Sixing Wu and Zhigang Yuan and Yongfeng Huang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.01.019},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Aspect extraction, Opinion target extraction, Unsupervised learning, GRU},
	pages = {66-73},
	title = {A hybrid unsupervised method for aspect term and opinion target extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300273},
	volume = {148},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300273},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.01.019}}

@article{CHEN20191,
	abstract = {Learning topics from short texts has become a critical and fundamental task for understanding the widely-spread streaming social messages, e.g., tweets, snippets and questions/answers. Up to date, there are two distinctive topic learning schemes: generative probabilistic graphical models and geometrically linear algebra approaches, with LDA and NMF being the representative works, respectively. Since these two methods both could uncover the latent topics hidden in the unstructured short texts, some interesting doubts are coming to our minds that which one is better and why? Are there any other more effective extensions? In order to explore valuable insights between LDA and NMF based learning schemes, we comprehensively conduct a series of experiments into two parts. Specifically, the basic LDA and NMF are compared with different experimental settings on several public short text datasets in the first part which would exhibit that NMF tends to perform better than LDA; in the second part, we propose a novel model called ``Knowledge-guided Non-negative Matrix Factorization for Better Short Text Topic Mining'' (abbreviated as KGNMF), which leverages external knowledge as a semantic regulator with low-rank formalizations, yielding up a time-efficient algorithm. Extensive experiments are conducted on three representative corpora with currently typical short text topic models to demonstrate the effectiveness of our proposed KGNMF. Overall, learning with NMF-based schemes is another effective manner in short text topic mining in addition to the popular LDA-based paradigms.},
	author = {Yong Chen and Hui Zhang and Rui Liu and Zhiwen Ye and Jianying Lin},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.08.011},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Short text mining, Topic modeling, Latent dirichlet allocation (LDA), Non-negative matrix factorization (NMF), Knowledge-based learning},
	pages = {1-13},
	title = {Experimental explorations on short text topic mining between LDA and NMF based Schemes},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118304076},
	volume = {163},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118304076},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.08.011}}

@article{WANG2017153,
	abstract = {Topic models, such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA), have shown impressive success in many fields. Recently, multi-view learning via probabilistic latent semantic analysis (MVPLSA), is also designed for multi-view topic modeling. These approaches are instances of generative model, whereas they all ignore the manifold structure of data distribution, which is generally useful for preserving the nonlinear information. In this paper, we propose a novel multiple graph regularized generative model to exploit the manifold structure in multiple views. Specifically, we construct a nearest neighbor graph for each view to encode its corresponding manifold information. A multiple graph ensemble regularization framework is proposed to learn the optimal intrinsic manifold. Then, the manifold regularization term is incorporated into a multi-view topic model, resulting in a unified objective function. The solutions are derived based on the Expectation Maximization optimization framework. Experimental results on real-world multi-view data sets demonstrate the effectiveness of our approach.},
	author = {Shaokai Wang and Eric Ke Wang and Xutao Li and Yunming Ye and Raymond Y.K. Lau and Xiaolin Du},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.01.022},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multi-view learning, Generative model, Manifold learning},
	pages = {153-162},
	title = {Multi-view learning via multiple graph regularized generative model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117300345},
	volume = {121},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117300345},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.01.022}}

@article{AN2022107623,
	abstract = {The growing popularity of location-based social networks gives rise to a tremendous amount of social check-ins data, which are broadly used in previous studies to produce dense venue representations for various trajectory mining tasks. In this work, we focus on the interpretability of venue representations, an essential property that existing methods fail to provide. We propose two novel models to generate interpretable and easy-to-understand venue representations. The first model, CEM, is a category-aware (a category may be a restaurant, a mall, etc.) check-in embedding model and generates venue and category representations by capturing the sequential patterns of check-in records. With the second model, XEM, each dimension of the venue representation corresponds to a semantic anchor (i.e., a category) and can be interpreted as a coherent topic. We conduct extensive experiments using real-world check-in datasets for venue similarity computation and venue semantic annotation, and empirically show that introducing interpretability to the venue representations improves the performance of various downstream tasks.},
	author = {Ning An and Meng Chen and Li Lian and Peng Li and Kai Zhang and Xiaohui Yu and Yilong Yin},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107623},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Venue semantic representation, Interpretable, Embedding learning, Semantic mapping, Check-ins},
	pages = {107623},
	title = {Enabling the interpretability of pretrained venue representations using semantic categories},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121008856},
	volume = {235},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121008856},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107623}}

@article{BEHPOUR2021106907,
	abstract = {Identifying the trending topics in journals and conferences is valuable for understanding the role of authors, institutions, and funding agencies in the progression of knowledge produced in the field. However, many available clustering methods do not accommodate a desire for temporally clustered results that are typical of trends, in part because time of publication is often neglected as a feature. As a demonstration of how time can be emphasized in trend detection, we use a novel approach of introducing a weighted temporal feature to bias a topic clustering toward articles in a similar time frame; this is performed over a set of finance journal abstracts from 1974 to 2020. Latent Dirichlet Allocation (LDA) is used to parameterize each abstract, followed by dimensionality reduction using Singular Value Decomposition (SVD). We detect trending finance topics that are not identifiable when we use a standard clustering approach with no temporal bias. To identify trending topics, we utilize a metric of the silhouette score divided by the standard deviation of clusters over time. We then isolate topics identified by this metric and validate them using expert judgment. Our clustering strategy using temporal bias can be readily utilized in other fields for discovering the rise and fall of trends.},
	author = {Sahar Behpour and Mohammadmahdi Mohammadi and Mark V. Albert and Zinat S. Alam and Lingling Wang and Ting Xiao},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106907},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Text mining, Trend detection, Temporal biased clustering, Machine learning},
	pages = {106907},
	title = {Automatic trend detection: Time-biased document clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001702},
	volume = {220},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001702},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106907}}

@article{LENG20171,
	abstract = {Topic model is a hot research topic which is attracting attentions from many fields. Recently, several studies have applied topic model to ASR (audio scene recognition). Among these studies, most of them use the document-word co-occurrence matrix for topic analysis. In this work, we propose a new ASR algorithm based on audio events and topic model, which uses the document-event co-occurrence matrix for topic analysis. Our work is based on the hypothesis that: for an audio document, compared with its word distribution, its event distribution is more in line with humans' way of thinking, and then the topic distribution obtained based on the document-event co-occurrence matrix can represent the audio document better. The contribution of this work lies in that: (1) we propose an ASR algorithm which uses document-event co-occurrence matrix for topic analysis. Compared with the current studies which use document-word co-occurrence matrix for topic analysis, the proposed algorithm can extract the topic distribution which can express the audio documents better, and then can get better recognition results; (2) we propose a much easier method to obtain the document-event co-occurrence matrix; (3) we propose a method to weight the event distribution of audio documents; this weighting method can emphasize the audio events that are important in reflecting the unique topics of the audio documents, and can suppress the audio events that are common to many topics. Experimental results on two public datasets verify the effectiveness of the proposed ASR algorithm, and also verify the necessity and effectiveness of the proposed weighting method. The innovative ideas in this work are not limited to ASR, but can be extended to many other fields, such as the video classification etc.},
	author = {Yan Leng and Nai Zhou and Chengli Sun and Xinyan Xu and Qi Yuan and Chuanfu Cheng and Yunxia Liu and Dengwang Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.04.001},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Audio scene recognition, Audio event, Topic model, PLSA, LDA, Support vector machine},
	pages = {1-12},
	title = {Audio scene recognition based on audio events and topic model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117301673},
	volume = {125},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117301673},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.04.001}}

@article{MARTIN2022109265,
	abstract = {Our society produces and shares overwhelming amounts of information through Online Social Networks (OSNs). Within this environment, misinformation and disinformation have proliferated, becoming a public safety concern in most countries. Allowing the public and professionals to efficiently find reliable evidence about the factual veracity of a claim is a crucial step to mitigate this harmful spread. To this end, we propose FacTeR-Check, a multilingual architecture for semi-automated fact-checking and hoaxes propagation analysis that can be used to implement applications designed for both the general public and for fact-checking organisations. FacTeR-Check implements three different modules relying on the XLM-RoBERTa Transformer architecture to evaluate semantic similarity, to calculate natural language inference and to build search queries through automatic keywords extraction and Named-Entity Recognition. The three modules have been validated using state-of-the-art benchmark datasets, exhibiting good performance in all of them. Besides, FacTeR-Check is employed to collect and label a dataset, called NLI19-SP, composed of more than 40,000 tweets supporting or denying 60 hoaxes related to COVID-19, released publicly. Finally, an analysis of the data collected in this dataset is provided, which allows to obtain a deep insight of how disinformation operated during the COVID-19 pandemic in Spanish-speaking countries.},
	author = {Alejandro Mart{\'\i}n and Javier Huertas-Tato and {\'A}lvaro Huertas-Garc{\'\i}a and Guillermo Villar-Rodr{\'\i}guez and David Camacho},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109265},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Misinformation, Transformers, COVID-19, Hoax, Natural language inference, Semantic similarity},
	pages = {109265},
	title = {FacTeR-Check: Semi-automated fact-checking through semantic similarity and natural language inference},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122006323},
	volume = {251},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122006323},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109265}}

@article{LIAO2022108665,
	abstract = {Attributed graph clustering is an important task for grouping the nodes in a graph. In recent years, the algorithms based on graph convolutional networks (GCN) have achieved promising performance. However, almost all existing methods ignore that the nonlinearity between two consecutive GCN layers is unnecessary for improving the performance of attributed graph clustering, and may even harm the efficiency of the model. In this paper, we propose a novel deep linear graph attention model for attributed graph clustering (DLGAMC), which consists of an attention-based aggregation module and a similarity preserve module. Specifically, we simply exploit cosine similarity to construct the attention for aggregation, which does not need to learn extra attention parameters. It is worth noting that the attention we designed not only explicitly considers the similarity of attribute information, but also implicitly takes into account the local graph structure. To select the proper order of aggregation, we propose an adaptive strategy to evaluate the smoothness of node representations, where intra-cluster distance and inter-cluster distance are the key indicators in this process. To learn node representations for clustering, we design a similarity preserve module to preserve local similarity and global dissimilarity of the smooth features obtained by multiple aggregations, which is different from the ideas in reconstruction-based methods. Finally, k-means is performed on the learned representations to obtain the cluster partition. Experiments on several datasets show that our algorithm achieves great performance in attributed graph clustering tasks.},
	author = {Huifa Liao and Jie Hu and Tianrui Li and Shengdong Du and Bo Peng},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108665},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Attributed graph clustering, Linear model, Attention-based fusion, Graph neural network, Graph representation learning},
	pages = {108665},
	title = {Deep linear graph attention model for attributed graph clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003057},
	volume = {246},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003057},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108665}}

@article{GUI201734,
	abstract = {There have been increasing interests in natural language processing to explore effective methods in learning better representations of text for sentiment classification in product reviews. However, most existing methods do not consider subtle interplays among words appeared in review text, authors of reviews and products the reviews are associated with. In this paper, we make use of a heterogeneous network to model the shared polarity in product reviews and learn representations of users, products they commented on and words they used simultaneously. The basic idea is to first construct a heterogeneous network which links users, products, words appeared in product reviews, as well as the polarities of the words. Based on the constructed network, representations of nodes are learned using a network embedding method, which are subsequently incorporated into a convolutional neural network for sentiment analysis. Evaluations on the product reviews, including IMDB, Yelp 2013 and Yelp 2014 datasets, show that the proposed approach achieves the state-of-the-art performance.},
	author = {Lin Gui and Yu Zhou and Ruifeng Xu and Yulan He and Qin Lu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.02.030},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment classification, Representation learning, Network embedding, Product reviews},
	pages = {34-45},
	title = {Learning representations from heterogeneous network for sentiment classification of product reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117301144},
	volume = {124},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117301144},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.02.030}}

@article{WANDABWA2021107249,
	abstract = {Consumption of content in short-text microblogs is necessitated to a large extent by individual users and their friendship network interests. Based on the dynamism in the data throughput on such platforms, e.g., Twitter, prevailing conditions are bound to determine the nature of consumed or disseminated content. Therefore, semantic interests differ over time even for individual users. Detecting this semantic change over time is integral in mapping user profiles over a time period, especially in microblogs where only the extrinsic user profile identifiers provide metadata that seldom evolve. This is vital in serving relevant third-party content as well as in the computation of topical interest variations over time. In essence, current, and relevant topics of interest to a user on such a platform may not be representative of the same users' interests a few months later. In our quest to identify the most user-representative interests at any given time, each topical term was modelled as the inner product between word embeddings and a time-based embedding representation of assigned topics at varied time periods. The model was fitted onto tweets as time-series documents. To validate the model, changes in the extracted user-representative interests over time were semantically weighed against a mirrored, time-variant dataset. Interest weights across the time-variant datasets were computed and validated in five sub-topics for a period spanning two and a half years. Linearity in the relationships between the test and validation sets could be identified, more so in emerging topics. A Pearson correlation coefficient as high as 0.871 was achieved in interest change verification over the tested period.},
	author = {Herman M. Wandabwa and M. Asif Naeem and Farhaan Mirza and Russel Pears},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107249},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {User profiling, Text mining, Neural Networks, Information retrieval, Short-text microblogs},
	pages = {107249},
	title = {Multi-interest semantic changes over time in short-text microblogs},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121005116},
	volume = {228},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121005116},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107249}}

@article{ZHANG201982,
	abstract = {With the exponential growth of massive image data, automatic image annotation is becoming more important in image management and retrieval. Traditional image region annotation methods, through machine learning and low-level visual features, typically yield incorrect annotation results owing to the influence of the Semantic Gap. We herein propose a novel label refinement method for improving the image region annotation results. A spatial position relation graph with co-occurrence relations and spatial position relations among labels is proposed to analyze the latent semantic correlations among image region labels. Moreover, an incremental iterative random-walking algorithm is proposed to reconstruct the region relation graph for detecting non-dependable regions whose labels do not fit the semantic context of an image. Subsequently, a graph matching algorithm with semantic correlation and spatial relation analysis is proposed for non-dependable region label completion. Experiments on Corel5K demonstrate that our proposed spatial-position-relation-graph- based label refinement method can achieve good performance for image region label refinement.},
	author = {Jing Zhang and Zhenkun Wang and Yakun Mu and Zhe Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.12.010},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Image region annotation, Label refinement, Spatial position relation graph, Random-walking, Graph matching},
	pages = {82-91},
	title = {Image region label refinement using spatial position relation graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118305975},
	volume = {166},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118305975},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.12.010}}

@article{LI2021107163,
	abstract = {Understanding word-level emotion in terms of both category and intensity has always been considered an essential step in addressing text emotion classification tasks. Existing studies have mainly adopted the categorical lexicons that are tagged by predefined emotion taxonomies to link affective words with discrete emotions. However, in these lexicons, emotion tags are restricted to a specific set of basic emotions. Moreover, the emotional intensity is ignored, making these methods less flexible and less informative. This paper proposes a novel method to generate a word-level emotion distribution (WED) vector by incorporating domain knowledge and dimensional lexicon. The proposed method can link a word with more generic and fine-grained emotion taxonomies with quantitatively computed intensities. We propose two schemas to utilize the WED vector implicitly and explicitly to facilitate classification. The implicit approach implements a rule-based conversion strategy to augment the information in the label space. The explicit approach exploits WED as an emotional word embedding to enhance the sentiment feature. We conduct extensive experiments on seven multiclass datasets. The results indicate that both proposed schemas produce competitive results compared with the state-of-the-art baselines.},
	author = {Zongxi Li and Haoran Xie and Gary Cheng and Qing Li},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107163},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotion classification, Emotional embedding, VAD, Emotional lexicon},
	pages = {107163},
	title = {Word-level emotion distribution with two schemas for short text emotion classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004263},
	volume = {227},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121004263},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107163}}

@article{PENTA2021107342,
	abstract = {Document clustering is a powerful method with numerous applications, where the core idea is to group text into smaller and more manageable pieces of semantically related information. While there has been progress in the research community on improving the quality of clusters, the extraction of information that explains the semantic content of the clusters is still mostly a manual activity, as it requires the inspection of sample documents. In this work, we propose three main cluster explanation approaches that extend the current state of the art, which is mostly based on predicting a label for each cluster (i.e. cluster labelling). The first approach is based on scores extracted from the word distributions; the second is based on augmenting the score-based explanations using external knowledge-bases, and the third one combines the first two methods to exploit the knowledge coming from a labelled dataset. We also discuss the limitations of the current metric presented in the literature and extend it to evaluate our approaches using ground truth. We also provided an extensive set of evaluations to verify the effectiveness of the proposed algorithms, by running multiple experiments to compare our approaches with different baselines. Results indicate that improved explanations are possible and that linking external knowledge can provide more general cluster descriptions. Semi-supervised approaches also demonstrated meaningful insight. User studies were also conducted highlighting that users prefer specialized explanations.},
	author = {Antonio Penta and Anandita Pal},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107342},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Document clustering, Text analytics, Explainability, Cluster summarization, Cluster labelling, Clusters explanations},
	pages = {107342},
	title = {What is this Cluster about? Explaining textual clusters by extracting relevant keywords},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121006043},
	volume = {229},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121006043},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107342}}

@article{LIU2020106435,
	abstract = {Nowadays, social network sites (SNSs) have been significant platforms for content sharing in our daily life. With the emergence of different kinds of social network sites and users' diverse needs for content sharing, their content sharing practices are generally taken place in multiple SNSs. To construct models that can characterize users' content sharing practices in a composite context constituted by multiple social network sites (cross-site user generated content modeling) has been an emerging research topic in web data mining and human behavior research. However, previous methods such as Dirichlet Multinomial Mixture model (DMM), Biterm Topic Model (BTM), Twitter-LDA and MultiLDA have limited representation ability or are based on unreliable assumption, which cannot characterize the user generated content (UGC) accurately from the perspective of multiple SNSs. In this paper, we first conduct an empirical study to investigate the characteristics of users' content sharing practices in cross-site context, based on which we propose a more reliable cross-site UGC model named CrossSite-LDA (C-LDA). We then evaluate the performances of the C-LDA model with four state-of-the-art models based on the two data sets sampled from Weibo--Douban and Facebook--Twitter. Results show that the C-LDA has better performances in perplexity, word coherence, topic KL divergence, UCI and UMass metrics compared with existing models, which suggests its superior accuracy on modeling users' content characteristics in cross-site context.},
	author = {Baoxi Liu and Peng Zhang and Tun Lu and Ning Gu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106435},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multiple social network sites, User generated content modeling, Topic model, Weibo, Douban},
	pages = {106435},
	title = {A reliable cross-site user generated content modeling method based on topic model},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305645},
	volume = {209},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305645},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106435}}

@article{YUELIU2019104794,
	abstract = {Users of Community Question Answering (CQA) could not manage their time conveniently because their questions are often not answered quickly enough. To address this problem, we try to provide a function for CQA sites to inform users when their questions will be answered. In this paper, we propose a Question--Answerer Model Matching based answerer's response time prediction named (QAM2), which consists of two parts: the construction of the Multi-feature based Question--Answerer Model (MQAM, including the answerer model and the question model) and the prediction of question response time based on MQAM Matching Strategy (QAMMS). Firstly, the MQAM is built according to some extracted deep features (e.g., answerer's interest, professional level, activity, question category and difficulty), which are neglected in most existing methods on the prediction of question response time. Herein, the Label Cluster Latent Dirichlet Allocation (LC-LDA) model was proposed to overcome the compulsive allocation behaviors caused by traditional topic models (e.g. LDA), which treats the words that are irrelevant or weakly related to the subject as the topic of short texts when extracting the feature of answerer's interest and question category. Meanwhile, an improved PageRank algorithm-topic sensitive weighted PageRank (TSWPR) is used to eliminate the impact of ``indiscriminate'' users who have answered many questions with low quality of answers. Secondly, we use the model matching strategy based on multiple classifier for matching MQAM and calculating the question response time of each answerer. Experiments conducted on two real data sets of Stack Overflow show that the proposed method can improve significantly the accuracy of question response time prediction in CQA.},
	author = {YueLiu and Aihua Tang and FeiCai and Pengfei Ren and Zhibin Sun},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.06.002},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Community Question Answering, Topic-sensitive model, Stack overflow, The prediction of question response time},
	pages = {104794},
	title = {Multi-feature based Question--Answerer Model Matching for predicting response time in CQA},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302576},
	volume = {182},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302576},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.06.002}}

@article{YANG2022108488,
	abstract = {Keyphrase generation is an important fundamental task of natural language processing, which can help users quickly obtain valuable information from a large number of documents especially when they are facing with informal social media text. Existing Recurrent Neural Network (RNN) based keyphrase generation approaches cannot properly model the dependency structure of the informal text, which is often implicit between those distant words and plays an important role in extracting salient information. To obtain core features of text, we apply Graph Convolutional Network (GCN) on document-level graph to capture dependency structure information. The GCN-based node representations are further fed into a predictor network to provide potential candidates for copying mechanism. Moreover, we utilize a novel variational selector network to determine the final selection probability of each word in a phrase, which relies on its probabilities of copying from a given document and being generated from a vocabulary. Eventually, we introduce an enhancement mechanism to maximize the mutual information between document and generated keyphrase, thus ensuring the consistency between them. Experiment results show that our model outperforms previous state-of-the-art baselines on three social datasets, including Weibo, Twitter and StackExchange.},
	author = {Peng Yang and Yanyan Ge and Yu Yao and Ying Yang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108488},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Keyphrase generation, Sequence-to-sequence, Graph convolutional network, Mutual information},
	pages = {108488},
	title = {GCN-based document representation for keyphrase generation enhanced by maximizing mutual information},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122002076},
	volume = {243},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122002076},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108488}}

@article{LI2021106827,
	abstract = {Because of its efficiency, word embedding has been widely used in many natural language processing and text modeling tasks. It aims to represent each word by a vector so such that the geometry between these vectors can capture the semantic correlations between words. An ambiguous word can often have diverse meanings in different contexts, a quality which is called polysemy. The bulk of studies aimed to generate only one single embedding for each word, whereas a few studies have made a small number of embeddings to present different meanings of each word. However, it is hard to determine the exact number of senses for each word, as meanings depend on contexts. To address this problem, this paper proposes a novel adaptive cross-contextual word embedding (ACWE) method for capturing the word polysemy in different contexts based on topic modeling, in which the word polysemy is defined over a latent interpretable semantic space. The proposed ACWE consists of two main parts, in the first of which an unsupervised cross-contextual probabilistic word embedding model is designed to obtain the global word embeddings, and each word is represented by an embedding in the unified latent semantic space. Based on the global word embeddings, an adaptive cross-contextual word embedding process is then devised in the second part to learn the local embeddings for each polysemous word in different contexts. In fact, a word embedding is adaptively adjusted and updated with respect to different contexts to generate different word embeddings tailored to the corresponding contexts. The proposed ACWE is validated on two datasets collected from Wikipedia and IMDb on different tasks including word similarity, polysemy induction, semantic interpretability, and text classification. Experimental results indicate that ACWE does not only outperform the established word embedding methods, which consider word polysemy on six popular benchmark datasets, but it also yields competitive performance compared with state-of-the-art deep learning-based approaches without considering polysemy. Moreover, the proposed ACWE significantly improves the performances of text classification both in precision and F1, and the visualizations of the semantics of words demonstrate the feasibility and advantage of the proposed ACWE model on polysemy.},
	author = {Shuangyin Li and Rong Pan and Haoyu Luo and Xiao Liu and Gansen Zhao},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106827},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Word polysemy, Representation learning, Adaptive word embeddings, Tailored word embedding, Topic modeling, Semantic learning},
	pages = {106827},
	title = {Adaptive cross-contextual word embedding for word polysemy with unsupervised topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121000903},
	volume = {218},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121000903},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106827}}

@article{QIN2021107160,
	abstract = {Emotion distribution learning aims to annotate unlabeled instances with a set of emotion categories and their strengths. Non-negative Matrix Tri-Factorization (NMTF) introduces an association matrix between document clusters and word clusters to help the domain adaptation task in emotion distribution learning. Nevertheless, many prior cross-domain emotion distribution learning methods had two major deficiencies. First, they hypothesize that there is a one-to-one correspondence between document clusters and emotion labels. In their experiments, the number of document clusters depends on the number of labels. Second, the prior work does not endow models with adequate constraints. In the real scenario of cross-domain emotion distribution learning, there are potential constraints that may improve the performance of such models. In order to address these problems, we propose a constrained optimization approach based on NMTF for cross-domain emotion distribution learning. In our model, the relationship between document clusters and emotion labels is not always one-to-one. A novel content-based constraint is also endowed based on the hypothesis that documents belonging to the same clusters must have similar content. We solve the optimization problem by an alternately iterative algorithm and show the proof of convergence. Experiments on 12 real-world cross-domain emotion distribution learning tasks validate the effectiveness of our method.},
	author = {Xiaorui Qin and Yufu Chen and Yanghui Rao and Haoran Xie and Man Leung Wong and Fu Lee Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107160},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotion distribution learning, Non-negative Matrix Tri-Factorization, Domain adaptation, Content-based constraint},
	pages = {107160},
	title = {A constrained optimization approach for cross-domain emotion distribution learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004238},
	volume = {227},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121004238},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107160}}

@article{QIAN2020105684,
	abstract = {Feature selection used for dimensionality reduction of the feature space plays an important role in multi-label learning where high-dimensional data are involved. Although most existing multi-label feature selection approaches can deal with the problem of label ambiguity which mainly focuses on the assumption of uniform distribution with logical labels, it cannot be applied to many practical applications where the significance of related label for every instance tends to be different. To deal with this issue, in this study, label distribution learning covered with a certain real number of labels is introduced to design a model for the labeling-significance. Nevertheless, multi-label feature selection is limited to handling only labels consisting of logical relations. In order to solve this problem, combining the random variable distribution with granular computing, we first propose a label enhancement algorithm to transform logical labels in multi-label data into label distribution with more supervised information, which can mine the hidden label significance from every instance. On this basis, to remove some redundant or irrelevant features in multi-label data, a label distribution feature selection algorithm using mutual information and label enhancement is developed. Finally, the experimental results show that the performance of the proposed method is superior to the other state-of-the-art approaches when dealing with multi-label data.},
	author = {Wenbin Qian and Jintao Huang and Yinglong Wang and Wenhao Shu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105684},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Feature selection, Multi-label data, Granular computing, Label enhancement, Mutual information},
	pages = {105684},
	title = {Mutual information-based label distribution feature selection for multi-label learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301210},
	volume = {195},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301210},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105684}}

@article{SEBASTIAN201766,
	abstract = {This paper presents HBIN-LBD, a novel literature-based discovery (LBD) method that exploits the lexico-citation structures within the heterogeneous bibliographic information network (HBIN) graphs. Unlike other existing LBD methods, HBIN-LBD harnesses the metapath features found in HBIN graphs for discovering the latent associations between scientific papers published in otherwise disconnected research areas. Further, this paper investigates the effects of incorporating semantic and topic modeling components into the proposed models. Using time-sliced historical bibliographic data, we demonstrate the performance of our method by reconstructing two LBD hypotheses: the Fish Oil and Raynaud's Syndrome hypothesis and the Migraine and Magnesium hypothesis. The proposed method is capable of predicting the future co-citation links between research papers of these previously disconnected research areas with up to 88.86% accuracy and 0.89 F-measure.},
	author = {Yakub Sebastian and Eu-Gene Siew and Sylvester Olubolu Orimaye},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2016.10.015},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Literature-based discovery, Heterogeneous bibliographic information network, Link prediction},
	pages = {66-79},
	title = {Learning the heterogeneous bibliographic information network for literature-based discovery},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705116303860},
	volume = {115},
	year = {2017},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBILi4vLi4vLi4vLi4vLi4vLi4vLi4vLi4vLlRyYXNoL1NjaWVuY2VEaXJlY3RfY2l0YXRpb25zXzE2NjY1NDIyMzgzNDIuYmliTxEBtAAAAAABtAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1NjaWVuY2VEaXJlY3RfY2l0YSNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAACAACAAAKIGN1AAAAAAAAAAAAAAAAAAYuVHJhc2gAAgBELzpVc2VyczpzYW11ZWxlY2VvbDouVHJhc2g6U2NpZW5jZURpcmVjdF9jaXRhdGlvbnNfMTY2NjU0MjIzODM0Mi5iaWIADgBUACkAUwBjAGkAZQBuAGMAZQBEAGkAcgBlAGMAdABfAGMAaQB0AGEAdABpAG8AbgBzAF8AMQA2ADYANgA1ADQAMgAyADMAOAAzADQAMgAuAGIAaQBiAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAQlVzZXJzL3NhbXVlbGVjZW9sLy5UcmFzaC9TY2llbmNlRGlyZWN0X2NpdGF0aW9uc18xNjY2NTQyMjM4MzQyLmJpYgATAAEvAAAVAAIAEv//AAAACAANABoAJABvAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAic=},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705116303860},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2016.10.015}}

@article{XU2021107225,
	abstract = {Textual emotion analysis is a challenging research topic in the field of natural language processing (NLP), which plays an important role in related NLP tasks, such as opinion mining and personalized recommendation. Existing research on emotion analysis has focused mostly on detecting types of emotions, and has solved problems using classification-based methods. Recently, fine-grained emotion analysis has attracted the attention of researchers for probing the essential elements of emotions, such as the causes, experiencers and results of emotion events, which could help further elucidate textual emotions in more depth. In this paper, we focus on the task of emotion cause extraction, aiming to recognize the causes in sentences that provoke certain emotions. We propose a two-stage supervised ranking method for accurately extracting the emotion causes based on information retrieval techniques. In the first stage, we measure the complexity of provoked emotions using query performance predictors to distinguish the number of causes for each emotion in contexts. In the second stage, we incorporate the emotion complexity into learning an autoencoder-enhanced ranking model for accurately extracting the causal clauses. We also extract abundant emotion-level clause features for clause representations as the learning samples. We evaluate the proposed method on an existing dataset for emotion cause extraction and demonstrate that our method significantly outperforms the state-of-the-art baseline methods. The proposed method is effective in extracting textual emotion causes in sentences, which can greatly benefit in-depth emotion analysis for effective cognitive computing.},
	author = {Bo Xu and Hongfei Lin and Yuan Lin and Kan Xu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107225},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotion cause extraction, Sentiment analysis, Ranking model, Natural language processing},
	pages = {107225},
	title = {Two-stage supervised ranking for emotion cause extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004871},
	volume = {228},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121004871},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107225}}

@article{LI2020105436,
	abstract = {Text representation, a crucial step for text mining and natural language processing, concerns about transforming unstructured textual data into structured numerical vectors to support various machine learning and data mining algorithms. For document classification, one classical and commonly adopted text representation method is Bag-of-Words (BoW) model. BoW represents document as a fixed-length vector of terms, where each term dimension is a numerical value such as term frequency or tf-idf weight. However, BoW simply looks at surface form of words. It ignores the semantic, conceptual and contextual information of texts, and also suffers from high dimensionality and sparsity issues. To address the aforementioned issues, we propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from external knowledge base, then conceptualizes words and phrases in the document into higher level semantics (i.e. concepts) in a probabilistic manner, and eventually represents a document as a distributed vector in the learned concept space. By utilizing background knowledge from knowledge base, BoC representation is able to provide more semantic and conceptual information of texts, as well as better interpretability for human understanding. We also propose Bag-of-Concept-Clusters (BoCCl) model which clusters semantically similar concepts together and performs entity sense disambiguation to further improve BoC representation. In addition, we combine BoCCl and BoW representations using an attention mechanism to effectively utilize both concept-level and word-level information and achieve optimal performance for document classification.},
	author = {Pengfei Li and Kezhi Mao and Yuecong Xu and Qi Li and Jiaheng Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105436},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Natural language processing, Text representation, Document classification, Knowledge base, Interpretability},
	pages = {105436},
	title = {Bag-of-Concepts representation for document classification based on automatic knowledge acquisition from probabilistic knowledge base},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306604},
	volume = {193},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306604},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105436}}

@article{ELAKROUCHI2021106650,
	abstract = {An extremely competitive business environment requires every company to monitor its competitors and anticipate future opportunities and risks, creating a dire need for competitive intelligence. In response to this need, foresight study became a prominent field, especially the concept of weak signal detection. This research area has been widely studied for its utility, but it is limited by the need of human expert judgments on these signals. Moreover, the increase in the volume of information on the Internet through blogs and web news has made the detection process difficult, which has created a need for automation. Recent studies have attempted topic modeling techniques, specifically latent Dirichlet allocation (LDA), for automating the weak signal detection process; however, these approaches do not cover all parts of the process. In this study, we propose a fully automatic LDA-based weak signal detection method, consisting of two filtering functions: the weakness function aimed at filtering topics, which potentially contains weak signals, and the potential warning function, which helps to extract only early warning signs from the previously filtered topics. We took this approach with a famous daily web news dataset, and we could detect the risk of the COVID19 pandemic at an early stage.},
	author = {Manal {El Akrouchi} and Houda Benbrahim and Ismail Kassou},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106650},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Weak signals, Topic modeling, Latent Dirichlet allocation},
	pages = {106650},
	title = {End-to-end LDA-based automatic weak signal detection in web news},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307796},
	volume = {212},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120307796},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106650}}

@article{WAN2022109551,
	abstract = {Data sparsity and cold start are two critical issues which need to be addressed in recommender systems (RSs). Currently, most methods address these issues by applying user history files or some side information to improve the user model and complete the rating matrix. However, such methods cannot perform well when labeled data is scarce or unavailable. In this paper, we propose a dual learning-based recommendation approach (DLRA). DLRA can trigger initial recommendation and improve the quality of recommendations by using the duality characteristics of RSs, even when the available labeled information is scarce. Specifically, DLRA regards the recommendation task as two independent subtasks --- primal task and dual task, and these two tasks show strong duality in DLRA. The primal task is item-centered which aims to find users who can rate high for items, while the dual task is user-centered that aims to recommend the most favorite items to users. These two tasks have strong dualities in terms of the recommendation space, selection probability and recommendation basis. Based on these dualities, we design three dual learning strategies to couple the whole recommendation process and realize the self-tuning and self-improvement of each task model, and finally optimize the whole recommendation model. Based on the dataset of Movielens and BookCrossing, we simulate data sparsity and cold start recommendation scenarios, the experimental results show that DLRA achieves substantial improvement when the labeled data is scare, and it outperforms other hybrid recommendation approaches and deep learning strategies with a smaller predictive error as well as better recommendation accuracy.},
	author = {Shanshan Wan and Ying Liu and Dongwei Qiu and James Chambua and Zhendong Niu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109551},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender system, Dual learning, Data sparsity, Duality, Hybrid filtering recommendation},
	pages = {109551},
	title = {A dual learning-based recommendation approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122007791},
	volume = {254},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122007791},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109551}}

@article{ZHANG2017255,
	abstract = {The journal Knowledge-based Systems (KnoSys) has been published for over 25 years, during which time its main foci have been extended to a broad range of studies in computer science and artificial intelligence. Answering the questions: ``What is the KnoSys community interested in?'' and ``How does such interest change over time?'' are important to both the editorial board and audience of KnoSys. This paper conducts a topic-based bibliometric study to detect and predict the topic changes of KnoSys from 1991 to 2016. A Latent Dirichlet Allocation model is used to profile the hotspots of KnoSys and predict possible future trends from a probabilistic perspective. A model of scientific evolutionary pathways applies a learning-based process to detect the topic changes of KnoSys in sequential time slices. Six main research areas of KnoSys are identified, i.e., expert systems, machine learning, data mining, decision making, optimization, and fuzzy, and the results also indicate that the interest of KnoSys communities in the area of computational intelligence is raised, and the ability to construct practical systems through knowledge use and accurate prediction models is highly emphasized. Such empirical insights can be used as a guide for KnoSys submissions.},
	author = {Yi Zhang and Hongshu Chen and Jie Lu and Guangquan Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.07.011},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Topic analysis, Topic detection and tracking, Bibliometrics, Text mining, Knowledge-based Systems},
	pages = {255-268},
	title = {Detecting and predicting the topic change of Knowledge-based Systems: A topic-based bibliometric analysis from 1991 to 2016},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117303271},
	volume = {133},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117303271},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.07.011}}

@article{LIU2021106917,
	abstract = {In many reality networks, nodes contain rich text attribute information that exhibits significant role in describing the properties of them and relationship between them. The integration of structural and textual information is beneficial to downstream network analysis tasks. In this work, we present an Enhanced Textual Information Network Embedding model, called ETINE, for learning network embeddings with not only global structural information but also deep semantic relationship between nodes. Specifically, we formulate the optimization of our proposed structure-based and text-based loss functions as a matrix approximation problem. Moreover, to enhance the efficiency and robustness of the proposed method, we propose to optimize the loss functions with an efficient randomized singular value decomposition (RSVD) method. Extensive experiments on four benchmarks demonstrate that our model outperforms other state-of-the-art baselines in multi-class node classification, network reconstruction and node clustering tasks.},
	author = {Wenfeng Liu and Maoguo Gong and Zedong Tang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106917},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Network embedding, Textual information networks, Structural proximity, Textual proximity, Matrix approximation},
	pages = {106917},
	title = {ETINE: Enhanced Textual Information Network Embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001805},
	volume = {220},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001805},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106917}}

@article{LIU2018158,
	abstract = {Many mobile devices are equipped with video shooting function, and users tend to use these mobile devices to produce user generated content (UGC), and share with friends or the public owing to the popularity of social media. To make the video to be attractive, embedding appropriate background music into the video is a popular way to enrich user experience, but it is a time-consuming and labor-intensive task to find music that fits the video. This work proposes to use latent factors to recommend a list of music songs for a given video, in which the recommendation is based on the proposed score function, which involves the weighted average of the latent factors for the video and music. Moreover, we use pairwise ranking to design the objective function, and use stochastic gradient descent to optimize the proposed objective function. In the experiments, we specify two hypotheses and design several experiments to assess the performance and the effectiveness of the proposed algorithm from different aspects, including accuracy, quantitative research, and qualitative research. The experimental results indicate that the proposed model is promising in accuracy and quantitative research. Furthermore, this work provides detailed analysis to investigate the fitness of the background music that recommended by the system through interviewing the subjects.},
	author = {Chien-Liang Liu and Ying-Chuan Chen},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.07.001},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Background music recommendation system, Moods, Latent factor model, Recommender systems, Collaborative filtering, Multimodal information retrieval},
	pages = {158-170},
	title = {Background music recommendation based on latent factors and moods},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118303484},
	volume = {159},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118303484},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.07.001}}

@article{LIU2022108495,
	abstract = {In many fields, how to catch the related-topic Web resources is crucial. As a vertical search method, focused crawler has received great attention in recent years. Currently, most focused crawlers consider multiple evaluating factors of the hyperlinks and use the weighted sum approach to compute the priorities of unvisited hyperlinks. However, the proper weighted coefficients are hard to determine, and their unsuitable values may even cause the direction of crawlers to deviate seriously from the topic. To overcome this issue, this article builds a multi-objective optimization model based on Web text and link structure and designs a crawler framework called the Web space evolution (WSE), where a hyperlink bank whose radius is gradually increased is introduced to extend the search scape of crawlers in Web space. To improve the uniformity and diversity of hyperlinks, a nearest and farthest candidate solution method is combined with the fast non-dominated sorting to choose Pareto-optimal solutions (hyperlinks). A domain ontology based on the formal concept analysis is applied to establish the topic model. By incorporating the WSE and the domain ontology into the focused crawling, a novel focused crawler called FCWSEO is proposed to collect topic-relevant webpages. The experimental results on the rainstorm disaster domain show that the FCWSEO outperforms other focused crawler strategies in terms of the quantity and quality of retrieved relevant webpages.},
	author = {Jingfa Liu and Xin Li and Qiansheng Zhang and Guo Zhong},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108495},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Focused crawler, Web space evolution, Multi-objective optimization, Pareto optimal, Ontology},
	pages = {108495},
	title = {A novel focused crawler combining Web space evolution and domain ontology},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512200212X},
	volume = {243},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512200212X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108495}}

@article{WU2019736,
	abstract = {To address the problem of query-focused multi-document summarisation, we present a novel unsupervised pattern-enhanced approach for representing coherent topics across documents, as well as the query relevance, in order to generate topically coherent summaries that meet the information needs of users. The proposed model employs not only a pattern-enhanced topic model to generate discriminative and semantic rich representations for topics and documents, but also a pattern-based relevance model for the query relevance of sentences. With these dual pattern-based representations for sentences, we are able to integrate various indicative metrics, such as rational coverage of document topics and sentence relevance, into a unified model. When evaluated on the datasets of the document understanding conferences of 2006 and 2007, the proposed approach shows a performance improvement as compared to a number of state-of-the-art methods and unsupervised baseline systems.},
	author = {Yutong Wu and Yuefeng Li and Yue Xu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.09.035},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Query-focused multi-document summarisation, Pattern mining, Topic modelling, Query expansion, Three-way decision theory, Unsupervised approach},
	pages = {736-748},
	title = {Dual pattern-enhanced representations model for query-focused multi-document summarisation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118304866},
	volume = {163},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118304866},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.09.035}}

@article{FU201981,
	abstract = {Aspect-level sentiment classification aims to predict the sentiment of a text in different aspects and it is a fine-grained sentiment analysis task. Recent work exploits an Attention-based Long Short-Term Memory Network to perform aspect-level sentiment classification. Most previous work are based on supervised learning that needs a large number of labeled samples, but the problem is that only a limited subset of data samples are labeled in practical applications. To solve this problem, we propose a novel Semi-supervised Aspect Level Sentiment Classification Model based on Variational Autoencoder (AL-SSVAE) for semi-supervised learning in the aspect-level sentiment classification. The AL-SSVAE model inputs a given aspect to an encoder a decoder based on a variational autoencoder (VAE), and it also has an aspect level sentiment classifier. It enables the attention mechanism to deal with different parts of a text when different aspects are taken as input as previous methods. Due to that the sentiment polarity of a word is usually sensitive to the given aspect, a single vector for a word is problematic. Therefore, we propose the aspect-specific word embedding learning from a topical word embeddings model to express a word and also append the corresponding sentiment vector into the word input vector. We compare our AL-SSVAE model with several recent aspect-level sentiment classification models on the SemEval 2016 dataset. The experimental results indicate that the proposed model is able to capture more accurate semantics and sentiment for the given aspect and obtain better performance on the task of the aspect level sentiment classification. Moreover, the AL-SSVAE model is able to learn with the semi-supervised mode in the aspect level sentiment classification, which enables it to learn efficiently using less labeled data.},
	author = {Xianghua Fu and Yanzhi Wei and Fan Xu and Ting Wang and Yu Lu and Jianqiang Li and Joshua Zhexue Huang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.02.008},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Aspect-level, Variational Autoencoder, Word embeddings, Semi-supervised, Sentiment classification},
	pages = {81-92},
	title = {Semi-supervised Aspect-level Sentiment Classification Model based on Variational Autoencoder},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119300619},
	volume = {171},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119300619},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.02.008}}

@article{YE2022108699,
	abstract = {Hyperbolic embedding aims to reveal the hidden space by capturing most of the properties observed in real networks. Most existing hyperbolic embedding models map to learn the representation vectors while preserving the microscopic adjacency, which cannot accurately represent the mesoscopic community structures. To this end, this study proposes a Community Preserving Hyperbolic Embedding model (CPHE). Specifically, we regularize the likelihood function of hyperbolic embedding by adding to the community co-occurrence relation (CCR). We then construct a closed loop for node embedding and community detection. Thus, the representation vectors and CCR alternately update. Finally, to avoid the distortion of the representing community, an equivalent majorization based on the sum of linear ratios programming is achieved for the numerical solution. Application experiments are implemented on both synthetic and real-world networks to evaluate the embedding performance. The accuracy and normalized mutual information (NMI) of community detection improved by approximately 3% and 2.4%, respectively, and the area under the receiver operating characteristic curve (AUROC) of link prediction improved by approximately 1.3%, demonstrating the advantages of the proposed model.},
	author = {Dongsheng Ye and Hao Jiang and Ying Jiang and Qiang Wang and Yulin Hu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108699},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Hyperbolic geometry, Network embedding, Community embedding},
	pages = {108699},
	title = {Community preserving mapping for network hyperbolic embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003227},
	volume = {246},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003227},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108699}}

@article{ZHOU2020105458,
	abstract = {Real-world networks usually consist of a large number of interacting, multi-typed components which are usually referred as heterogeneous information networks (HIN). HIN that associated with various attributes on nodes is defined as attributed HIN (or AHIN). Clustering is a fundamental task for HIN and AHIN. However, most of the current existing methods focus on single type nodes and there is very limited existing work that groups objects of different types into the same cluster. This is largely due to the reasons that object similarities can either be attribute-based or link-based between same type of nodes and it is challenging to incorporate both in a unified framework. To bridge this gap, in this paper, we propose a framework, namely Cross Multi-Type Objects Clustering in Attributed Heterogeneous Information Network, or CMOC-AHIN, to integrate both the attribute information and multi-type node clustering in a principled way. We empirically show superior performances of CMOC-AHIN on three large scale challenging data sets and also summarize insights on the performances compared to other state-of-the-arts methodologies.},
	author = {Sheng Zhou and Jiajun Bu and Zhen Zhang and Can Wang and Lingzhou Ma and Jianfeng Zhang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105458},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Heterogeneous information network, Clustering, Attributed network},
	pages = {105458},
	title = {Cross Multi-Type Objects Clustering in Attributed Heterogeneous Information Network},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306719},
	volume = {194},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306719},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105458}}

@article{PENG2019429,
	abstract = {Emotional conversation generation has elicited a wide interest in both academia and industry. However, existing emotional neural conversation systems tend to ignore the necessity to combine topic and emotion in generating responses, possibly leading to a decline in the quality of responses. This paper proposes a topic-enhanced emotional conversation generation model that incorporates emotional factors and topic information into the conversation system, by using two mechanisms. First, we use a Twitter latent Dirichlet allocation (LDA) model to obtain topic words of the input sequences as extra prior information, ensuring the consistency of content between posts and responses for emotional conversation generation. Second, the system uses a dynamic emotional attention mechanism to adaptively acquire content-related and affective information of the input texts and extra topics. The advantage of this study lies in the fact that the presented model can generate abundant emotional responses, with the contents being related and diverse. To demonstrate the effectiveness of our method, we conduct extensive experiments on large-scale Weibo post--response pairs. Experimental results show that our method achieves good performance, even outperforming some existing models.},
	author = {Yehong Peng and Yizhen Fang and Zhiwen Xie and Guangyou Zhou},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.09.006},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Emotional conversation, Topic model, Sequence-to-sequence, Attention mechanism},
	pages = {429-437},
	title = {Topic-enhanced emotional conversation generation with attention mechanism},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511830457X},
	volume = {163},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511830457X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.09.006}}

@article{WANG2019104812,
	abstract = {Most of the LDA algorithms make the same limiting assumption based on a fixed vocabulary. When these algorithms process data streams in real time, the non-existent words in the vocabulary are discounted. Unexpected words that appear in the streams are incapable to be processed, as the atoms in the Dirichlet distribution are fixed. In order to address the drawbacks as mentioned above, ivLDA with topic--word distribution stemming from the Dirichlet process that has infinite atoms instead of Dirichlet distribution is proposed. ivLDA involves an incremental vocabulary that enables the topic models to process data streams. Besides, two methods are presented to manage the indices of the words, namely, ivLDA-Perp and ivLDA-PMI. ivLDA-Perp is capable of achieving high accuracy and ivLDA-PMI is able to identify the most valuable words to represent the topic. As indicated by experiments, ivLDA-Perp and ivLDA-PMI can achieve superior performance to infvoc-LDA and other state-of-the-art algorithms with fixed vocabulary.},
	author = {Meng Wang and Lu Yang and JianFeng Yan and Jianwei Zhang and Jie Zhou and Peng Xia},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.06.020},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Topic model, Belief Propagation, Stick-breaking process, Online algorithm},
	pages = {104812},
	title = {Topic model with incremental vocabulary based on Belief Propagation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302874},
	volume = {182},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302874},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.06.020}}

@article{QIN2020105750,
	abstract = {In recent years, many efforts based on deep learning have been made to address the issue of text tagging. However, these work generally neglect to consider the neighborhood effect which may help improve the accuracy of predictions. For this, we present a neighborhood-aware deep model for text tagging (NATT). A neural component which combines bi-directional recurrent neural network and self-attention mechanism, is firstly selected as the text encoder to encode the target document into one feature vector. Then, k-nearest-neighbor documents of the target document are identified and encoded into feature vectors one by one with the same text encoder. Simultaneously, an independent attention module is introduced to aggregate these neighboring documents into a special feature vector, which will represent features of the neighborhood. Finally, the two feature vectors are fused to match the embedding vectors of tags. To optimize the NATT model, we build the objective function with pairwise hinge loss and specially develop a neighborhood-aware negative sampling strategy to form training data. Experimental results on four datasets demonstrate that NATT outperforms some state-of-the-art neural models. Additionally, NATT is economical on achieving the best results with less training epochs and a smaller number of nearest neighbors.},
	author = {Shaowei Qin and Hao Wu and Rencan Nie and Jun He},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105750},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Neighborhood-aware, Negative sampling, Deep neural networks, Text tagging},
	pages = {105750},
	title = {Deep model with neighborhood-awareness for text tagging},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120301623},
	volume = {196},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120301623},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105750}}

@article{GANGAVARAPU2020105321,
	abstract = {In hospitals, caregivers are trained to chronicle the subtle changes in the clinical conditions of a patient at regular intervals, for enabling decision-making. Caregivers' text-based clinical notes are a significant source of rich patient-specific data, that can facilitate effective clinical decision support, despite which, this treasure-trove of data remains largely unexplored for supporting the prediction of clinical outcomes. The application of sophisticated data modeling and prediction algorithms with greater computational capacity have made disease prediction from raw clinical notes a relevant problem. In this paper, we propose an approach based on vector space and topic modeling, to structure the raw clinical data by capturing the semantic information in the nursing notes. Fuzzy similarity based data cleansing approach was used to merge anomalous and redundant patient data. Furthermore, we utilize eight supervised multi-label classification models to facilitate disease (ICD-9 code group) prediction. We present an exhaustive comparative study to evaluate the performance of the proposed approaches using standard evaluation metrics. Experimental validation on MIMIC-III, an open database, underscored the superior performance of the proposed Term weighting of unstructured notes AGgregated using fuzzy Similarity (TAGS) model, which consistently outperformed the state-of-the-art structured data based approach by 7.79% in AUPRC and 1.24% in AUROC.},
	author = {Tushaar Gangavarapu and Aditya Jayasimha and Gokul S. Krishnan and Sowmya Kamath S.},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105321},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Clinical decision support systems, Disease prediction, Healthcare analytics, ICD-9 code group prediction, Machine learning, Natural language processing},
	pages = {105321},
	title = {Predicting ICD-9 code groups with fuzzy similarity based supervised multi-label classification of unstructured clinical nursing notes},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119305982},
	volume = {190},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119305982},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105321}}

@article{KAUR2022108014,
	abstract = {Lately, cross-modal retrieval has attained plenty of attention due to enormous multi-modal data generation every day in the form of audio, video, image, and text. One vital requirement of cross-modal retrieval is to reduce the heterogeneity gap among various modalities so that one modality's results can be efficiently retrieved from the other. So, a novel unsupervised cross-modal retrieval framework based on associative learning has been proposed in this paper where two traditional SOMs are trained separately for images and collateral text and then they are associated together using the Hebbian learning network to facilitate the cross-modal retrieval process. Experimental outcomes on a popular Wikipedia dataset demonstrate that the presented technique outshines various existing state-of-the-art approaches.},
	author = {Parminder Kaur and Avleen Kaur Malhi and Husanbir Singh Pannu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108014},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Self organizing maps, Cross-modal retrieval, Hebbian learning, Zernike moments, Machine learning},
	pages = {108014},
	title = {Hybrid SOM based cross-modal retrieval exploiting Hebbian learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011175},
	volume = {239},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011175},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108014}}

@article{YERA2022109216,
	abstract = {The need of increasing trustworthiness and transparency in artificial intelligence (AI)-based systems that adhere ethical principles of respect for human autonomy, prevention of harm, fairness, and explainability; has boosting the development of systems that incorporate such issues as a key component. Recommender systems (RSs) are included in such AI-based systems, because they use intelligent algorithms for providing the most suitable items to active users according to other users' preferences. The RSs success is based on how much customers trust on the system, therefore recommendation explainability has become a crucial dimension for RSs adoption in real-world scenarios. Among the different successful applications of RS, it is remarkable the recent and exponential importance of recommendations for health and wellness areas. Hence, this paper aims at exploring, adapting and applying explanations for nutrition/recipes recommendations, that not only explain why the recommendation is enjoyable but also, it is aware of how healthy is the recommendation. Among the different methodologies to explain recommendations, this paper is focused on post-hoc explainability approaches and its adaptation, application and evaluation for nutrition/recipes recommendation. Eventually, it is included a comprehensive experimental study for characterizing the strengths and weaknesses of such explainability approaches in the recipe recommendation context.},
	author = {Raciel Yera and Ahmad A. Alzahrani and Luis Mart{\'\i}nez},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109216},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Explainable recommendation, Cooking recipes, Post-hoc explanation, Trustworthiness},
	pages = {109216},
	title = {Exploring post-hoc agnostic models for explainable cooking recipe recommendations},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122006050},
	volume = {251},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122006050},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109216}}

@article{XU201844,
	abstract = {Nowadays plenty of user-generated posts, e.g., sina weibos, are published on the social media. The posts contain the public's sentiments (i.e., positive or negative) towards various topics. Bursty sentiment-aware topics from these posts reveal sentiment-aware events which have attracted much attention. To detect sentiment-aware topics, we attempt to utilize Joint Sentiment/Topic models, these models are achieved with Latent Dirichlet Allocation (LDA) based models. However, most of the existing sentiment/topic models cannot be directly utilized to detect sentiment-aware topics on the posts, since applying the models to the posts directly suffers from the context sparsity problem. In this paper, we propose a Time-User Sentiment/Topic Latent Dirichlet Allocation (TUS-LDA) which simultaneously models sentiments and topics for posts. Thereinto, TUS-LDA aggregates posts in the same timeslices or from the same users as pseudo-documents to alleviate the context sparsity problem. Based on TUS-LDA, we further design an approach to detect bursty sentiment-aware topics and these sentiment-ware topics can reflect bursty real-world events. Experiments on the Chinese sina weibos show that TUS-LDA outperforms previous models in the tasks of sentiment classification and burst detection in sentiment-aware topics. Finally, we visualize the bursty sentiment-aware topics discovered by TUS-LDA.},
	author = {Kang Xu and Guilin Qi and Junheng Huang and Tianxing Wu and Xuefeng Fu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.11.007},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Burst detection, Sentiment topic model, Sina weibo},
	pages = {44-54},
	title = {Detecting bursts in sentiment-aware topics from social media},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117305282},
	volume = {141},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117305282},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.11.007}}

@article{VENUGOPALAN2022108668,
	abstract = {Aspect level sentiment analysis is a fine-grained task in sentiment analysis. It extracts aspects and their corresponding sentiment polarity from opinionated text. The first subtask of identifying the opinionated aspects is called aspect extraction, which is the focus of the work. Social media platforms are an enormous resource of unlabeled data. However, data annotation for fine-grained tasks is quite expensive and laborious. Hence unsupervised models would be highly appreciated. The proposed model is an unsupervised approach for aspect term extraction, a guided Latent Dirichlet Allocation (LDA) model that uses minimal aspect seed words from each aspect category to guide the model in identifying the hidden topics of interest to the user. The guided LDA model is enhanced by guiding inputs using regular expressions based on linguistic rules. The model is further enhanced by multiple pruning strategies, including a BERT based semantic filter, which incorporates semantics to strengthen situations where co-occurrence statistics might fail to serve as a differentiator. The thresholds for these semantic filters have been estimated using Particle Swarm Optimization strategy. The proposed model is expected to overcome the disadvantage of basic LDA models that fail to differentiate the overlapping topics that represent each aspect category. The work has been evaluated on the restaurant domain of SemEval 2014, 2015 and 2016 datasets and has reported an F-measure of 0.81, 0.74 and 0.75 respectively, which is competitive in comparison to the state of art unsupervised baselines and appreciable even with respect to the supervised baselines.},
	author = {Manju Venugopalan and Deepa Gupta},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108668},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Aspect term extraction, Guided LDA, BERT, Semantic similarity},
	pages = {108668},
	title = {An enhanced guided LDA model augmented with BERT based semantic strength for aspect term extraction in sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122003069},
	volume = {246},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122003069},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108668}}

@article{AYETIRAN2021106902,
	abstract = {Several language applications often require word semantics as a core part of their processing pipeline either as precise meaning inference or semantic similarity. Multi-sense embeddings (m-se) can be exploited for this important requirement. m-se seeks to represent each word by their distinct senses in order to resolve the conflation of meanings of words as used in different contexts. Previous works usually approach this task by training a model on a large corpus and often ignore the effect and usefulness of the semantic relations offered by lexical resources. However, even with large training data, coverage of all possible word senses is still an issue. In addition, a considerable percentage of contextual semantic knowledge are never learned because a huge amount of possible distributional semantic structures are never explored. In this paper, we leverage the rich semantic structures in WordNet using a graph-theoretic walk technique over word senses to enhance the quality of multi-sense embeddings. This algorithm composes enriched texts from the original texts. Furthermore, we derive new distributional semantic similarity measures for m-se from prior ones. We adapt these measures to word sense disambiguation (wsd) aspect of our experiment. We report evaluation results on 11 benchmark datasets involving wsd and Word Similarity tasks and show that our method for enhancing distributional semantic structures improves embeddings quality on the baselines. Despite the small training data, it achieves state-of-the-art performance on some of the datasets.},
	author = {Eniafe Festus Ayetiran and Petr Sojka and V{\'\i}t Novotn{\'y}},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.106902},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Multi-sense embeddings, Graph walk, Language generation, Distributional semantics, Distributional structures, Word sense disambiguation, Knowledge-based systems, Word similarity, Semantic applications},
	pages = {106902},
	title = {EDS-MEMBED: Multi-sense embeddings based on enhanced distributional semantic structures via a graph walk over word senses},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001659},
	volume = {219},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121001659},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.106902}}

@article{FACCHINETTI2022109266,
	abstract = {Systematic Literature Review (SLR) is nowadays a challenging task due to the large number of papers that typically compose the scientific material of the topic to review. Recently, a lot of research effort has been devoted to automate, even partially, the stages of an SLR. This paper proposes the design and implementation of a workflow and a set of tools -- called slr-kit -- to support key tasks in an SLR. The proposed approach leverages a semi-supervised strategy, in which time-consuming processes are carried out using automatic tools, whereas manual tasks have been optimized by carefully designed support tools to reduce the overall required effort. Important parts of the workflow include the extraction of key terms directly from the abstracts of the papers to survey, and the subsequent topic modeling that allows for a thematic clustering of the corpus of papers. In the proposed workflow, the former task is carried out by exploiting a novel tool, called FAst WOrd Classifier (FAWOC). The latter, instead, is designed to be automatically carried out by leveraging an ad-hoc solution based on the application of the Latent Dirichlet Allocation (LDA) algorithm. The result of the process consists in a set of statistics regarding the relationship among papers, topics, and their trend of publication on journals and conference proceedings. The validity of the method is demonstrated with an application to a dataset related to the scientific field of NLP, while its accuracy is assessed by the manual examination of the results by domain experts.},
	author = {Tullio Facchinetti and Guido Benetti and Davide Giuffrida and Antonino Nocera},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.109266},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Natural language processing, Topic modeling, Systematic literature review, Tagging, Performance evaluation},
	pages = {109266},
	title = {slr-kit: A semi-supervised machine learning framework for systematic literature reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122006335},
	volume = {251},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122006335},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.109266}}

@article{ZHAO2022108550,
	abstract = {Currently, sequence/graph-to-sequence models for abstractive dialogue summarization are being studied extensively. However, previous methods strive to integrate complex events spanning multiple utterances, and the generated summaries are often filled with incorrect facts. In this study, we first utilize the speaker-aware structure to model the information interaction process in the dialogue, which shows an excellent ability to settle the cross-sentence dependency. Then, we incorporate the factual representations via a dual-copy decoder to obtain summaries conditioned on both the tokens from source sequences and the factual knowledge from our designed fact graph, which enhances the factual consistency for dialogue summarization. We also propose some fact-level factual consistency metrics. Adequate experimental results demonstrate that our model outperforms the state-of-the-art baselines by a significant margin on the SAMSum and DialSumm datasets. A comprehensive analysis also proves the effectiveness of our model. Furthermore, human judges confirm that the outputs of our model contain more informative and faithful information.},
	author = {Lulu Zhao and Weiran Xu and Chunyun Zhang and Jun Guo},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108550},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Abstractive dialogue summarization, Speaker-aware structure, Dual-copy, Factual consistency},
	pages = {108550},
	title = {Leveraging speaker-aware structure and factual knowledge for faithful dialogue summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512200243X},
	volume = {245},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512200243X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108550}}

@article{GONZALEZSANTOS2021107113,
	abstract = {Topic modeling is a growing field within the area of text analysis that extracts underlying topics from document collections. Several objectives can be simultaneously considered when designing an approach for topic modeling. A multi-objective optimization approach based on the swarm intelligence of a bee colony (MOABC, Multi-Objective Artificial Bee Colony) has been designed, implemented, and tested. This new approach has been evaluated by using documents from the Reuters-21578 and TagMyNews datasets. Three objective functions (coherence, coverage, and perplexity) and three multi-objective metrics (hypervolume, set coverage, and distance to the ideal point) have been considered in two topic scenarios. Results show that MOABC provides relevant improvements with respect to LDA (Latent Dirichlet Allocation, the most used approach for topic modeling) and MOEA/D (Multi-Objective Evolutionary Algorithm based on Decomposition, the only multi-objective approach published to date). This demonstrates that the multi-criteria nature of topic modeling should be exploited with multi-objective optimization approaches.},
	author = {Carlos Gonz{\'a}lez-Santos and Miguel A. Vega-Rodr{\'\i}guez and Carlos J. P{\'e}rez},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107113},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Artificial bee colony, Evolutionary computing, Latent Dirichlet allocation, Multi-objective optimization, Text analysis, Topic modeling},
	pages = {107113},
	title = {Addressing topic modeling with a multi-objective optimization approach based on swarm intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121003762},
	volume = {225},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121003762},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107113}}

@article{ZENGINALP2018211,
	abstract = {Social media web sites have become major media platforms to share personal information, news, photos, videos and more. Users can even share live streams whenever they want to reach out to many other. This prevalent usage of social media attracted companies, data scientists, and researchers who are trying to infer meaningful information from this vast amount of data. Information diffusion and maximizing the spread of words is one of the most important focus for researchers working on social media. This information can serve many purposes such as; user or content recommendation, viral marketing, and user modeling. In this research, finding topical influential/authority users on Twitter is addressed. Since Twitter is a good platform to spread knowledge as a word of mouth approach and it has many more public profiles than protected ones, it is a target media for marketers. In this paper, we introduce a novel methodology, called Personalized PageRank, that integrates both the information obtained from network topology and the information obtained from user actions and activities in Twitter. The proposed approach aims to determine the topical influencers who are experts on a specific topic. Experimental results on a large dataset consisting of Turkish tweets show that using user specific features like topical focus rate, activeness, authenticity and speed of getting reaction on specific topics positively affects identifying influencers and lead to higher information diffusion. Algorithms are implemented on a distributed computing environment which makes high-cost graph processing more efficient.},
	author = {Zeynep {Zengin Alp} and {\c S}ule {G{\"u}nd{\"u}z {\"O}{\u g}{\"u}d{\"u}c{\"u}}},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.11.021},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Social influence, Topical influence, Twitter, User modeling, PageRank, Topic modeling, LDA, Evaluation metrics, Viral marketing},
	pages = {211-221},
	title = {Identifying topical influencers on twitter based on user behavior and network topology},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117305439},
	volume = {141},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117305439},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.11.021}}

@article{ALI2020106438,
	abstract = {Researchers face millions of research papers on various digital libraries. Therefore, finding relevant research work that meets the preferences of a researcher is a challenging task. Hence, different paper recommendation models have been proposed to address this issue. However, these models lack in exploiting prominent information factors, namely: papers' citations proximity, authors' information, papers' topical relevance, venues' information, researchers' preference dynamics, and labels information to produce quality recommendations. Additionally, these models encounter problems such as cold start papers and data sparsity. To overcome these problems, this paper presents a weighted probabilistic paper recommendation model termed as PR-HNE, which jointly learns researchers' and papers' dynamics by encoding information from six information networks into a joint latent space. Specifically, it captures papers' citation proximity, authors' collaboration proximity, venues' information, labeled information, and topical relevance to generate personalized paper recommendations. Compared to state-of-the-art models, the results generated by PR-HNE over publicly available datasets prove 4% and 6% improvement in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) metrics, respectively. Further, in the cold-start papers problem, the proposed model produced 8% better recall score than its counterparts.},
	author = {Zafar Ali and Guilin Qi and Khan Muhammad and Bahadar Ali and Waheed Ahmed Abro},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106438},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Paper recommendation, Citation recommendation, Heterogeneous network embedding, Neural networks, Cold-start},
	pages = {106438},
	title = {Paper recommendation based on heterogeneous network embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305670},
	volume = {210},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305670},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106438}}

@article{ZHU2021106511,
	abstract = {Travel package recommendation is a critical task in the tourism e-commerce recommender systems. Recently, an increasing number of studies proposed various travel package recommendation algorithms to improve Online Travel Agencies (OTAs) service, such as collaborative filtering-based, matrix factorization-based and neural network-based methods. Despite their value, however, the main challenges that incorporating complex descriptive information of the travel packages and capturing complicated users' long-term preferences for fine-grained travel package recommendation are still not fully resolved. In terms of these issues, this paper propose a novel model named Neural Attentive Travel package Recommendation (NATR) for tourism e-commerce by combining users' long-term preferences with short-term preferences. Specifically, NATR mainly contains two core modules, namely, travel package encoder and user encoder. The travel package encoder module is developed to learn a unified travel package representation by an attentive multi-view learning approach including word-level and view-level attention mechanisms. The user encoder module is designed to study long-term and short-term preference of the user by Bidirectional Long Short-Term Memory (Bi-LSTM) neural networks with package-level attention mechanism. In addition, we further adopt a gated fusion approach to coalesce these two kinds of preferences for learning high-quality the user's representation. Extensive experiments are conducted on a real-life tourism e-commerce dataset, the results demonstrate the proposed model yields significant performance advantages over several competitive methods. Further analyses from different attention weights provide insights of attentive multi-view learning and gated fusion network, respectively.},
	author = {Guixiang Zhu and Youquan Wang and Jie Cao and Zhan Bu and Shuxin Yang and Weichao Liang and Jingting Liu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106511},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Travel recommendations, Sequential behaviors, Neural networks, Personalized attention},
	pages = {106511},
	title = {Neural Attentive Travel package Recommendation via exploiting long-term and short-term behaviors},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120306407},
	volume = {211},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120306407},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106511}}

@article{HUANG2019104791,
	abstract = {Prefetching proactively resources at datanodes within distribution networks plays a key role in improving the efficiency of data access for e-learning, which requires assistance from semantic knowledge on educational applications and resource popularity. To capture such information, we should exploit the characteristics of education end-users' requests with high spatial--temporal locality. This paper aims to develop resource popularity modeling techniques for enhancing the performance of educational resource prefetching. Specifically, a novel topic model, built on an accelerated spectral clustering and an ontology concept similarity, is proposed to support resource access based on semantic features, including topic relevance and spatial--temporal locality. Using the proposed model, an adaptive prefetching inference approach is presented to associate possible popular resources in the future data requests. Also, an efficient prefetching management mechanism incorporating with replica techniques is suggested to design resource cloud storage systems for geo-distributed educational applications. Experiments over a simulation setting and a real-world case study with seven million users across China are carried out. Results demonstrate that the proposed method performs favorably compared to the state-of-the-art approaches.},
	author = {Qionghao Huang and Changqin Huang and Jin Huang and Hamido Fujita},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.05.034},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Spatial--temporal locality, Topic information, Semantic inference, Prefetching mechanism, Educational resources},
	pages = {104791},
	title = {Adaptive resource prefetching with spatial--temporal and topic information for educational cloud storage systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119302540},
	volume = {181},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119302540},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.05.034}}

@article{GUO2021107454,
	abstract = {Text matching is a fundamental and critical problem in natural language understanding (NLU), where multi-level semantics matching is the most challenging task. Human beings can always leverage their semantic knowledge, while neural computer systems first learn sentence semantic representations and then perform text matching based on learned representation. However, without sufficient semantic information, computer systems will not perform very well. To bridge the gap, we propose a novel Frame-based Multi-level Semantics Representation (FMSR) model, which utilizes frame knowledge to extract multi-level semantic information within sentences explicitly for the text matching task. Specifically, different from existing methods that only rely on the sophisticated architectures, FMSR model, which leverages both frame and frame elements in FrameNet, is designed to integrate multi-level semantic information with attention mechanisms to learn better sentence representations. Our extensive experimental results show that FMSR model performs better than the state-of-the-art technologies on two text matching tasks.},
	author = {Shaoru Guo and Yong Guan and Ru Li and Xiaoli Li and Hongye Tan},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107454},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Text matching, Frame semantics, Multi-level semantic representation},
	pages = {107454},
	title = {Frame-based Multi-level Semantics Representation for text matching},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121007164},
	volume = {232},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121007164},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107454}}

@article{DECAMPOS2020105337,
	abstract = {In the information age we are living in today, not only are we interested in accessing multimedia objects such as documents, videos, etc. but also in searching for professional experts, people or celebrities, possibly for professional needs or just for fun. Information access systems need to be able to extract and exploit various sources of information (usually in text format) about such individuals, and to represent them in a suitable way usually in the form of a profile. In this article, we tackle the problems of profile-based expert recommendation and document filtering from a machine learning perspective by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested. The experts will then be represented by means of multi-faceted profiles. Our experiments show that this is a valid technique to improve the performance of expert finding and document filtering.},
	author = {Luis M. {de Campos} and Juan M. Fern{\'a}ndez-Luna and Juan F. Huete and Luis Redondo-Exp{\'o}sito},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.105337},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Clustering, Content-based recommendation, Expert finding, Filtering, User profiling},
	pages = {105337},
	title = {Automatic construction of multi-faceted user profiles using text clustering and its application to expert recommendation and filtering problems},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119306069},
	volume = {190},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119306069},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.105337}}

@article{LIANG2022108050,
	abstract = {Temporal dynamics such as short term and long term effects, recency effects, periodic and seasonal temporal factors in information networks are of great importance for many real-world applications. However, existing network embedding learning approaches mainly focus on semantic information or temporal phenomenon such as recency or dynamic process. They failed to have the capability of incorporating multiple temporal factors/phenomenon in information networks. To bridge the gap, this paper proposes a general time-aware network representation learning framework TNE for temporal applications. TNE contains a temporally annotated network TAN, a temporally annotated meta-path based random walk method, and a self-supervised embedding learning approach. We introduce temporal nodes and relations to existing information networks to construct TAN that can incorporate multiple temporal factors. We propose a temporally annotated meta-path based random walk approach to form a time-aware hybrid neighbourhood context that considers both semantic and temporal factors. Based on the time-aware context, self-supervised representation learning approaches are used to simultaneously preserve both semantic and temporal factors in embeddings. Extensive experiments of two large scale real-life datasets show that the proposed framework is effective in various temporal applications such as temporal similarity search and temporal recommendations.},
	author = {Huizhi Liang and Thanet Markchom},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.108050},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Representation learning, Heterogeneous information network, User profiling, Network embeddings, Temporal dynamics},
	pages = {108050},
	title = {TNE: A general time-aware network representation learning framework for temporal applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121011382},
	volume = {240},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121011382},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.108050}}

@article{SINOARA2019955,
	abstract = {Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of word- and word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios.},
	author = {Roberta A. Sinoara and Jose Camacho-Collados and Rafael G. Rossi and Roberto Navigli and Solange O. Rezende},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.10.026},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Semantic representation, Document embeddings, Text classification, Text mining},
	pages = {955-971},
	title = {Knowledge-enhanced document embeddings for text classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118305124},
	volume = {163},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118305124},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.10.026}}

@article{XU2020106391,
	abstract = {Rumors can be propagated across online microblogs at a relatively low cost, but result in a series of major problems in our society. Traditional rumor detection approaches focus on exploring various propagation patterns or data interactions between a source microblog and its subsequent reactions. It is obvious that this causes missing interaction on rumor detection, especially in the absence of retweets or reactions. According to the communication theory of Allport and Postman (1947), Chorus (1953) and Rosnow (1988), the topic of a post can help determine its potential of being a rumor or not. Therefore, we develop a novel topic-driven rumor detection (TDRD) framework to determine whether a post is a rumor only according to its source microblog. Specifically, we first automatically perform topic classification on source microblogs, and then we successfully incorporate the predicted topic vector of the source microblogs into rumor detection. Our extensive experimental results demonstrate that our TDRD significantly outperforms state-of-the-art methods on both two English and two Chinese benchmark datasets.},
	author = {Fan Xu and Victor S. Sheng and Mingwen Wang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106391},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Rumor detection, Topic vector, Latent dirichlet allocation (LDA), Source microblogs},
	pages = {106391},
	title = {Near real-time topic-driven rumor detection in source microblogs},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120305281},
	volume = {207},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120305281},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106391}}

@article{ZHENG2018200,
	abstract = {With the increasing amount of Linked Data on the Web, large numbers of linked entities often make it difficult for users to find the entities of interest quickly for further exploration. Clustering as a fundamental approach, has been adopted to organize entities into meaningful groups. In general, link and entity class are semantically labelled and can be used to group linked entities. However, entities are usually associated with many links and classes. To avoid information overload, we propose a novel hierarchical co-clustering approach to simultaneously group links and entity classes. In our approach, we define a measure of intra-link similarity and intra-class similarity respectively, and then incorporate them into co-clustering. Our proposed approach is implemented in a Linked Data browser called CoClus. We compare it with other three browsers by conducting a task-based user study and the experimental results show that our approach provides useful support for entity exploration. We also compare our algorithm with three baseline co-clustering algorithms and the experimental results indicate that it outperforms baselines in terms of the Clustering Index score.},
	author = {Liang Zheng and Yuzhong Qu and Xinqi Qian and Gong Cheng},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2017.11.017},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Linked Data, Entity exploration, Hierarchical co-clustering},
	pages = {200-210},
	title = {A hierarchical co-clustering approach for entity exploration over Linked Data},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705117305397},
	volume = {141},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705117305397},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2017.11.017}}

@article{DONG2022108954,
	abstract = {A significant remaining challenge for existing recommender systems is that users may not trust recommender systems for either inaccurate recommendation or lack of explanation. Thus, it becomes critical to embrace a trustworthy recommender system. This survey provides a systematic summary of three categories of trust issues in recommender systems: social-aware recommender systems, which leverage users' social trust relationships; robust recommender systems, which filter untruthful information, noises and enhance attack resistance; and explainable recommender systems, which provide explanations of the recommended items. We focus on the work based on deep learning techniques, which is an emerging area in the recommendation research.},
	author = {Manqing Dong and Feng Yuan and Lina Yao and Xianzhi Wang and Xiwei Xu and Liming Zhu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.108954},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender systems, Deep learning, Systematic survey},
	pages = {108954},
	title = {A survey for trust-aware recommender systems: A deep learning perspective},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122004622},
	volume = {249},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122004622},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.108954}}

@article{YIN201868,
	abstract = {With the rapid prevalence of smart mobile devices and the dramatic proliferation of mobile applications (Apps), App recommendation becomes an emergent task that will benefit different stockholders of mobile App ecosystems. However, the extreme sparsity of user-App matrix and many newly emerging Apps create severe challenges, causing CF-based methods to degrade significantly in their recommendation performance. Besides, unlike traditional items, Apps have rights to access users' personal resources (e.g., location, message and contact) which may lead to security risk or privacy leak. Thus, users' choosing of Apps are influenced by not only their personal interests but also their privacy preferences. Moreover, user privacy preferences vary with App categories. In light of the above challenges, we propose a mobile sparse additive generative model (Mobi-SAGE) to recommend Apps by considering both user interests and category-aware user privacy preferences in this paper. To overcome the challenges from data sparsity and cold start, Mobi-SAGE exploits both textual and visual content associated with Apps to learn multi-view topics for user interest modeling. We collected a large-scale and real-world dataset from 360 App store - the biggest Android App platform in China, and conducted extensive experiments on it. The experimental results demonstrate that our Mobi-SAGE consistently and significantly outperforms the other existing state-of-the-art methods, which implies the importance of exploiting category-aware user privacy preferences and the multi-modal App content data on personalized App recommendation.},
	author = {Hongzhi Yin and Weiqing Wang and Liang Chen and Xingzhong Du and Quoc Viet {Hung Nguyen} and Zi Huang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.05.028},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Recommender system, Mobile applications, User modeling, Privacy, Sparse additive generative model, Cold start},
	pages = {68-80},
	title = {Mobi-SAGE-RS: A sparse additive generative model-based mobile application recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118302491},
	volume = {157},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118302491},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.05.028}}

@article{YANG2019106,
	abstract = {Community question answering (CQA) is an important but challenging task. Meantime, as the theory of deep learning develops, remarkable progress has been made by deep neural networks. This paper studies an advanced deep neural network that not only uses external knowledge to learn better representations of questions and answers but also improves representation learning by considering question categorization as an auxiliary task. Specifically, we propose a novel Multi-task and Knowledge enhanced Multi-head Interactive Attention network for Community Question Answering (MKMIA-CQA). It contains a document modeling module responsible for utilizing external commonsense knowledge to help identify background information (entity mentions and their relations) and filter out noise information from the long text which has complicated semantic and syntactic structures. Moreover, the model is trained in a multi-task manner. It regards community question answering as the primary task and question categorization as the auxiliary task, which aims to learn a category-aware encoder and improve the quality of locating the salient information of a long question. The experimental results on three widely used CQA datasets demonstrate that our model achieves impressive results compared to other strong competitors.},
	author = {Min Yang and Wenting Tu and Qiang Qu and Wei Zhou and Qiao Liu and Jia Zhu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2019.02.006},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Community question answering, Knowledge base, Multitask learning, Multi-head attention, Interactive attention},
	pages = {106-119},
	title = {Advanced community question answering by leveraging external knowledge and multi-task learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119300516},
	volume = {171},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705119300516},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2019.02.006}}

@article{ZOU2022107927,
	abstract = {The recent deep cross-modal hashing (DCMH) has achieved superior performance in effective and efficient cross-modal retrieval and thus has drawn increasing attention. Nevertheless, there are still two limitations for most existing DCMH methods: (1) single labels are usually leveraged to measure the semantic similarity of cross-modal pairwise instances while neglecting that many cross-modal datasets contain abundant semantic information among multi-labels. (2) several DCMH methods utilized the multi-labels to supervise the learning of hash functions. Nevertheless, the feature space of multi-labels suffers the weakness of sparse, resulting in sub-optimization for the hash functions learning. Thus, this paper proposed a multi-label modality enhanced attention-based self-supervised deep cross-modal hashing (MMACH) framework. Specifically, a multi-label modality enhanced attention module is designed to integrate the significant features from cross-modal data into multi-labels feature representations, aiming to improve its completion. Moreover, a multi-label cross-modal triplet loss is defined based on the criterion that the feature representations of cross-modal pairwise instances with more common categories should preserve higher semantic similarity than other instances. To the best of our knowledge, the multi-label cross-modal triplet loss is the first time designed for cross-modal retrieval. Extensive experiments on four multi-label cross-modal datasets demonstrate the effectiveness and efficiency of our proposed MMACH. Moreover, the MMACH also achieved superior performance and outperformed several state-of-the-art methods on the task of cross-modal retrieval. The source code of MMACH is available at https://github.com/SWU-CS-MediaLab/MMACH.},
	author = {Xitao Zou and Song Wu and Nian Zhang and Erwin M. Bakker},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107927},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Deep cross-modal hashing, Attention mechanism, Multi-label semantic learning},
	pages = {107927},
	title = {Multi-label modality enhanced attention based self-supervised deep cross-modal hashing},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512101073X},
	volume = {239},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512101073X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107927}}

@article{LIU2020105918,
	abstract = {Email data has unique characteristics, involving multiple topics, lengthy replies, formal language, high variance in length, high duplication, anomalies, and indirect relationships that distinguish it from other social media data. In order to better model Email documents and to capture complex sentiment structures in the content, we develop a framework for document-level multi-topic sentiment classification of Email data. Note that, a large volume of labeled Email data is rarely publicly available. We introduce an optional data augmentation process to increase the size of datasets with synthetically labeled data to reduce the probability of overfitting and underfitting during the training process. To generate segments with topic embeddings and topic weighting vectors as inputs for our proposed model, we apply both latent Dirichlet allocation topic modeling and semantic text segmentation to post-process Email documents. Empirical results obtained with multiple sets of experiments, including performance comparison against various state-of-the-art algorithms with and without data augmentation and diverse parameter settings, are analyzed to demonstrate the effectiveness of our proposed framework.},
	author = {Sisi Liu and Kyungmi Lee and Ickjai Lee},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105918},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment classification, Email sentiment, Multi-topic sentiment, Bidirectional LSTM, Data augmentation},
	pages = {105918},
	title = {Document-level multi-topic sentiment classification of Email data with BiLSTM and data augmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120302574},
	volume = {197},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120302574},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105918}}

@article{DHELIM2020106227,
	abstract = {With the emergence of online social networks and microblogging websites, user interest mining has been an active research topic for the past few years. However, most of the existing works suffer from two significant drawbacks, firstly, they focus on the user's explicit content and social network structure to predicate the user's interests, neglecting the fact that the user's personality might be a rich source to infer the topical interests. Secondly, they represent the user's content using the bag-of-words model that ignores the chronological order of the posted content, hence the predicted interests might contain outdated topics that the user does not interest anymore. In this paper, we propose a novel user interest mining system based on Big Five personality traits and dynamic interests. To prove the effectiveness of incorporating the user's personality traits in the interest mining process, we have implemented a social network for news sharing and conducted different experiments on the collected data. The experiment results show that considering personality traits can increase the precision and recall of interest mining systems, as well as can help to tackle the cold start problem.},
	author = {Sahraoui Dhelim and Nyothiri Aung and Huansheng Ning},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106227},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Interest mining, Personality computing, User modeling, Interest graph, Personality traits, Big five, User interest, Social computing},
	pages = {106227},
	title = {Mining user interest based on personality-aware hybrid filtering in social networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120304354},
	volume = {206},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120304354},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106227}}

@article{CHEN2020105546,
	abstract = {In order to facilitate human decision making, trust evaluation has received widespread attention in many fields, especially for online services. Most of the existing methods consider trust in a person as a value which does not vary across different scenarios without any attention to the distinction of domains or communities where trust is derived. However, the notion of context is a significant and indispensable factor for trust evaluation in practice. Due to the lack of the consideration of context, traditional methods cannot resolve the issue that arises when a highly trustworthy person in one domain is likely to dominate the results of trust assessment in others where the person is in fact less authoritative. To solve this problem, in this paper, we develop a general approach to accomplish topic-sensitive trust evaluation by considering the context of trust. We first propose a general framework which presents the well-organized architecture of topic-sensitive trust evaluation in online communities. Then, a user-topic model is proposed to automatically extract topic data from user-generated content based on the Labeled Latent Dirichlet Allocation (LLDA) model. To compare the topic differences between users, we design a topic coverage function for revealing their trust relationships in diverse topics. Moreover, we employ two traditional methods and extend them to accomplish trust prediction for people with multiple domain knowledge. Experiments based on a real-world dataset show that extended topic-sensitive approaches are more adaptive and accurate than those topic-free trust evaluation approaches, especially when the trust application scenario features multiple topics.},
	author = {Xu Chen and Yuyu Yuan and Mehmet Ali Orgun and Lilei Lu},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.105546},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Topic-sensitive analysis, Trust evaluation, Trust propagation, Context-dependency, Labeled LDA},
	pages = {105546},
	title = {A topic-sensitive trust evaluation approach for users in online communities},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120300435},
	volume = {194},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705120300435},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.105546}}

@article{LOPEZ2021107455,
	abstract = {Opinion summarisation is concerned with generating structured summaries of multiple opinions in order to provide insightful knowledge to end users. We present the Aspect Discovery for OPinion Summarisation (ADOPS) methodology, which is aimed at generating explainable and structured opinion summaries. ADOPS is built upon aspect-based sentiment analysis methods based on deep learning and Subgroup Discovery techniques. The resultant opinion summaries are presented as interesting rules, which summarise in explainable terms for humans the state of the opinion about the aspects of a specific entity. We annotate and release a new dataset of opinions about a single entity on the restaurant review domain for assessing the ADOPS methodology, and we call it ORCo. The results show that ADOPS is able to generate interesting rules with high values of support and confidence, which provide explainable and insightful knowledge about the state of the opinion of a certain entity.},
	author = {Miguel L{\'o}pez and Eugenio Mart{\'\i}nez-C{\'a}mara and M. Victoria Luz{\'o}n and Francisco Herrera},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2021.107455},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Opinion summarisation, Deep learning, Aspect extraction, Subgroup discovery, Interesting rules},
	pages = {107455},
	title = {ADOPS: Aspect Discovery OPinion Summarisation Methodology based on deep learning and subgroup discovery for generating explainable opinion summaries},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121007176},
	volume = {231},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705121007176},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107455}}

@article{ASDAGHI2019198,
	abstract = {Web spam is an illegal and immoral way to increase the ranking of web pages by deceiving search engine algorithms. Therefore, different methods have been proposed to detect and improve the quality of results. Since a web page can be viewed from two aspects of the content and the link, the number of extracting features is high. Thus, selection of features with high separating ability can be considered as a preprocessing step in order to decrease computational time and cost. In this study, a new backward elimination approach is proposed for feature selection. The main idea of this method is measuring the impact of eliminating a set of features on the performance of a classifier instead of a single feature which is similar to the sequential backward selection. This method seeks for the largest feature subset that their omission from whole set features not only reduces the efficiency of the classifier but also improves it. Implementations on WEBSPAM-UK2007 dataset with Na{\"\i}ve Bayes classifier show that the proposed method selects fewer features in comparison with other methods and improves the performance of the classifier in the IBA index about 7%.},
	author = {Faeze Asdaghi and Ali Soleimani},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.12.026},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Web spam, Feature selection, Content-based features, Link-based features, Unbalanced data, Index of balanced accuracy (IBA)},
	pages = {198-206},
	title = {An effective feature selection method for web spam detection},
	url = {https://www.sciencedirect.com/science/article/pii/S095070511830621X},
	volume = {166},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070511830621X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.12.026}}

@article{JUNG2022110020,
	abstract = {The shared interest among existing research topics matures over time until it emerges as a topic of its own. This paper detects emerging topics as well as general predictor models spanning multiple research domains through the network-based topic evolution approach, which offers additional topic evolution capabilities such as extrapolation of data and separation of topic transition and correlation. Topics are represented as their neighbors in the past, or ancestors, and their structural properties are used to train binary classification models in capturing the materialization of such topics. The entirety of 197 million publications within the Microsoft Academic Graph was used to build multiple datasets, where machine learning algorithms were trained with structural features resulting in over 0.98 area under the precision--recall curve. General topic emergence predictor equations are then proposed based on the models trained specifically for each domain, which were able to capture a common pattern shared by emerging topics in general.},
	author = {Sukhwan Jung and Aviv Segev},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2022.110020},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Topic prediction, Scientometrics, Knowledge management, Machine learning},
	pages = {110020},
	title = {Identifying a common pattern within ancestors of emerging topics for pan-domain topic emergence prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122011133},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705122011133},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2022.110020}}

@article{KUNDU2021106535,
	abstract = {Here, we propose a topic sensitive hybrid expertise retrieval system in community question answering services. We introduce three new expertise signatures: knowledge, reputation, and authority. These signatures consider the questions, and hence, their answerers from a topic sensitive perspective. We estimate the knowledge of an answerer on a new question based on the previously answered subset of questions with similar topic distributions to the new question. The reputation of an answerer, moreover, is derived from the qualities of previously answered questions by the answerer with similar distributions of topics. Furthermore, we propose a topic sensitive authority model. It considers some topic related information associated with questions and the relationships among their answerers. We compare the proposed method with 26 existing methods on 4 real-world datasets using 5 performance measures. It outperforms the comparing algorithms in 91.73% (477 out of 520) cases.},
	author = {Dipankar Kundu and Rajat Kumar Pal and Deba Prasad Mandal},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2020.106535},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Community question answering, Expertise retrieval, Social network analysis, Topic sensitive model},
	pages = {106535},
	title = {Topic sensitive hybrid expertise retrieval system in community question answering services},
	url = {https://www.sciencedirect.com/science/article/pii/S095070512030664X},
	volume = {211},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095070512030664X},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2020.106535}}

@article{FU201843,
	abstract = {Topic sentiment joint model aims to deal with the problem about the mixture of topics and sentiment simultaneously from online reviews. Most of existing topic sentiment modeling algorithms are mainly based on the state-of-art latent Dirichlet allocation (LDA) and probabilistic latent semantic analysis (PLSA), which infer sentiment and topic distributions from the co-occurrence of words. These methods have been proposed and successfully used for topic and sentiment analysis. However, when the training corpus is small or when the documents are short, the textual features become sparse, so that the results of the sentiment and topic distributions might be not very satisfied. In this paper, we propose a novel topic sentiment joint model called weakly supervised topic sentiment joint model with word embeddings (WS-TSWE), which incorporates word embeddings and HowNet lexicon simultaneously to improve the topic identification and sentiment recognition. The main contributions of WS-TSWE include the following two aspects. (1) Existing models generate the words only from the sentiment-topic-to-word Dirichlet multinomial component, but the WS-TSWE model replaces it with a mixture of two components, a Dirichlet multinomial component and a word embeddings component. Since the word embeddings are trained on a very large corpora and can be used to extend the semantic information of the words, they can provide a certain solution for the problem of the textual sparse. (2) Most of previous models incorporate sentiment knowledge in the  priors. And the priors are usually set from a dictionary and completely rely on previous domain knowledge to identify positive and negative words. In contrast, the WS-TSWE model calculates the sentiment orientation of each word with the HowNet lexicon and automatically infers sentiment-based  priors for sentiment analysis and opinion mining. Furthermore, we implement WS-TSWE with Gibbs sampling algorithms. The experimental results on Chinese and English data sets show that WS-TSWE achieved significant performance in the task of detecting sentiment and topics simultaneously.},
	author = {Xianghua Fu and Xudong Sun and Haiying Wu and Laizhong Cui and Joshua Zhexue Huang},
	date-added = {2022-10-23 18:23:48 +0200},
	date-modified = {2022-10-23 18:23:48 +0200},
	doi = {https://doi.org/10.1016/j.knosys.2018.02.012},
	issn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Sentiment analysis, Topic model, Topic sentiment joint model, Word embeddings},
	pages = {43-54},
	title = {Weakly supervised topic sentiment joint model with word embeddings},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705118300649},
	volume = {147},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0950705118300649},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2018.02.012}}

@article{CHUA2019113142,
	abstract = {Social media is increasingly being used for communication by individuals and organizations. Social media stores vast amounts of publicly available data that provides a rich source of information and insights. Often, social media users can easily infer meaning from short text such as microblogs and Facebook posts because they understand the context and terminology used. Although automated data-mining can be effective for gaining insights from text data, a significant challenge is to accurately infer meaning from social media text derived from a single social media account. This is difficult because social media communication uses very short, or sparse, text, which yields a relatively small sample of usable words for analysis. Furthermore, interpreting the contextual meaning from a relatively small set of words is challenging. This research proposes a methodology for extracting semantic lexical chains from frequently occurring words in a single social media account and using these chains to mine short text structures to infer the overall themes of the user. The methodology is based on a proposed clustering algorithm and illustrated with examples from Facebook posts. The algorithm is tested and illustrated by comparing it to existing work and further applying it to a variety of news posts. This methodology could be useful for gaining decision-making insights from social media, or other online forms with short or sparse text.},
	author = {Cecil Eng Huang Chua and Veda C. Storey and Xiaolin Li and Mala Kaul},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113142},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Social media, Semantic clustering, Lexical chain, Short text, Text mining, Word sense disambiguation},
	pages = {113142},
	title = {Developing insights from social media using semantic lexical chains to mine short text structures},
	url = {https://www.sciencedirect.com/science/article/pii/S016792361930171X},
	volume = {127},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792361930171X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113142}}

@article{HOLSAPPLE201832,
	abstract = {A substantial portion of internet usage today involves social media applications. Aside from personal use, given the vast amount of content stored, and rapid diffusion of information, in social media, businesses have begun exploiting social media for competitive advantage. Its popularity has led to the recognition of Social Media Analytics (SMA) as a distinct, albeit formative, sub-field within the Analytics field. Against this backdrop, we examine available characterizations of SMA that collectively identify various considerations of interest. However, their diversity suggests the need for adopting a concise, unifying SMA definition. We present a definition that subsumes salient aspects of existing characterizations and incorporates novel features of interest to Business SMA. Further, we examine available conceptual frameworks for Business SMA and advance a framework that comprehensively models the Business SMA phenomenon. We also conduct a survey of recently published SMA research in the premier, academic Management Information Systems journals and use some of the surveyed papers to validate our framework.},
	author = {Clyde W. Holsapple and Shih-Hui Hsiao and Ram Pakath},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2018.03.004},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Analytics, Business social media analytics, Conceptual framework, Social media, Social media analytics},
	pages = {32-45},
	title = {Business social media analytics: Characterization and conceptual framework},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923618300502},
	volume = {110},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923618300502},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2018.03.004}}

@article{NAM2019100,
	abstract = {With the advent of the Big Data era and the development of machine learning technologies, predicting stock movements by analyzing news articles, which are unstructured data, has been studied actively. However, so far no attempts have been made to utilize the asymmetric relationship of firms. Thus far, most papers focus on only the target firm, and few papers focus on the target firm and relevant firms together. In this article, we propose a novel machine learning model to forecast stock price movement based on the financial news considering causality. Specifically, our method analyzes the causal relationship between companies, and it accounts for the directional impact within the Global Industry Classification Standard sectors. In our proposed method, transfer entropy is used to find causality, and multiple kernel learning is used to combine features of target firm and causal firms. Based on a Korean market dataset and out-of-sample test, our experimental results reveal that the proposed causal analytic-based framework outperforms two traditional state-of-the-art algorithms. Furthermore, the experimental results show that the proposed method can predict the stock price directional movements even when there is no financial news on the target firm, but financial news is published on causal firms. Our findings reveal that identifying causal relationship is important in prediction problems, and we suggest that it is important to develop machine learning algorithms and it is also important to find connections with well-established theories such as the complex system theory.},
	author = {KiHwan Nam and NohYoon Seong},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2018.11.004},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Stock movement prediction, Transfer entropy, Causal relationship, Multiple kernel learning, Text mining},
	pages = {100-112},
	title = {Financial news-based stock movement prediction using causality analysis of influence in the Korean stock market},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923618301957},
	volume = {117},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923618301957},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2018.11.004}}

@article{XU2021113525,
	abstract = {The online review systems of digital platforms feature a variety of designs. This study examines how closed-form evaluations and open-ended textual comment options affect customers' online review writing behaviors and reflect satisfaction with hotels. We find that direct and spillover effects exist. These refer to the effects of customers' closed-form evaluations of a product or service attribute on their reviews of the same attribute and other attributes in their open-ended comments, respectively. Regarding the direct effect, positive closed-form evaluations of attributes reduce customers' behaviors of writing details about the same attributes in open-ended comments, but negative closed-form evaluations of attributes increase that behavior. The increasing effect is greater than the reducing effect. Regarding the spillover effect, closed-form evaluations of certain attributes increase customers' behaviors of writing details about other attributes in open-ended comments. Additionally, we find that the heterogeneity effect lies in the different roles of closed-form evaluations and open-ended textual comments in reflecting customers' overall satisfaction. Open-ended comments about the advantages of the attributes have a more significant effect than the closed-form evaluations of the same attributes in reflecting overall customer satisfaction, but closed-form evaluations better reflect customers' low overall satisfaction with the disadvantages of the attributes. The direct, spillover, and heterogeneity effects on customers' evaluations and comments regarding independent hotels are higher than those effects on evaluations and comments regarding chain hotels. The findings provide insights for companies to design review systems, understand customer perceptions, and use the positive electronic word-of-mouth effect.},
	author = {Xun Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113525},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Online customer reviews, Review systems, Closed-form evaluation, Textual comments, Customer satisfaction},
	pages = {113525},
	title = {Closed-form evaluations and open-ended comment options: How do they affect customer online review behavior and reflect satisfaction with hotels?},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362100035X},
	volume = {145},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362100035X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113525}}

@article{POURNARAKIS201798,
	abstract = {The proliferation of Big Data & Analytics in recent years has compelled marketing practitioners to search for new methods when faced with assessing brand performance during brand equity appraisal. One of the challenges of current practices is that these methods rely heavily on traditional data collection and analysis methods such as questionnaires, and face to face or telephone interviews, which have a significant time lag. In this paper we introduce a computational model that combines topic and sentiment classification to elicit influential subjects from consumer perceptions in social media. Our model devises a novel genetic algorithm to improve clustering of tweets in semantically coherent groups, which act as an essential prerequisite when searching for prevailing topics and sentiment in big pools of data. To illustrate the validity of our model, we apply it to the Uber transportation network, from data collected through Twitter for the period between January and April 2015. The results obtained present consumer perceptions and produce insights for two fundamental brand equity dimensions: brand awareness and brand meaning. Simultaneously, they improve clustering results, in comparison to the k-means approach.},
	author = {Demitrios E. Pournarakis and Dionisios N. Sotiropoulos and George M. Giaglis},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2016.09.018},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Social media, Big data, Consumer perceptions},
	pages = {98-110},
	title = {A computational model for mining consumer perceptions in social media},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923616301671},
	volume = {93},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923616301671},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2016.09.018}}

@article{CHEN201896,
	abstract = {With the rapid development of e-commerce, a new type of secondhand e-commerce website has appeared in recent years. Any user can have his or her own shop and list superfluous items for sale online without much supervision. These secondhand e-commerce platforms maximize the economic value of secondhand markets online, but buyers risk conducting unpleasant transactions with low-reputation sellers. The main contribution of our research is the design of a text analytics framework to assess secondhand sellers' reputation. In addition, we develop a new aspect-extraction method that combines the results of domain ontology and topic modeling to extract topical features from product descriptions. We conduct our experiments based on a real-word dataset crawled from XianYu. The experimental results reveal that our ontology-based topic model method outperforms a traditional topic model method. Furthermore, the proposed framework performs well in different item categories. The managerial implication of our research is that potential buyers can prejudge the reputation of secondhand sellers when making purchase decisions. The results can support a more effective development of online secondhand markets.},
	author = {Runyu Chen and Yitong Zheng and Wei Xu and Minghao Liu and Jiayue Wang},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2018.02.008},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Secondhand e-commerce, Reputation assessment, Text analytics, Aspect extraction},
	pages = {96-106},
	title = {Secondhand seller reputation in online markets: A text analytics framework},
	url = {https://www.sciencedirect.com/science/article/pii/S016792361830037X},
	volume = {108},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792361830037X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2018.02.008}}

@article{KAZMAIER2020113304,
	abstract = {The increased exposure of the average citizen and customer to polarised content from various sources has been of significant consequence for companies and governmental organisations. Such content has, for example, served as a catalyst for violent uprisings and shifts in stock market prices. The collection and study of opinion have therefore become a necessity in many industries. Due to the vast nature of such data, manual approaches to this problem are no longer feasible. Several computational approaches have been proposed within the field of sentiment analysis, which successfully address many aspects of this problem, such as the classification of data into one of several sentiment categories. The research in the field is lacking, however, with respect to the integration and application of these techniques in practice, as well as their incorporation into the decision-making process of affected entities. In this paper, a generic framework for sentiment analysis is proposed, with a focus on facilitating the model development process for a user in a manner such that good performance may be achieved irrespective of the problem domain, as well as facilitating a flexible, exploratory analysis of model results in combination with existing structured attributes in order to gain actionable insights. The objective of the framework is to aid organisations in successfully leveraging unstructured, opinion-bearing data in combination with structured data sources to inform decision making.},
	author = {Jacqueline Kazmaier and Jan H. {van Vuuren}},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113304},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Sentiment analysis, Machine learning, Natural language processing, Decision Support Systems},
	pages = {113304},
	title = {A generic framework for sentiment analysis: Leveraging opinion-bearing data to inform decision making},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620300592},
	volume = {135},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620300592},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113304}}

@article{WANG201887,
	abstract = {Automobile insurance fraud represents a pivotal percentage of property insurance companies' costs and affects the companies' pricing strategies and social economic benefits in the long term. Automobile insurance fraud detection has become critically important for reducing the costs of insurance companies. Previous studies on automobile insurance fraud detection examined various numeric factors, such as the time of the claim and the brand of the insured car. However, the textual information in the claims has rarely been studied to analyze insurance fraud. This paper proposes a novel deep learning model for automobile insurance fraud detection that uses Latent Dirichlet Allocation (LDA)-based text analytics. In our proposed method, LDA is first used to extract the text features hiding in the text descriptions of the accidents appearing in the claims, and deep neural networks then are trained on the data, which include the text features and traditional numeric features for detecting fraudulent claims. Based on the real-world insurance fraud dataset, our experimental results reveal that the proposed text analytics-based framework outperforms a traditional one. Furthermore, the experimental results show that the deep neural networks outperform widely used machine learning models, such as random forests and support vector machine. Therefore, our proposed framework that combines deep neural networks and LDA is a suitable potential tool for automobile insurance fraud detection.},
	author = {Yibo Wang and Wei Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2017.11.001},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Insurance fraud, Fraud detection, Text analytics, Topic modeling, Deep learning},
	pages = {87-95},
	title = {Leveraging deep learning with LDA-based text analytics to detect automobile insurance fraud},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923617302130},
	volume = {105},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923617302130},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2017.11.001}}

@article{EIRASFRANCO2019113141,
	abstract = {Gaining relevant insight from a dyadic dataset, which describes interactions between two entities, is an open problem that has sparked the interest of researchers and industry data scientists alike. However, the existing methods have poor explainability, a quality that is becoming essential in certain applications. We describe an explainable and scalable method that, operating on dyadic datasets, obtains an easily interpretable high-level summary of the relationship between entities. To do this, we propose a quality measure, which can be configured to a level that suits the user, that factors in the explainability of the model. We report experiments that confirm better results for the proposed method over alternatives, in terms of both explainability and accuracy. We also analyse the method's capacity to extract relevant actionable information and to handle large datasets.},
	author = {Carlos Eiras-Franco and Bertha Guijarro-Berdi{\~n}as and Amparo Alonso-Betanzos and Antonio Bahamonde},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113141},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Dyadic data, Machine learning, Interpretable machine learning, Explainable artificial intelligence, Scalable machine learning},
	pages = {113141},
	title = {A scalable decision-tree-based method to explain interactions in dyadic data},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619301708},
	volume = {127},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619301708},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113141}}

@article{DUTTA2022113662,
	abstract = {Text databases have grown tremendously in number, size, and volume over the last few decades. Optical Character Recognition (OCR) software is used to scan the text and make them available in online repositories. The OCR transcription process is often not accurate resulting in large volumes of garbled text in the repositories. Spell correction and other post-processing of OCR text often prove to be very expensive and time-consuming. While it is possible to rely on the OCR model to assess the quality of text in a corpus, many natural language processing and information retrieval tasks prefer the extrinsic evaluation of the effect of noise on the task at hand. This paper examines the effect of noise on the unsupervised ranking of person name entities by first populating a list of person names using an out-of-the-box Named Entity Recognition (NER) software, extracting content-based features for the identified entities, and ranking them using a novel unsupervised Kernel Density Estimation (KDE) based ranking algorithm. This generative model has the ability to learn rankings using the data distribution and therefore requires limited manual intervention. Empirical results are presented on a carefully curated parallel corpus of OCR and clean text and ``in the wild'' using a large real-world corpus. Experiments on the parallel corpus reveals that even with a reasonable degree of noise in the dataset, it is possible to generate ranked lists using the KDE algorithm with a high degree of precision and recall. Furthermore, since the KDE algorithm has comparable performance to state-of-the-art unsupervised rankers, using it on real-world corpora is feasible. The paper concludes by reflecting on other methods for enhancing the performance of the unsupervised algorithm on OCR text such as cleaning entity names, disambiguating names concatenated to one another and correcting OCR errors that are statistically significant in the corpus.},
	author = {Haimonti Dutta and Aayushee Gupta},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113662},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Unsupervised ranking, Kernel density estimation, OCR noise, Named entity recognition},
	pages = {113662},
	title = {PNRank: Unsupervised ranking of person name entities from noisy OCR text},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362100172X},
	volume = {152},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362100172X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113662}}

@article{CHENG2022113864,
	abstract = {Priming is challenging when consumers start shortlisting products before the final purchase. This is because this shortlisting process is performed in multiple user sessions online across time, the shortlist does not stay as a static list, and product comparison in this stage uses the heuristics internal to individual consumers. The goal of this study is two folds: (1) to approximate user heuristics after B&B product shortlisting using NLP and deep learning techniques, and (2) to identify optimized deep learning models for the representation of key elements of consumer heuristics. This offers an extension of the priming theory into product comparison and shortlisting stages that were traditionally difficult for marketers to tap into. By analyzing the B&B product information repeated visited in user sessions, the formation of shortlists is identified and products in the shortlists can then be compared. Subsequent priming and promotions can therefore be performed closer to the actual purchase. Our study also provides marketers keywords and their associated activated words relevant for crafting marketing messages. As these activation words are extracted from the B&B sites and product reviews that the users had visited repeatedly in long-term tracking sessions, they are analogous to effects produced from user participatory design, an approach popular in the IT world. Our work shows that opportunities for marketing decision support, especially into the shortlisting phase, are now possible through machine learning techniques. Both theoretical and practical implications are provided.},
	author = {Li-Chen Cheng and Kuanchin Chen},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113864},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Text mining, Session-based recommendation, Priming theory, -commerce},
	note = {Business and Government Applications of Text Mining & Natural Language Processing (NLP) for Societal Benefit},
	pages = {113864},
	title = {Mining longitudinal user sessions with deep learning to extend the boundary of consumer priming},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362200135X},
	volume = {162},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362200135X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113864}}

@article{LI2022113863,
	abstract = {Extracting useful knowledge from multimodal data is the core of many multimedia applications, such as recommendation systems, and cross-modal retrieval. In this paper, we propose a label-based multimodal topic (LB-MMT) model to jointly model text and image data tagged with multiple labels. Specifically, we use the labels as supervised information to generate the text and image data. In the LB-MMT model, we assume that the textual words and visual words related to each text and image are drawn from a mixture of latent topics, where each topic is represented as a group of textual words and visual words. Moreover, we introduce multiple topics for each label, to build the top-down relationship from label to text and image. To investigate the effectiveness of the proposed approach, we conduct extensive experiments on a real-world multimodal dataset with labels. The results show the proposed approach obtains superior performances on topic coherence and label prediction compared with previous competitors. In addition, we show that our model yields interesting insights about multimodal topics. The proposed model provides important practical implications, e.g., designing more attractive multimodal contents for marketers.},
	author = {Hao Li and Yang Qian and Yuanchun Jiang and Yezheng Liu and Fan Zhou},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113863},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Multimodal data, Topic modeling, Label data, Supervised model, Image representation},
	pages = {113863},
	title = {A novel label-based multimodal topic model for social media analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622001348},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622001348},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113863}}

@article{BISWAS2022113651,
	abstract = {Online hacker communities are meeting spots for aspiring and seasoned cybercriminals where they engage in technical discussions, share exploits and relevant hacking tools to be used in launching cyber-attacks on business organizations. Sometimes, the affected organizations can detect these attacks in advance, with the help of cyber-threat intelligence derived from the explicit and implicit features of hacker communication in these forums. Herein, we proposed a novel text-mining based cyber-risk assessment and mitigation framework, which performs the following critical tasks. (i) Cyber-risk Assessment - to identify hacker expertise (i.e., newbie, beginner, intermediate, and advanced) using explicit and implicit features applying various classification algorithms. Among these features, cybersecurity keywords, sharing of attachments, and sentiments emerged as significant. Further, we found that expert hackers demonstrate leadership in the online forums that eventually serve as communities of practice. Consequently, novice hackers gradually develop their cyber-attack skills through prolonged observations, interactions, and external influences in this social learning process. (ii) Cyber-risk mitigation -- computes financial impact for every {hacker expertise, attack-type} combination, and then by ranking them on a {likelihood, impact} decision-matrix to prioritize mitigation strategies in affected organizations. Through these novel recommendations, our framework can guide managers to decide on appropriate cybersecurity controls using an {expected loss, probability, attack-type, hacker expertise} metric against financial losses due to cyber-attacks.},
	author = {Baidyanath Biswas and Arunabha Mukhopadhyay and Sudip Bhattacharjee and Ajay Kumar and Dursun Delen},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113651},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Information security, Cyber risks, Hacker forum, Machine learning, Sentiment analysis},
	pages = {113651},
	title = {A text-mining based cyber-risk assessment and mitigation framework for critical analysis of online hacker forums},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923621001615},
	volume = {152},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923621001615},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113651}}

@article{ROEDER2022113770,
	abstract = {Other than banks, non-financial companies also continuously monitor and analyze their credit risk exposure to avoid possible counterparty defaults. Credit default swaps are commonly used financial instruments that provide information on a counterparty's creditworthiness. Although this metric can provide crucial insights, the underlying price dynamics often remain unknown and require further explanation. Data-driven decision-making is a key concept for identifying these reasons and supporting and justifying decisions. In this paper, we provide such justifications by applying sentiment and topic analysis to company-related financial analyst reports. While the contents of financial news have been analyzed in the past, analyst reports can offer additional insights, as seasoned analysts use them to disseminate in-depth research to experienced investors. This analysis examines 3386 analyst reports covering constituents of the Dow Jones Industrial Average Index in the period from 2009 to 2020. The results suggest that even when established credit risk indicators and financial news are considered, the sentiment and a subset of topics are correlated with changes in the credit default swap spread, indicating a fundamental relationship between quantitative risk metric and analyst reports. We find that analyst reports contain information related to the change in credit default swap spreads, an insight that helps to improve our understanding of existing risk assessments. The outcome indicates that banks or corporate risk managers can benefit from complementing established financial metrics and even financial news data with new insights derived from analyst reports.},
	author = {Jan Roeder and Matthias Palmer and Jan Muntermann},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113770},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Credit risk, Data-driven decision-making, Unstructured data, Text mining, Sentiment analysis, Topic mining},
	pages = {113770},
	title = {Data-driven decision-making in credit risk management: The information value of analyst reports},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000410},
	volume = {158},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000410},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113770}}

@article{SINGH201781,
	abstract = {Online product reviews have been shown to be a viable source of information for helping customers make informed purchasing decisions. In many cases, users of online shopping platforms have the ability to rate products on a numerical scale, and also provide textual feedback pertaining to a purchased product. Beyond using online product review platforms as customer decision support systems, this information rich data source could also aid designers seeking to increase the chances of their products being successful in the market through a deeper understanding of market needs. However, the increasing size and complexity of products on the market makes manual analysis of such data challenging. Information obtained from such sources, if not mined correctly, risks misrepresenting a product's true success/failure (e.g., a customer leaves a one star rating because of the slow shipping service of a product, not necessarily that he/she dislikes the product). The objective of this paper is three fold: i) to propose a machine learning approach that disambiguates online customer review feedback by classifying them into one of three direct product characteristics (i.e., form, function or behavior) and two indirect product characteristics (i.e., service and other), ii) to discover the machine learning algorithm that yields the highest and most generalizable results in achieving objective i) and iii) to quantify the correlation between product ratings and direct and indirect product characteristics. A case study involving review data for products mined from e-commerce websites is presented to demonstrate the validity of the proposed method. A multilayered (i.e., k-fold and leave one out) validation approach is presented to explore the generalizability of the proposed method. The resulting machine learning model achieved classification accuracies of 82.44% for within product classification, 80.84% for across product classification, 79.03% for across product type classification and 80.64% for across product domain classification. Furthermore, it was determined that the form of a product had the highest Pearson Correlation Coefficient relating to a product's star rating, with a value of 0.934. The scientific contributions of this work have the potential to transform the manner in which both product designers and customers incorporate product reviews into their decision making processes by quantifying the relationship between product reviews and product characteristics.},
	author = {Abhinav Singh and Conrad S. Tucker},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2017.03.007},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Machine learning, Product attribute extraction, Text mining, Product reviews, Product design},
	pages = {81-91},
	title = {A machine learning approach to product review disambiguation based on function, form and behavior classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923617300477},
	volume = {97},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923617300477},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2017.03.007}}

@article{DEHGHAN2020113425,
	abstract = {Expert finding in Community Question Answering (CQA) networks such as Stack Overflow is a practical issue facing a challenging problem called vocabulary gap. A widely used approach to overcome this problem is translation model. Different from prior works that only consider the relevancy of translations to a query, we intend to diversify query translations for better coverage of query topics. In this work, we have utilized the idea of clustering to group relevant translations to a given query into different clusters and then select representatives from each cluster as a set of diverse translations. We have proposed two new approaches to cluster translations. In the first one, the Mutual Information was primarily utilized as a similarity measure during clustering. In the second approach, the relevant translations are embedded in a topic space and then clustered in that space. After clustering, we propose two batch and sequential methods to select a diverse set of translations from the resultant clusters. The batch method selects the top most relevant translations from each cluster proportional to the relevancy of that cluster to the user query. The sequential one is an iterative method that looks for the most diverse set of translations considering the previously selected ones. Finally, to rank users, a regression model was utilized to learn how expert and non-expert users differ in using a set of diverse translations in their documents. Experiments on a large dataset generated from Stack Overflow demonstrate that the proposed methods improve the ranking performance over baselines in the expert finding.},
	author = {Mahdi Dehghan and Ahmad Ali Abin and Mahmood Neshati},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113425},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Expert finding, Question answering, Stack overflow, Translations diversification},
	pages = {113425},
	title = {An improvement in the quality of expert finding in community question answering networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620301809},
	volume = {139},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620301809},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113425}}

@article{WANG2020113171,
	abstract = {We investigate multiple disease risk prediction modeling, aimed at assessing future disease risks for an individual who is ready for discharge after hospitalization. We propose a novel framework that combines directed disease network and recommendation system techniques to substantially enhance multiple disease risk predictive modeling. Firstly, a directed disease network considering temporal information is developed. Then based on this directed disease network, we look into different disease risk score computing approaches. We validate the proposed approaches with two real-world datasets from two independent hospitals. The predicted results can be promisingly utilized as a reference for medical experts to offer effective healthcare guidance for both inpatients and outpatients. The proposed framework can also be utilized for developing an innovative tool that helps individuals create and maintain a better healthcare plan over time.},
	author = {Tingyan Wang and Robin G. Qiu and Ming Yu and Runtong Zhang},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113171},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Multiple-disease risk prediction, Health risk assessment, Disability adjusted life year, Directed network, Disease temporal relations},
	pages = {113171},
	title = {Directed disease networks to facilitate multiple-disease risk assessment modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619302003},
	volume = {129},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619302003},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113171}}

@article{JUNG2019113074,
	abstract = {Online reviews have become a significant information source for business practitioners to know about customers' opinions of their products or services. Previous studies examined product or service satisfaction factors of customers by analyzing online consumer reviews. However, examining job satisfaction factors of employees through online employee reviews has rarely been studied. In this study, we first identified job satisfaction factors from 35,063 online employee reviews posted on jobplanet.co.kr using Latent Dirichlet Allocation (LDA). Then, we conducted a series of analyses based on the factors. We measured the sentiment and importance of each job satisfaction factor at industry, company, group, and chronological levels. Dominance analysis examined the relative importance of each star-rated job satisfaction factor on overall job satisfaction. Further, the association strength between each job satisfaction factor and overall job satisfaction is computed from correspondence analysis. The results from this study will provide business managers with profound insights into making decisions on managing job satisfaction of their employees in various aspects.},
	author = {Yeonjae Jung and Yongmoo Suh},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113074},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Online employee reviews, Job satisfaction, Latent Dirichlet Allocation, Sentiment analysis, Dominance analysis, Correspondence analysis},
	pages = {113074},
	title = {Mining the voice of employees: A text mining approach to identifying and analyzing job satisfaction factors from online employee reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619301034},
	volume = {123},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619301034},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113074}}

@article{LIU2021113609,
	abstract = {Understanding customer experience is an essential part of service operations for sustaining business in the sharing economy. We investigate the relationship among customer experience, perceived value, and customer loyalty from a stimulus-organism-response (S-O-R) perspective. Using a dataset of 4166 listings in a leading Chinese accommodation-sharing platform and text mining and econometric methods to analyze online customer reviews, we find that customer experience manifests in the physical environment and human interaction dimensions. The results show a positive association among customer experience, perceived value, and customer loyalty. Notably, the physical environment and human interaction are equally important in influencing customers' value judgments about their consumption experience. Moreover, perceived value has a stronger positive effect on attitudinal loyalty than on behavioral loyalty. This study adds new insights into the customer experience-perceived value-customer loyalty path by showing that the physical environment and human interaction have the same importance in affecting perceived value and identifying the subtle difference between attitudinal and behavioral loyalty influenced by perceived value in the accommodation-sharing economy. Furthermore, these findings provide managerial insights for service operations management and marketing strategy planning.},
	author = {Fuzhen Liu and Kee-Hung Lai and Jiang Wu and Wenjing Duan},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2021.113609},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Customer experience, Perceived value, Attitudinal loyalty, Behavioral loyalty, Sharing economy},
	pages = {113609},
	title = {Listening to online reviews: A mixed-methods investigation of customer experience in the sharing economy},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923621001196},
	volume = {149},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923621001196},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2021.113609}}

@article{ZHANG2020113288,
	abstract = {Trust plays an important role in sharing transactions on short-term rental platforms. However, the impact of host self-description on trust perception and whether trust perception can influence purchase behavior remain under-studied. Therefore, a text analytics framework was proposed to research the relationships among host self-description, trust perception and purchase behavior on Airbnb. Specifically, a deep-learning-based method was designed to automatically code trust perception of host self-descriptions. And the linguistic and semantic features of description texts were extracted with text mining methods. The estimated order quantity was used to quantify purchase behavior. Then, the influence of linguistic and semantic features on trust perception was identified, and the relationship between trust perception and purchase behavior was also verified. The empirical analysis derives the following findings: i. The readability of self-description is positively associated with trust perception; ii. Perspective taking expressed in self-description is also helpful; iii. Excessive positive sentiment expression can raise barriers to trust building; iv. Paying more attention to family relationship, openness, service and travel experience in self-description would be helpful; v. Trust perception can promote purchases. These findings can help hosts write better self-description, which contributes to trust building and purchases on short-term rental platforms.},
	author = {Le Zhang and Qiang Yan and Leihan Zhang},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113288},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Self-description, Trust perception, Purchase behavior, Airbnb, Text mining},
	pages = {113288},
	title = {A text analytics framework for understanding the relationships among host self-description, trust perception and purchase behavior on Airbnb},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620300439},
	volume = {133},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620300439},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113288}}

@article{IBRAHIM201937,
	abstract = {The purpose of this study is to identify the customers' primary topics of concern regarding online retail brands that are shared among Twitter users. This study collects tweets associated with five leading UK online retailers covering the period from Black Friday to Christmas and New Year's sales. We use a combination of text analytical approaches including topic modelling, sentiment analysis, and network analysis to analyse the tweets. Through the analysis, we identify that delivery, product and customer service are among the most-discussed topics on Twitter. We also highlight the areas that receive the most negative customer sentiments such as delivery and customer service. Interestingly, we also identify emerging topics such as online engagement and in-store experience that are not captured by the existing literature on online retailing. Through a network analysis, we underscore the relationships among those important topics. This study derives insights on how well the leading online retail brands are performing and how their products and services are perceived by their customers. These insights can help businesses understand customers better and enable them to convert the information into meaningful knowledge to improve their business performance. The study offers a novel approach of transforming social media data into useful knowledge about online retailing. The incorporation of three analytical approaches offers insights for researchers to understand the hidden content behind the large collections of unstructured bodies of text, and this information can be used to improve online retailing services and reach out to customers.},
	author = {Noor Farizah Ibrahim and Xiaojun Wang},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.03.002},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Online retailing, Social media research, Text analytics, Topic modelling, Sentiment analysis},
	pages = {37-50},
	title = {A text analytics approach for online retailing service improvement: Evidence from Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619300405},
	volume = {121},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619300405},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.03.002}}

@article{KUNDU2020113164,
	abstract = {Here, we propose a preference enhanced hybrid expertise retrieval (PEHER) system in community question answering services. PEHER consists of three segments, namely, preferability estimator, authority estimator, and expertise estimator. The preferability estimator utilizes the textual information to determine both intra-profile and inter-profile preferences of answerers for each term. The intra-profile preferences consider the preference of a term using the answering history of a given answerer. The inter-profile preferences incorporate the preferences of all answerers for a term. These preferences are then used to determine the preferability of each answerer for each of the archived questions. The authority estimator considers the textual familiarity between each archived question and the profile of each answerer as the weight of the associated link in the network. The expertise estimator is composed of three blocks, namely, question similarity finder, proficiency estimator, and expert list generator. The question similarity finder finds the similarities between the new question and each of the archived questions. The proficiency estimator uses the said similarities of the archived questions along with their preferabilities to decide the proficiencies of answerers for the new question. Finally, the expert list generator considers the authorities and proficiencies to generate a list of experts for a given question. We compare PEHER with twenty existing methods on four real-world datasets using five performance measures. We find that PEHER outperforms the comparing algorithms in 92.00% (368 out of 400) cases.},
	author = {Dipankar Kundu and Rajat Kumar Pal and Deba Prasad Mandal},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113164},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Community question answering, Expertise retrieval, Hybrid systems, Social network analysis},
	pages = {113164},
	title = {Preference enhanced hybrid expertise retrieval system in community question answering services},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619301939},
	volume = {129},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619301939},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113164}}

@article{GOLDBERG2022113751,
	abstract = {In recent years, online reviews have offered a rich new medium for consumers to express their opinions and feedback. Product designers frequently aim to consider consumer preferences in their work, but many firms are unsure of how best to harness this online feedback given that textual data is both unstructured and voluminous. In this study, we use text mining tools to propose a method for rapid prioritization of online reviews, differentiating the reviews pertaining to innovation opportunities that are most useful for firms. We draw from the innovation and entrepreneurship literature and provide an empirical basis for the widely accepted attribute mapping framework, which delineates between desirable product attributes that firms may want to capitalize upon and undesirable attributes that they may need to remedy. Based on a large sample of reviews in the countertop appliances industry, we demonstrate the performance of our technique, which offers statistically significant improvements relative to existing methods. We validate the usefulness of our technique by asking senior managers at a large manufacturing firm to rate a selection of online reviews, and we show that the selected attribute types are more useful than alternative reviews. Our results offer insight in how firms may use online reviews to harness vital consumer feedback.},
	author = {David M. Goldberg and Alan S. Abrahams},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113751},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Online reviews, Text mining, Data mining, Innovation, Business intelligence},
	pages = {113751},
	title = {Sourcing product innovation intelligence from online reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000227},
	volume = {157},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000227},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113751}}

@article{YANG2022113813,
	abstract = {Depression is a leading mental health problem affecting 300 million people globally. Recent studies show that social networks provide a tremendous potential for mental health professionals as a source of supplemental information about their patients. This study presents a methodological framework for clinical decision support systems (CDSSs) through analysis of social network data to distinguish the language usage of individuals with early signs of depression (i.e., contrast language analysis). By analyzing the contrast language patterns of different user groups, we are able to uncover constructive and actionable insights into the pain points and characteristics of users with signs of depression as decision support mechanisms for clinicians during intervention, (early) diagnosis and treatment plans. First, we discover terms that represent contrasting language by analyzing the percentage difference of terms in two user groups, labeled as''depressed'' and''non-depressed'' for ease of reference. Second, by building topic models based on social network contents, the topic-level contrast features are discovered. Finally, we consider the structure of the social network to discover the network-level contrast features. To illustrate the effectiveness of the proposed framework, we present a case study on early depression detection using a real-world dataset. The proposed framework has methodological contributions in enhancing the features and functionalities of CDSS for clinicians. It also contributes to evidence-based health research through cost-effective data and analytical insights that can supplement or improve the traditional survey and time-consuming interview methods.},
	author = {Xingwei Yang and Alexandra Joukova and Anteneh Ayanso and Morteza Zihayat},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113813},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Contrast language analysis, Clinical Decision Support System (CDSS), Depression detection, Social network},
	pages = {113813},
	title = {Social influence-based contrast language analysis framework for clinical decision support systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000847},
	volume = {159},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000847},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113813}}

@article{KUMAR2022113792,
	abstract = {The COVID-19 pandemic has had a severe impact on mankind, causing physical suffering and deaths across the globe. Even those who have not contracted the virus have experienced its far-reaching impacts, particularly on their mental health. The increased incidences of psychological problems, anxiety associated with the infection, social restrictions, economic downturn, etc., are likely to aggravate with the virus spread and leave a longer impact on humankind. These reasons in aggregation have raised concerns on mental health and created a need to identify novel precursors of depression and suicidal tendencies during COVID-19. Identifying factors affecting mental health and causing suicidal ideation is of paramount importance for timely intervention and suicide prevention. This study, thus, bridges this gap by utilizing computational intelligence and Natural Language Processing (NLP) to unveil the factors underlying mental health issues. We observed that the pandemic and subsequent lockdown anxiety emerged as significant factors leading to poor mental health outcomes after the onset of COVID-19. Consistent with previous works, we found that psychological disorders have remained pre-eminent. Interestingly, financial burden was found to cause suicidal ideation before the pandemic, while it led to higher odds of depressive (non-suicidal) thoughts for individuals who lost their jobs. This study offers significant implications for health policy makers, governments, psychiatric practitioners, and psychologists.},
	author = {Rahul Kumar and Shubhadeep Mukherjee and Tsan-Ming Choi and Lalitha Dhamotharan},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113792},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Mental health, Depression, Suicidal ideation, Natural language processing, Social-media, COVID-19, Pandemic},
	note = {Business and Government Applications of Text Mining & Natural Language Processing (NLP) for Societal Benefit},
	pages = {113792},
	title = {Mining voices from self-expressed messages on social-media: Diagnostics of mental distress during COVID-19},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362200063X},
	volume = {162},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362200063X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113792}}

@article{XU2020113162,
	abstract = {With the rapid development of information technology, platform-facilitated collaborative consumption has recently become attractive to consumers. A comparative study of consumers' online review behavior and its impact on overall satisfaction and demand in the accommodation-sharing economy and the hotel industry indicates that consumers' perceptions and behavior change gradually with changes in the level of sharing---from no sharing when staying in hotel rooms to intensive sharing when sharing rooms through collaborative consumption. Online consumer reviews focus on product and service attributes, and the influential factors of customer satisfaction and demand differ when consumers are at different accommodation-sharing levels. Not all attributes described in online reviews influence overall customer satisfaction. With a higher level of sharing, consumers' valuation changes from more to less tangible attributes. Consumers at a higher sharing level care more about social interaction and economic value than consumers at a lower sharing level. Transaction costs, particularly the information search and acquisition costs, play an important role in influencing customer purchase decisions in the sharing economy. Consumers refer to direct information for tangible attributes and to previous consumers' online reviews for intangible attributes to familiarize themselves with details before making purchase decisions. Our study provides implications that help platforms and hosts better target consumer segments with different sharing levels and more effectively utilize online reviews to generate positive electronic word of mouth to enhance consumer demand and the performance of platform economics.},
	author = {Xun Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.113162},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Sharing economy, Customer satisfaction, Product and service attributes, Online reviews},
	pages = {113162},
	title = {How do consumers in the sharing economy value sharing? Evidence from online reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619301915},
	volume = {128},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619301915},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.113162}}

@article{XU2021113467,
	abstract = {The on-demand economy has prospered with the rapid development of digital platforms. Many customers use on-demand service platforms to order services and then post online reviews. Using text-mining approaches, this study examines customers' online review-writing behavior and their overall satisfaction with restaurants in the context of on-demand food service. We use customers' overall ratings in their reviews to measure their overall satisfaction. We find that customers comment on the main service provider, the restaurants, and the auxiliary service providers, which include the drivers and on-demand service platform; in their online customer reviews, which are posted on the restaurants' web pages on the on-demand service platform. From text regressions, we found the determinants of customer satisfaction with the restaurants through their online reviews. There is a spillover effect from the performance of auxiliary providers on customer satisfaction with the main provider. That is, the performance of the drivers and the platform affects customers' overall satisfaction with the restaurants. In addition, we find that a higher cost of the order makes customers comment more on the attributes offered by the restaurants to show their overall satisfaction. Further, we find the type of listed merchants categorized by their properties (i.e., chain or independent) and participation in the platform programs affect the influence of the various attributes, offered by different providers, on customer satisfaction with the main provider. The findings shed light on the determinants of customers' overall satisfaction and urge improvement in collaboration and coordination between various participants in the on-demand service context.},
	author = {Xun Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113467},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {On-demand service, Customer satisfaction, Online customer reviews, Spillover effect},
	pages = {113467},
	title = {What are customers commenting on, and how is their satisfaction affected? Examining online reviews in the on-demand food service context},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620302220},
	volume = {142},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620302220},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113467}}

@article{ZHANG2019113064,
	abstract = {Feature selection has received significant attention in knowledge management and decision support systems in the past decades. In this study, kernel-based sparse representation and feature dependence analysis are integrated into a feature assessment and ranking framework. The proposed method utilizes the advantages of the kernel-based sparse representation technique and of the information theoretic metric to iteratively obtain the salient feature cluster. Then, a novel approximate dependence analysis is applied to further maintain complementarity while eliminating redundancy among the features selected by nonlinear orthogonal matching pursuit (NOMP). This can effectively prevent the significant bias caused by the pairwise correlation analysis for a large-scale feature set. To illustrate the effectiveness of the proposed method, classification experiments are conducted with three representative classifiers, on nine well-known datasets. The experimental results show the superiority of the proposed method compared with the representative information theoretic and model-based methods in classification for data-driven decision support systems.},
	author = {Yishi Zhang and Qi Zhang and Zhijun Chen and Jennifer Shang and Haiying Wei},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.05.004},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Feature selection, Dimensionality reduction, Classification, Sparse representation, Dependence analysis},
	pages = {113064},
	title = {Feature assessment and ranking for classification with nonlinear sparse representation and approximate dependence analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619300806},
	volume = {122},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619300806},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.05.004}}

@article{LI2022113755,
	abstract = {For narrative products such as movies, books, and TV shows, electronic word of mouth (eWOM) can be a double-edged sword. It provides consumers with useful information while potentially revealing the storyline, that is, spoiling the surprise of what will happen. Prior studies have focused on the impact of spoilers on consumers' experiences through psychological experiments. However, the relationship between spoilers and narrative product sales has rarely been empirically explored. To fill this gap, the current study explores the impact of spoilers on the movie box office revenue and how this impact evolves over time. We collected 279,433 reviews of 465 films on a leading community website in China and constructed a dynamic generalized method of moments (GMM) model with instrumental variables to empirically examine the spoiler effect in the movie market. Our findings indicate that spoilers have a negative influence on the movie box office revenue. However, this impact is limited to the first 6 days after the movie is released. We also discover that spoilers have a stronger negative effect on narrative-based movies than non-narrative-based movies. Furthermore, eWOM volume and eWOM variance negatively moderate the spoiler effect on the box office revenue, but the moderating effect of eWOM valence is not significant. These findings can deepen movie industry decision makers' and platform providers' understanding of spoilers, helping them devise more feasible eWOM operation strategies.},
	author = {Yang Li and Xin (Robert) Luo and Kai Li and Xiaobo Xu},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113755},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Spoilers, eWOM, Movie reviews, Box office revenue, Narrative},
	pages = {113755},
	title = {Exploring the spoiler effect in the digital age: Evidence from the movie industry},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000264},
	volume = {157},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000264},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113755}}

@article{GARCIALOZANO201718,
	abstract = {Tracking how discussion topics evolve in social media and where these topics are discussed geographically over time has the potential to provide useful information for many different purposes. In crisis management, knowing a specific topic's current geographical location could provide vital information to where, or even which, resources should be allocated. This paper describes an attempt to track online discussions geographically over time. A distributed geo-aware streaming latent Dirichlet allocation model was developed for the purpose of recognizing topics' locations in unstructured text. To evaluate the model it has been implemented and used for automatic discovery and geographical tracking of election topics during parts of the 2016 American presidential primary elections. It was shown that the locations correlated with the actual election locations, and that the model provides a better geolocation classification compared to using a keyword-based approach.},
	author = {Marianela {Garc{\'\i}a Lozano} and Jonah Schreiber and Joel Brynielsson},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2017.05.006},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Social media, Topic modeling, Geo-awareness, Trend analysis, Latent Dirichlet allocation, Streaming media},
	note = {Location Analytics and Decision Support},
	pages = {18-29},
	title = {Tracking geographical locations using a geo-aware topic model for analyzing social media data},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923617300842},
	volume = {99},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923617300842},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2017.05.006}}

@article{ZHANG2022113765,
	abstract = {The task of detecting fraudulent reviewers is of great importance to E-commerce platforms. Existing research has invested much effort into developing comprehensive features and advanced techniques to detect fraudulent reviewers. However, most of these studies have ignored the data imbalance problem inherent in fraudulent reviewer detection: non-fraudulent reviewers are the majority, while fraudulent reviewers are the minority in real practice. To fill this gap, we propose a novel approach called ImDetector to detect fraudulent reviewers while handling data imbalance based on weighted latent Dirichlet allocation (LDA) and Kullback--Leibler (KL) divergence. Specifically, we develop a weighted LDA model to extract the latent topics of reviewers distributed on the review features. Asymmetric KL divergence is adopted to make the similarity measure between reviewers biased toward the fraudulent minority when using the K-nearest-neighbor for classification. By mapping the reviewers to the latent topics of features derived from the weighted LDA model and measuring the similarities between reviewers using asymmetric KL divergence, the data imbalance problem in fraudulent reviewer detection is alleviated. Extensive experiments on the Yelp.com dataset demonstrate that the proposed ImDetector approach is superior to the state-of-the-art techniques used for fraudulent reviewer detection. We also explain the experimental results and present the managerial implications of this paper.},
	author = {Wen Zhang and Rui Xie and Qiang Wang and Ye Yang and Jian Li},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2022.113765},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {-commerce, Fraudulent reviewer detection, Imbalanced data, Weighted LDA, Kullback--Leibler divergence},
	pages = {113765},
	title = {A novel approach for fraudulent reviewer detection based on weighted topic modelling and nearest neighbors with asymmetric Kullback--Leibler divergence},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923622000367},
	volume = {157},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923622000367},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2022.113765}}

@article{GUAN201958,
	abstract = {With the rapid proliferation of images on e-commerce platforms today, embracing and integrating versatile information sources have become increasingly important in recommender systems. Owing to the heterogeneity in information sources and consumers, it is necessary and meaningful to consider the potential synergy between visual and textual content as well as consumers' different cognitive styles. This paper proposes a multi-view model, namely, Deep Multi-view Information iNtEgration (Deep-MINE), to take multiple sources of content (i.e., product images, descriptions and review texts) into account and design an end-to-end recommendation model. In doing so, stacked auto-encoder networks are deployed to map multi-view information into a unified latent space, a cognition layer is added to depict consumers' heterogeneous cognition styles and an integration module is introduced to reflect the interaction of multi-view latent representations. Extensive experiments on real world data demonstrate that Deep-MINE achieves high accuracy in product ranking, especially in the cold-start case. In addition, Deep-MINE is able to boost overall model performance compared with models taking a single view, further verifying the proposed model's effectiveness on information integration.},
	author = {Yue Guan and Qiang Wei and Guoqing Chen},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2019.01.003},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Multi-view information, Deep learning, Information integration, Personalized recommendation, Representation learning},
	pages = {58-69},
	title = {Deep learning based personalized recommendation with multi-view information integration},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923619300119},
	volume = {118},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923619300119},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2019.01.003}}

@article{ZHENG2020113369,
	abstract = {Product defects are a major concern for manufacturers and customers. Detecting product defects is vital for manufacturers to prevent enormous product failure costs. As the surge of social media is in vogue, social media data become an important information source for manufacturers to collect defect information. In this study, we propose a novel probabilistic graphic model to discover defects from social media data. We first use three filters, namely, sentiment filter, component-symptom filter and similarity filter, to select informative data. Second, we analyze the remaining data via the proposed probabilistic graphic model and identify defect-related data. Our method provides detailed defect information including defect types, defective components and defect symptoms which is omitted by previous research. A case study in the automobile industry validates the effectiveness and superior performance of our method compared to prior approaches.},
	author = {Lu Zheng and Zhen He and Shuguang He},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113369},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Product defect detection, Social media data, Probabilistic graphic model, Text analysis},
	pages = {113369},
	title = {A novel probabilistic graphic model to detect product defects from social media data},
	url = {https://www.sciencedirect.com/science/article/pii/S016792362030124X},
	volume = {137},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S016792362030124X},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113369}}

@article{WANG2021113465,
	abstract = {Comparable entity identification plays an essential role in the decision making of both consumers and firms in competitive environment. In contrast to traditional cooccurrence approaches, this paper proposes a novel method, namely, ICE (identifying comparable entities) for effectively identifying comparable entities from web search logs, which are online user-generated contents that reflect users' attention and preferences. ICE consists of two stages: the formulation of directly and indirectly associative relations, followed by a generative procedure that is designed for deriving a broad set of candidate entities that are indirectly associative with a specified focal entity; and a deep-learning-based semantic analysis with a word embedding procedure for measuring the similarities between entity profiles so as to target comparable entities from the candidate set. Extensive experiments show that ICE outperforms several baseline methods in the identification of accurate, broad and novel comparable entities with suitable rankings.},
	author = {Liye Wang and Jin Zhang and Guoqing Chen and Dandan Qiao},
	date-added = {2022-10-23 18:22:47 +0200},
	date-modified = {2022-10-23 18:22:47 +0200},
	doi = {https://doi.org/10.1016/j.dss.2020.113465},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Comparable entity identification, Web search logs, Indirectly associative relation, Semantic analysis},
	pages = {113465},
	title = {Identifying comparable entities with indirectly associative relations and word embeddings from web search logs},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923620302207},
	volume = {141},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167923620302207},
	bdsk-url-2 = {https://doi.org/10.1016/j.dss.2020.113465}}

@article{LI2020241,
	abstract = {In cross-language question retrieval (CLQR), users employ a new question in one language to search the community question answering (CQA) archives for similar questions in another language. In addition to the ranking problem in monolingual question retrieval, one needs to bridge the language gap in CLQR. The existing adversarial models for cross-language learning normally rely on a single adversarial component. Since natural languages consist of units of different abstract levels, we argue that crossing the language gap adaptatively on different levels with multiple adversarial components should lead to smoother text representation and better CLQR performance. To this end, we first encode questions into multi-layer representations of different abstract levels with a CNN based model which enhances conventional models with diverse kernel shapes and the corresponding pooling strategy so as to capture different aspects of a text segment. We then impose a set of adversarial components on different layers of question representation so as to decide the appropriate abstract levels and their role in performing cross-language mapping. Experimental results on two real-world datasets demonstrate that our model outperforms state-of-the-art models for CLQR, which is on par with the strong machine translation baselines and most monolingual baselines.},
	author = {Bo Li and Xiaodong Du and Meng Chen},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-language question retrieval, Adversarial learning, Community question answering},
	pages = {241-252},
	title = {Cross-language question retrieval with multi-layer representation and layer-wise adversary},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300359},
	volume = {527},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300359},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.035}}

@article{SHEN202063,
	abstract = {Hierarchical classification (HC) is effective when categories are organized hierarchically. However, the blocking problem makes the effect of hierarchical classification greatly reduced. Blocking means that samples are easily getting misclassified in high-level classifiers so that the samples are blocked at the high-level of the hierarchy. This issue is caused by the inconsistency between the artificially defined hierarchy and the actual hierarchy of the raw data. Another issue is that it is flippant to strictly process data following the hierarchy. Therefore, special treatment is required for some uncertain data. To address the first issue, we learn category relationships and modify the hierarchy. To address the second issue, we introduce three-way decisions (3WD) to targetedly deal with the ambiguous data. We extend original studies and propose two HC models based on 3WD, collectively referred to as TriHC, for carefully modifying the hierarchy to alleviate the blocking problem. The proposed TriHC model learns new category hierarchies by the following three steps: (1) mining category relations; (2) modifying category hierarchies according to the latent category relations; and (3) using 3WD to divide observed objects into three regions: positive region, boundary region, and negative region, and making decisions based on different strategies. Specifically, based on different category relation mining methods, there are two versions of TriHC, cross-level blocking priori knowledge based TriHC (CLPK-TriHC) and expert classifier based TriHC (EC-TriHC). The CLPK-TriHC model defines a cross-level blocking distribution matrix to mine the category relations between the higher and lower levels. To better exploit category hierarchical relations, the EC-TriHC model builds expert classifiers using topic model to learn latent category topics. Experimental results validate that the proposed methods can simultaneously reduce the blocking and improve the classification accuracy.},
	author = {Wen Shen and Zhihua Wei and Qianwen Li and Hongyun Zhang and Duoqian Miao},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.02.020},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical classification, Blocking reduction, Three-way decisions, Category relation mining, Topic model},
	pages = {63-76},
	title = {Three-way decisions based blocking reduction models in hierarchical classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300955},
	volume = {523},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300955},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.02.020}}

@article{KHAN202269,
	abstract = {Knowledge Graph Embedding (KGE)-enhanced recommender systems are effective in providing accurate and personalized recommendations in diverse application scenarios. However, such techniques that exploit entire embedded Knowledge Graph (KG) without data relevance approval constraints fail to stop noise penetration into the data. Additionally, approaches that pay no heed to tackle semantic relations among entities remain unable to effectively capture semantical structure of Heterogeneous Information Graph (HIG). Therefore, in this paper, we propose Similarity Attributed Graph-embedding Enhancement (SAGE) approach to model similarity-aware semantic connections among entities according to their triplets' granularity. SAGE is a novel Knowledge Graph Embedding Enhancement (KGEE) method that constructs Entity-relevance-based Similarity-attributed Subgraph (ESS) to remove noise from the underlying data. It propagates interactions-enhanced knowledge over ESS to learn higher-order semantic connections among entities; and simultaneously utilizes feedbacks to enhance the interactions and regularize the model to highlight influential targets (nodes). Further, it samples influential targets in KG, independently move their preferences to the Local Central Nodes (LCN) of current influential areas, and streamline the collected information from all LCN to the main unit. Finally, a prediction module is used to determine generalized preferences for recommendation. We performed extensive experiments on benchmark datasets to evaluate the performance of SAGE where it outperformed the state-of-the-art methods with significant improvements in effectively providing the desired explainable recommendations.},
	author = {Nasrullah Khan and Zongmin Ma and Aman Ullah and Kemal Polat},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.124},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Knowledge graph, KGE and KGEE, Recommender systems, Similarity, Interactions, Features},
	pages = {69-95},
	title = {Similarity attributed knowledge graph embedding enhancement for item recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522010441},
	volume = {613},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522010441},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.124}}

@article{ZHANG2020306,
	abstract = {With the fast development of online E-commerce Websites and mobile applications, users' auxiliary information as well as products' textual information can be easily collected to form a vast amount of training data. Therefore, research efforts are urgently needed to make customized recommendations using such large but sparse data. Deep recommendation model is a natural choice for this research issue. However, most existing approaches try to investigate either user's auxiliary information such as age and zipcode, or item's textual information such as product descriptions, reviews or comments. Therefore, it is desired to see whether user's auxiliary information and item's textual information could be modeled simultaneously. This paper proposes a novel approach which is essentially a hybrid probabilistic matrix factorization model. Particularly, it has two sub components. One component tries to predict user's rating scores by capturing user's personal preferences extracted from auxiliary information. Another component tries to model item's textual attractiveness to different users via a proposed attention based convolutional neural network. We then propose a global objective function and optimize these two sub components under a unified framework. Extensive experiments are performed on five real-world datasets, i.e., ML-100K, ML-1M, ML-10M, AIV and Amazon sub dataset. The promising experimental results have demonstrated the superiority of our proposed approach when compared with both baseline models and state-of-the-art deep recommendation approaches, i.e., PMF, CDL, CTR, ConvMF, ConvMF+ and D-Attn with respect to RMSE criterion.},
	author = {Xiaofeng Zhang and Huijie Liu and Xiaoyun Chen and Jingbin Zhong and Di Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.044},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Probabilistic matrix factorization, Deep learning, Recommendation systems},
	pages = {306-316},
	title = {A novel hybrid deep recommendation system to differentiate user's preference and item's attractiveness},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300554},
	volume = {519},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300554},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.044}}

@article{BAI2022,
	abstract = {With the widespread development of the internet, multi-view text documents have become increasingly common, which has led to extensive research on multi-view text document modeling. As opposed to traditional single-view document modeling, which treats each document independently and learns each document as a single topic representation, the views of multi-view text documents have complicated correlation relationships that include both the global and local underlying topical information. In this study, we introduce a deep generative model for multi-view document modeling known as Hierarchical Variational Auto-Encoder (HVAE), which combines the advantages of the probability generative model for learning interpretable latent information and the deep neural network for efficient parameter inference. Specifically, a set of hierarchical topic representations is learned for each multi-view document to capture the document-level global topical information and view-level local topical information for each view. A two-level hierarchical topic inference network is investigated as the encoder network of HVAE, which is designed using an aligned variational auto-encoder, to learn the hierarchical topic representations. Subsequently, multi-view documents are generated through a two-layered generation network, considering both the view-level local and document-level global topic representations. Experiments on three real datasets of different scales for various tasks demonstrate the satisfactory results of the proposed method.},
	author = {Ruina Bai and Ruizhang Huang and Yongbin Qin and Yanping Chen and Chuan Lin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.10.052},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {deep variational inference, variational auto-encoder, multi-view document modeling, hierarchical topic representation, probability generative model},
	title = {HVAE: A Deep Generative Model via Hierarchical Variational Auto-Encoder for Multi-view Document Modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522011732},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522011732},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.10.052}}

@article{DAU20201279,
	abstract = {With the developments of e-commerce websites, user textual review has become an important source of information for improving the performance of recommendation systems, as they contain fine-grained users' opinions that generally reflect their preference towards products. However, most of the classical recommender systems (RSs) often ignore such user opinions and therefore fail to precisely capture users' specific sentiments on products. Although a few of the approaches have attempted to utilize fine-grained users' opinions for enhancing the accuracy of recommendation systems to some extent, most of these methods basically rely on handcrafted and rule-based approaches that are generally known to be time-consuming and labour-intensive. As such, their application is limited in practice. Thus, to overcome the above problems, this paper proposes a recommendation system that utilizes aspect-based opinion mining (ABOM) based on the deep learning technique to improve the accuracy of the recommendation process. The proposed model consists of two parts: ABOM and rating prediction. In the first part, we use a multichannel deep convolutional neural network (MCNN) to better extract aspects and generate aspect-specific ratings by computing users' sentiment polarities on various aspects. In the second part, we integrate the aspect-specific ratings into a tensor factorization (TF) machine for the overall rating prediction. Experimental results using various datasets show that our proposed model achieves significant improvements compared with the baseline methods.},
	author = {Aminu Da'u and Naomie Salim and Idris Rabiu and Akram Osman},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.10.038},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Aspect Based Opinion Mining, Sentiment classification, Deep CNN, Tensor factorization, Recommendation systems, Ratings prediction},
	pages = {1279-1292},
	title = {Recommendation system exploiting aspect-based opinion mining with deep learning method},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519310060},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519310060},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.10.038}}

@article{XIAO2021262,
	abstract = {Topic detection aims to discover valuable topics from the massive online news. It can help people to capture what is happening in real world and alleviate the burden of information overload. It also has great significance since the online news is experiencing an explosive growth. Topic detection is typically transformed into a document clustering problem, whose core idea is to cluster news documents that report on the same topic to the same group based on document similarity. Due to the complex structure and long length of news documents, the similarity measurement of news is very challenging. Existing term-based methods represent news documents based on a set of informative keywords in the document with a vector space model (VSM) and then the relationship between documents is calculated by cosine similarity. However, VSM ignores the relationship between words and has sparse semantics, which leads to low precision of topic detection. In recent years, the probabilistic methods and the graph analytical methods have been proposed for topic detection. However, both of them have high time complexity. To cope with these problems, we first present a novel document representation approach based on graphical decomposition, which decomposes each news document into different semantic units and then relationship between the semantic units is constructed to form a capsule semantic graph (CSG). The CSG can retain the relationship between words and alleviate the sparse semantics compared to VSM representation. We next introduce the graph kernel to measure the similarity between the CSGs based on their substructures. Finally, we use an incremental clustering method to cluster the news documents, in which the documents are represented by CSGs and the similarity between documents is calculated by graph kernel. The experiment results on three standard datasets show that our method obtains higher precision, recall and F1 score than several state-of-the-art methods. Moreover, the experiment results on a large news dataset show that our CSG-SM has lower time complexity than probabilistic methods and graph analytical methods.},
	author = {Kejing Xiao and Zhaopeng Qian and Biao Qin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.04.029},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Topic detection, Word co-occurrence, Capsule semantic graph, Graph kernel, Community detection},
	pages = {262-277},
	title = {A graphical decomposition and similarity measurement approach for topic detection from online news},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552100356X},
	volume = {570},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552100356X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.04.029}}

@article{YANG2021185,
	abstract = {The investigation of user preferences through user comments has attracted significant attention. Although topic models have been verified as useful tools to facilitate the understanding of textual contents, they cannot be directly applied to accomplish this task because of two problems. The first problem is the severe data sparsity suffered by user comments because they are generally short. The second problem is the mixture of opinions from both user comments and the original documents the users commented on. To simultaneously solve the data sparsity problem and explore clean user preferences, we propose an author co-occurring topic model (AOTM) for normal documents and their short user comments. By considering authorship, AOTM allows each author of short texts to have a probability distribution over a set of topics represented only short texts. Accordingly, the individual user preferences can be investigated based on these author-level distributions. We verify the performance of AOTM using two news article datasets and one e-commerce dataset. Extensive experiments demonstrate that the AOTM outperforms several state-of-the-art methods in topic learning and topic representation of documents. The potential usage of AOTM in exploring individual user preferences is further illustrated by drawing user portraits and predicting user posting behaviors.},
	author = {Yang Yang and Feifei Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.04.060},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Authorship, Short texts, Topic model, User preference},
	pages = {185-199},
	title = {Author topic model for co-occurring normal documents and short texts to explore individual user preferences},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521003893},
	volume = {570},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521003893},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.04.060}}

@article{ZHAO2021283,
	abstract = {Current research on subset selection for opinion analysis assumes that their methods can retrieve the opinions expressed in documents from general text features. However, such relaxed conditions can hardly maintain the performance of the analysis in opinion mining, especially when given strict limitations on the subset size. In this paper, we propose a framework for opinion subset selection. This framework can select a small set of instances from original data to convey a subjective representation for opinion classification and regression. Compared with our framework, the conventional submodular based subset selection approach cannot capture the fine-grained opinion features expressed in the corpus. Specifically, we propose a monotone non-decreasing score function and a framework based on topic modeling and submodular maximization for filtering irrelevant information and selecting the subsets. Our work further introduces an opinion-sensitive algorithm for optimizing the proposed function for opinion subset construction. We perform extensive experiments and comparative analysis of different subset selection methods in this work. The experimental result shows that the proposed opinion subset selection framework can compress the original text training set and preserve the test set's classification and regression metric performance at the same time.},
	author = {Yang Zhao and Tommy W.S. Chow},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.12.083},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Subset selection, Opinion mining, Opinion summarization, Submodular function optimization, Opinion subset selection, Sentiment analysis},
	pages = {283-306},
	title = {Opinion subset selection via submodular maximization},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000141},
	volume = {560},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000141},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.12.083}}

@article{PRADHAN2021212,
	abstract = {Scholarly venue recommendation is an emerging field due to a rapid surge in the number of scholarly venues concomitant with exponential growth in interdisciplinary research and cross collaboration among researchers. Finding appropriate publication venues is confronted as one of the most challenging aspects in paper publication as a larger proportion of manuscripts face rejection due to a disjunction between the scope of the venue and the field of research pursued by the research article. We present CLAVER--an integrated framework of Convolutional Layer, bi-directional LSTM with an Attention mechanism-based scholarly VEnue Recommender system. The system is the first of its kind to integrate multiple deep learning-based concepts, that requires only the abstract and the title of a manuscript to identify academic venues. An extensive and exhaustive set of experiments conducted on the DBLP dataset certify that the postulated model CLAVER performs better than most of the modern techniques as entrenched by standard metrics such as stability, accuracy, MRR, average venue quality, precision@k, nDCG@k and diversity.},
	author = {Tribikram Pradhan and Prashant Kumar and Sukomal Pal},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.12.024},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Recommendation system, Convolution neural network, Long short-term memory (LSTM), Attention mechanism, Deep learning},
	pages = {212-235},
	title = {CLAVER: An integrated framework of convolutional layer, bidirectional LSTM with attention mechanism based scholarly venue recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520311890},
	volume = {559},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520311890},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.12.024}}

@article{COSTA2021226,
	abstract = {An innovative model-based approach to coupling text clustering and topic modeling is introduced, in which the two tasks take advantage of each other. Specifically, the integration is enabled by a new generative model of text corpora. This explains topics, clusters and document content via a Bayesian generative process. In this process, documents include word vectors, to capture the (syntactic and semantic) regularities among words. Topics are multivariate Gaussian distributions on word vectors. Clusters are assigned corresponding topic distributions as their semantics. Content generation is ruled by text clusters and topics, which act as interacting latent factors. Documents are at first placed into respective clusters, then the semantics of these clusters is then repeatedly sampled to draw document topics, which are in turn sampled for word-vector generation. Under the proposed model, collapsed Gibbs sampling is derived mathematically and implemented algorithmically with parameter estimation for the simultaneous inference of text clusters and topics. A comparative assessment on real-world benchmark corpora demonstrates the effectiveness of this approach in clustering texts and uncovering their semantics. Intrinsic and extrinsic criteria are adopted to investigate its topic modeling performance, whose results are shown through a case study. Time efficiency and scalability are also studied.},
	author = {Gianni Costa and Riccardo Ortale},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.01.019},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Document clustering, Topic modeling, Word embeddings, Bayesian text analysis},
	pages = {226-240},
	title = {Jointly modeling and simultaneously discovering topics and clusters in text corpora using word vectors},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000463},
	volume = {563},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000463},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.01.019}}

@article{TANG2019190,
	abstract = {Fine-grained sentiment analysis for online reviews plays more and more important role in many applications. The key techniques here are how to efficiently extract multi-grained aspects, identify associated opinions, and classify sentiment polarity. Although various topic models have been proposed to process some of these tasks in recent years, there was little work available for effective sentiment analysis. In this paper, we propose a joint aspect based sentiment topic (JABST) model that jointly extracts multi-grained aspects and opinions through modeling aspects, opinions, sentiment polarities and granularities simultaneously. Moreover, by means of the supervised learning, we then propose a maximum entropy based JABST model (MaxEnt--JABST) to improve accuracy and performance in extracting opinions and aspects. Comprehensive evaluation results on real-world reviews for electronic devices and restaurants demonstrate that our JABST and MaxEnt--JABST models significantly outperform related proposals.},
	author = {Feilong Tang and Luoyi Fu and Bin Yao and Wenchao Xu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.02.064},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment analysis, Online reviews, Topic model, Fine-grain aspects and opinions, Classifier},
	pages = {190-204},
	title = {Aspect based fine-grained sentiment analysis for online reviews},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519301872},
	volume = {488},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519301872},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.02.064}}

@article{YANG2019271,
	abstract = {Modeling the interests of authors over time from documents has important applications in broad applications such as recommendation systems, authorship identification and opinion extraction. In this paper, we propose an Ordering-sensitive and Semantic-aware Dynamic Author Topic Model (OSDATM), which monitors the evolution of author interest in time-stamped documents. The model further uses the discovered author interest information to discover better topics. Unlike traditional topic models, OSDATM is sensitive to the ordering of words, thus it extracts more information from the semantic meaning of the context. The experimental results show that OSDATM learns better topics than state-of-the-art topic models. In addition, the dynamic interests of authors that the OSDATM model discovers are interpretable and consistent with the truth.},
	author = {Min Yang and Qiang Qu and Xiaojun Chen and Wenting Tu and Ying Shen and Jia Zhu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.02.040},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Topic model, Dynamic author topic model, Ordering-sensitive, Semantic-aware},
	pages = {271-286},
	title = {Discovering author interest evolution in order-sensitive and Semantic-aware topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516308222},
	volume = {486},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516308222},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.02.040}}

@article{GULLO2017199,
	abstract = {Uncertain data clustering has become central in mining data whose observed representation is naturally affected by imprecision, staling, or randomness that is implicit when storing this data from real-word sources. Most existing methods for uncertain data clustering follow a partitional or a density-based clustering approach, whereas little research has been devoted to the hierarchical clustering paradigm. In this work, we push forward research in hierarchical clustering of uncertain data by introducing a well-founded solution to the problem via an information-theoretic approach, following the initial idea described in our earlier work [26]. We propose a prototype-based agglomerative hierarchical clustering method, dubbed U-AHC, which employs a new uncertain linkage criterion for cluster merging. This criterion enables the comparison of (sets of) uncertain objects based on information-theoretic as well as expected-distance measures. To assess our proposal, we have conducted a comparative evaluation with state-of-the-art algorithms for clustering uncertain objects, on both benchmark and real datasets. We also compare with two basic definitions of agglomerative hierarchical clustering that are treated as baseline methods in terms of accuracy and efficiency of the clustering results, respectively. Main experimental findings reveal that U-AHC generally outperforms competing methods in accuracy and, from an efficiency viewpoint, is comparable to the fastest baseline version of agglomerative hierarchical clustering.},
	author = {Francesco Gullo and Giovanni Ponti and Andrea Tagarelli and Sergio Greco},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.03.030},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Clustering, Hierarchical clustering, Uncertain data, Information theory, Probability distributions, Mixture models},
	pages = {199-215},
	title = {An information-theoretic approach to hierarchical clustering of uncertain data},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517306266},
	volume = {402},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517306266},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.03.030}}

@article{QIN202237,
	abstract = {Online reviews play an important role in consumers' purchasing decisions. However, many online reviews confuse consumers when they wish to make a purchase but lack experience. To solve the problem of product ranking based on online reviews, two important issues must be addressed: sentiment analysis and product ranking based on multi-criteria decision-making (MCDM) methods. Therefore, this paper proposes an integrated MCDM method for product ranking through online reviews based on evidential reasoning (ER) theory and stochastic dominance (SD) rules. First, online reviews are preprocessed to obtain product attributes and weight values. Then, we use naive Bayes (NB), logistic regression (LR), and support vector machines (SVM) for the sentiment analysis of online reviews, and the results of the three classifiers are aggregated using ER theory. In addition, according to the confidence distribution matrix of sentiment orientations, SD rules are used to determine the stochastic dominance relations between pairwise alternatives for each attribute. Furthermore, we use the stochastic multi-criteria acceptability analysis (SMAA)-PROMETHEE method to obtain the final product ranking results and conduct sensitivity analysis. Finally, a case study on ranking computer products from JD Mall through online reviews is provided to illustrate the validity of the proposed method.},
	author = {Jindong Qin and Mingzhi Zeng},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.070},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Online reviews, Product ranking, Evidential reasoning, Stochastic dominance, SMAA-PROMETHEE},
	pages = {37-61},
	title = {An integrated method for product ranking through online reviews based on evidential reasoning theory and stochastic dominance},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522009811},
	volume = {612},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522009811},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.070}}

@article{HOU2022215,
	abstract = {Previous online policy opinion analyses based on social media data have focused on topic detection and sentiment classification of policy opinion after a given period following policy implementation. These approaches are limited and inefficient because they provide no opportunity to change citizens' opinions once they have been formed. Furthermore, incorporating auxiliary information to enrich semantic representations is vital and challenging due to limited texts, and a lack of both semantic information and strict syntactic structure. Therefore, we propose a novel framework to extract and integrate multidimensional features from user-related and policy-related social media information and predict policy comment polarity in the policy release phase. First, we construct four machine learning models for model-induced features to capture topic-related and opinion-related features and identify the policy-opinion nexus. In addition, we integrate basic and behavioral user features. Then, we leverage multidimensional features to construct a stacked learning model for predicting the policy opinion. Finally, we conduct experiments on 20 policy comment datasets to demonstrate that our prediction framework can effectively predict public opinion about a policy once it is released. Our model provides key insights into policy opinions in advance and can enable policymakers to engage in better policy communication before opinion formation.},
	author = {Wenju Hou and Ying Li and Yijun Liu and Qianqian Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.004},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Policy opinion, Sentiment prediction, Deep learning, Feature engineering},
	pages = {215-234},
	title = {Leveraging multidimensional features for policy opinion sentiment prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522008842},
	volume = {610},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522008842},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.004}}

@article{BICALHO201766,
	abstract = {Short texts are everywhere in the Web, including messages posted in social media, status messages and blog comments, and uncovering the topics of this type of messages is crucial to a wide range of applications, e.g., context analysis and user characterization. Extracting topics from short text is challenging because of the dependence of conventional methods, such as Latent Dirichlet Allocation, in words co-occurrence, which in short text is rare and make these methods suffer from severe data sparsity. This paper proposes a general framework for topic modeling of short text by creating larger pseudo-document representations from the original documents. In the framework, document components (e.g., words or bigrams) are defined over a metric space, which provides information about the similarity between them. We present two simple, effective and efficient methods that specialize our general framework to create larger pseudo-documents. While the first method considers word co-occurrence to define the metric space, the second relies on distributed word vector representations. The pseudo-documents generated can be given as input to any topic modeling algorithm. Experiments run in seven datasets and compared against state-of-the-art methods for extracting topics by generating pseudo-documents or modifying current topic modeling methods for short text show the methods significantly improve results in terms of normalized pointwise mutual information. A classification task was also used to evaluate the quality of the topics in terms of document representation, where improvements in F1 varied from 1.5 to 15%.},
	author = {Paulo Bicalho and Marcelo Pita and Gabriel Pedrosa and Anisio Lacerda and Gisele L. Pappa},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.02.007},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Topic modeling, Short text, Word vector representation, Pseudo-documents},
	pages = {66-81},
	title = {A general framework to expand short text for topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517304206},
	volume = {393},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517304206},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.02.007}}

@article{CHAI20221029,
	abstract = {Crowdsourcing plays a vital role in today's AI industry. However, existing crowdsourcing research mainly focuses on those simple tasks that are often formulated as label classification, while complex open-ended tasks such as question answering and translation have not received much attention. Such tasks usually have open solution spaces and non-unique true answers, which pose great challenges for designing effective crowdsourcing algorithms. In this work, we are concerned specifically with complex text annotation crowdsourcing tasks, where each answer of a task is in the form of free text. We propose an error consistency-based approach to inferring a satisfying result from a set of open-ended answers. First, each answer is represented with two vectors that capture the local word collocation and the global sentence semantics respectively. Second, the true answer is approximated by the sum of the answer vectors weighted by the reciprocals of their respective errors. Third, an algorithm called AEC (Aggregation based on Error Consistency) is designed to infer the aggregated result by maximizing the consistency of the errors of an answer in two vector spaces. Experimental results on two datasets demonstrate the effectiveness of our approach.},
	author = {Lei Chai and Hailong Sun and Zizhe Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.07.001},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Crowdsourcing, Open-ended text annotation, Answer aggregation, Annotation error consistency},
	pages = {1029-1044},
	title = {An error consistency based approach to answer aggregation in open-ended crowdsourcing},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522006909},
	volume = {608},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522006909},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.07.001}}

@article{WU2020100,
	abstract = {Text is one of the most common unstructured data, and usually, the most primary task in text mining is to transfer the text into a structured representation. However, the existing text representation models split the complete semantic unit and neglect the order of words, finally lead to understanding bias. In this paper, we propose a novel phrase-based text representation method that takes into account the integrity of semantic units and utilizes vectors to represent the similarity relationship between texts. First, we propose HPMBP (Hierarchical Phrase Mining Based on Parsing) which mines hierarchical phrases by parsing and uses BOP (Bag Of Phrases) to represent text. Then, we put forward three phrase embedding models, called Phrase2Vec, including Skip-Phrase, CBOP (Continuous Bag Of Phrases), and GloVeFP (Global Vectors For Phrase Representation). They learn the phrase vector with semantic similarity, further obtain the vector representation of the text. Based on Phrase2Vec, we propose PETC (Phrase Embedding based Text Classification) and PETCLU (Phrase Embedding based Text Clustering). PETC utilizes the phrase embedding to get the text vector, which is fed to a neural network for text classification. PETCLU gets the vectorization expression of text and cluster center by Phrase2Vec, furthermore extends the K-means model for text clustering. To the best of our knowledge, it is the first work that focuses on the phrase-based English text representation. Experiments show that the introduced Phrase2Vec outperforms state-of-the-art phrase embedding models in the similarity task and the analogical reasoning task on Enwiki, DBLP, and Yelp dataset. PETC is superior to the baseline text classification methods in the F1-value index by about 4%. PETCLU is also ahead of the prevalent text clustering methods in entropy and purity indicators. In summary, Phrase2Vec is a promising approach to text mining.},
	author = {Yongliang Wu and Shuliang Zhao and Wenbin Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.12.031},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Text representation, Phrase mining, Phrase embedding, Parsing, Text classification, Text clustering},
	pages = {100-127},
	title = {Phrase2Vec: Phrase embedding based on parsing},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519311429},
	volume = {517},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519311429},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.12.031}}

@article{ZHANG2017125,
	abstract = {Image classification refers to the task of automatically classifying the categories of images based on the contents. This task is typically solved using visual features with the histogram based classification scheme. Although effective, this strategy has two drawbacks. On one hand, histogram based representation often disregards the object layout which is very important for classification. On the other hand, visual features are unable to fully separate different images due to the semantic gap. To solve these two problems, in this paper, we propose a novel image classification method by explicitly and implicitly representing the images with searching strategy. First, to make use of object layouts, we randomly select a number of regions and then use these regions for image representations. Second, we generate the explicitly semantic representations using a number of pre-learned semantic models. Third, we measure the visual similarities with the Internet images and use the text information for implicitly semantic representations. Since Internet images are contaminated with noise, the resulting representations only implicitly reflect the contents of images. Finally, both the explicitly and implicitly semantic representations are jointly modeled for image classifications by training bi-linear classifiers. We evaluate the effectiveness of the proposed image classification by search with explicitly and implicitly semantic representations method (EISR) on the Scene-15 dataset, the MIT-Indoor dataset, the UIUC-Sports dataset and the PASCAL VOC 2007 dataset. The experimental results prove the usefulness of the proposed method.},
	author = {Chunjie Zhang and Guibo Zhu and Qingming Huang and Qi Tian},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2016.10.019},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Explicit representation, Implicit representation, Semantic modeling, Image classification},
	pages = {125-135},
	title = {Image classification by search with explicitly and implicitly semantic representations},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516312336},
	volume = {376},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516312336},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2016.10.019}}

@article{LI201883,
	abstract = {Nowadays, massive short texts, such as social media posts and newspaper titles, are available on the Internet. Analyzing these short texts is very significant for many content analysis tasks. However, the commonly used text analysis tools, i.e., topic models, lose effectiveness on short texts because of the sparsity and noise problems. Recent topic models mainly attempt to solve the sparsity problem, but neglect the noise issue. To address this, we propose a common semantics topic model (CSTM) in this paper. The key idea is to introduce a new type of topic, namely common topic, to gather the noise words. The experimental results on real-world datasets indicate that our CSTM outperforms the existing short text topic models on the traditional tasks.},
	author = {Ximing Li and Yue Wang and Ang Zhang and Changchun Li and Jinjin Chi and Jihong Ouyang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.04.071},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Short text, Topic modeling, Noise words, Topic inference},
	pages = {83-96},
	title = {Filtering out the noise in short text topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518303323},
	volume = {456},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518303323},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.04.071}}

@article{ASGHARI2022184,
	abstract = {A primary focus of the healthcare industry is to improve patient experience and quality of service. Practitioners and health workers are generating large volumes of text that are captured in Electronic Medical Records, clinical reports, and publications. Additionally, patients post millions of comments on social media related to healthcare, on diverse topics such as hospital services, disease symptoms, and drugs effects. Unifying various data sources can guide physicians and healthcare workers to avoid unnecessary, irrelevant information and expedite access to helpful information. The main challenge to creating Biomedical Natural Language Understanding is the lack of standard datasets and the extensive computational resources needed to develop different models. This paper proposes a model trained on low-tier GPU computers, producing comparable results to larger models like BioBERT. We propose BINER, a Biomedical Named Entity Recognition architecture using limited data and computational resources.},
	author = {Mohsen Asghari and Daniel Sierra-Sosa and Adel S. Elmaghraby},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.04.037},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Natural Language Processing, Named entity recognition, Deep learning, Biomedical text, Transfer Learning, Computational efficiency},
	pages = {184-200},
	title = {BINER: A low-cost biomedical named entity recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522003838},
	volume = {602},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522003838},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.04.037}}

@article{CHEN2021343,
	abstract = {Network representation learning (NRL) aims at modeling network graph by encoding vertices and edges into a low-dimensional space. These learned representations can be used for subsequent applications, such as vertex classification and link prediction. Negative Sampling (NS) is the most widely used method for boosting the performance of NRL. However, most of the existing work only randomly draws negative samples based on vertex frequencies, i.e., the vertices with higher frequency are more likely to be drawn, which ignores the situation that the sampled one may not be a true negative sample, thus, lead to undesirable embeddings. In this paper, we propose a new negative sampling method, called Hierarchical Negative Sampling (HNS), which is able to model the latent structures of vertices and learn the relations among them. During sampling, HNS can draw more appropriate negative samples and thereby obtain better performance on network embeddings. Firstly, we theoretically demonstrate the superiority of HNS over NS. And then we use experimental results to show that our proposed method outperforms the state-of-the-art models on vertex classification tasks at different training scales in real-world networks.},
	author = {Junyang Chen and Zhiguo Gong and Wei Wang and Weiwen Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.07.015},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical negative sampling, Network representation learning, Network embeddings},
	pages = {343-356},
	title = {HNS: Hierarchical negative sampling for network representation learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520306770},
	volume = {542},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520306770},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.07.015}}

@article{WANG2018110,
	abstract = {Exploring the demographic attributes and social networks of Internet users is widely employed by many applications, such as recommendation systems. The popularity of mobile devices (notably smartphones) and location-based Internet services (e.g., Google Maps) facilitates the collection of users' locations over time. There have been recent efforts to predict users' attributes (e.g., age and gender) from this data, and location-based social networks such as Foursquare and Gowalla are based on using the rich location context knowledge of points of interest (e.g., the name, type and description of restaurants and hotels) where users check-in online. However, little attention has been paid to inferring the attributes and social networks of mobile device users based on their spatiotemporal trajectories where there is little or no location context knowledge. In this paper, we collect logs of thousands of mobile devices' network connections to wireless access points (APs) of two campuses, and investigate whether one can infer mobile device users' demographic attributes and social networks solely from their spatiotemporal AP-trajectories. We develop a tensor factorization-based method Dinfer to infer mobile device users' attributes from their AP-trajectories by leveraging prior knowledge. Compared with our previous work, which only considered users' social networks, Dinfer further utilizes AP spatial information and achieves a 2% improvement. We also propose a novel method Sinfer to learn social networks between mobile device users by exploring patterns of their AP-trajectories, such as fine-grained co-occurrence events (e.g., co-coming, co-leaving, and co-presenting duration). Experimental results on real-world datasets demonstrate the effectiveness and efficiency of our methods.},
	author = {Pinghui Wang and Feiyang Sun and Di Wang and Jing Tao and Xiaohong Guan and Albert Bifet},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.06.029},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {User profiling, Spatiotemporal trajectories, Social network},
	pages = {110-128},
	title = {Predicting attributes and friends of mobile users from AP-Trajectories},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518304699},
	volume = {463-464},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518304699},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.06.029}}

@article{ZHOU20221030,
	abstract = {Multi-topic sentiment analysis, which aims to identify the topics and classify their corresponding sentiment, is of great value in understanding consumers' behaviour and improving services. Because of the high cost of manual annotation of the datasets, topic model-based approaches that model the joint distributions of both topics and sentiments have been studied previously. Some studies proposed models that leverage the prior knowledge derived from the pre-trained word embeddings and have proven effective. However, most of the existing models are based on the assumption that words and topics are conditionally independent, ignoring the dependency relations among them. Additionally, the fine-tuning of the pre-trained word embeddings to incorporate the contextual information is also neglected in these models. This could result in the ambiguous representations of topics. In this paper, we propose a novel weakly-supervised graph-based joint sentiment topic model (W-GJST) that integrates an edge-gated graph convolutional network (E-GCN) into a joint sentiment-topic model. An importance sampling-based training method is proposed to learn the contextual representations of topics and words efficiently. Additionally, a self-training multi-topic classifier is designed for the multi-label topic identification. Experiments on two benchmark datasets demonstrate the superiority of the proposed W-GJST compared to the baseline models in terms of topic modelling, topic identification and topic-sentiment identification.},
	author = {Tao Zhou and Kris Law and Douglas Creighton},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.07.126},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multi-topic sentiment analysis, Edge-gated graph convolutional network, Joint sentiment-topic model, Self-training},
	pages = {1030-1051},
	title = {A weakly-supervised graph-based joint sentiment topic model for multi-topic sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522008192},
	volume = {609},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522008192},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.07.126}}

@article{KONG202039,
	abstract = {Sentiment classification is an important research task in Natural Language Processing. To fulfill this type of classification, previous works have focused on leveraging task-specific features. However, they only notice part of the related features. Also, state-of-the-art methods based on neural networks often ignore traditional features. This paper proposes a novel text sentiment classification method that learns the representation of texts by hierarchically incorporating multiple features. More specifically, we design different representations for sentiment words according to the polarity of labeled texts and whether negation exists; we distinguish words with different part-of-speech tags; emoticons, if there are, are to optimize the word vectors obtained in the previous step; apart from word embeddings, character embeddings are also trained. We use a deep neural network to get a sentence-level representation from both word and character sequence. For documents with at least two sentences, we use a hierarchical structure and design a rule to give more weight to import sentences empirically to get a document-level representation. Experimental results on open datasets demonstrate that our method could effectively improve the sentiment classification performance compared with the basic models and state-of-the-art methods.},
	author = {Li Kong and Chuanyi Li and Jidong Ge and FeiFei Zhang and Yi Feng and Zhongjin Li and Bin Luo},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.012},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment classification, Deep learning, Feature engineering},
	pages = {39-55},
	title = {Leveraging multiple features for document sentiment classification},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552030013X},
	volume = {518},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552030013X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.012}}

@article{LEI202098,
	abstract = {Recommendation system (RS) is designed to provide personalized services based on the users' historical data. It has been applied in various fields and is expected to recommend the suitable services for the different kinds of users. Considering the importance of individual privacy, current users gradually tend not to expose personal information. This means RS may face the highly sparse datasets in the fields of cloud security. In general, the accuracy of recommendation will be improved with the growth of individual data, but the cold start problem is exactly in this contradictory phenomenon: this question evolves to produce sufficiently accurate recommendation result under the data scarcity problem. RS has to recommend services for the rarely historical data users and the latent users might drain along with the production of counter effects. To alleviate data scarcity problem in cloud security environment, this work is to introduce similar domain knowledge based on the transfer learning. Besides, the content and location based methods have been proved that these ideas work under this situation. So, this work also employs latent dirichlet allocation (LDA) to analysis the service descriptions and explore the relationship between the content and location information. In this framework, the suitable combination of LDA and word2vec models will balance the accuracy and speed which benefit service recommendation particularly. The related experiments demonstrate the effectiveness on the real word dataset. It can be found that the transfer learning based word2vec model shows the potentiality to explore the relationship between topic words, and improve the LDA algorithm from the content relationship. This proves that in both cold start environment and warm start environment, the proposed algorithm is more robust than other model-based state-of-art methods.},
	author = {Chao Lei and Hongjun Dai and Zhilou Yu and Rui Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.10.004},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Service recommendation, Transfer learning, Cloud security},
	pages = {98-111},
	title = {A service recommendation algorithm with the transfer learning based matrix factorization to improve cloud security},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309582},
	volume = {513},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519309582},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.10.004}}

@article{DONG2020203,
	abstract = {There has been considerable interest in transforming unstructured social tagging data into structured knowledge for semantic-based retrieval and recommendation. Research in this line mostly exploits data co-occurrence and often overlooks the complex and ambiguous meanings of tags. Furthermore, there have been few comprehensive evaluation studies regarding the quality of the discovered knowledge. We propose a supervised learning method to discover subsumption relations from tags. The key to this method is quantifying the probabilistic association among tags to better characterise their relations. We further develop an algorithm to organise tags into hierarchies based on the learned relations. Experiments were conducted using a large, publicly available dataset, Bibsonomy, and three popular, human-engineered or data-driven knowledge bases: DBpedia, Microsoft Concept Graph, and ACM Computing Classification System. We performed a comprehensive evaluation using different strategies: relation-level, ontology-level, and knowledge base enrichment based evaluation. The results clearly show that the proposed method can extract knowledge of better quality than the existing methods against the gold standard knowledge bases. The proposed approach can also enrich knowledge bases with new subsumption relations, having the potential to significantly reduce time and human effort for knowledge base maintenance and ontology evolution.},
	author = {Hang Dong and Wei Wang and Frans Coenen and Kaizhu Huang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.04.002},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Knowledge discovery, Knowledge base enrichment, Ontology learning, Social tagging, Probabilistic association analysis, Classification},
	pages = {203-220},
	title = {Knowledge base enrichment by relation learning from social tagging data},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520303017},
	volume = {526},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520303017},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.04.002}}

@article{WANG2021762,
	abstract = {A recommender system utilizes information filtering techniques to help users obtain accurate information effectively and efficiently. The existing recommender systems, however, recommend items based on the overall ratings or the click-through rate, and emotions expressed by users are neglected. Conversely, the cold-start problem and low model scalability are the two main problems with recommender systems. The cold-start problem is encountered when the system lacks initial rating. Low model scalability indicates that a model is incapable of coping with high-dimensional data. These two problems may mislead the recommender system, and thus, users will not be satisfied with the recommended items. A hybrid recommender system is proposed to mitigate the negative effects caused by these problems. Additionally, ontologies are applied to integrate the extracted features into topics to reduce dimensionality. Topics mentioned in the items are displayed in the form of a topic map, and users can refer to these similar items for further information.},
	author = {Hei-Chia Wang and Hsu-Tung Jhou and Yu-Shan Tsai},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.04.015},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hybrid recommender system, Cold start, Social network, Ontology, Sentiment analysis},
	pages = {762-778},
	title = {Adapting topic map and social influence to the personalized hybrid recommender system},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518302718},
	volume = {575},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518302718},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.04.015}}

@article{CHAUDHURI2022,
	abstract = {Extraneous growth of scientific information over the Internet makes the searching task non-trivial and as a consequence researchers are facing difficulties in finding relevant papers from the millions of research papers in digital repositories. The research paper recommendation systems have been advocated to address this problem. The existing research paper recommendation systems lack in exploiting prominent information of papers, such as relevancy with the current time, novelty, scientific contribution, writing complexity of the papers, etc. Further, the existing models emphasize only on user's preference rather than user's intention that may change with time. Furthermore, the existing models do not consider a sound ranking strategy to unleash the personalization aspect and relevancy of papers. This work aims to address the existing limitations and proposes a systematic hidden attribute-based recommendation engine (SHARE). SHARE utilizes a feature engineering technique to unfold valuable insights of papers through multiple hidden features. These features are used as a context for users as well as multiple criteria for ranking papers. Additionally, SHARE predicts a user's intention beyond the user's preference to capture the dynamic notion of a user. Finally, a novel ranking strategy is proposed to retrieve personalized and the most important papers. SHARE is flexible for recommending both old and new users. In order to evaluate the effectiveness of SHARE both user studies and system evaluations were performed. Experimental results substantiate the efficacy of the proposed approach and are comparable to the existing systems.},
	author = {Arpita Chaudhuri and Monalisa Sarma and Debasis Samanta},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.09.064},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Research paper recommendation, Content-based recommendation, User modeling, Multi-criteria analysis, Hybrid ranking, Personalized recommendation},
	title = {SHARE: Designing Multiple Criteria-Based Personalized Research Paper Recommendation System},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552201115X},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552201115X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.09.064}}

@article{LIU2018219,
	abstract = {Social media offers a new communication channel for users and affords an interactive opportunity between users and the firms about the products and the brands. Understanding what topics are important to users and the corresponding internal motivation is crucial for managers to successfully engage customers and promote business through social media. Assuming topic preference is the outcome of intrinsic factors such as gender, age and personality traits, this paper proposes an improved nonparametric hierarchical Bayesian topic (NHBT) model to investigate the multiple-to-multiple generative relationships from intrinsic factors to topic preferences. The proposed NHBT model employs a three-level generation framework based on Dirichlet process to study the impact of intrinsic factors on users topic preference. Our study of Facebook data shows that NHBT model is able to draw valuable latent topics (e.g. music band, chemical biology, cosplay) from the open social media environment, and reveal the internal motivation for users topic selection behaviors (e.g. users with low conscientiousness and high extraversion personality prefer topics about campus party). Our experiments also show that NHBT model can identify the intrinsic factors dominating topic preferences for individual users, and provide foundations to predict the intrinsic factors for new user generated contents.},
	author = {Yezheng Liu and Jiajia Wang and Yuanchun Jiang and Jianshan Sun and Jennifer Shang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.09.041},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Social media, Topic preference, Intrinsic factor, Nonparametric hierarchical Bayesian model},
	pages = {219-234},
	title = {Identifying impact of intrinsic factors on topic preferences in online social media: A nonparametric hierarchical Bayesian approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516313706},
	volume = {423},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516313706},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.09.041}}

@article{ZHENG2022211,
	abstract = {Friend link prediction is an important research problem in recommender systems. Existing network embedding and knowledge embedding methods mainly consider the structural relationships of entities, ignoring the explanatory role of text contents. In this paper, we present an explainable friend link recommendation method that leverages direct and indirect similarity of user pairs via fusion embedding of heterogeneous context information. In social networks, while considering user content interests, first, a fusion user embedding method was developed by incorporating external knowledge semantics. Second, for a user pair, we calculate their direct similar relationship using fusion user embeddings. Additionally, based on intermediate neighbors, we compute their indirect similar relationship by using an attention mechanism, which explains neighbors' bootable and transitive influences for learning the social relationship of user pairs. Then, a hybrid personalized and neighbor attention model for friend link prediction was proposed by considering direct and indirect factors. Finally, experiments were conducted on the Sina Weibo datasets, which indicates that the proposed method effectively predicts the friends of users and provides a good interpretation for link prediction recommendation results.},
	author = {Jianxing Zheng and Zifeng Qin and Suge Wang and Deyu Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.03.010},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Explainable friend link prediction, Attention mechanism, Fusion user embedding, Heterogeneous context information},
	pages = {211-229},
	title = {Attention-based explainable friend link prediction with heterogeneous context information},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522002092},
	volume = {597},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522002092},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.03.010}}

@article{DAS2021279,
	abstract = {Nowadays, the increase in criminal activities has resulted in a massive generation of crime reports describing the details of the crime incidents. Analyzing these reports for crime type prediction helps the law enforcement agencies deal with crime prevention strategies. But it is quite a demanding and difficult task to consider these reports individually and determine their crime types. In the proposed work, an efficient classifier has been designed to analyze the crime reports which not only predict the crime types of the reports but at the same time upgrades itself with the help of new crime reports. Therefore, this task demands an incremental supervised learning technique that continuously learns the existing classifier based on the new set of reports and information already extracted from the old set of reports. Developing an incremental classifier infuses the knowledge that keep coming from the newly generated reports and help in increasing the report-discriminating power of the classifier. In this work, we have applied a Bi-objective Particle Swarm Optimization technique for generating an efficient incremental classifier for classifying and predicting the crime reports dynamically. Crime reports of different countries, such as India, the United States of America, and the United Arab Emirates, have been collected from online classified newspapers to measure the performance of the proposed as well as some state-of-the-art classifiers. Also, the method has been evaluated based on an unbiased police witness narrative crime reports and finally, a statistical test has been performed using all four considered datasets to measure the statistical significance of the proposed methodology.},
	author = {Priyanka Das and Asit Kumar Das and Janmenjoy Nayak and Danilo Pelusi and Weiping Ding},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.02.002},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Crime report prediction, Particle Swarm Optimization, Incremental classifier, Named entity recognition, Multi-objective optimization},
	pages = {279-303},
	title = {Incremental classifier in crime prediction using bi-objective Particle Swarm Optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521001225},
	volume = {562},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521001225},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.02.002}}

@article{PHAN2021243,
	abstract = {Social networks are a very popular channel for people to communicate with, to find, to reference other users before making decisions, especially those concerning purchase. How can users' opinions within social networks be used in making decisions cost-effective and reliable? In this paper, we propose an approach for supporting decision-making based on measuring the user satisfaction level by analyzing the sentiment of aspects and mining the fuzzy decision trees. Our proposal has been proved to overcome some of the disadvantages of previous methods. Specifically, we consider the fuzzy sentiments of users for aspects and the effects of user satisfaction, dissatisfaction, and hesitation for decision-making. The proposed method comprises four main stages. The first stage identifies a topic, which the user is interested. In the second stage, aspects of the topic and their sentiments within tweets are extracted. At the third stage, the user satisfaction level is calculated according to each kind of sentiment identified in the second step. Finally, a decision matrix is constructed, and the fuzzy decision tree is built to generate a set of rules for supporting users in decision-making. The experiments using tweets show that the proposed method achieves promising results regarding the accuracy and gained information.},
	author = {Huyen Trang Phan and Ngoc Thanh Nguyen and Van Cuong Tran and Dosam Hwang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.01.008},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment analysis, Decision-making, Fuzzy decision tree, User satisfaction level, Fuzzy sentiment phrase, BiLSTM-CRF model},
	pages = {243-273},
	title = {An approach for a decision-making support system based on measuring the user satisfaction level on Twitter},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000098},
	volume = {561},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000098},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.01.008}}

@article{NAKAMURA2021482,
	abstract = {Most work on musical score models (a.k.a.musical language models) for music transcription has focused on describing the local sequential dependence of notes in musical scores and failed to capture their global repetitive structure, which can be a useful guide for transcribing music. Focusing on rhythm, we formulate several classes of Bayesian Markov models of musical scores that describe repetitions indirectly using the sparse transition probabilities of notes or note patterns. This enables us to construct piece-specific models for unseen scores with an unfixed repetitive structure and to derive tractable inference algorithms. Moreover, to describe approximate repetitions, we explicitly incorporate a process for modifying the repeated notes/note patterns. We apply these models as prior musical score models for rhythm transcription, where piece-specific score models are inferred from performed MIDI data by Bayesian learning, in contrast to the conventional supervised construction of score models. Evaluations using the vocal melodies of popular music showed that the Bayesian models improved the transcription accuracy for most of the tested model types, indicating the universal efficacy of the proposed approach. Moreover, we found an effective data representation for modelling rhythms that maximizes the transcription accuracy and computational efficiency.},
	author = {Eita Nakamura and Kazuyoshi Yoshii},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.04.100},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Music transcription, Musical language model, Bayesian modelling, Symbolic music processing, Musical rhythm},
	pages = {482-500},
	title = {Musical rhythm transcription based on Bayesian piece-specific score models capturing repetitions},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521004412},
	volume = {572},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521004412},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.04.100}}

@article{WANG2021136,
	abstract = {Cross-modal hashing has been intensively studied to efficiently retrieve multi-modal data across modalities. Supervised cross-modal hashing methods leverage the labels of training data to improve the retrieval performance. However, most of these methods still assume that the semantic labels of training data are ideally complete and noise-free. This assumption is too optimistic for real multi-modal data, whose label annotations are, in essence, error-prone. To achieve effective cross-modal hashing on multi-modal data with noisy labels, we introduce an end-to-end solution called Noise-robust Deep Cross-modal Hashing (NrDCMH). NrDCMH contains two main components: a noise instance detection module and a hash code learning module. In the noise detection module, NrDCMH firstly detects noisy training instance pairs based on the margin between the label similarity and feature similarity, and specifies weights to pairs using the margin. In the hash learning module, NrDCMH incorporates the weights into a likelihood loss function to reduce the impact of instances with noisy labels and to learn compatible deep features by applying different neural networks on multi-modality data in a unified end-to-end framework. Experimental results on multi-modal benchmark datasets demonstrate that NrDCMH performs significantly better than competitive methods with noisy label annotations. NrDCMH also achieves competitive results in `noise-free' scenarios.},
	author = {Runmin Wang and Guoxian Yu and Hong Zhang and Maozu Guo and Lizhen Cui and Xiangliang Zhang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.09.030},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-modal hashing, Noise labels, Deep learning, Feature similarity, Label similarity},
	pages = {136-154},
	title = {Noise-robust Deep Cross-Modal Hashing},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521009610},
	volume = {581},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521009610},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.09.030}}

@article{FENG202279,
	abstract = {As one of the prevalent topic mining methods, neural topic modeling has attracted a lot of interests due to the advantages of low training costs and strong generalisation abilities. However, the existing neural topic models may suffer from the feature sparsity problem when applied to short texts, due to the lack of context in each message. To alleviate this issue, we propose a Context Reinforced Neural Topic Model (CRNTM), whose characteristics can be summarized as follows. First, by assuming that each short text covers only a few salient topics, the proposed CRNTM infers the topic for each word in a narrow range. Second, our model exploits pre-trained word embeddings by treating topics as multivariate Gaussian distributions or Gaussian mixture distributions in the embedding space. Extensive experiments on two benchmark short corpora validate the effectiveness of the proposed model on both topic discovery and text classification.},
	author = {Jiachun Feng and Zusheng Zhang and Cheng Ding and Yanghui Rao and Haoran Xie and Fu Lee Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.05.098},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Neural topic model, Short texts, Context reinforcement},
	pages = {79-91},
	title = {Context reinforced neural topic modeling over short texts},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522005369},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522005369},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.05.098}}

@article{DAI2018216,
	abstract = {Representing variable length texts (e.g., sentences, documents) with low-dimensional continuous vectors has been a topic of recent interest due to its successful applications in various NLP tasks. During the learning process, most of existing methods tend to treat all the words equally regardless of their possibly different intrinsic nature. We believe that for some types of documents (e.g., news articles), entity mentions are more informative than ordinary words and it can be beneficial for certain tasks if they are properly utilized. In this paper, we propose a novel approach for learning low-dimensional vector representations of documents. The learned representations captures information of not only the words in documents, but also the entity mentions in documents and the connections between different entities. Experimental results demonstrate that our approach is able to significantly improve text clustering, text classification performance and outperform previous studies on the TAC-KBP entity linking benchmark.},
	author = {Hongliang Dai and Siliang Tang and Fei Wu and Yueting Zhuang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.11.032},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Distributed representation, Text clustering, Text classification, Entity linking},
	pages = {216-227},
	title = {Entity mention aware document representation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517310952},
	volume = {430-431},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517310952},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.11.032}}

@article{SONG201883,
	abstract = {This paper presents a novel framework for high-level activity analysis based on late fusion using multi-independent temporal perception layers. The method allows us to handle temporal diversity of high-level activities. The framework consists of multi-temporal analysis, multi-temporal perception layers, and late fusion. We build two types of perception layers based on situation graph trees (SGT) and support vector machines (SVMs). The results obtained from the multi-temporal perception layers are fused into an activity score through a step of late fusion. To verify this approach, we apply the framework to violent events detection in visual surveillance and experiments are conducted by using three datasets: BEHAVE, NUS--HGA and some videos from YouTube that show real situations. We also compare the proposed framework with existing single-temporal frameworks. The experiments produced results with accuracy of 0.783 (SGT-based, BEHAVE), 0.702 (SVM-based, BEHAVE), 0.872 (SGT-based, NUS--HGA), and 0.699 (SGT-based, YouTube), thereby showing that using our multi-temporal approach has advantages over single-temporal methods.},
	author = {Donghui Song and Chansu Kim and Sung-Kee Park},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.02.065},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Computer vision, Multi-temporal framework, High-level activity analysis, Violent event detection, Late fusion, Visual surveillance},
	pages = {83-103},
	title = {A multi-temporal framework for high-level activity analysis: Violent event detection in visual surveillance},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518301592},
	volume = {447},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518301592},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.02.065}}

@article{LIU2020227,
	abstract = {Question retrieval is an extremely important research field in Community Question Answering (CQA). Most existing question retrieval methods depend on semantic analysis of questions, whose effectiveness suffers from the short texts of the noise words in the question corpus. In order to recommend the questions with more advanced knowledge to users, the influence of the questions' popularity should be considered during retrieving questions. To make retrieved questions with both similar semantics and high popularity, we propose an Integrated Retrieval Framework for Similar Questions named Word-semantic Embedded Label Clustering -- LDA with Question Life Cycle (WELQLC-QR), consisting of Word-semantic Embedded Label Clustering -- LDA (WEL) and Question Life Cycle Optimization Similar Question List Strategy (QLC). Firstly, WEL is proposed for question retrieval from the perspective of semantic matching. It not only overcomes the problem of over-generalization of the semantic information extracted by topic models when facing short questions with multi-levels labels, but also avoids the influence of noise vocabularies during semantic extracting of the questions. Then, based on the internal factors (i.e., the number of comments and answers to the question) and external factors (i.e., programming language ranking information) of questions, QLC constructs a popularity-predicted model to optimize the similar question set searched by WEL, making the final retrieval results both semantically similar and popular. Finally, experiments are conducted on CQADupStack dataset, and results show that the MRR@N of WELQLC-QR model has an average increase of 8.99%, 8.3%, 4.74% and 3.56% compared with that of L-LDA, LC-LDA, BM25 and Word2vec, respectively.},
	author = {Yue Liu and Aihua Tang and Zhibin Sun and Weize Tang and Fei Cai and Chengjin Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.05.014},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {CQA, Question retrieval, Product life cycle, Semantic representation},
	pages = {227-245},
	title = {An integrated retrieval framework for similar questions: Word-semantic embedded label clustering -- LDA with question life cycle},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520304060},
	volume = {537},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520304060},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.05.014}}

@article{ETEMADI2022,
	abstract = {Finding a qualified individual who can independently answer a question on a community question answering platform is becoming more challenging due to the increasing multidisciplinary nature of posted questions. As such, finding a group of experts to collaboratively answer the questions is of paramount importance. To this end, we propose a novel approach to form teams of experts who can collectively answer new questions. The proposed approach, called team2box, learns neural embedding representations based on the content of the posted questions, experts' engagement with these questions, and past expert collaboration history in order to form a team to answer the posted question. It embeds experts and questions as points and existing teams as regions within the embedding space. Therefore, team2box forms a team whose members (1) collectively cover the knowledge required to answer a question, (2) have successful past experience in jointly answering similar questions, and (3) can work efficiently together to answer the question. Extensive experiments on real-life datasets from Stack Exchange show that team2box outperforms the state-of-the-art by discovering teams with on average 38.97% more covering the skills required to answer new questions and employing experts with collectively a high expertise level.},
	author = {Roohollah Etemadi and Morteza Zihayat and Kuan Feng and Jason Adelman and Ebrahim Bagheri},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.09.036},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Team Formation, Network Embedding, Learn to Ranking, Skill Coverage},
	title = {Embedding-based Team Formation for Community Question Answering},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522010829},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522010829},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.09.036}}

@article{CHEN2018519,
	abstract = {While web services avail a rapid growth of data volume for use, identifying helpful information is of great value, especially when users face with an unwilling glut of information. Thus, it is deemed relevant and meaningful to provide users with a representative subset (i.e., small set) that could well reflect the original information corpus (i.e., large set). In such a large--small context, this paper addresses the issues of representativeness in light of measurement and extraction by reviewing our previous efforts. Specifically, we first discuss various metrics from different perspectives of representativeness, then present a series of related representativeness extraction methods. Finally as a supplement and extension, a recent effort is introduced, which aims to take information quality into account in deriving a ranked subset. The proposed extraction method is justified by extensive real-world data experiments, showing its superiority to others in both effectiveness and efficiency.},
	author = {Guoqing Chen and Cong Wang and Mingyue Zhang and Qiang Wei and Baojun Ma},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.08.096},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Representative, Coverage, Redundancy, Consistency, Compactness, Extraction},
	pages = {519-540},
	title = {How ``small'' reflects ``large''?---Representative information measurement and extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517309246},
	volume = {460-461},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517309246},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.08.096}}

@article{LIN2019483,
	abstract = {Internet has become the most popular platform for people to exchange opinions and express stances. The stances implied in web texts indicate people's fundamental beliefs and viewpoints. Understanding the stances people take is beneficial and critical for many security and business related applications, such as policy design, emergency response and marketing management. Most previous work on stance detection focuses on identifying the supportive or unsupportive attitudes towards a specific target. However, another important type of stance detection, i.e. multiple standpoint detection, has been largely ignored. Multiple standpoint detection aims to identify the distinct standpoints people hold among multiple parties, which reflects people's intrinsic values and judgments. When expressing standpoints, people tend to discuss diverse topics, and the word usage in the topics of different standpoints often varies a lot. As topics can provide latent information for identifying various standpoints, in this paper, we propose a topic-based approach to detecting multiple standpoints in Web texts, by enhancing generative classification model as well as feature representation of texts. In addition, we develop an adaptive process to determine parameter values in our approach automatically. Experimental studies on several real-world datasets verify the effectiveness of our proposed approach in detecting multiple standpoints.},
	author = {Junjie Lin and Qingchao Kong and Wenji Mao and Lei Wang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.05.068},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multiple standpoint detection, Topic model, Topic enhanced approach, Adaptive parameter determination},
	pages = {483-494},
	title = {A topic enhanced approach to detecting multiple standpoints in web texts},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519304852},
	volume = {501},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519304852},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.05.068}}

@article{LI201961,
	abstract = {With the rapid development of web technology, the social networks have become the largest information portals. In the social platforms, the text information can effectively reflect the user opinions or the public opinions for a certain entity, such as company, celebrity service, product and so on. Therefore, mining user opinions from social networks have become an imperative requirement for the service groups. In this paper, an opinion community detection method is proposed by considering the content similarity, the time similarity and the topology structure of users. The integrated similarity between two users, which includes the content similarity, the time similarity and the topology structure of users, is achieved. Then, based on the integrated similarities, the opinion communities are detected. Furthermore, in order to identify the opinion leader, an opinion leader detection method is proposed based on the user influence and emotional analysis. The users with the same topic form the opinion community. Meanwhile, a directed graph is created to formulate the interaction relationship between users in the opinion community. Then, the user influence model and emotional analysis model are presented. Moreover, the occurrence frequencies of the negative words are also considered in the emotional analysis model. Then, a model of influence value for each user in the opinion community is built. The user with the highest influence value is considered as the opinion leader. Finally, the performances of the proposed algorithms are evaluated in a distributed computing environment. Meanwhile, the extensive experiments are conducted. The results indicate that our proposed opinion community detection algorithm can effectively detect the opinion communities. Also, the proposed opinion leader detection algorithm can significantly identify the opinion leader in the social networks.},
	author = {Chunlin Li and Jingpan Bai and Lei Zhang and Hengliang Tang and Youlong Luo},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.06.060},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Social networks, Opinion community detection, Opinion leader detection, Cloud computing},
	pages = {61-83},
	title = {Opinion community detection and opinion leader detection based on text information and network topology in cloud environment},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519305985},
	volume = {504},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519305985},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.06.060}}

@article{XUAN2017263,
	abstract = {After a news event, many different websites publish coverage of that event, each expressing their own unique commentary, perspectives, and viewpoints. Websites form around a specific set of interests to cater to different audiences, and discovering these interests can help audiences C especially people and organizations that are interested in news C select the most appropriate websites to use as their sources of information. This paper presents three methods for formally defining and mining a websites interests, each of which is explicitly or implicitly based on a hierarchial structure: website-webpage-keyword. The first, and most straightforward, method explicitly uses keyword-layer network communities and the mapping relations between websites and keywords. The second method expands upon the first method with an iterative algorithm that combines both the mapping relations and the network relations from the website-webpage-keyword structure to further refine the keyword-layer network communities. In the third method, a website topic model implicitly captures the mapping relations among the websites, webpages, and keywords. The performance of three proposed methods in website interest mining is compared using a bespoke evaluation metric. The experimental results show that the iterative procedure designed in the second method is able to improve website interest mining performance, and the website topic model in the third method achieves the best performance among the three methods.},
	author = {Junyu Xuan and Xiangfeng Luo and Jie Lu and Guangquan Zhang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.08.056},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Text mining, Web mining, News event, Website interest},
	pages = {263-277},
	title = {Explicitly and implicitly exploiting the hierarchical structure for mining website interests on news events},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517308952},
	volume = {420},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517308952},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.08.056}}

@article{LU202087,
	abstract = {Heaven Ape (HAPE) is an integrated big knowledge graph platform supporting the construction, management, and operation of large to massive scale knowledge graphs. Its current version described in this paper is a prototype, which consists of three parts: a big knowledge graph knowledge base, a knowledge graph browser on the client side, and a knowledge graph operating system on the server side. The platform is programmed in two high level scripting languages: JavaScript for programming the client side functions and Python for the server side functions. For making the programming more suitable for big knowledge processing and more friendly to knowledge programmers, we have developed two versions of knowledge scripting languages, namely K-script-c and K-script-s, for performing very high level knowledge programming of client resp. server side functions. HAPE borrows ideas from some well-known knowledge graph processing techniques and also invents some new ones as our creation. As an experiment, we transformed a major part of the DBpedia knowledge base and reconstructed it as a big knowledge graph. It works well in some application tests and provides acceptable efficiency.},
	author = {Ruqian LU and Chaoqun FEI and Chuanqing WANG and Shunfeng GAO and Han QIU and Songmao ZHANG and Cungen CAO},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.08.051},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Big knowledge, Big knowledge system, Big knowledge graph, Knowledge graph browser, Knowledge graph operating system, Knowledge scripting language, Big knowledge security},
	pages = {87-103},
	title = {HAPE: A programmable big knowledge graph platform},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551930800X},
	volume = {509},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002551930800X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.08.051}}

@article{KHODABAKHSH20221,
	abstract = {The focus of our work is the ad hoc table retrieval task, which aims to rank a list of structured tabular objects in response to a user query. Given the importance of this task, various methods have already been proposed in the literature that focus on syntactic, semantic and neural representations of tables for determining table relevance. However, recent works have highlighted queries that are consistently difficult for baseline methods to satisfy, referred to as hard queries. For this reason, the objectives of this paper include: (1) effectively satisfying hard queries by proposing three classes of qualitative measures, namely coherence, interpretability and exactness, (2) offering a systematic approach to interpolate these three classes of measures with each other and with baseline table retrieval methods, and (3) performing extensive experiments using a range of baseline retrieval methods to show the feasibility of the proposed measures for hard queries. We demonstrate that the consideration of the proposed qualitative measures will lead to improved performance for hard queries on a range of state-of-the-art ad hoc table retrieval baselines. We further show that our proposed measures are synergistic and will lead to even higher performance improvements over the baselines when interpolated with each other. The improvements measure up to 22.94% on the Semantic Table Retrieval (STR) method with an NDCG@20 of 0.5, which is superior to the performance of any state-of-the-art baseline for hard queries in the ad hoc table retrieval task.},
	author = {Maryam Khodabakhsh and Ebrahim Bagheri},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.05.080},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Qualitative measures, Ad hoc table retrieval, Deep contextual embeddings},
	pages = {1-26},
	title = {Qualitative measures for ad hoc table retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522005126},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522005126},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.05.080}}

@article{LU201959,
	abstract = {User requirements for result diversification in image retrieval have been increasing with the explosion of image resources. Result diversification requires that image retrieval systems are made capable of handling semantic gaps between image visual features and semantic concepts, and providing both relevant and diversified image results. Context information, such as captions, descriptions, and tags, provides opportunities for image retrieval systems to improve their result diversification. This study explores a mechanism for improving result diversification using the semantic distance of image social tags. We design and compare nine strategies that combine three different semantic distance algorithms (WordNet, Google Distance, and Explicit Semantic Analysis) with three re-ranking algorithms (MMR, xQuAD, and Score Difference) for result diversification. In order to better prove the effectiveness of our strategy of applying semantic information, we also make use of visual features of images for result diversification experiment and make comparison. Our data for experimentation were extracted from 269,648 images selected from the NUS-WIDE datasets with manually annotated subtopics. Experimental results affirm the effectiveness of applying semantic information for improving result diversification in image retrieval. In particular, WordNet-based semantic distance combined with the Score Difference (WordNet-DivScore) outperformed other strategies in diversifying image retrieval results.},
	author = {Wei Lu and Mengqi Luo and Zhenyu Zhang and Guobiao Zhang and Heng Ding and Haihua Chen and Jiangping Chen},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.06.020},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Image retrieval, Result diversification, Semantic distance algorithm, Social tag, Re-ranking algorithm},
	pages = {59-75},
	title = {Result diversification in image retrieval based on semantic distance},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519305584},
	volume = {502},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519305584},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.06.020}}

@article{HUANG202018,
	abstract = {Co-clustering aims to explore coherent patterns by simultaneously clustering samples and features of data. Several co-clustering methods have been proposed in the past decades. However, in real-world applications, datasets are often with multiple modalities or composed of multiple representations (i.e., views), which provide different yet complementary information. Hence, it is essential to develop multi-view co-clustering models to solve the multi-view application problems. In this paper, a novel multi-view co-clustering method based on bipartite graphs is proposed. To make use of the duality between samples and features of multi-view data, a bipartite graph for each view is constructed such that the co-occurring structure of data can be extracted. The key point of utilizing the bipartite graphs to deal with the multi-view co-clustering task is to reasonably integrate these bipartite graphs and obtain an optimal consensus one. As for this point, the proposed method can learn an optimal weight for each bipartite graph automatically without introducing an additive parameter as previous methods do. Furthermore, an efficient algorithm is proposed to optimize this model with theoretically guaranteed convergence. Extensive experimental results on both toy data and several benchmark datasets have demonstrated the effectiveness of the proposed model.},
	author = {Shudong Huang and Zenglin Xu and Ivor W. Tsang and Zhao Kang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.09.079},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multi-view learning, Co-clustering, Bipartite graph learning, Auto-weighted strategy},
	pages = {18-30},
	title = {Auto-weighted multi-view co-clustering with bipartite graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309302},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519309302},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.09.079}}

@article{TERRAGNI2020581,
	abstract = {Relational topic models (RTM) have been widely used to discover hidden topics in a collection of networked documents. In this paper, we introduce the class of Constrained Relational Topic Models (CRTM), a semi-supervised extension of RTM that, apart from modeling the structure of the document network, explicitly models some available domain knowledge. We propose two instances of CRTM that incorporate prior knowledge in the form of document constraints. The models smooth the probability distribution of topics such that two constrained documents can either share the same topics or denote distinct themes. Experimental results on benchmark relational datasets show significant performances of CRTM on a semi-supervised document classification task.},
	author = {Silvia Terragni and Elisabetta Fersini and Enza Messina},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.09.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Constrained Relational Topic Models, Semi-supervised model, Latent Dirichlet Allocation, Domain knowledge},
	pages = {581-594},
	title = {Constrained Relational Topic Models},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519308850},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519308850},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.09.039}}

@article{KIM201915,
	abstract = {The purpose of document classification is to assign the most appropriate label to a specified document. The main challenges in document classification are insufficient label information and unstructured sparse format. A semi-supervised learning (SSL) approach could be an effective solution to the former problem, whereas the consideration of multiple document representation schemes can resolve the latter problem. Co-training is a popular SSL method that attempts to exploit various perspectives in terms of feature subsets for the same example. In this paper, we propose multi-co-training (MCT) for improving the performance of document classification. In order to increase the variety of feature sets for classification, we transform a document using three document representation methods: term frequency--inverse document frequency (TF--IDF) based on the bag-of-words scheme, topic distribution based on latent Dirichlet allocation (LDA), and neural-network-based document embedding known as document to vector (Doc2Vec). The experimental results demonstrate that the proposed MCT is robust to parameter changes and outperforms benchmark methods under various conditions.},
	author = {Donghwa Kim and Deokseong Seo and Suhyoun Cho and Pilsung Kang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.10.006},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Document classification, Semi-supervised learning, TF--IDF, LDA, Doc2vec, Co-training},
	pages = {15-29},
	title = {Multi-co-training for document classification using various document representations: TF--IDF, LDA, and Doc2Vec},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518308028},
	volume = {477},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518308028},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.10.006}}

@article{MAO2019269,
	abstract = {Recommender systems are emerging in e-commerce as important promotion tools to assist customers to discover potentially interesting items. Currently, most of these are single-objective and search for items that fit the overall preference of a particular user. In real applications, such as restaurant recommendations, however, users often have multiple objectives such as group preferences and restaurant ambiance. This paper highlights the need for multi-objective recommendations and provides a solution using hypergraph ranking. A general User--Item--Attribute--Context data model is proposed to summarize different information resources and high-order relationships for the construction of a multipartite hypergraph. This study develops an improved balanced hypergraph ranking method to rank different types of objects in hypergraph data. An overall framework is then proposed as a guideline for the implementation of multi-objective recommender systems. Empirical experiments are conducted with the dataset from a review site Yelp.com, and the outcomes demonstrate that the proposed model performs very well for multi-objective recommendations. The experiments also demonstrate that this framework is still compatible for traditional single-objective recommendations and can improve accuracy significantly. In conclusion, the proposed multi-objective recommendation framework is able to handle complex and changing demands for e-commerce customers.},
	author = {Mingsong Mao and Jie Lu and Jialin Han and Guangquan Zhang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.07.029},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Recommender systems, E-commerce, User personalization, Hypergraph},
	pages = {269-287},
	title = {Multiobjective e-commerce recommendations based on hypergraph ranking},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518305437},
	volume = {471},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518305437},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.07.029}}

@article{AMPLAYO2018200,
	abstract = {Sentiment topic models are used as unsupervised methods to solve the specific problems of the general aspect-based sentiment analysis (ABSA) problem. One of the main problems of the technique is its substandard aspect term extraction, which leads to difficulties in aspect label determination. This paper is focused on improving the aspect term extraction of topic models by incorporating product descriptions to the current state-of-the-art sentiment topic model, Aspect Sentiment Unification Model (ASUM). We present two models that extend from ASUM differently to leverage on the information found in the product description: Seller-aided Aspect-based Sentiment Model (SA-ASM) and Seller-aided Product-based Sentiment Model (SA-PSM). SA-ASM has its topic distribution inside the review while SA-PSM has its topic distribution inside the product description. Based on experiments conducted to reviews of laptops and mobile phones, results show that SA-ASM performs better in micro-level problems such as sentiment classification and aspect assignment and SA-PSM performs better in macro-level problems like aspect category detection. Both models achieve better performances compared to current topic modeling methods for the ABSA problem.},
	author = {Reinald Kim Amplayo and Seanie Lee and Min Song},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.04.079},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Aspect-based sentiment analysis, Aspect extraction, Topic models, Product description},
	pages = {200-215},
	title = {Incorporating product description to sentiment topic models for improved aspect-based sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518303499},
	volume = {454-455},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518303499},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.04.079}}

@article{BONDIELLI201938,
	abstract = {False or unverified information spreads just like accurate information on the web, thus possibly going viral and influencing the public opinion and its decisions. Fake news and rumours represent the most popular forms of false and unverified information, respectively, and should be detected as soon as possible for avoiding their dramatic effects. The interest in effective detection techniques has been therefore growing very fast in the last years. In this paper we survey the different approaches to automatic detection of fake news and rumours proposed in the recent literature. In particular, we focus on five main aspects. First, we report and discuss the various definitions of fake news and rumours that have been considered in the literature. Second, we highlight how the collection of relevant data for performing fake news and rumours detection is problematic and we present the various approaches, which have been adopted to gather these data, as well as the publicly available datasets. Third, we describe the features that have been considered in fake news and rumour detection approaches. Fourth, we provide a comprehensive analysis on the various techniques used to perform rumour and fake news detection. Finally, we identify and discuss future directions.},
	author = {Alessandro Bondielli and Francesco Marcelloni},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.05.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Fake news, Rumours, Natural language processing, Data mining, Text mining, Classification, Machine learning, Deep learning},
	pages = {38-55},
	title = {A survey on fake news and rumour detection techniques},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519304372},
	volume = {497},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519304372},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.05.035}}

@article{JUNIOR2021116,
	abstract = {With the advent of online social networks, people became more eager to express and share their opinions and sentiment about all kinds of targets. The overwhelming amount of opinion texts soon attracted the interest of many entities (industry, e-commerce, celebrities, etc.) that were interested in analyzing the sentiment people express about what they produce or communicate. This interest has led to the surge of the sentiment analysis (SA) field. One of the most studied subfields of SA is polarity detection, which is the problem of classifying a text as positive, negative, or neutral. This classification problem is difficult to solve automatically, and many hand-adjusted resources are needed to overcome the difficulties in detecting sentiment from text. These resources include hand-adjusted textual features as well as lexicons. Deciding which resource and which combination of resources are more appropriate to a given scenario is a time-consuming trial-and-error process. Thus, in this work, we propose the use of Genetic Programming (GP) as a tool for automatically choosing, combining, and classifying sentiment from text. We propose a series of functions that allow GP to deal with preprocessing tasks, handcrafted features, and automatic weighting of lexicons for a given training set. Our experiments show that our GP solution is competitive and sometimes better than SVM and superior to na{\"\i}ve Bayes, logistic regression, and stochastic gradient descent, which are methods used in SA competitions.},
	author = {Airton Bordin Junior and N{\'a}dia F{\'e}lix F. {da Silva} and Thierson Couto Rosa and Celso G.C. Junior},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.01.025},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentiment analysis, Genetic programming, Lexicon, Classifiers},
	pages = {116-135},
	title = {Sentiment analysis with genetic programming},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521000529},
	volume = {562},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521000529},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.01.025}}

@article{MA2017325,
	abstract = {With the development of social networking applications, microblog has turned to be an indispensable online communication network in our daily life. For microblog users, recommending high quality information is a demanding service. Some microblog services encourage users to annotate themselves with tags, which are used to describe their interests or attributes. However, few users are willing to create tags and available tags are not fully exploited for microblog recommendation. Besides, following/follower relationship in microblog is asymmetric, which can be used not only for communicating with friends or acquaintances but also for getting information on particular subjects. So far, there is no microblog recommendation algorithm which employs all the above mentioned information. This paper aims to investigate a joint framework to combine tag correlation and user social relation for microblog recommendation. Our approach identifies users' interests via their personal tags and social relations. More specifically, a user tag retrieval strategy is established to add tags for users without or with few tags, and the user-tag matrix is then built and user-tag weights are then obtained. In order to solve the problem of sparsity of the matrix, both inner and outer correlation between tags are investigated to update the user-tag matrix. Considering the significance of user social relation for microblog recommendation, a user--user social relation similarity matrix is constructed. Moreover, an iterative updating scheme is developed to get the final tag-user matrix for computing the similarities between microblogs and users. We illustrate the capability of our algorithm by making experiments on real microblog datasets. Experimental results show that the algorithm is effective for microblog recommendation.},
	author = {Huifang Ma and Meihuizi Jia and Di Zhang and Xianghong Lin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2016.12.047},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Microblog recommendation, Tag retrieval, User-tag matrix, User-tag weight, Tag correlation, User--user social relation},
	pages = {325-337},
	title = {Combining tag correlation and user social relation for microblog recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516322794},
	volume = {385-386},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516322794},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2016.12.047}}

@article{ZHONG2021178,
	abstract = {As semi-supervised feature selection is becoming much more popular among researchers, many related methods have been proposed in recent years. However, many of these methods first compute a similarity matrix prior to feature selection, and the matrix is then fixed during the subsequent feature selection process. Clearly, the similarity matrix generated from the original dataset is susceptible to the noise features. In this paper, we propose a novel adaptive discriminant analysis for semi-supervised feature selection, namely, SADA. Instead of computing a similarity matrix first, SADA simultaneously learns an adaptive similarity matrix S and a projection matrix W with an iterative process. Moreover. we introduce the 2,p norm to control the sparsity of S by adjusting p. Experimental results show that S will become sparser with the decrease of p. The experimental results for synthetic datasets and nine benchmark datasets demonstrate the superiority of SADA, in comparison with 6 semi-supervised feature selection methods.},
	author = {Weichan Zhong and Xiaojun Chen and Feiping Nie and Joshua Zhexue Huang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.02.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Feature selection, Semi-supervised feature selection, Discriminant analysis},
	pages = {178-194},
	title = {Adaptive discriminant analysis for semi-supervised feature selection},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521001778},
	volume = {566},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521001778},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.02.035}}

@article{LI20221023,
	abstract = {In the continental law system, it is appropriate for judges to find relevant laws and consider rules defined in them when dealing with legal cases. Therefore, recommending relevant laws quickly and accurately based on case content is crucial in improving the efficiency of case processing. There have been researched works of recommender systems in various fields, but few of them lucubrates systems that recommend statutes for cases. To the best of our knowledge, there is no research on recommending statutes by modeling the relationship between case content and law content with interpretable hand-crafted features. In this paper, we define five novel types of features for calculating relevance between a case and a statute for resorting all statutes retrieved through collaborative filtering for the input case. Both pair-wise and list-wise ranking models are trained based on all these features for re-ranking the statutes list. Besides, we also test the combinations of different learning algorithms and popular pre-trained language models. Experimental results show that adopting the proposed novel features in pair-wise ranking achieves the best performance. It improves the recommendation recall of the Top 1 statute by almost 5% compared with the collaborative filtering approach.},
	author = {Chuanyi Li and Jidong Ge and Kun Cheng and Bin Luo and Victor Chang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.06.042},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Statute recommender, Learning to rank, Text matching, Text relation modeling},
	pages = {1023-1040},
	title = {Statute recommendation: Re-ranking statutes by modeling case-statute relation with interpretable hand-crafted features},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522006363},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522006363},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.06.042}}

@article{MOURINOGARCIA201712,
	abstract = {This paper presents the application of a Wikipedia-based bag of concepts (WikiBoC) document representation to cross-language text classification (CLTC). Its main objective is to alleviate the major drawbacks of the state-of-the-art CLTC approaches -- typically based on the machine translation (MT) of documents, which are represented as bags of words (BoW). We propose a technique called cross-language concept matching (CLCM), to convert concept-based representations of documents from one language to another using Wikipedia correspondences between concepts in different languages and thus not relying on automated full-text translations. We describe two proposals: the first proposal consists in the use of the WikiBoC representation in conjunction with the CLCM technique (WikiBoC-CLCM) to classify documents written in a language L1 by using a SVM algorithm that was trained with documents written in another language L2; the second proposal consists of a hybrid model for representing documents that combines WikiBoC-CLCM with the classic BoW-MT approach. To evaluate the two proposals we conducted several experiments with three cross-lingual corpora: the JRC-Acquis corpus and two purpose-built corpora composed of Wikipedia articles. The first proposal outperforms state-of-the-art approaches when training sequences are short, achieving performance increases up to 233.33%. The second proposal outperforms state-of-the-art approaches in the whole range of training sequences, achieving performance increases up to 23.78%. Results obtained show the benefits of the WikiBoC-CLCM approach, since concepts extracted from documents add useful information to the classifier, thus improving its performance.},
	author = {Marcos Antonio {Mouri{\~n}o Garc{\'\i}a} and Roberto {P{\'e}rez Rodr{\'\i}guez} and Luis {Anido Rif{\'o}n}},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.04.024},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-language text classification, Wikipedia Miner, Bag of concepts, Bag of words, Hybrid, Document representation},
	pages = {12-28},
	title = {Wikipedia-based cross-language text classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517306680},
	volume = {406-407},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517306680},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.04.024}}

@article{LIANG202194,
	abstract = {Explainable recommendation systems (ERSs) have attracted increasing attention from researchers, which generate high-quality recommendations with intuitive explanations to help users make appropriate decisions. However, most of the existing ERSs are designed with an offline setting, which can hardly adjust their models using the online feedback instantly for improved performance. To overcome the limitations of ERSs with the offline setting, we propose a novel online setting for ERSs and devise an effective model called O3ERS in this online setting, which can perform online learning with good scalability and rigorous theoretical guides for better online recommendations and online explanations. O3ERS also addresses two challenging problems in real scenarios, namely, the sparsity and delay of online explanations' feedback as well as the partialness and insufficiency of online recommendations' feedback. Specifically, O3ERS not only instantly leverages the knowledge learned from the recommendations' feedback to adjust the sparse and delayed explanations' feedback for better explanations but also utilizes a novel exploitation--exploration strategy that incorporates the explanations' feedback to adjust the partial and insufficient recommendations' feedback for better recommendations. Our theoretical analysis and empirical studies on one simulated and two real-world datasets show that our model outperforms the state-of-the-art models in online scenarios remarkably.},
	author = {Qianqiao Liang and Xiaolin Zheng and Yan Wang and Mengying Zhu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.12.070},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Explainable recommendation systems, Online learning, Factorization bandit},
	pages = {94-115},
	title = {O3ERS: An explainable recommendation system with online learning, online recommendation, and online explanation},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520312366},
	volume = {562},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520312366},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.12.070}}

@article{FIGUEIREDO201965,
	abstract = {Hashtags have become a crucial social media tool. The categorization of posts in a simple and informal way helps to spread the content through the web. At the same time, it enables users to easily find messages within a specific topic. However, the flexibility provided to use and create a hashtag carries some problems. Equivalent expressions, like synonyms, are handled like entirely different words. On the other hand, the same hashtag may refer to different topics. In this paper, we present TORHID (Topic Relevant Hashtag Identification), a method that employs topic modeling with the purpose of retrieving and identifying hashtags relevant to a specific topic in Twitter streams, starting from a seed hashtag and resorting to a classifier to remove non relevant hashtags. The result is a network of hashtags related to the seed, that we can use to deepen the initial search.},
	author = {Filipe Figueiredo and Alipio Jorge},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.07.062},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Text mining, Topic modeling, Latent Dirichlet Allocation, Support vector machines, Twitter, Hashtag recommendation},
	pages = {65-83},
	title = {Identifying topic relevant hashtags in Twitter streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519306668},
	volume = {505},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519306668},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.07.062}}

@article{LIU2021129,
	abstract = {Facing the increasingly fierce competition, app developers have to update features of their products continually. In this process, developers need to not only consider users' demands but also pay attention to what other similar apps do so that they can stay one step ahead in the competition. App stores provide large-scale useable data for achieving this goal while how to use them efficiently becomes a new challenge for developers. In this paper, we aim at helping developers make feature updating strategies of their apps by analyzing data of similar products in App stores. Firstly, we identify similar apps by using texts in app descriptions and UI. Then, we gain and integrate the information of updated features in these apps from their release texts. Furthermore, we match reviews with the related updated features, which helps developers predict the payback if they adopt a similar updating strategy. To validate the proposed approach, we conducted a series of experiments based on Google Play. The results show that our approach can analyze the data reasonably and provide useful information for developers making feature updating strategy in the evolution of their own products.},
	author = {Huaxiao Liu and Yihui Wang and Yuzhou Liu and Shanquan Gao},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.08.050},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {App store mining, Feature extraction, Reviews analysis, App evolution},
	pages = {129-151},
	title = {Supporting features updating of apps by analyzing similar products in App stores},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521008562},
	volume = {580},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521008562},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.08.050}}

@article{DING201912,
	abstract = {Diabetes and its complications have been recognized worldwide as a major public health threat. Predicting diabetic complications is regarded as a highly effective technique for increasing the survival rate of diabetic patients. While many studies currently use medical images and structured medical records, very limited efforts have been dedicated to applying data mining techniques for unstructured textual medical records, such as admission and discharge records. Moreover, the similarities among medical records that are overlooked by existing approaches could potentially improve the accuracy of prediction models. In this paper, we propose an approach for diabetic complication prediction based on a similarity-enhanced latent Dirichlet allocation (seLDA) model. Specifically, we first estimate the similarity between textual medical records after data preprocessing, and then we perform seLDA-based diabetic complication topic mining based on similarity constraints. Finally, we construct a prediction model by solving a multilabel classification problem with support vector machines (SVMs). The experimental results show that our approach outperforms the conventional LDA-based approach in similarity indices by 22.49%. Additionally, our approach shows significant improvements in prediction accuracy over four other representative seLDA-based approaches, including random forests (RF), k-nearest neighbors (KNN), logistic regression (LR) and deep neural networks (DNNs).},
	author = {Shuai Ding and Zhenmin Li and Xiao Liu and Hui Huang and Shanlin Yang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.05.037},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Diabetic complication prediction, Similarity enhancement, Latent Dirichlet allocation, Topic mining, Multilabel classification},
	pages = {12-24},
	title = {Diabetic complication prediction using a similarity-enhanced latent Dirichlet allocation model},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519304402},
	volume = {499},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519304402},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.05.037}}

@article{CHEN201932,
	abstract = {With the explosive growth of short documents generated from streaming textual sources (e.g., Twitter), latent topic discovery has become a critical task for short text stream clustering. However, most online clustering models determine the probability of producing a new topic by manually setting some hyper-parameter/threshold, which becomes barrier to achieve better topic discovery results. Moreover, topics generated by using existing models often involve a wide coverage of the vocabulary which is not suitable for online social media analysis. Therefore, we propose a nonparametric model (NPMM) which exploits auxiliary word embeddings to infer the topic number and employs a ``spike and slab'' function to alleviate the sparsity problem of topic-word distributions in online short text analyses. NPMM can automatically decide whether a given document belongs to existing topics, measured by the squared Mahalanobis distance. Hence, the proposed model is free from tuning the hyper-parameter to obtain the probability of generating new topics. Additionally, we propose a nonparametric sampling strategy to discover representative terms for each topic. To perform inference, we introduce a one-pass Gibbs sampling algorithm based on Cholesky decomposition of covariance matrices, which can further be sped up using a Metropolis-Hastings step. Our experiments demonstrate that NPMM significantly outperforms the state-of-the-art algorithms.},
	author = {Junyang Chen and Zhiguo Gong and Weiwen Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.07.048},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Data mining, Clustering, Topic model, Online topic discovery, Nonparametric model, Word embeddings},
	pages = {32-47},
	title = {A nonparametric model for online topic discovery with word embeddings},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519306541},
	volume = {504},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519306541},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.07.048}}

@article{VANLIERDE2019212,
	abstract = {Existing graph-based methods for extractive document summarization represent sentences of a corpus as the nodes of a graph in which edges depict relationships of lexical similarity between sentences. This approach fails to capture semantic similarities between sentences when they express a similar information but have few words in common and are thus lexically dissimilar. To overcome this issue, we propose to extract semantic similarities based on topical representations of sentences. Inspired by the Hierarchical Dirichlet Process, we propose a topic model to infer topic distributions of sentences. As each topic defines a semantic connection among sentences with a certain degree of membership for each sentence, we propose a fuzzy hypergraph model in which nodes are sentences and fuzzy hyperedges are topics. To produce an informative summary, we extract a set of sentences from the corpus by simultaneously maximizing their relevance to a user-defined query, their centrality in the fuzzy hypergraph and their coverage of topics present in the corpus. We formulate an algorithm building on the theory of submodular functions to solve the associated optimization problem. A thorough comparative analysis with other graph-based summarizers demonstrates the superiority of our method in terms of content coverage of the summaries.},
	author = {Hadrien {Van Lierde} and Tommy W.S. Chow},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.05.020},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Automatic text summarization, Fuzzy graphs, Probabilistic topic models, Hierarchical Dirichlet process, Personalized PageRank, Submodular set functions},
	pages = {212-224},
	title = {Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519304244},
	volume = {496},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519304244},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.05.020}}

@article{STEIN2019216,
	abstract = {Efficient distributed numerical word representation models (word embeddings) combined with modern machine learning algorithms have recently yielded considerable improvement on automatic document classification tasks. However, the effectiveness of such techniques has not been assessed for the hierarchical text classification (HTC) yet. This study investigates the application of those models and algorithms on this specific problem by means of experimentation and analysis. We trained classification models with prominent machine learning algorithm implementations---fastText, XGBoost, SVM, and Keras' CNN---and noticeable word embeddings generation methods---GloVe, word2vec, and fastText---with publicly available data and evaluated them with measures specifically appropriate for the hierarchical context. FastText achieved an lcaF1 of 0.893 on a single-labeled version of the RCV1 dataset. An analysis indicates that using word embeddings and its flavors is a very promising approach for HTC.},
	author = {Roger Alan Stein and Patricia A. Jaques and Jo{\~a}o Francisco Valiati},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.09.001},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical text classification, Word embeddings, Gradient tree boosting, fastText, Support vector machines},
	pages = {216-232},
	title = {An analysis of hierarchical text classification using word embeddings},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518306935},
	volume = {471},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518306935},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.09.001}}

@article{PANG2017190,
	abstract = {Inspired by the photometric invariance of color space, this paper proposes a simple yet powerful descriptor for object detection and recognition, called Rotative Maximal Pattern (RMP). The effectiveness of RMP comes from the two components: Rotatable Couple Templates (RCTs) with max pooling, and Normalized Histogram Intersection (NHI) with the theoretical guarantee. More concretely, RCTs are the combination of two templates to code the possible rotations. NHI serves as the similarity between two color histograms. We have conducted extensive experiments on INRIA pedestrian and Pascal VOC2007 data sets for object detection tasks; we also show that our approach leads to a promising performance on Caltech 101, Scene 15, UIUCsport and Stanford 40 action data sets.},
	author = {Junbiao Pang and Jing Huang and Lei Qin and Weigang Zhang and Laiyun Qing and Qingming Huang and Baocai Yin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.04.011},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Object detection, Object recognition, Max pooling, Translation invariance, Self similarity, Photometric invariance},
	pages = {190-206},
	title = {Rotative maximal pattern: A local coloring descriptor for object classification and recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517306527},
	volume = {405},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517306527},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.04.011}}

@article{LU201754,
	abstract = {Along with the development of Web2.0 technologies and the prevalence of digital capture devices, there has been an increased popularity of various photo-sharing websites, e.g., Flickr, Instagram, and Pinterest. More and more people today are creating and sharing millions of personal photos using these photo-sharing websites. Previous studies have shown that an individual may register and participate with several online social networking websites, and is referred to as an ``overlapped user''. In this work, we use Flickr and Instagram as test platforms to conduct a comparative study on the behaviors of overlapped users on different photo-sharing websites, including their temporal distribution, location distribution, photo annotation, and social attributes. We theoretically show that observations based on the overlapped users are more significant than those that are based on independent subject groups. Moreover, the derived observations are not only helpful in understanding the multi-online social networking (OSN) participation phenomenon in the context of photo-sharing behaviors, but they provide practical instructions to photo-sharing-based user modeling applications.},
	author = {Dongyuan Lu and Ruoshan Wu and Jitao Sang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2016.10.005},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Photo-sharing websites, User behavior analysis, Overlapped user, Multi-OSN participation},
	pages = {54-70},
	title = {Overlapped user-based comparative study on photo-sharing websites},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516311501},
	volume = {376},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516311501},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2016.10.005}}

@article{HU2022239,
	abstract = {A method to enhance Web service clustering is proposed in this paper. Since current service clustering methods usually face low quality of service representation vectors and lack consideration of service collaboration, we try to provide an improved topic model to generate high-quality service representation vectors and design a service clustering method to integrate function similarity and collaboration similarity. First, by introducing feature word extraction and probability distribution correction into GSDMM, we present a model called TE-GSDMM (topic enhanced Gibbs sampling algorithm for the Dirichlet Multinomial Mixture model). Then, a service collaboration graph is put forward to model cooperation relationships and generate service collaboration vectors. Collaboration similarity is assessed by the similarity of service collaboration vectors. Finally, the K-means++ algorithm is employed to cluster Web services by evaluating service function similarity and collaboration similarity. Experiments show that TE-GSDMM outperforms other topic models in generating high-quality service representation vectors for service clustering. Moreover, service clustering performance is further improved by integrating collaboration similarity. Thus, the proposed method effectively enhances Web service clustering by improving the quality of service representation vectors and integrating service collaboration similarity.},
	author = {Qiang Hu and Jiaji Shen and Kun Wang and Junwei Du and Yuyue Du},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.11.087},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Web service, Service clustering, Topic model, Service collaboration graph, GSDMM},
	pages = {239-260},
	title = {A Web service clustering method based on topic enhanced Gibbs sampling algorithm for the Dirichlet Multinomial Mixture model and service collaboration graph},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521012159},
	volume = {586},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521012159},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.11.087}}

@article{HAN2020177,
	abstract = {As high-resolution remote-sensing (HRRS) images have become increasingly widely available, scene classification focusing on the smart classification of land cover and land use has also attracted more attention. However, mainstream methods encounter a severe problem in that many annotation samples are required to obtain an ideal model for scene classification. In the remote sensing community, there is no dataset with a comparative scale to ImageNet (which contains over 14 million images) to meet the sample requirements of the convolutional neural network (CNN)-based methods. In addition, labeling new images is both labor intensive and time consuming. To address these problems, we present a new generative adversarial network (GAN)-based remote-sensing image generation method (GAN-RSIGM) that can be applied to create high-resolution annotated samples for scene classification. In GAN-RSIGM, the Wasserstein distance is used to measure the difference between the generator distribution and the real data distribution. This addresses the problem of the gradient disappearing during sample generation, and distinctly promotes a generator distribution close to the real data distribution. An auxiliary classifier is added to the discriminator, guiding the generator to produce consistent and distinct images. With regard to the network structure, the discriminator and the generator are implemented by stacking residual blocks, which further stabilize the training process of the GAN-RSIGM. Extensive experiments were conducted to evaluate the proposed method with two public HRRS datasets. The experimental results demonstrated that the proposed method could achieve satisfactory performance for high-quality annotation sample generation, scene classification, and data augmentation.},
	author = {Wei Han and Lizhe Wang and Ruyi Feng and Lang Gao and Xiaodao Chen and Ze Deng and Jia Chen and Peng Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.06.018},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Generative adversarial network, Scene classification, High-resolution remote sensing, Sample generation},
	pages = {177-194},
	title = {Sample generation based on a supervised Wasserstein Generative Adversarial Network for high-resolution remote-sensing scene classification},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552030606X},
	volume = {539},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552030606X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.06.018}}

@article{FROLOV2020595,
	abstract = {This paper proposes a novel method, referred to as ParGenFS, for finding a most specific generalization of a query set represented by a fuzzy set of topics assigned to leaves of the rooted tree of a taxonomy. The query set is generalized by ``lifting'' it to one or more ``head subjects'' in the higher ranks of the taxonomy. The head subjects should cover the query set, with the possible addition of some ``gaps'', taxonomy nodes covered by the head subject but irrelevant to the query set. To decrease the numbers of gaps, we admit some ``offshoots'', nodes belonging to the query set but not covered by a head subject. The method globally minimizes the total number of head subjects, gaps and offshoots, each suitably weighted. Our algorithm is applied to the structural analysis and description of a collection of 17,685 abstracts of research papers published in 17 Springer journals related to Data Science for the 20-year period 1998--2017. Our taxonomy of Data Science (TDS) is extracted from the Association for Computing Machinery Computing Classification System 2012 (ACM-CCS), a six-level hierarchical taxonomy manually developed by a team of ACM experts. The TDS also includes a number of additional leaves that we added to cater for recent developments not represented in the ACM-CCS taxonomy. We find fuzzy clusters of leaf topics over the text collection, using specially developed machinery. Three of the clusters are indeed thematic, relating to the Data Science sub-areas of (a) learning, (b) information retrieval, and (c) clustering. These three clusters are then lifted in the TDS using ParGenFS, which allows us to draw some conclusions about tendencies in developments in these areas.},
	author = {Dmitry Frolov and Susana Nascimento and Trevor Fenner and Boris Mirkin},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.09.082},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Hierarchical taxonomy, Parsimony, Generalization, Additive fuzzy cluster, Spectral clustering, Annotated suffix tree},
	pages = {595-615},
	title = {Parsimonious generalization of fuzzy thematic sets in taxonomies applied to the analysis of tendencies of research in data science},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519309454},
	volume = {512},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519309454},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.09.082}}

@article{DESMET201861,
	abstract = {Online communication platforms are increasingly used to express suicidal thoughts. There is considerable interest in monitoring such messages, both for population-wide and individual prevention purposes, and to inform suicide research and policy. Online information overload prohibits manual detection, which is why keyword search methods are typically used. However, these are imprecise and unable to handle implicit references or linguistic noise. As an alternative, this study investigates supervised text classification to model and detect suicidality in Dutch-language forum posts. Genetic algorithms were used to optimise models through feature selection and hyperparameter optimisation. A variety of features was found to be informative, including token and character ngram bags-of-words, presence of salient suicide-related terms and features based on LSA topic models and polarity lexicons. The results indicate that text classification is a viable and promising strategy for detecting suicide-related and alarming messages, with F-scores comparable to human annotators (93% for relevant messages, 70% for severe messages). Both types of messages can be detected with high precision and minimal noise, even on large high-skew corpora. This suggests that they would be fit for use in a real-world prevention setting.},
	author = {Bart Desmet and V{\'e}ronique Hoste},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.02.014},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Suicide prevention, Social media, Text classification, Machine learning, Feature selection, Optimisation},
	pages = {61-78},
	title = {Online suicide prevention through optimised text classification},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551830094X},
	volume = {439-440},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002551830094X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.02.014}}

@article{LI2022186,
	abstract = {Sentence representation approaches have been widely used and proven to be effective in many text modeling tasks and downstream applications. Many recent proposals are available on learning sentence representations based on deep neural frameworks. However, these methods are pre-trained in open domains and depend on the availability of large-scale data for model fitting. As a result, they may fail in some special scenarios, where data are sparse and embedding interpretations are required, such as legal, medical, or technical fields. In this paper, we present an unsupervised learning method to exploit representations of sentences for some closed domains via topic modeling. We reformulate the inference process of the sentences with the corresponding contextual sentences and the associated words, and propose an effective context-enhanced process called the bi-Directional Context-enhanced Sentence Representation Learning (bi-DCSR). This method takes advantage of the semantic distributions of the nearby contextual sentences and the associated words to form a context-enhanced sentence representation. To support the bi-DCSR, we develop a novel Bayesian topic model to embed sentences and words into the same latent interpretable topic space called the Hybrid Priors Topic Model (HPTM). Based on the defined topic space by the HPTM, the bi-DCSR method learns the embedding of a sentence by the two-directional contextual sentences and the words in it, which allows us to efficiently learn high-quality sentence representations in such closed domains. In addition to an open-domain dataset from Wikipedia, our method is validated using three closed-domain datasets from legal cases, electronic medical records, and technical reports. Our experiments indicate that the HPTM significantly outperforms on language modeling and topic coherence, compared with the existing topic models. Meanwhile, the bi-DCSR method does not only outperform the state-of-the-art unsupervised learning methods on closed domain sentence classification tasks, but also yields competitive performance compared to these established approaches on the open domain. Additionally, the visualizations of the semantics of sentences and words demonstrate the interpretable capacity of our model.},
	author = {Shuangyin Li and Weiwei Chen and Yu Zhang and Gansen Zhao and Rong Pan and Zhenhua Huang and Yong Tang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.05.113},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Sentence representations learning, Closed domains, Bayesian sentence embedding, Bi-directional context-enhanced, Semantic interpretability, Topic modeling},
	pages = {186-210},
	title = {A context-enhanced sentence representation learning method for close domains with topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522005540},
	volume = {607},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522005540},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.05.113}}

@article{RONG2019158,
	abstract = {Nowadays, the amount of user-generated contents (UGCs) or texts has surged exponentially. Therefore, recognizing emotions from these texts can bring about lots of advantages. In this paper, we have proposed a novel model named Deep Rolling to predict emotion for target participant in a multi-participant communication context. First, the proposed method converts a text collection into a set of n-dimension vectors for emotion representation and re-organizes texts into a sequence in time order. Then, Deep Rolling can predict the emotion of target participant corresponding to a future time point. Second, apart from simply taking in texts posted by target participant via LSTM, the proposed method has also incorporated texts posted by other participants at every time step by CNN. In this way, Deep Rolling can predict target participant's emotion by processing emotions from both the target and all the other participants in an ensemble way. Finally, data factorization has also been introduced into Deep Rolling to enhance the overall prediction efficiency. According to experimental results, compared with the state-of-art methods, our proposed model has achieved the best prediction precision on different target participants. At the same time, Deep Rolling has also maintained the prediction efficiency at an acceptable level.},
	author = {Huan Rong and Tinghuai Ma and Jie Cao and Yuan Tian and Abdullah Al-Dhelaan and Mznah Al-Rodhaan},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.03.023},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Emotion recognition, Time series forecasting, Natural language processing, Deep learning},
	pages = {158-180},
	title = {Deep rolling: A novel emotion prediction model for a multi-participant communication context},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519302208},
	volume = {488},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519302208},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.03.023}}

@article{YAN201715,
	abstract = {To solve the big topic modeling problem, we need to reduce both the time and space complexities of batch latent Dirichlet allocation (LDA) algorithms. Although parallel LDA algorithms on multi-processor architectures have low time and space complexities, their communication costs among processors often scale linearly with the vocabulary size and the number of topics, leading to a serious scalability problem. To reduce the communication complexity among processors to achieve improved scalability, we propose a novel communication-efficient parallel topic modeling architecture based on a power law, which consumes orders of magnitude less communication time when the number of topics is large. We combine the proposed communication-efficient parallel architecture with the online belief propagation (OBP) algorithm, referred to as POBP, for big topic modeling tasks. Extensive empirical results confirm that POBP has the following advantages for solving the big topic modeling problem when compared with recent state-of-the-art parallel LDA algorithms on multi-processor architectures: (1) high accuracy, (2) high communication efficiency, (3) high speed, and (4) constant memory usage.},
	author = {JianFeng Yan and Jia Zeng and Zhi-Qiang Liu and Lu Yang and Yang Gao},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2016.12.014},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Big topic modeling, Latent Dirichlet allocation, Communication complexity, Multi-processor architecture, Online belief propagation, Power law},
	pages = {15-31},
	title = {Towards big topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516320345},
	volume = {390},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516320345},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2016.12.014}}

@article{HUYNHTHE2020112,
	abstract = {Recently, skeleton-based human action recognition has received more interest from industrial and research communities for many practical applications thanks to the popularity of depth sensors. A large number of conventional approaches, which have exploited handcrafted features with traditional classifiers, cannot learn high-level spatiotemporal features to precisely recognize complex human actions. In this paper, we introduce a novel encoding technique, namely Pose-Transition Feature to Image (PoT2I), to transform skeleton information to image-based representation for deep convolutional neural networks (CNNs). The spatial joint correlations and temporal pose dynamics of an action are exhaustively depicted by an encoded color image. For learning action models, we fine-tune end-to-end a pre-trained network to thoroughly capture multiple high-level features at multi-scale action representation. The proposed method is benchmarked on several challenging 3D action recognition datasets (e.g., UTKinect-Action3D, SBU-Kinect Interaction, and NTU RGB+D) with different parameter configurations for performance analysis. Outstanding experimental results with the highest accuracy of 90.33% on the most challenging NTU RGB+D dataset demonstrate that our action recognition method with PoT2I outperforms state-of-the-art approaches.},
	author = {Thien Huynh-The and Cam-Hao Hua and Trung-Thanh Ngo and Dong-Seong Kim},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2019.10.047},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Pose-transition feature to image (PoT2I) encoding technique, Depth camera, Human action recognition, Deep convolutional neural networks},
	pages = {112-126},
	title = {Image representation of pose-transition feature for 3D skeleton-based action recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025519310151},
	volume = {513},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025519310151},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2019.10.047}}

@article{SONG2020138,
	abstract = {Anomaly detection is an important application field of evolutionary algorithm. Unlike traditionly anomaly detection, group anomaly detection aims to discover the anomalous aggregate behaviors in data points. Over past decades, a large number of promising methods have been successfully applied for group anomaly detection. However, they inherently neglect the correlations among groups in data points, limiting their abilities. This paper presents a correlated hierarchical generative model, which can model the intricate correlations hidden in groups by introducing a logistic normal distribution to capture the correlations among groups. With the proposed model, we construct a full variational Bayesian framework, which can data-adaptively optimize the model parameters of the proposed model. The model is designed and trained using Genetic Algorithm (GA), which helps automating the use of generative model. Further, a new score function is proposed as an anomaly criterion to estimate final anomaly groups in data points. Several experiments on synthetic data and real astronomical star data from Sloan Digital Sky Survey demonstrate the effectiveness of proposed method compared with the-state-of-art methods, in terms of average accurac (AP) and area under the Receiver Operating Characteristic(ROC) curve(AUC).},
	author = {Wanjuan Song and Wenyong Dong and Lanlan Kang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.03.110},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Group correlation, Genetic algorithm, Anomaly group detection, Logistic normal distribution, Variational inference},
	pages = {138-149},
	title = {Group anomaly detection based on Bayesian framework with genetic algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520302905},
	volume = {533},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520302905},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.03.110}}

@article{XIAO2017114,
	abstract = {In social networks, user behavior is affected by complex dynamic factors. Here, we investigate the internal and external factors that drive users to participate in social hotspot s. By analyzing user behavior, we discover the differences between driving factors and quantify their driving strength. First, four factors that influence the user's behavior are proposed, including explicit links (E), implicit links (I), personal interest (P), and a random factor (R). In particular, based on a cloud model, an implicit link creation method is designed. This method can quantify the driving strength of the implicit relation between users, and avoid the multiple attribute weighting defects in subjective and objective aspects. Next, considering the maximum likelihood estimation theory, a user behavior influence model (EIPR) of a hotspot topic is proposed to measure the causes of user behavior behind the social hotspots. Experimental results show that the model can be used to find different dynamic factors of user behavior in social hot topics. Among these external factors, the implicit link plays an significantly important role in driving user behavior.},
	author = {Yunpeng Xiao and Na Li and Ming Xu and Yanbing Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.02.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Social network, Hotspot topic, Influence analysis, Implicit link, Cloud model},
	pages = {114-126},
	title = {A user behavior influence model of social hotspot under implicit link},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517305340},
	volume = {396},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517305340},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.02.035}}

@article{SUN2019456,
	abstract = {With the rise of the Internet of Things, malicious attacks pose serious threats to the massive vulnerable IoT devices. Recently, attackers have initiated increasingly coordinated attack activities commonly pertaining to botnets. However, how to effectively detect the botnet based on attacker activities is proven challenging. In this paper, we propose a Machine Learning-based method for modeling attacker activities based on the following intuitive observations: attackers in the same botnet tend to launch temporally close attacks. We then directly model attack temporal patterns using a special class of point process called Multivariate Hawkes Process. Intuitively, Multivariate Hawkes Process identifies the latent influences between attackers by utilizing the mutually exciting properties. We then cluster the attacker activities based on the inferred weighted influence matrix with resort to the graph-based clustering approach. To evaluate the applicability of our method, we deployed 10 honeypots in a real-world environment, and conduct experiments on the collected dataset. The results show that we can identify the activity pattern and the structure of botnets effectively.},
	author = {Peiyuan Sun and Jianxin Li and Md Zakirul {Alam Bhuiyan} and Lihong Wang and Bo Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.04.065},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {IoT, Botnet, Multivariate Hawkes Process},
	pages = {456-471},
	title = {Modeling and clustering attacker activities in IoT through machine learning techniques},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518303311},
	volume = {479},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518303311},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.04.065}}

@article{LIU201713,
	abstract = {Human activity recognition has become a key research topic in a variety of applications. Modeling activity events and their rich relations using high-level human understandable activity models such as semantic-based knowledge base hold promise. However, formulas in current semantic-based approaches are generally manually encoded, which is rather unrealistic in situations where event relations are intricate. Moreover, current approaches for learning event relations often lack the capability to handle uncertainties. To address these issues, we present a framework to learn an event knowledge base (EKB) of probabilistic interval-based event relations and use them to infer varied semantic-level queries about activity occurrences under uncertainty. Specifically, we formalize an activity model to represent eight temporal and hierarchical event relations and four commonly performed queries. We leverage pattern mining techniques to learn an EKB associated with these relations and queries in a unified way. Experimental results show that the proposed framework with the learned EKB involving temporal and hierarchical dependencies leads to a significant performance improvement on activity recognition, particularly in the presence of incomplete or incorrect observations.},
	author = {Li Liu and Shu Wang and Guoxin Su and Bin Hu and Yuxin Peng and Qingyu Xiong and Junhao Wen},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.07.022},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Activity model, Semantic-based representation, Probabilistic event relation learning, Pattern mining},
	pages = {13-33},
	title = {A framework of mining semantic-based probabilistic event relations for complex activity recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516313457},
	volume = {418-419},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516313457},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.07.022}}

@article{QIU2017102,
	abstract = {Recently user clustering has become an increasingly important subject because of the high popularity of social medias like Twitter, Weibo and Facebook. The state-of-the-art algorithms of user clustering are all focused on the long or short text streams without considering factors of social network information, making them either unable to capture the social connectivity from short text streams or unable to conform to the sparsity, high-dimensionality and dynamically changing topics of short text streams. To address these issues, we propose a user clustering method named dynamic social network topic model (DSM) in this paper to cluster users by modeling their topics with dynamic features and social connectivity in short text streams. Experimental results show our topic model outperforms the state-of-the-art methods in the context of short text streams with social network information.},
	author = {Zhangcheng Qiu and Hong Shen},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.05.018},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {User clustering, Social network, Topic model, Text clustering},
	pages = {102-116},
	title = {User clustering in a dynamic social network topic model for short text streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025516319958},
	volume = {414},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025516319958},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.05.018}}

@article{ZHENG201955,
	abstract = {Ontology user profiles describe users' structural semantic interests. Studying similar relationships between user profiles is crucial to detecting interest overlapping communities. The novel view assumes that hierarchical interests of user profiles can generate multiple similarity relations, which is conducive to forming interest clusters. In this research, we develop a hierarchical interest overlapping community (HIOC) detection method and present a personalized recommendation model. First, content interest closeness and semantic interest closeness between user profiles are computed to measure multi-granularity subject similarity of users. Then, using the multi-granularity subject similarity and follow similarity of users, a heterogeneous hypergraph is constructed to represent an interest network. By application of the interest density peaks mechanism, the HIOC detection method is adopted for identifying communities of interest. Further, personalized interest prediction is implemented by consideration of the memberships of a user in a community and a subject distributed in a community. Finally, we verify the performance of the HIOC detection algorithm on several real networks and validate the effectiveness of the proposed recommendation approach. The experimental results illustrate that the proposed approach outperforms classical recommendation methods in precision and recall.},
	author = {Jianxing Zheng and Suge Wang and Deyu Li and Bofeng Zhang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.11.054},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Ontology user profile, Multi-granularity similarity, Hierarchical interest overlapping community, Personalized recommendation},
	pages = {55-75},
	title = {Personalized recommendation based on hierarchical interest overlapping community},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551830940X},
	volume = {479},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002551830940X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.11.054}}

@article{ZHOU2022809,
	abstract = {Cross-lingual information retrieval (CLIR) methods have quickly made the transition from translation-based approaches to semantic-based approaches. In this paper, we examine the limitations of current unsupervised neural CLIR methods, especially those leveraging aligned cross-lingual word embedding (CLWE) spaces. At the moment, CLWEs are normally constructed on the monolingual corpus of bilingual texts through an iterative induction process. Homonymy and polysemy have become major obstacles in this process. On the other hand, contextual text representation methods often fail to outperform static CLWE methods significantly for CLIR. We propose a method utilizing a novel neural generative model with Wasserstein autoencoders to learn neural topic-enhanced CLWEs for CLIR purposes. Our method requires minimal or no supervision at all. On the CLEF test collections, we perform a comparative evaluation of the state-of-the-art semantic CLWE methods along with our proposed method for neural CLIR tasks. We demonstrate that our method outperforms the existing CLWE methods and multilingual contextual text encoders. We also show that our proposed method obtains significant improvements over the CLWE methods based upon representative topical embeddings.},
	author = {Dong Zhou and Wei Qu and Lin Li and Mingdong Tang and Aimin Yang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.06.081},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Cross-Lingual Information Retrieval, Cross-lingual Word Embeddings, Neural Generative Models, Word Embedding Models},
	pages = {809-824},
	title = {Neural topic-enhanced cross-lingual word embeddings for CLIR},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522006752},
	volume = {608},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522006752},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.06.081}}

@article{BAEK2022235,
	abstract = {Graph mining has been widely studied to analyze real-world graph properties and applied to various applications. In particular, graph subspace clustering performance, defined as partitioning high-dimensional graph data into several clusters by finding minimum weights for the edges, has been consistently improved by exploiting deep learning algorithms with Euclidean features extracted from Euclidean domains (image datasets). Most subspace clustering algorithms tend to extract features from the Euclidean domain to identify graph characteristics and structures, and hence are limited for real-world data applications in non-Euclidean domains. This paper proposes a self-supervised deep geometric subspace clustering algorithm optimized for non-Euclidean high-dimensional graph data by emphasizing spatial features and geometric structures while simultaneously reducing redundant nodes and edges. Quantitative and qualitative experimental results verified the proposed approach is effective for graph clustering compared with previous state-of-the-art algorithms on public datasets.},
	author = {Sangwon Baek and Gangjoon Yoon and Jinjoo Song and Sang Min Yoon},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.08.006},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Subspace clustering, Graph mining, Deep geometric learning},
	pages = {235-245},
	title = {Self-supervised deep geometric subspace clustering network},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552200888X},
	volume = {610},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002552200888X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.08.006}}

@article{ANNAMORADNEJAD2022144,
	abstract = {With thousands of new questions posted every day on popular Q&A websites, there is a need for automated and accurate software solutions to replace manual moderation. In this paper, we address the critical drawbacks of crowdsourcing moderation actions in Q&A communities and demonstrate the ability to automate moderation using the latest machine learning models. From a technical point, we propose a multi-view approach that generates three distinct feature groups that examine a question from three different perspectives: 1) question-related features extracted using a BERT-based regression model; 2) context-related features extracted using a named-entity-recognition model; and 3) general lexical features derived using statistical and analytical methods. As a last step, we train a gradient boosting classifier to predict a moderation action. For evaluation purposes, we created a new dataset consisting of 60,000 Stack Overflow questions classified into three choices of moderation actions. Based on cross-validation on the novel dataset, our approach reaches 95.6% accuracy as a multiclass task and outperforms all state-of-the-art and previously-published models. Our results clearly demonstrate the high influence of our feature generation components on the overall success of the classifier.},
	author = {Issa Annamoradnejad and Jafar Habibi and Mohammadamin Fazli},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.03.085},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Automatic moderation, User-generated content, Community question answering, Multi-view learning, Decision support system, Stack overflow},
	pages = {144-154},
	title = {Multi-view approach to suggest moderation actions in community question answering sites},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522003127},
	volume = {600},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522003127},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.03.085}}

@article{GAO2022170,
	abstract = {Food recommendation has attracted increasing attentions to various food-related applications and services. The food recommender models aim to match users' preferences with recipes, where the key lies in the representation learning of users and recipes. However, ranging from early content-based filtering and collaborative filtering methods to recent hybrid methods, the existing work overlooks the various food-related relations, especially the ingredient-ingredient relations, leading to incomprehensive representations. To bridge this gap, we propose a novel model Food recommendation with Graph Convolutional Network (FGCN), which exploits ingredient-ingredient, ingredient-recipe, and recipe-user relations deeply. FGCN employs the information propagation mechanism and adopts multiple embedding propagation layers to model high-order connectivity across different food-related relations and enhance the representations. Specifically, we develop three types of information propagation: (1) ingredient-ingredient information propagation, (2) ingredient-recipe information propagation, and (3) recipe-user information propagation. To validate the effectiveness and rationality of FGCN, we conduct extensive experiments on a real-world dataset. The results show that the proposed FGCN outperforms the state-of-the-art baselines. Further in-depth analyses reveal that FGCN could alleviate the sparsity issue in food recommendation.},
	author = {Xiaoyan Gao and Fuli Feng and Heyan Huang and Xian-Ling Mao and Tian Lan and Zewen Chi},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.10.040},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Food recommendation, Food-related relations, Graph convolutional network, Information propagation, High-order connectivity},
	pages = {170-183},
	title = {Food recommendation with graph convolutional network},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521010549},
	volume = {584},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521010549},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.10.040}}

@article{DAI2017198,
	abstract = {Many real-world networks contain multiple types of interactions and relations. Link prediction in such multi-relational networks has become an important area in network analysis. For link prediction in multi-relational networks, we should consider the similarity and influence between different types of relations. In this paper, we propose a link prediction algorithm in multi-relational networks based on relational similarity. In the algorithm, a belief propagation method is presented to calculate the belief of each node and to construct the belief vector for each type of link. We use the similarity between belief vectors to measure the influence between different types of relations. Based on the influence between different relations, we present a nonnegative matrix factorization -based method for link prediction in multi-relational networks. The convergence and correctness of the presented method are proved. Our experimental results show that our method can achieve higher-quality prediction results than other similar algorithms.},
	author = {Caiyan Dai and Ling Chen and Bin Li and Yun Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2017.02.003},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multi-relational networks, Link prediction, Similarity},
	pages = {198-216},
	title = {Link prediction in multi-relational networks based on relational similarity},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025517304139},
	volume = {394-395},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025517304139},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2017.02.003}}

@article{YANG202241,
	abstract = {Traditional topic classification usually adopts the closed-world assumption that all the test topics have been seen in training. However, in open dynamic environments, the potential new topics may appear in testing due to the evolution of text data over time. Considering the uncertainty and multi-granularity of dynamic text data, such open topic classification needs to detect unseen topics by mining the boundary region continually, and incrementally update the previous models by knowledge accumulation. To address these challenge issues, this paper introduces a unified framework of three-way multi-granularity learning to open topic classification based on the fusion of three-way decision and granular computing. First, we propose the multilevel granular structure of tasks from the temporal-spatial multi-granularity perspective. Then, we construct an adaptive decision boundary and use the centroids and the corresponding radius to discover unknowns by the reject option. Subsequently, we further explore the unknown topics by three-way enhanced clustering and the uncertain instances will be re-investigated in the next stage. Besides, we design a built-in knowledge base represented as the centroid of each topic to store the topic knowledge. Finally, the experiments are conducted to compare the performances of proposed models and the efficiency of knowledge accumulation with classic models.},
	author = {Xin Yang and Yujie Li and Dan Meng and Yuxuan Yang and Dun Liu and Tianrui Li},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2021.11.035},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Three-way decision, Multi-granularity learning, Open topic, Uncertainty, Knowledge accumulation},
	pages = {41-57},
	title = {Three-way multi-granularity learning towards open topic classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521011555},
	volume = {585},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025521011555},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.11.035}}

@article{XIAO20211,
	abstract = {Link prediction is one of the core problems in social network analysis. Considering the complexity of features in social networks, we propose a link prediction method based on feature representation and fusion. Firstly, based on the sparseness and high-dimensionality of network structure, network embedding is applied to represent the network structure as low-dimensional vectors, which identifies the spatial relationships and discovers the relevance among users. Second, owing to the diversity and complexity of text semantics, the user text is converted into vectors by word embedding models. As user behaviors can reflect the dynamic change of links, a time decay function is introduced to process the text vector to quantify the impact of user text on link establishment. Meanwhile, to simplify the complexity, we choose the top-k relevant users for each user. Finally, due to the attention mechanism can improve the expression of user's interests in text information, a link prediction method with attention-based convolutional neural network is proposed. By fusing and mining structural and text features, the purpose of synthetically predict link is finally achieved. Experimental results show that the proposed model can effectively improve the performance of link prediction.},
	author = {Yunpeng Xiao and Rui Li and Xingyu Lu and Yanbing Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.09.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Link prediction, Social networks, Network embedding, Convolutional neural network, Feature fusion},
	pages = {1-17},
	title = {Link prediction based on feature representation and fusion},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520309452},
	volume = {548},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520309452},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.09.039}}

@article{LIU2022395,
	abstract = {Expert finding is an important research field in community question answering (CQA). Traditional expert finding methods mainly exploit topic analysis and authority calculation methods to identify high-quality experts in certain fields. To avoid recommending questions to those experts who do not display the willingness or ability to provide high-quality answers, user interest drift and user quality should be considered. This study proposes a novel method named high-quality domain expert finding in CQA based on multi-granularity semantic analysis and interest drift (HQExpert). Firstly, HQExpert considers different semantic granularities by employing two models, a coarse-grained topic model LC-LDA and a fine-grained model (BERT), to capture the domain information of questions and users more accurately. Secondly, to address the diverse interests of the users, a user interest drift model in HQExpert is developed to dynamically represent the changes in the interests of the users at different periods. In addition, a user quality model is developed to further optimize the professional level of the user, finding experts who can provide high-quality answers and are interested in the current question. Finally, extensive experiments on two datasets from different domains demonstrate that the proposed HQExpert model can significantly improve the accuracy of finding high-quality experts.},
	author = {Yue Liu and Weize Tang and Zitu Liu and Lin Ding and Aihua Tang},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2022.02.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Community question answering, Domain expert finding, Semantic analysis, Interest drift, User quality},
	pages = {395-413},
	title = {High-quality domain expert finding method in CQA based on multi-granularity semantic analysis and interest drift},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522001748},
	volume = {596},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025522001748},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2022.02.039}}

@article{XIAO2019526,
	abstract = {Traffic classification has been widely used in networking and security issues. Previous works have involved many different techniques for mapping traffic to the application. However, little attention has been paid to traffic classification for dynamic network stream. In this paper, we propose a Dynamic Multiple Traffic Classification System (DMTCS). We first introduce the time-based distribution of the traffic protocol information to the traffic classification problem, as the traffic data is a data stream with time continuity. The packets are treated as documents and protocols are seen as topics. Thus, we can apply topic models to cluster packets. In our system, after initialization, packets arrived at a time point are classified as of some protocols. Then, these packets are assembled to clusters according to the protocol distribution at the last time point. Finally, we use these clusters to classify packets arrived at the next time point. Our method has several advantages: 1) does not require the prior knowledge of target applications; 2) tolerant with both TCP and UDP protocols; 3) support multiple classification; 4) preserve high accuracy for the traffic stream with dynamic and imbalanced traffic distribution. Evaluations on DMTCS are carried on two different datasets, and the experimental results demonstrate that DMTCS has an impressive performance in classification on the real-world network stream and the dynamic simulation stream. Whats more, DMTCS outperforms other state-of-the-art models in our experiment.},
	author = {Xi Xiao and Rui Li and Hai-Tao Zheng and Runguo Ye and Arun KumarSangaiah and Shutao Xia},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.10.039},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Multiple classification, Topic model, Machine learning, Traffic classification, Network security},
	pages = {526-541},
	title = {Novel dynamic multiple classification system for network traffic},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518308569},
	volume = {479},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025518308569},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.10.039}}

@article{WAN2020243,
	abstract = {The Latent Dirichlet Allocation (LDA) model, which is a document-level probabilistic model, has been widely used in topic modeling. However, an essential issue of the LDA is its shortage in identifying co-occurrence relationships (e.g., aspect-aspect, aspect-opinion, etc.) in sentences. To address the problem, we propose an association constrained LDA (AC-LDA) for effectively capturing the co-occurrence relationships. Specifically, based on the basic features of the syntactic structure in product reviews, we formalize three major types of word association combinations and then carefully design corresponding identifications. For reducing the influence of global aspect words on the local distribution, we apply an important constraint on global aspects. Finally, the constraint and related association combinations are merged into the LDA to guide the topic-words allocation in the learning process. Based on the experiments on real-world product review data, we demonstrate that our model can effectively capture the relationships hidden in local sentences and further increase the extraction rate of fine-grained aspects and opinion words. Our results confirm the superiority of the AC-LDA over the state-of-the-art methods in terms of the extraction accuracy. We also verify the strength of our method in identifying irregularly appeared terms, such as non-aspect opinions, low-frequency words, and secondary aspects.},
	author = {Changxuan Wan and Yun Peng and Keli Xiao and Xiping Liu and Tengjiao Jiang and Dexi Liu},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2020.01.036},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Aspect words, Association constraint, LDA model, Opinion words},
	pages = {243-259},
	title = {An association-constrained LDA model for joint extraction of product aspects and opinions},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520300360},
	volume = {519},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0020025520300360},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2020.01.036}}

@article{J201823,
	abstract = {Opinion mining (also called sentiment analysis) is a type of natural language processing for computing people's opinions and emotions. It detects opinions from structured, semi-structured, and unstructured social media contents at different levels, such as the document, word, sentence, and aspect levels. In all these levels except aspect, opinion mining identifies the overall subjectivity or sentiment polarities. An aspect level is described as a part or an attribute of an entity. It exactly describes people's likes and dislikes in social media contents. In this paper, we propose a new framework for ranking products based on aspects. First, the system identifies the aspects of products. Second, the aspects and their opinion words are identified and visualized from the products' reviews using a Harel--Koren fast multiscale layout. Third, the network visualization is constructed and modeled, and a Spearman's rank correlation coefficient based opinion ranking method is applied to rank the products based on positive and negative ranks. Fourth, the supervised learning methods (Na{\"\i}ve Bayes, Maximum Entropy, and Support Vector Machine) are employed for the aspect-based sentiment classification task. Finally, the performance of the system is measured by the experimental results.},
	author = {Ashok Kumar J and Abirami S},
	date-added = {2022-10-23 18:21:58 +0200},
	date-modified = {2022-10-23 18:21:58 +0200},
	doi = {https://doi.org/10.1016/j.ins.2018.05.003},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Opinion mining, Sentiment analysis, Opinion ranking, Social media, Visualization, Spearman's rank correlation},
	pages = {23-41},
	title = {Aspect-based opinion ranking framework for product reviews using a Spearman's rank correlation coefficient method},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551830358X},
	volume = {460-461},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S002002551830358X},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2018.05.003}}

@article{BALLESTER2022101224,
	abstract = {Approaches for estimating the similarity between individual publications are an area of long-standing interest in the scientometrics and informetrics communities. Traditional techniques have generally relied on references and other metadata, while text mining approaches based on title and abstract text have appeared more frequently in recent years. In principle, topic models have great potential in this domain. But, in practice, they are often difficult to employ successfully, and are notoriously inconsistent as latent space dimension grows. In this manuscript we identify the three properties all usable topic models should have: robustness, descriptive power and reflection of reality. We develop a novel method for evaluating the robustness of topic models and suggest a metric to assess and benchmark descriptive power as number of topics scale. Employing that procedure, we find that the neural-network-based paragraph embedding approach seems capable of providing statistically robust estimates of the document--document similarities, even for topic spaces far larger than what is usually considered prudent for the most common topic model approaches.},
	author = {Omar Ballester and Orion Penner},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101224},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Scientometrics, Topic modelling, Stability, Robustness, Similarity, Informetrics},
	number = {1},
	pages = {101224},
	title = {Robustness, replicability and scalability in topic modelling},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772100095X},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772100095X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101224}}

@article{XIE2021101201,
	abstract = {Scholar performance assessment plays an important role in reward evaluation, funding allocation, and promotion and recruitment decisions. However, raw publication counts and raw citation count-based scholar performance assessment indicators, such as H-index or author citations, have shortcomings; for example, they ignore the impact of different citation patterns under different research topics, leading to authorship credit inflation due to full citation allocation to each author in multi-author publications. This study proposes a new scholar performance assessment indicator called the normalized scholar academic productivity (NSAP) indicator, which overcomes the issues posed by raw citation counts and publication counts-related scholar performance indicators by considering diverse aspects of scholar research achievements. The NSAP indicator considers the research topic, author sequence and author role in the author list, field-normalized journal impact when allocating citation counts to scholars, and published time. The research topic is generated by the co-keyword embedding and semantic relatedness of each keyword in order to make NSAP topic-dependent; the author sequence and role affect authorship credit allocation strategy; and field-normalized journal impact was used to assign different weights on raw publication counts and citation counts. Finally, awardees of the Derek de Solla Price Memorial Medal and the Association for Information Science and Technology's awards were used to evaluate the validity of NSAP for calculating scholar performance assessment. Results reveal outstanding topic-related scholar performance assessment properties compared to raw citation count indicators, such as H-index, author citations, and cited-by counts (i.e., total number of citing authors).},
	author = {Qing Xie and Xinyuan Zhang and Min Song},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101201},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Scholar performance assessment, Topic-dependent indicator, Authorship credit allocation, Field-normalized journal impact, Derek de Solla price memorial medal, The association for information science and technology's awards},
	number = {4},
	pages = {101201},
	title = {A network embedding-based scholar assessment indicator considering four facets: Research topic, author credit allocation, field-normalized journal impact, and published time},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000729},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000729},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101201}}

@article{ZHOU2021101162,
	abstract = {The global spread of COVID-19 has caused pandemics to be widely discussed. This is evident in the large number of scientific articles and the amount of user-generated content on social media. This paper aims to compare academic communication and social communication about the pandemic from the perspective of communication preference differences. It aims to provide information for the ongoing research on global pandemics, thereby eliminating knowledge barriers and information inequalities between the academic and the social communities. First, we collected the full text and the metadata of pandemic-related articles and Twitter data mentioning the articles. Second, we extracted and analyzed the topics and sentiment tendencies of the articles and related tweets. Finally, we conducted pandemic-related differential analysis on the academic community and the social community. We mined the resulting data to generate pandemic communication preferences (e.g., information needs, attitude tendencies) of researchers and the public, respectively. The research results from 50,338 articles and 927,266 corresponding tweets mentioning the articles revealed communication differences about global pandemics between the academic and the social communities regarding the consistency of research recognition and the preferences for particular research topics. The analysis of large-scale pandemic-related tweets also confirmed the communication preference differences between the two communities.},
	author = {Qingqing Zhou and Chengzhi Zhang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101162},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {COVID-19, Global pandemic, Academic communication, Social communication, Topic mining, Sentiment analysis},
	number = {3},
	pages = {101162},
	title = {Breaking community boundary: Comparing academic and social communication preferences regarding global pandemics},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772100033X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772100033X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101162}}

@article{ALSUDAIS2021101139,
	abstract = {While several aspects of open research software libraries have been studied, their in-code citation practices remain an unexplored area. In-code citations are citations of published scientific contributions added in the programming source code of research software libraries. In this paper, 73 such libraries are analyzed to determine the availability and consistency of in-code citations and reference sections. Findings indicate that 54 (73.9 %) of these libraries have at least one in-code citation. However, 47 had at least one incomplete citation and 89.3 % of libraries with several citations used multiple formats for citations. For reference sections, 36 of the libraries investigated in this study had at least one section. Still, inconsistencies in formats were also present as 83.3 % of the libraries with two or more sections used multiple formats for the sections, which may prevent automated programmers from indexing and collecting the list of references. Most importantly, this study investigates the availability of a systematic method that allows for the linking of references and in-code citations. Findings indicate that only six of the libraries had such a method, although many did not fully implement the method.},
	author = {Abdulkareem Alsudais},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101139},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Research software, In-code citation, Software citation, Open software, Citation networks},
	number = {2},
	pages = {101139},
	title = {In-code citation practices in open research software libraries},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000109},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000109},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101139}}

@article{SHU20171080,
	abstract = {Maps of scientific knowledge are generally created by analyzing scientific literature including journal articles, conference proceedings, books, and monographs. Although citation analysis is the most popular method for generating maps of science from scientific journal articles and their citations, other relationships between scientific topics can be used to map science. This study offers a map of science generated from examining non-fiction book topics and their relationships as defined by Library of Congress Subject Heading (LCSH) co-assignments. The resulting map reveals which sub-disciplines of science must be learned together, showing that Physics and Mathematics are the central topics required to practice science, which is not revealed by previous studies. This novel LCSH-based science map reveals new relations between the major sub-disciplines of science to produce a more complete representation of scientific domains and how they interact.},
	author = {Fei Shu and Jesse David Dinneen and Banafsheh Asadi and Charles-Antoine Julien},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2017.08.008},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Map of science, LCSH, Gephi, Assignment, Citation},
	number = {4},
	pages = {1080-1094},
	title = {Mapping science using Library of Congress Subject Headings},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157717300950},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157717300950},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2017.08.008}}

@article{HAJIBABAEI2022101275,
	abstract = {Gender disparity in science is one of the most focused debating points among authorities and the scientific community. Over the last few decades, numerous initiatives have endeavored to accelerate gender equity in academia and research society. However, despite the ongoing efforts, gaps persist across the world, and more measures need to be taken. Using social network analysis, natural language processing, and machine learning, in this study, we comprehensively analyzed gender-specific patterns in the highly interdisciplinary and evolving field of artificial intelligence for the period of 2000--2019. Our findings suggest an overall increasing rate of mixed-gender collaborations. From the observed gender-specific collaborative patterns, the existence of disciplinary homophily at both dyadic and team levels is confirmed. However, a higher preference was observed for female researchers to form homophilous collaborative links. Our core-periphery analysis indicated a significant positive association between having diverse collaboration and scientific performance and experience. We found evidence in support of expecting the rise of new female superstar researchers in the artificial intelligence field.},
	author = {Anahita Hajibabaei and Andrea Schiffauerova and Ashkan Ebadi},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101275},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Gender disparity, Interdisciplinary research, Artificial intelligence, Research performance, Collaboration},
	number = {2},
	pages = {101275},
	title = {Gender-specific patterns in the artificial intelligence scientific ecosystem},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772200027X},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772200027X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101275}}

@article{XU2020101014,
	abstract = {In the modern world, science and technology jointly determine the evolutionary path of scientific innovation, with an increasingly close relationship between them. Therefore, it is important to study the identification method of the innovation path, based on the linkage of topics in science and technology. This study focuses on connected topics utilizing bibliometric analysis, thereby exploring the identification method for innovation paths based on the linkage of scientific and technological topics. The internal mechanism of knowledge dissemination and the relationship between science and technology are revealed and described in detail by measuring the linkage of knowledge units. For practical bibliometric analyses, research papers and patent literature were used to characterize scientific research and technological research to reveal the innovation path for the interaction of science and technology quantitatively, automatically, and visually. Experimental study shows that analysis of the topic-linked path of science and technology, along with the integration of multi-relationships, can effectively identify important science- and technology-related topics in a field in the evolution process, and help grasp the key points of basic research and applied research.},
	author = {Haiyun Xu and Jos Winnink and Zenghui Yue and Ziqiang Liu and Guoting Yuan},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101014},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Evolution path, Scientific innovation, Science-technology linkage, Knowledge dissemination},
	number = {2},
	pages = {101014},
	title = {Topic-linked innovation paths in science and technology},
	url = {https://www.sciencedirect.com/science/article/pii/S175115771930210X},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115771930210X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101014}}

@article{ZHANG2020101032,
	abstract = {Compared with journal articles, books can provide broader, deeper and more comprehensive information, and often have higher expertise and academic depth. However, most researches on book assessment focus on measuring academic value of books (e.g. citations analysis) or identifying attitudes of readers (e.g. book review mining), depth and breadth reflected by book contents is neglected. Therefore, in this paper, we measure books' depth and breadth by mining books' tables of contents, so as to enrich resources and methods for book assessment research, help users understand book contents quickly and improve efficiency of book selection. Specifically, we measured books' depth and breadth based on books' tables of contents via two levels: topic level and feature level. Firstly, we obtained topic-level metrics by identifying topics expressed in tables of contents and calculating topic distributions. Then, we got feature-level results via feature extraction and feature distribution calculation. Finally, we compared depth and breadth metrics and other book assessment metrics. Experimental results reveal that, books' depth and breadth at two levels are different, and substantial differences between disciplines and book types are obvious. In addition, books' depth and breadth can provide alternative and supplementary information for assessing multi-dimensional values of books.},
	author = {Chengzhi Zhang and Qingqing Zhou},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101032},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Book impact assessment, Depth and breadth analysis, Topic extraction, Feature extraction},
	number = {2},
	pages = {101032},
	title = {Assessing books' depth and breadth via multi-level mining on tables of contents},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719302238},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719302238},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101032}}

@article{LEE2021101126,
	abstract = {This study aims at representing research patterns of bibliographic entities (e.g., scholars, papers, and venues) with a fixed-length vector. Bibliographic network structures rooted in the entities are incredibly diverse, and this diversity increases in the outstanding entities. Thus, despite their significant volume, the outstanding entities obtain minimal learning opportunities, whereas low-performance entities are over-represented. This study solves the problem by representing the patterns of the entities rather than depicting individual entities in a precise manner. First, we describe structures rooted in the entities using the Weisfeiler--Lehman (WL) relabeling process. Each subgraph generated by the relabeling process provides information on the scholars, kinds of papers they published, standards of venues in which the papers were published, and types of their collaborators. We assume that a subgraph depicts the research patterns of bibliographic entities, such as the preference of a scholar in choosing either a few highly impactful papers or numerous papers of moderate impact. Then, we simplify the subgraphs according to multiple levels of detailedness. Original subgraphs represent the individuality of the entities, and simplified subgraphs represent the entities sharing the same research patterns. In addition, simplified subgraphs balance the learning opportunities of high- and low-performance entities by co-occurring with both types of entities. We embed the subgraphs using the Skip-Gram method. If the results of the embedding represent the research patterns of the entities, the obtained vectors should be able to represent various aspects of the research performance in both the short-term and long-term durations regardless of the performances of the entities. Therefore, we conducted experiments for predicting 23 performance indicators during four time periods for four performance groups (top 1%, 5%, 10%, and all entities) using only the vector representations. The proposed model outperformed the existing network embedding methods in terms of both accuracy and variance.},
	author = {O-Joun Lee and Hyeon-Ju Jeon and Jason J. Jung},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101126},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Bibliographic network embedding, Skewed distribution, Multi-resolution representation learning, Level-wise simplification, Outstanding scholars},
	number = {1},
	pages = {101126},
	title = {Learning multi-resolution representations of research patterns in bibliographic networks},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772030643X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772030643X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101126}}

@article{AMON2022101284,
	abstract = {In competitive research environments, scholars have a natural interest to maximize the prestige associated with their scientific work. In order to identify factors that might help them address this goal more effectively, the scientometric literature has tried to link linguistic and meta characteristics of academic papers to the associated degree of scientific prestige, conceptualized as cumulative citation counts. In this paper, we take an alternative approach that instead understands scientific prestige in terms of the rankings of the journals that the articles appeared in, as such rankings are routinely used as surrogate research quality indicators. For the purpose of determining the most important drivers of suchlike prestige, we use state-of-the-art text mining tools to extract 344 interpretable features from a large corpus of over 200,000 journal articles in economics. We then estimate beta regression models to investigate the relationship between these predictors and a cross-sectionally standardized version of SCImago Journal Rank (SJR) in multiple topically homogeneous clusters. In so doing, we also reinvestigate the bafflegab theory, according to which more prestigious research papers tend to be less readable, in a methodologically novel way. Our results show the consistently most informative predictors to be associated with the length of the paper, the span of coreference chains in its full text, the deployment of a personal and moderately informal writing style, the ``density'' of the article in terms of sentences per page, international and institutional collaboration in research teams and the references cited in the paper. Moreover, we identify various linguistic intricacies that matter in the association between readability and scientific prestige, which suggest this relationship to be more complicated than previously assumed.},
	author = {Julian Amon and Kurt Hornik},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101284},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Research impact, SJR indicator, NLP, Readability, Gradient boosting, GLMLSS},
	number = {2},
	pages = {101284},
	title = {Is it all bafflegab? -- Linguistic and meta characteristics of research articles in prestigious economics journals},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000360},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000360},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101284}}

@article{REHS2021101166,
	abstract = {Author-level scientometric indicators are an important tool in individual and institutional-based research assessment and require high-quality author-publication profiles. To address this need, our study developed a robust supervised machine learning approach in combination with graph community detection methods to disambiguate author names in the Web of Science publication database. We used the unique author identifier Researcher ID to retrieve true authorship data of 1,904 scientists and trained a random forest and a logistic regression classifier on 1.2 million corresponding publication pairs with authors that share the same last name and first name initial. To do this, we reviewed a vast set of paper and author characteristics and randomly included missing data to make our machine learning robust to quality changes of new publication data. In the application on an unseen test set, we achieved F1 scores of 0.82 in the random forest and 0.75 in the logistic regression model. Subsequently, we evaluate feature performance and apply the infomap graph community detection algorithm to identify all publications belonging to an author. The community detection results in reasonable cluster metrics (Mean K-Metric in logistic regression-based model = 0.78 and = 0.81 in random forest-based model). Finally, we test our algorithm on a large surname-initial block (``Muller, M.'') and demonstrate speed and predictive performance.},
	author = {Andreas Rehs},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101166},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Author name disambiguation, Machine learning, Pairwise classification, Random forest, Community detection, Web of science},
	number = {3},
	pages = {101166},
	title = {A supervised machine learning approach to author disambiguation in the Web of Science},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000377},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000377},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101166}}

@article{DOGAN2020101076,
	abstract = {The effective representation of the relationship between the documents and their contents is crucial to increase classification performance of text documents in the text classification. Term weighting is a preprocess aiming to represent text documents better in Vector Space by assigning proper weights to terms. Since the calculation of the appropriate weight values directly affects performance of the text classification, in the literature, term weighting is still one of the important sub-research areas of text classification. In this study, we propose a novel term weighting (MONO) strategy which can use the non-occurrence information of terms more effectively than existing term weighting approaches in the literature. The proposed weighting strategy also performs intra-class document scaling to supply better representations of distinguishing capabilities of terms occurring in the different quantity of documents in the same quantity of class. Based on the MONO weighting strategy, two novel supervised term weighting schemes called TF-MONO and SRTF-MONO were proposed for text classification. The proposed schemes were tested with two different classifiers such as SVM and KNN on 3 different datasets named Reuters-21578, 20-Newsgroups, and WebKB. The classification performances of the proposed schemes were compared with 5 different existing term weighting schemes in the literature named TF-IDF, TF-IDF-ICF, TF-RF, TF-IDF-ICSDF, and TF-IGM. The results obtained from 7 different schemes show that SRTF-MONO generally outperformed other schemes for all three datasets. Moreover, TF-MONO has promised both Micro-F1 and Macro-F1 results compared to other five benchmark term weighting methods especially on the Reuters-21578 and 20-Newsgroups datasets.},
	author = {Turgut Dogan and Alper Kursat Uysal},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101076},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Text classification, Supervised term weighting, Max-occurrence, Non-occurrence},
	number = {4},
	pages = {101076},
	title = {A novel term weighting scheme for text classification: TF-MONO},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157720300705},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157720300705},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101076}}

@article{HUANG2021101145,
	abstract = {Policy documents have become increasingly valuable in the field of bibliometrics because they contain important information such as the intentions and behaviors of policymakers. Policy instruments are the central elements of policy documents; therefore, identifying core policy instruments can help researchers in the field better understand the important methodological measures taken by government organizations to achieve specific economic or social goals. However, existing identification methods often focus on the effectiveness of a policy instrument along one dimension (e.g., economic indicators), while ignoring the relationship between individual policy instruments. This paper attempts to fill this gap by designing a network-based framework incorporating structural holes theory to identify the core policy instruments implied in the policy documents. We first identify ``policy target-policy instrument'' patterns in relevant policy documents and then establish a ``policy target-policy instrument'' network that maps onto real-world policy systems. Finally, using structural holes theory, we identify core policy instruments and analyze the policy mix system upon this basis. We use China's nuclear energy policy as a case study to evaluate the proposed approach. Our proposed method is useful for quantitatively analyzing complex policy systems and for identifying core policy instruments and targets within them.},
	author = {Cui Huang and Chao Yang and Jun Su},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101145},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Bibliometrics, Policy documents, Structural holes, Policy instruments, ``Policy target-policy instrument'', Patterns},
	number = {2},
	pages = {101145},
	title = {Identifying core policy instruments based on structural holes: A case study of China's nuclear energy policy},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772100016X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772100016X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101145}}

@article{WANG2021101214,
	abstract = {Recent research has shifted to investigating knowledge integration in an interdisciplinary field and measuring the interdisciplinarity. Conventional citation analysis does not consider the context of citations, which limits the understanding of interdisciplinary knowledge integration. This study introduces a novel analytical framework to characterize interdisciplinary knowledge integration by both the content, i.e., integrated knowledge phrases (IKPs), and location of citances (i.e., citing sentences) in addition to citations. Seven knowledge categories are used to classify IKPs, including Research Subject, Theory, Research Methodology, Technology, Human Entity, Data, and Others. The eHealth field is explored as an exemplar interdisciplinary field in the case study. The result reveals that the ranks of source disciplines quantified by the integrated knowledge phrases are different from those by citations, especially in terms of average knowledge integration density. The distributions of the IKPs over the knowledge categories differ among source disciplines, indicating their different contributions to knowledge integration of eHealth field. The knowledge from adjacent disciplines is integrated into the field faster than that from other disciplines. Knowledge distributions over sections of articles are also different among source disciplines, and a correlation between knowledge categories and the sections they were used is observed. The analytical framework offers a way to better understand an interdisciplinary field by disclosing the characteristics of interdisciplinary knowledge integration from the perspective of knowledge content and usage.},
	author = {Shiyun Wang and Jin Mao and Kun Lu and Yujie Cao and Gang Li},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101214},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Knowledge flow, Citation age, Interdisciplinary research, Citation context, eHealth},
	number = {4},
	pages = {101214},
	title = {Understanding interdisciplinary knowledge integration through citance analysis: A case study on eHealth},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721000857},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721000857},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101214}}

@article{EBADI2020101018,
	abstract = {Research and development activities are regarded as one of the most influencing factors of the future of a country. Large investments in research can yield a tremendous outcome in terms of a country's overall wealth and strength. However, public financial resources of countries are often limited which calls for a wise and targeted investment. Scientific publications are considered as one of the main outputs of research investment. Although the general trend of scientific publications is increasing, a detailed analysis is required to monitor the research trends and assess whether they are in line with the top research priorities of the country. Such focused monitoring can shed light on scientific activities evolution as well as the formation of new research areas, thus helping governments to adjust priorities, if required. But monitoring the output of the funded research manually is not only very expensive and difficult, it is also subjective. Using structural topic models, in this paper we evaluated the trends in academic research performed by federally funded Canadian researchers during the time-frame of 2000--2018, covering more than 140,000 research publications. The proposed approach makes it possible to objectively and systematically monitor research projects, or any other set of documents related to research activities such as funding proposals, at large-scale. Our results confirm the accordance between the performed federally funded research projects and the top research priorities of Canada.},
	author = {Ashkan Ebadi and St{\'e}phane Tremblay and Cyril Goutte and Andrea Schiffauerova},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101018},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Text mining, Topic modeling, Machine learning, Funded research, Publications, Government research priorities, Canada},
	number = {2},
	pages = {101018},
	title = {Application of machine learning techniques to assess the trends and alignment of the funded research output},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157718301901},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157718301901},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101018}}

@article{CITRON2018181,
	abstract = {How does the collaboration network of researchers coalesce around a scientific topic? What sort of social restructuring occurs as a new field develops? Previous empirical explorations of these questions have examined the evolution of co-authorship networks associated with several fields of science, each noting a characteristic shift in network structure as fields develop. Historically, however, such studies have tended to rely on manually annotated datasets and therefore only consider a handful of disciplines, calling into question the universality of the observed structural signature. To overcome this limitation and test the robustness of this phenomenon, we use a comprehensive dataset of over 189,000 scientific articles and develop a framework for partitioning articles and their authors into coherent, semantically related groups representing scientific fields of varying size and specificity. We then use the resulting population of fields to study the structure of evolving co-authorship networks. Consistent with earlier findings, we observe a global topological transition as the co-authorship networks coalesce from a disjointed aggregate into a dense giant connected component that dominates the network. We validate these results using a separate, complimentary corpus of scientific articles, and, overall, we find that the previously reported characteristic structural evolution of a scientific field's associated co-authorship network is robust across a large number of scientific fields of varying size, scope, and specificity. Additionally, the framework developed in this study may be used in other scientometric contexts in order to extend studies to compare across a larger range of scientific disciplines.},
	author = {Daniel T. Citron and Samuel F. Way},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2017.12.008},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Collaboration networks, Network assembly, Social network analysis, Topic modeling, Scientometrics},
	number = {1},
	pages = {181-190},
	title = {Network assembly of scientific communities of varying size and specificity},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157717301797},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157717301797},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2017.12.008}}

@article{CHUNG2021101103,
	abstract = {Scouting young and talented human resources with firm-specific domain knowledge has a great impact on performance and sustainable growth among technology-based firms. Previous studies have proposed key researcher identification and recommendation approaches, but few studies have focused on identifying prospective human resources---young and talented people suitable for a firm's technology strategy. Thus, this study proposes an inventor profile mining approach for identifying such human resources. The proposed approach is as follows: 1) collecting patent data related to a target firm and preprocessing candidate inventors' patents; 2) identifying the inventors' technology fields and measuring their invention performance and career; 3) generating performance-career portfolio maps for invention fields to identify prospective inventors aligned with the target firm's technology development directions. We show that this approach can identify prospective inventors through a case study and statistical validation. This approach is expected to be used as a human resources management tool by technology-based firms to help them identify and scout young and talented human resources the most suitable for technology strategies.},
	author = {Jaemin Chung and Namuk Ko and Hyeonsu Kim and Janghyeok Yoon},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101103},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Inventor profile mining, Human resource, Scouting, Technology-based firm, Patent analysis},
	number = {1},
	pages = {101103},
	title = {Inventor profile mining approach for prospective human resource scouting},
	url = {https://www.sciencedirect.com/science/article/pii/S175115772030328X},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S175115772030328X},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101103}}

@article{THELWALL2019149,
	abstract = {Although the gender gap in academia has narrowed, females are underrepresented within some fields in the USA. Prior research suggests that the imbalances between science, technology, engineering and mathematics fields may be partly due to greater male interest in things and greater female interest in people, or to off-putting masculine cultures in some disciplines. To seek more detailed insights across all subjects, this article compares practising US male and female researchers between and within 285 narrow Scopus fields inside 26 broad fields from their first-authored articles published in 2017. The comparison is based on publishing fields and the words used in article titles, abstracts, and keywords. The results cannot be fully explained by the people/thing dimensions. Exceptions include greater female interest in veterinary science and cell biology and greater male interest in abstraction, patients, and power/control fields, such as politics and law. These may be due to other factors, such as the ability of a career to provide status or social impact or the availability of alternative careers. As a possible side effect of the partial people/thing relationship, females are more likely to use exploratory and qualitative methods and males are more likely to use quantitative methods. The results suggest that the necessary steps of eliminating explicit and implicit gender bias in academia are insufficient and might be complemented by measures to make fields more attractive to minority genders.},
	author = {Mike Thelwall and Carol Bailey and Catherine Tobin and Noel-Ann Bradshaw},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2018.12.002},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Gender, Academia, Disciplines, Underrepresentation, STEM},
	number = {1},
	pages = {149-169},
	title = {Gender differences in research areas, methods and topics: Can people and thing orientations explain the results?},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157718303596},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157718303596},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2018.12.002}}

@article{JUNG2022101320,
	abstract = {Topic emergence detection aids in pinpointing prominent topics within a given domain, providing practical insights into all interested parties on where to focus the limited resources. This paper employs the network-based topic evolution approach to overcome limitations in text-based topic evolution, providing prospective topic emergence prediction capabilities by representing emergent topics by their ancestors. A descendant-aware clustering algorithm is proposed to generate non-exhaustive and overlapping clusters, utilizing the pace of collaborations and structural similarities between topics with iterative edge removal and addition processes. Over 100 datasets specific to a research topic were extracted from the Microsoft Academic Graph dataset for the experiments, where the proposed algorithm consistently outperformed existing clustering algorithms in generating clusters with a higher likelihood of being ancestors to an emergent topic up to three years in the future. Regression-based cluster filtering using five structural cluster features and topic cluster qualities showed that the prediction performance can be enhanced by automatically classifying undesirable clusters from previously known data. The results showed that the proposed algorithm can enhance topic emergence predictions on a wide range of research domains regardless of their maturities, popularities, and magnitudes without having access to the data in the predicted year, paving a road to prospective predictions on emergent topics.},
	author = {Sukhwan Jung and Aviv Segev},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101320},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Topic evolution, Topic prediction, Clustering, Topic emergence prediction, Scientometrics},
	number = {3},
	pages = {101320},
	title = {DAC: Descendant-aware clustering algorithm for network-based topic emergence prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000724},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000724},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101320}}

@article{ZHANG20181099,
	abstract = {Topic extraction presents challenges for the bibliometric community, and its performance still depends on human intervention and its practical areas. This paper proposes a novel kernel k-means clustering method incorporated with a word embedding model to create a solution that effectively extracts topics from bibliometric data. The experimental results of a comparison of this method with four clustering baselines (i.e., k-means, fuzzy c-means, principal component analysis, and topic models) on two bibliometric datasets demonstrate its effectiveness across either a relatively broad range of disciplines or a given domain. An empirical study on bibliometric topic extraction from articles published by three top-tier bibliometric journals between 2000 and 2017, supported by expert knowledge-based evaluations, provides supplemental evidence of the method's ability on topic extraction. Additionally, this empirical analysis reveals insights into both overlapping and diverse research interests among the three journals that would benefit journal publishers, editorial boards, and research communities.},
	author = {Yi Zhang and Jie Lu and Feng Liu and Qian Liu and Alan Porter and Hongshu Chen and Guangquan Zhang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2018.09.004},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Bibliometrics, Topic analysis, Cluster analysis, Text mining},
	number = {4},
	pages = {1099-1117},
	title = {Does deep learning help topic extraction? A kernel k-means clustering method with word embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157718300257},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157718300257},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2018.09.004}}

@article{JUNG2020101040,
	abstract = {Topic modeling methods aim to extract semantic topics from unstructured documents, and topic evolution is one of its branches seeking to analyze how temporal topics in a set of documents evolve and has shown successful identification of content transitions within static topics over time; yet, the inherent limitations of topic modeling methods inhibit traditional topic evolution methods from highlighting topical correlations between different, dynamic topics. The authors propose an alternative topic modeling method conscious of the topical correlation in the academic domain by introducing the notion of the common interest authors (CIA11CIA: Common Interest Authors), defining a topic as a set of shared common research interests of a researcher group. Publication records related to the Human Computer Interaction field were extracted from the Microsoft Academic Graph dataset, with virtual reality as the target field of research. The result showed that the proposed alternative topic modeling is capable of successfully model coherent topics regardless of the topic size with only the meta-data of the document set, indicating that the alternative approach is not only capable of allowing topic correlation analysis during the topic evolution but also able to generate coherent topics at the same time.},
	author = {Sukhwan Jung and Wan Chul Yoon},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101040},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Topic modeling, Bibliographic network, Topic evolution, Scientometric},
	number = {3},
	pages = {101040},
	title = {An alternative topic model based on Common Interest Authors for topic evolution analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719303517},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719303517},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101040}}

@article{CHEN2022101281,
	abstract = {Main Path Analysis (MPA) is widely used to trace the developmental trajectory of a technological field through a citation network. The citation-based traversal weight is usually utilized to cherry-pick the most significant path. However, the theme of documents along a main path may not be so coherent, and it is very possible to miss the main paths of significant sub-fields overall in a domain. Furthermore, the global path search algorithm in conventional MPA also suffers from high space complexity due to the exhaustive strategy. To address these limitations, a new method, named as semantic MPA (sMPA), is proposed by leveraging semantic information in two steps of candidate path generation and main path selection. In the meanwhile, the resulting source code can be freely accessed. To demonstrate the advantages of our method, extensive experiments are conducted on a patent dataset pertaining to lithium-ion battery in electric vehicle. Experimental results show that our sMPA is capable of discovering more knowledge flows from important sub-fields, and improving the topical coherence of candidate paths as well.},
	author = {Liang Chen and Shuo Xu and Lijun Zhu and Jing Zhang and Haiyun Xu and Guancan Yang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101281},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Main path analysis, Developmental trajectory, Patent mining, Topic coherence, Lithium-ion battery},
	number = {2},
	pages = {101281},
	title = {A semantic main path analysis method to identify multiple developmental trajectories},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000335},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000335},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101281}}

@article{HUANG2022101317,
	abstract = {This research proposes a new approach that considers citation relevance in main path analysis (MPA). Traditional MPA assumes that all citations have equal weight, but in practice treating every citation equally may not find the main paths that truthfully reflect the knowledge flow in a target science field. To address the issue, this study suggests taking the level of relevance among documents into consideration. For demonstration purposes, the level of relevance is determined by similarity in both citation structure and key phrases among documents. The approach not only achieves convergence of development trajectories, but also helps frame the topics on the main paths to a specific concept from a wide range of research domains. This study takes health interoperability fields as the demonstration case to show the effects of converging the trajectories toward a target domain.},
	author = {Chen-Hao Huang and John S. Liu and Mei Hsiu-Ching Ho and Tzu-Chuan Chou},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101317},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Citation relevance, Literature review, Main path analysis, Health interoperability standards},
	number = {3},
	pages = {101317},
	title = {Towards more convergent main paths: A relevance-based approach},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000694},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000694},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101317}}

@article{CASSI20171095,
	abstract = {Science policy is increasingly shifting towards an emphasis in societal problems or grand challenges. As a result, new evaluative tools are needed to help assess not only the knowledge production side of research programmes or organisations, but also the articulation of research agendas with societal needs. In this paper, we present an exploratory investigation of science supply and societal needs on the grand challenge of obesity -- an emerging health problem with enormous social costs. We illustrate a potential approach that uses topic modelling to explore: (a) how scientific publications can be used to describe existing priorities in science production; (b) how policy records (in this case here questions posed in the European parliament) can be used as an instance of mapping discourse of social needs; (c) how the comparison between the two may show (mis)alignments between societal concerns and scientific outputs. While this is a technical exercise, we propose that this type of mapping methods can be useful to domain experts for informing strategic planning and evaluation in funding agencies.},
	author = {Lorenzo Cassi and Ag{\'e}nor Lahatte and Ismael Rafols and Pierre Sautier and {\'E}lisabeth {de Turckheim}},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2017.09.010},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Research agenda, Science mapping, Societal needs, Obesity, Topic modeling},
	number = {4},
	pages = {1095-1113},
	title = {Improving fitness: Mapping research priorities against societal needs on obesity},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157717301542},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157717301542},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2017.09.010}}

@article{XU2019100983,
	abstract = {Emerging research topic detection can benefit the research foundations and policy-makers. With the long-term and recent interest in detecting emerging research topics, various approaches are proposed in the literature. Though, there is still a lack of well-established linkages between the clear conceptual definition of emerging research topics and the proposed indicators for operationalization. This work follows the definition by Wang (2018), and several machine learning models are together used to detect and foresight the emerging research topics. Finally, experimental results on gene editing dataset discover three emerging research topics, which make clear that it is feasible to identify emerging research topics with our framework.},
	author = {Shuo Xu and Liyuan Hao and Xin An and Guancan Yang and Feifei Wang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2019.100983},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Emerging research topics, Topic modeling, Dynamic Influence Model, Citation Influence Model, Machine learning},
	number = {4},
	pages = {100983},
	title = {Emerging research topics detection with multiple machine learning models},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719300367},
	volume = {13},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719300367},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2019.100983}}

@article{KIM2022101242,
	abstract = {Main path analysis (MPA) is the most widely accepted approach to tracing knowledge transfer in a research field. In this study, we extracted multiple longest paths from the multidisciplinary academic field's citation network and integrating topic modeling to the extracted paths. We consider three main aspects of trajectory analysis when analyzing the represented documents through the extracted paths: emergence, authority, and topic dynamics. For path extraction, we adopt the longest path algorithm that consists of the following three steps: 1) topological sort, 2) edge relaxation, and 3) multiple path extraction. For topic integration into multiple paths, we employ latent Dirichlet allocation (LDA) by utilizing the topic-document matrix that LDA derives to select an article's topic from the citation network, where each article is labeled with the topic that is assigned with the highest topical probability for that article. We conduct a series of experiments to examine the results on a dataset from the field of healthcare informatics that PubMed provides.},
	author = {Erin H.J. Kim and Yoo Kyung Jeong and YongHwan Kim and Min Song},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2021.101242},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Citation analysis, Healthcare informatics, Longest path, Main path analysis, Topic modeling},
	number = {1},
	pages = {101242},
	title = {Exploring scientific trajectories of a large-scale dataset using topic-integrated path extraction},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157721001139},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157721001139},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2021.101242}}

@article{BU2017810,
	abstract = {Author cocitation analysis (ACA) is a branch of bibliometrics and knowledge representation that aims to map knowledge domains. However, ACA has been criticized because count-based measurement is too simple, and resulting maps are insufficiently informative. Since different scholarly relationships, e.g., coauthorship and author bibliographic coupling relationships, can extract out different relationships among authors in various perspectives, combining them with ACA for constructing knowledge domain mappings is our major purpose. The proposed method constructs the hybrid matrix from all relationships in four steps: relationship normalization, calculating the similarity between scholarly relationships, calculating adjustment parameters, and constructing hybrid relationships. The important parameters for integrating these matrices are calculated according to the distance in the hyperspace transformed from the similarity among the scholarly relationships by exploratory factor analysis. Compared with ACA, the results of the proposed method show: (1) More sub-fields in the given discipline can be identified when combining other scholarly relationships; (2) The more scholarly relationships added into ACA, the more details in terms of research area the method will find; (3) Good visualization in clustering is depicted when we combine other scholarly relationships. As a result, the proposed method offers a good choice to understand researchers and to map knowledge domains in a study field for integrating more scholarly relationships at the same time.},
	author = {Yi Bu and Shaokang Ni and Win-bin Huang},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2017.06.004},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Author cocitation analysis, Coauthorship analysis, Author bibliographic coupling analysis, Scholarly network, Scientific intellectual structure, Knowledge domain mapping, Exploratory factor analysis (EFA), Bibliometrics},
	number = {3},
	pages = {810-822},
	title = {Combining multiple scholarly relationships with author cocitation analysis: A preliminary exploration on improving knowledge domain mappings},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157716303674},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157716303674},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2017.06.004}}

@article{MCLEVEY2017176,
	abstract = {metaknowledge is a full-featured Python package for computational research in information science, network analysis, and science of science. It is optimized to scale efficiently for analyzing very large datasets, and is designed to integrate well with reproducible and open research workflows. It currently accepts raw data from the Web of Science, Scopus, PubMed, ProQuest Dissertations and Theses, and select funding agencies. It processes these raw data inputs and outputs a variety of datasets for quantitative analysis, including time series methods, Standard and Multi Reference Publication Year Spectroscopy, computational text analysis (e.g. topic modeling, burst analysis), and network analysis (including multi-mode, multi-level, and longitudinal networks). This article motivates the use of metaknowledge and explains its design and core functionality.},
	author = {John McLevey and Reid McIlroy-Young},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2016.12.005},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Informetrics, Scientometrics, Bibliometrics, Networks, Computational, Big data, Software, RPYS, Gender, Topic models, Burst analysis, Python},
	number = {1},
	pages = {176-197},
	title = {Introducing metaknowledge: Software for computational research in information science, network analysis, and science of science},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157716302000},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157716302000},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2016.12.005}}

@article{KIM2022101255,
	abstract = {This study explores the topic-based interdisciplinarity in the research domain of literacy. A text corpus of keywords was generated through a deep keyword generation model from abstracts of 346,387 articles published in 296 disciplines from 1917 to 2021. Dirichlet-Multinomial Regression topic modeling, interdisciplinarity indices, and network analysis were employed to analyze the collected corpus. Topic modeling uncovered 15 dominant research topics in the literacy field, as well as their up-and-down trends from 2000 to 2021. For each topic, keywords were then replaced with disciplines, and interdisciplinarity was measured using four indices: variety, balance, disparity, and diversity. Finally, the interdisciplinarity of each topic, connectivity between topics, and topic trends were comprehensively analyzed on the keyword co-occurrence network. Our methodology reaches beyond connectivity limited to a few disciplines and provides insight into the direction of collaboration between disciplines centered on a research domain. Moreover, the study's deep keyword generation model has methodological implications for forming a corpus spanning numerous disciplines as a bottom-up approach.},
	author = {Hyeyoung Kim and Hyelin Park and Min Song},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2022.101255},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Disciplinarity, Interdisciplinary cooperation, Topic diversity, Keyword generation, DMR topic modeling, Deep learning},
	number = {2},
	pages = {101255},
	title = {Developing a topic-driven method for interdisciplinarity analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157722000074},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157722000074},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2022.101255}}

@article{QIAN2020101047,
	abstract = {Detecting what type of knowledge constitutes a discipline, tracking how the knowledge changes, and understanding why the changes are triggered are the key issues in analyzing scientific development from a macro perspective, which is usually analyzed by the topic of evolution. However, traditional methods assume that the disciplinary structure is flat with only one-layer topics, rather than a tree-like structure with hierarchical topics, which leads to the inability of existing methods to effectively examine the details of the evolution, such as the interactions between different research directions. In this paper, we take artificial intelligence (AI) as a case in which we study its hierarchical structural evolution, more precisely inspecting disciplinary development, by analyzing 65,887 AI-related research papers published during a 10-year period from 2009 to 2018. From a hierarchical topic model that can construct a topic-tree with multi-layer organizations, we design a visual analysis model for the topic-tree to systematically and visually investigate how knowledge transfers along the topic-tree and how the topic-tree changes over time. Moreover, some assistant indicators are employed to help in the exploration of the complicated structural evolution. Then, we discover the latent relationship between the sub-structures within a topic as well as the triggering reason for the knowledge migration. Based on these results, we conclude that different topics have different development patterns and that the recent artificial intelligence revolution stems from the interactions among the different topics.},
	author = {Yue Qian and Yu Liu and Quan Z. Sheng},
	date-added = {2022-10-23 18:21:01 +0200},
	date-modified = {2022-10-23 18:21:01 +0200},
	doi = {https://doi.org/10.1016/j.joi.2020.101047},
	issn = {1751-1577},
	journal = {Journal of Informetrics},
	keywords = {Topic evolution, Artificial intelligence, Hierarchical knowledge structure, Nonnegative matrix factorization, Evolutionary patterns, Visual analysis approach},
	number = {3},
	pages = {101047},
	title = {Understanding hierarchical structural evolution in a scientific discipline: A case study of artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S1751157719302925},
	volume = {14},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1751157719302925},
	bdsk-url-2 = {https://doi.org/10.1016/j.joi.2020.101047}}

@article{JIAN2021108100,
	abstract = {As the Internet confronts the multimedia explosion, it becomes urgent to investigate personalized recommendation for alleviating information overload and improving users' experience. Most personalized recommendation approaches pay their attention to collaborative filtering over users' interactions, which suffers greatly from the highly sparse interactions. In image recommendation, visual correlations among images that users consumed provide a piece of intrinsic evidence to reveal users' interests. It inspires us to investigate image recommendation over the dense visual graph of images instead of the sparse user interaction graph. In this paper, we propose a semantic manifold modularization-based ranking (MMR) for image recommendation. MMR leverages the dense visual manifold to propagate users' historical records and infer user-image correlations for image recommendation. Especially, it constrains interest propagation within semantic visual compact groups by manifold modularization to make a tradeoff between users' personality and graph smoothness in propagation. Experimental results demonstrate that user-consumed visual correlations play actively to capture users' interests, and the proposed MMR can infer user-image correlations via visual manifold propagation for image recommendation.},
	author = {Meng Jian and Jingjing Guo and Chenlin Zhang and Ting Jia and Lifang Wu and Xun Yang and Lina Huo},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108100},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Manifold propagation, Modularization, Image recommendation, User interest},
	pages = {108100},
	title = {Semantic manifold modularization-based ranking for image recommendation},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321002879},
	volume = {120},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321002879},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108100}}

@article{IJJINA2017504,
	abstract = {In this paper, we propose an approach for recognizing human actions based on motion sequence information in RGB-D video using deep learning. A new representation that gives emphasis to the key poses associated with each action is presented. The features obtained from motion in RGB and depth video streams are given as input to the convolutional neural network to learn the discriminative features. The efficacy of the proposed approach is demonstrated on MIVIA action, NATOPS gesture, SBU Kinect interaction, and Weizmann datasets.},
	author = {Earnest Paul Ijjina and Krishna Mohan Chalavadi},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.07.013},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Multi-modal action recognition, Deep learning, Motion information, Extreme learning machines},
	pages = {504-516},
	title = {Human action recognition in RGB-D videos using motion sequence information and deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317302844},
	volume = {72},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317302844},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.07.013}}

@article{CHENG2018474,
	abstract = {In this paper, we present a feature description method called semantic descriptor with objectness (SDO) for scene recognition. Most existing scene representation methods exploit the characteristics of constituent objects in scenes with inter-class independence, which ignore the negative effects caused by the common objects among different scenes. The generic characteristics of the common objects cause some generality among different scenes, which weakens the discriminative characteristics among scenes. To address this problem, we exploit the correlations of object configurations among different scenes by the co-occurrence pattern of all objects across scenes to choose representative and discriminative objects which enhances the inter-class discriminability. Specifically, we capture the statistic information of objects appearing in each scene to compute the distribution of each object across scenes, which obtains the co-occurrence pattern of objects. Moreover, we represent the image descriptors with the occurrence probabilities of discriminative objects in image patches to eliminate the negative effects of common objects. To make image descriptors more discriminative, we discard the patches with non-discriminative objects to enhance the intra-class generalized characteristics. Experimental results on three widely used scene recognition datasets show that our method outperforms the state-of-the-art methods.},
	author = {Xiaojuan Cheng and Jiwen Lu and Jianjiang Feng and Bo Yuan and Jie Zhou},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.09.025},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Scene recognition, Deep learning, Co-occurrence pattern},
	pages = {474-487},
	title = {Scene recognition with objectness},
	url = {https://www.sciencedirect.com/science/article/pii/S003132031730376X},
	volume = {74},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132031730376X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.09.025}}

@article{TALAVERA2020107330,
	abstract = {Developing tools to understand and visualize lifestyle is of high interest when addressing the improvement of habits and well-being of people. Routine, defined as the usual things that a person does daily, helps describe the individuals' lifestyle. With this paper, we are the first ones to address the development of novel tools for automatic discovery of routine days of an individual from his/her egocentric images. In the proposed model, sequences of images are firstly characterized by semantic labels detected by pre-trained CNNs. Then, these features are organized in temporal-semantic documents to later be embedded into a topic models space. Finally, Dynamic-Time-Warping and Spectral-Clustering methods are used for final day routine/non-routine discrimination. Moreover, we introduce a new EgoRoutine-dataset, a collection of 104 egocentric days with more than 100.000 images recorded by 7 users. Results show that routine can be discovered and behavioural patterns can be observed.},
	author = {Estefania Talavera and Carolin Wuerich and Nicolai Petkov and Petia Radeva},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107330},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Routine, Egocentric vision, Lifestyle, Behaviour analysis, Topic modelling},
	pages = {107330},
	title = {Topic modelling for routine discovery from egocentric photo-streams},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301333},
	volume = {104},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301333},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107330}}

@article{PATRO2021107586,
	abstract = {In this paper, we propose a probabilistic framework for solving the task of `Visual Dialog'. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable.},
	author = {Badri N. Patro and Anupriy and Vinay P. Namboodiri},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107586},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {CNN, LSTM, Uncertainty, Aleatoric uncertainty, Epistemic uncertainty vision and language, Visual dialog, VQA, Answer generation, Question generation, Bayesian deep learning},
	pages = {107586},
	title = {Probabilistic framework for solving visual dialog},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320303897},
	volume = {110},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320303897},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107586}}

@article{ZHANG2022108217,
	abstract = {Cross-modal retrieval has become a hot research topic in both computer vision and natural language processing areas. Learning intermediate common space for features of different modalities has become one of mainstream methods. In this paper, we propose a novel multi-task framework based on feature separation and reconstruction (mFSR) for cross-modal retrieval based on common space learning methods, which introduces feature separation module to deal with information asymmetry between different modalities, and introduces image and text reconstruction module to improve the quality of feature separation module. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that feature separation and specific information reconstruction can significantly improve the baseline performance of cross-modal image-caption retrieval.},
	author = {Li Zhang and Xiangqian Wu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108217},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Cross-modal retrieval, Feature separation, Image reconstruction, Text reconstruction},
	pages = {108217},
	title = {Multi-task framework based on feature separation and reconstruction for cross-modal retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321003988},
	volume = {122},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321003988},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108217}}

@article{HOU201766,
	abstract = {The recent proliferation of advertising (ad) videos has driven the research in multiple applications, ranging from video analysis to video indexing and retrieval. Among them, classifying ad video is a key task because it allows automatic organization of videos according to categories or genres, and this further enables ad video indexing and retrieval. However, classifying ad video is challenging compared to other types of video classification because of its unconstrained content. While many studies focus on embedding ads relevant to videos, to our knowledge, few focus on ad video classification. In order to classify ad video, this paper proposes a novel ad video representation that aims to sufficiently capture the latent semantics of video content from multiple views in an unsupervised manner. In particular, we represent ad videos from four views, including bag-of-feature (BOF), vector of locally aggregated descriptors (VLAD), fisher vector (FV) and object bank (OB). We then devise a multi-layer multi-view topic model, mlmv_LDA, which models the topics of videos from different views. A topical representation for video, supporting category-related task, is finally achieved by the proposed method. Our empirical classification results on 10,111 real-world ad videos demonstrate that the proposed approach effectively differentiate ad videos.},
	author = {Sujuan Hou and Ling Chen and Dacheng Tao and Shangbo Zhou and Wenjie Liu and Yuanjie Zheng},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.03.003},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Video representation, Ad video classification, Multi-layer, Multi-view, Topic model},
	pages = {66-81},
	title = {Multi-layer multi-view topic model for classifying advertising video},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317301036},
	volume = {68},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317301036},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.03.003}}

@article{JIA2018691,
	abstract = {Short text clustering is an increasingly important methodology but faces the challenges of sparsity and high-dimensionality of text data. Previous concept decomposition methods have obtained concept vectors via the centroids of clusters using k-means-type clustering algorithms on normal, full texts. In this study, we propose a new concept decomposition method that creates concept vectors by identifying semantic word communities from a weighted word co-occurrence network extracted from a short text corpus or a subset thereof. The cluster memberships of short texts are then estimated by mapping the original short texts to the learned semantic concept vectors. The proposed method is not only robust to the sparsity of short text corpora but also overcomes the curse of dimensionality, scaling to a large number of short text inputs due to the concept vectors being obtained from term-term instead of document-term space. Experimental tests have shown that the proposed method outperforms state-of-the-art algorithms.},
	author = {Caiyan Jia and Matthew B. Carson and Xiaoyang Wang and Jian Yu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.09.045},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Short text clustering, Concept decomposition, Spherical -means, Semantic word community, Community detection},
	pages = {691-703},
	title = {Concept decompositions for short text clustering by identifying word communities},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317303953},
	volume = {76},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317303953},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.09.045}}

@article{XIE2020107205,
	abstract = {With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted.},
	author = {Lin Xie and Feifei Lee and Li Liu and Koji Kotani and Qiu Chen},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107205},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Scene recognition, Patch feature encoding, Spatial layout pattern learning, Discriminative region detection, Convolutional neural networks, Deep learning},
	pages = {107205},
	title = {Scene recognition: A comprehensive survey},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032030011X},
	volume = {102},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032030011X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107205}}

@article{LU2018228,
	abstract = {Multilayer network is a structure commonly used to describe and model the complex interaction between sets of entities/nodes. A three-layer example is the author-paper-word structure in which authors are linked by co-author relation, papers are linked by citation relation, and words are linked by semantic relation. Network embedding, which aims to project the nodes in the network into a relatively low-dimensional space for latent factor analysis, has recently emerged as an effective method for a variety of network-based tasks, such as collaborative filtering and link prediction. However, existing studies of network embedding both focus on the single-layer network and overlook the structural properties of the network, e.g., the degree distribution and communities, which are significant for node characterization, such as the preferences of users in a social network. In this paper, we propose four multilayer network embedding algorithms based on Nonnegative Matrix Factorization (NMF) with consideration given to four structural properties: whole network (NNMF), community (CNMF), degree distribution (DNMF), and max spanning tree (TNMF). Experiments on synthetic data show that the proposed algorithms are able to preserve the desired structural properties as designed. Experiments on real-world data show that multilayer network embedding improves the accuracy of document clustering and recommendation, and the four embedding algorithms corresponding to the four structural properties demonstrate the differences in performance on these two tasks. These results can be directly used in document clustering and recommendation systems.},
	author = {Jie Lu and Junyu Xuan and Guangquan Zhang and Xiangfeng Luo},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.11.004},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Multilayer network, Network embedding, Nonnegative matrix factorization},
	pages = {228-241},
	title = {Structural property-aware multilayer network embedding for latent factor analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317304569},
	volume = {76},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317304569},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.11.004}}

@article{DENITTO2020107318,
	abstract = {Biclustering can be defined as the simultaneous clustering of rows and columns in a data matrix and it has been recently applied to many scientific scenarios such as bioinformatics, text analysis and computer vision to name a few. In this paper we propose a novel biclustering approach, that is based on the concept of dominant-set clustering and extends such algorithm to the biclustering problem. In more detail, we propose a novel encoding of the biclustering problem as a graph so to use the dominant set concept to analyse rows and columns simultaneously. Moreover, we extend the Dominant Set Biclustering approach to facilitate the insertion of prior knowledge that may be available on the domain. We evaluated the proposed approach on a synthetic benchmark and on two computer vision tasks: multiple structure recovery and region-based correspondence. The empirical evaluation shows that the method achieves promising results that are comparable to the state-of-the-art and that outperforms competitors in various cases.},
	author = {M. Denitto and M. Bicego and A. Farinelli and S. Vascon and M. Pelillo},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107318},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Biclustering, Dominant set, Replicator dynamics, Prior knowledge},
	pages = {107318},
	title = {Biclustering with dominant sets},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301217},
	volume = {104},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301217},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107318}}

@article{CHEN2018404,
	abstract = {A two-way subspace weighting partitional co-clustering method TWCC is proposed. In this method, two types of subspace weights are introduced to simultaneously weight the data in two ways, i.e., columns on row clusters and rows on column clusters. An objective function that uses the two types of weights in the distance function to determine the co-clusters of data is defined, and an iterative TWCC co-clustering algorithm to optimize the objective function is proposed, in which the two types of subspace weights are automatically computed. A series of experiments on both synthetic and real-life data were conducted to investigate the properties of TWCC, compare the two-way clustering results of TWCC with those of eight co-clustering algorithms, and compare one-way clustering results of TWCC with those of six clustering algorithms. The results have shown that TWCC is robust and effective for large high-dimensional data.},
	author = {Xiaojun Chen and Min Yang and Joshua {Zhexue Huang} and Zhong Ming},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.10.026},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Data mining, Co-clustering, Subspace clustering, Variable weighting},
	pages = {404-415},
	title = {TWCC: Automated Two-way Subspace Weighting Partitional Co-Clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317304326},
	volume = {76},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317304326},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.10.026}}

@article{URICCHIO2017144,
	abstract = {Automatic image annotation is among the fundamental problems in computer vision and pattern recognition, and it is becoming increasingly important in order to develop algorithms that are able to search and browse large-scale image collections. In this paper, we propose a label propagation framework based on Kernel Canonical Correlation Analysis (KCCA), which builds a latent semantic space where correlation of visual and textual features are well preserved into a semantic embedding. The proposed approach is robust and can work either when the training set is well annotated by experts, as well as when it is noisy such as in the case of user-generated tags in social media. We report extensive results on four popular datasets. Our results show that our KCCA-based framework can be applied to several state-of-the-art label transfer methods to obtain significant improvements. Our approach works even with the noisy tags of social users, provided that appropriate denoising is performed. Experiments on a large scale setting show that our method can provide some benefits even when the semantic space is estimated on a subset of training images.},
	author = {Tiberio Uricchio and Lamberto Ballan and Lorenzo Seidenari and Alberto {Del Bimbo}},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.05.019},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Automatic image annotation, Image tagging, Label transfer, Canonical correlation, Semantic space},
	pages = {144-157},
	title = {Automatic image annotation via label transfer in the semantic space},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317302066},
	volume = {71},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317302066},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.05.019}}

@article{RUIZ2019298,
	abstract = {Solving a supervised learning problem requires to label a training set. This task is traditionally performed by an expert, who provides a label for each sample. The proliferation of social web services (e.g., Amazon Mechanical Turk) has introduced an alternative crowdsourcing approach. Anybody with a computer can register in one of these services and label, either partially or completely, a dataset. The effort of labeling is then shared between a great number of annotators. However, this approach introduces scientifically challenging problems such as combining the unknown expertise of the annotators, handling disagreements on the annotated samples, or detecting the existence of spammer and adversarial annotators. All these problems require probabilistic sound solutions which go beyond the naive use of majority voting plus classical classification methods. In this work we introduce a new crowdsourcing model and inference procedure which trains a Gaussian Process classifier using the noisy labels provided by the annotators. Variational Bayes inference is used to estimate all unknowns. The proposed model can predict the class of new samples and assess the expertise of the involved annotators. Moreover, the Bayesian treatment allows for a solid uncertainty quantification. Since when predicting the class of a new sample we might have access to some annotations for it, we also show how our method can naturally incorporate this additional information. A comprehensive experimental section evaluates the proposed method with synthetic and real experiments, showing that it consistently outperforms other state-of-the-art crowdsourcing approaches.},
	author = {Pablo Ruiz and Pablo Morales-{\'A}lvarez and Rafael Molina and Aggelos K. Katsaggelos},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.11.021},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Crowdsourcing, Classification, Gaussian processes, Bayesian modeling, Variational inference},
	pages = {298-311},
	title = {Learning from crowds with variational Gaussian processes},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318304060},
	volume = {88},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318304060},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.11.021}}

@article{LIU20191,
	abstract = {Human activity recognition has been actively studied in the last three decades. Compared to human action performed by a single person, human interaction is more complex due to the involvement of more subjects and the interdependence between them. Recently, motivated by the remarkable success of deep learning techniques, many learning-based feature representations have been developed for activity recognition. This paper provides a comprehensive review of human action and interaction recognition methods, covering both hand-crafted features and learning-based features, with a special focus on data captured by RGB-D sensors. Furthermore, this review reveals practical challenges in human activity analysis along with their promising solutions and potential future directions.},
	author = {Bangli Liu and Haibin Cai and Zhaojie Ju and Honghai Liu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2019.05.020},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Survey, RGB-D sensing, Action recognition, Interaction recognition, Deep learning},
	pages = {1-12},
	title = {RGB-D sensing based human action and interaction analysis: A survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319301955},
	volume = {94},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320319301955},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2019.05.020}}

@article{WANG2022108230,
	abstract = {Node clustering aims to partition the vertices in a graph into multiple groups or communities. Existing studies have mostly focused on developing deep learning approaches to learn a latent representation of nodes, based on which simple clustering methods like k-means are applied. These two-step frameworks for node clustering are difficult to manipulate and usually lead to suboptimal performance, mainly because the graph embedding is not goal-directed, i.e., designed for the specific clustering task. In this paper, we propose a clustering-directed deep learning approach, Deep Neighbor-aware Embedded Node Clustering (DNENC for short) for clustering graph data. Our method focuses on attributed graphs to sufficiently explore the two sides of information in graphs. It encodes the topological structure and node content in a graph into a compact representation via a neighbor-aware graph autoencoder, which progressively absorbs information from neighbors via a convolutional or attentional encoder. Multiple neighbor-aware encoders are stacked to build a deep architecture followed by an inner-product decoder for reconstructing the graph structure. Furthermore, soft labels are generated to supervise a self-training process, which iteratively refines the node clustering results. The self-training process is jointly learned and optimized with the graph embedding in a unified framework, to benefit both components mutually. Experimental results compared with state-of-the-art algorithms demonstrate the good performance of our framework.},
	author = {Chun Wang and Shirui Pan and Celina P. Yu and Ruiqi Hu and Guodong Long and Chengqi Zhang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108230},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Attributed graph, Node clustering, Graph attention network, Graph convolutional network, Network representation},
	pages = {108230},
	title = {Deep neighbor-aware embedding for node clustering in attributed graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321004118},
	volume = {122},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321004118},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108230}}

@article{GUO2022108334,
	abstract = {Graph clustering based on embedding aims to divide nodes with higher similarity into several mutually disjoint groups, but it is not a trivial task to maximumly embed the graph structure and node attributes into the low dimensional feature space. Furthermore, most of the current advanced methods of graph nodes clustering adopt the strategy of separating graph embedding technology and clustering algorithm, and ignore the potential relationship between them. Therefore, we propose an innovative end-to-end graph clustering framework with joint strategy to handle the complex problem in a non-Euclidean space. In terms of learning the graph embedding, we propose a new variational graph auto-encoder algorithm based on the Graph Convolution Network (GCN), which takes into account the boosting influence of joint generative model of graph structure and node attributes on the embedding output. On the basis of embedding representation, we implement a self-training mechanism through the construction of auxiliary distribution to further enhance the prediction of node categories, thereby realizing the unsupervised clustering mode. In addition, the loss contribution of each cluster is normalized to prevent large clusters from distorting the embedding space. Extensive experiments on real-world graph datasets validate our design and demonstrate that our algorithm has highly competitive in graph clustering over state-of-the-art methods.},
	author = {Lin Guo and Qun Dai},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108334},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Graph convolution neural network, Variational graph embedding, Graph clustering, Variational graph auto-encoder},
	pages = {108334},
	title = {Graph Clustering via Variational Graph Embedding},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321005148},
	volume = {122},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321005148},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108334}}

@article{KANDEMIR2018226,
	abstract = {Topic modeling is a powerful approach for modeling data represented as high-dimensional histograms. While the high dimensionality of such input data is extremely beneficial in unsupervised applications including language modeling and text data exploration, it introduces difficulties in cases where class information is available to boost up prediction performance. Feeding such input directly to a classifier suffers from the curse of dimensionality. Performing dimensionality reduction and classification disjointly, on the other hand, cannot enjoy optimal performance due to information loss in the gap between these two steps unaware of each other. Existing supervised topic models introduced as a remedy to such scenarios have thus far incorporated only linear classifiers in order to keep inference tractable, causing a dramatical sacrifice from expressive power. In this paper, we propose the first Bayesian construction to perform topic modeling and non-linear classification jointly. We use the well-known Latent Dirichlet Allocation (LDA) for topic modeling and sparse Gaussian processes for non-linear classification. We combine these two components by a latent variable encoding the empirical topic distribution of each document in the corpus. We achieve a novel variational inference scheme by adapting ideas from the newly emerging deep Gaussian processes into the realm of topic modeling. We demonstrate that our model outperforms other existing approaches such as: (i) disjoint LDA and non-linear classification, (ii) joint LDA and linear classification, (iii) joint non-LDA linear subspace modeling and linear classification, and (iv) non-linear classification without topic modeling, in three benchmark data sets from two real-world applications: text categorization and image tagging.},
	author = {Melih Kandemir and Taygun Keke{\c c} and Reyyan Yeniterzi},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.12.019},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Latent Dirichlet allocation, Nonparametric Bayesian inference, Gaussian processes, Variational inference, Supervised topic models},
	pages = {226-236},
	title = {Supervising topic models with Gaussian processes},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317305150},
	volume = {77},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317305150},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.12.019}}

@article{CHEN2022108849,
	abstract = {Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.},
	author = {Lifei Chen and Haiyan Wu and Wenxuan Kang and Shengrui Wang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108849},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Sequence representation, Hidden Markov model, State clustering, Hierarchical model selection, Activity recognition},
	pages = {108849},
	title = {Symbolic sequence representation with Markovian state optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322003302},
	volume = {131},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322003302},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108849}}

@article{ZHENG2020107126,
	abstract = {Business information networks involve diverse users and rich content and have emerged as important platforms for enabling business intelligence and business decision making. A key step in an organizations business intelligence process is to cluster users with similar interests into social audiences and discover the roles they play within a business network. In this article, we propose a novel machine-learning approach, called CBIN, that co-clusters business information networks to discover and understand these audiences. The CBIN framework is based on co-factorization. The audience clusters are discovered from a combination of network structures and rich contextual information, such as node interactions and node-content correlations. Since what defines an audience cluster is data-driven, plus they often overlap, pre-determining the number of clusters is usually very difficult. Therefore, we have based CBIN on an overlapping clustering paradigm with a hold-out strategy to discover the optimal number of clusters given the underlying data. Experiments validate an outstanding performance by CBIN compared to other state-of-the-art algorithms on 13 real-world enterprise datasets.},
	author = {Yu Zheng and Ruiqi Hu and Sai-fu Fung and Celina Yu and Guodong Long and Ting Guo and Shirui Pan},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2019.107126},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Machine learning, Clustering, Business information networks, Social networks},
	pages = {107126},
	title = {Clustering social audiences in business information networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319304273},
	volume = {100},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320319304273},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2019.107126}}

@article{BOSE2021107784,
	abstract = {Soft computing provides the framework for dealing with the uncertainty and imprecision inherent in real-life applications. Soft computing has become a long-standing notable paradigm for medical image processing. A typical fuzzy clustering uses the fuzzy membership function. Nevertheless, there is an alternative membership representation, known as typicality or possibilistic membership. Unlike fuzzy membership that is probabilistic in nature, typicality represents an absolute membership and it is the degree of belonging of an object to a class that does not depend on its distances from the other classes. However, both fuzzy membership and typicality play important role in assigning membership to an object. This study proposes a novel clustering model that creates a vague environment enriched with the concept of fuzzy membership and typicality, while the use of type-reduction plays an essential role in capturing all the vagueness present in the data set. The proposed model is called type-reduced vague possibilistic fuzzy clustering (TVPFC), and we use MRI images to demonstrate its superior robustness over that of FCM (fuzzy c-means), PCM (possibilistic c-means), VCM (vague c-means) and IPFCM (interval-valued possibilistic fuzzy c-means).},
	author = {Ankita Bose and Kalyani Mali},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107784},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Fuzzy membership, Typicality, Vague set, Type-reduction, Medical images},
	pages = {107784},
	title = {Type-reduced vague possibilistic fuzzy clustering for medical images},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320305872},
	volume = {112},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320305872},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107784}}

@article{KURBAN2022108621,
	abstract = {In this paper, we propose a new temporal template approach for action recognition and person identification based on motion sequence information in masked depth video streams obtained from RGB-D data. This new representation creates a membership function that models the change in motion based on the correlation between frames that occur during motion flow. The energy images created with this function emphasize the intervals of motion with more change, while the intervals with less change are suppressed. To understand the distinctive features, the obtained energy images by using the proposed function are given as input to the convolutional neural networks and different handcrafted classifiers. The proposed method was observed on the BodyLogin, NATOPS, and SBU Kinect datasets and compared with the existing temporal templates and recent methods. The results indicate that the proposed method provides both higher performance and better motion representation.},
	author = {Onur Can Kurban and Nurullah Calik and T{\"u}lay Yildirim},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108621},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Motion recognition, Human recognition, Correlation coefficients, Deep learning, Behavioral biometrics},
	pages = {108621},
	title = {Human and action recognition using adaptive energy images},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322001029},
	volume = {127},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322001029},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108621}}

@article{JOO2020107514,
	abstract = {This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the inverse cumulative distribution function of the Gamma distribution, which is a component of the Dirichlet distribution. This approximation on a new prior led an investigation on the component collapsing, and DirVAE revealed that the component collapsing originates from two problem sources: decoder weight collapsing and latent value collapsing. The experimental results show that 1) DirVAE generates the result with the best log-likelihood compared to the baselines; 2) DirVAE produces more interpretable latent values with no collapsing issues which the baselines suffer from; 3) the latent representation from DirVAE achieves the best classification accuracy in the (semi-)supervised classification tasks on MNIST, OMNIGLOT, COIL-20, SVHN, and CIFAR-10 compared to the baseline VAEs; and 4) the DirVAE augmented topic models show better performances in most cases.},
	author = {Weonyoung Joo and Wonsung Lee and Sungrae Park and Il-Chul Moon},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107514},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Representation learning, Variational autoencoder, Deep generative model, Multi-modal latent representation, Component collapse},
	pages = {107514},
	title = {Dirichlet Variational Autoencoder},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320303174},
	volume = {107},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320303174},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107514}}

@article{HU2021107734,
	abstract = {Cross-modal retrieval aims at retrieving relevant points across different modalities, such as retrieving images via texts. One key challenge of cross-modal retrieval is narrowing the heterogeneous gap across diverse modalities. To overcome this challenge, we propose a novel method termed as Cross-modal discriminant Adversarial Network (CAN). Taking bi-modal data as a showcase, CAN consists of two parallel modality-specific generators, two modality-specific discriminators, and a Cross-modal Discriminant Mechanism (CDM). To be specific, the generators project diverse modalities into a latent cross-modal discriminant space. Meanwhile, the discriminators compete against the generators to alleviate the heterogeneous discrepancy in this space, i.e., the generators try to generate unified features to confuse the discriminators, and the discriminators aim to classify the generated results. To further remove the redundancy and preserve the discrimination, we propose CDM to project the generated results into a single common space, accompanying with a novel eigenvalue-based loss. Thanks to the eigenvalue-based loss, CDM could push as much discriminative power as possible into all latent directions. To demonstrate the effectiveness of our CAN, comprehensive experiments are conducted on four multimedia datasets comparing with 15 state-of-the-art approaches.},
	author = {Peng Hu and Xi Peng and Hongyuan Zhu and Jie Lin and Liangli Zhen and Wei Wang and Dezhong Peng},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107734},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Adversarial learning, Cross-modal representation learning, Cross-modal retrieval, Discriminant adversarial network, Cross-modal discriminant mechanism, Latent common space},
	pages = {107734},
	title = {Cross-modal discriminant adversarial network},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320305379},
	volume = {112},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320305379},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107734}}

@article{SELOSSE2020107315,
	abstract = {Recently, different studies have demonstrated the use of co-clustering, a data mining technique which simultaneously produces row-clusters of observations and column-clusters of features. The present work introduces a novel co-clustering model to easily summarize textual data in a document-term format. In addition to highlighting homogeneous co-clusters as other existing algorithms do we also distinguish noisy co-clusters from significant co-clusters, which is particularly useful for sparse document-term matrices. Furthermore, our model proposes a structure among the significant co-clusters, thus providing improved interpretability to users. The approach proposed contends with state-of-the-art methods for document and term clustering and offers user-friendly results. The model relies on the Poisson distribution and on a constrained version of the Latent Block Model, which is a probabilistic approach for co-clustering. A Stochastic Expectation-Maximization algorithm is proposed to run the model's inference as well as a model selection criterion to choose the number of co-clusters. Both simulated and real data sets illustrate the efficiency of this model by its ability to easily identify relevant co-clusters.},
	author = {Margot Selosse and Julien Jacques and Christophe Biernacki},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107315},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Co-Clustering, Document-term matrix, Latent block model},
	pages = {107315},
	title = {Textual data summarization using the Self-Organized Co-Clustering model},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301199},
	volume = {103},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301199},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107315}}

@article{ZHANG2019196,
	abstract = {Action recognition is a challenging task in the field of computer vision. The deficiency in training samples is a bottleneck problem in the current action recognition research. With the explosive growth of Internet data, some researchers try to use prior knowledge learned from various video sources to assist in recognizing the action video of the target domain, which is called knowledge adaptation. Based on this idea, we propose a novel framework for action recognition, called Semantic Adaptation based on the Vector of Locally Max Pooled deep learned Features (SA-VLMPF). The proposed framework consists of three parts: Two-Stream Fusion Network (TSFN), Vector of Locally Max-Pooled deep learned Features (VLMPF) and Semantic Adaptation Model (SAM). TSFN adopts a cascaded convolution fusion strategy to combine the convolutional features extracted from two-stream network. VLMPF retains the long-term information in videos and removes the irrelevant information by capturing multiple local features and extracting the features with the highest response to action category. SAM first maps the data of the auxiliary domain and the target domain into the high-level semantic representation through the deep network. Then the obtained high-level semantic representations from auxiliary domain are adapted into target domain in order to optimize the target classifier. Compared with the existing methods, the proposed methods can utilize the advantages of deep learning methods in obtaining the high-level semantic information to improve the performance of knowledge adaptation. At the same time, SA-VLMPF can make full use of the auxiliary data to make up for the insufficiency of training samples. Multiple experiments are conducted on several couples of datasets to validate the effectiveness of the proposed framework. The results show that the proposed SA-VLMPF outperforms the state-of-the-art knowledge adaptation methods.},
	author = {Junxuan Zhang and Haifeng Hu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2019.01.027},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Knowledge adaptation, Two-stream network, Video representation, Action recognition, Cascaded convolution fusion strategy},
	pages = {196-209},
	title = {Domain learning joint with semantic adaptation for human action recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319300470},
	volume = {90},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320319300470},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2019.01.027}}

@article{JEON2022108592,
	abstract = {We address the data association problem and propose a Bayesian approach based on a mixture of Gaussian Processes (GPs) having two key components, the assignment probabilities and the GPs. In the proposed approach, the two key components are simultaneously updated according to observations through an efficient Expectation-Maximization (EM) algorithm that we develop. The proposed approach is thus more adaptive to the observations than the existing approaches for data association. To validate the performance of the proposed approach, we provide experimental results with real data sets as well as two synthetic data sets. We also provide a theoretical analysis to show the effectiveness of the Bayesian update.},
	author = {Younghwan Jeon and Ganguk Hwang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108592},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Gaussian processes, Bayesian models, Variational inference, Expectation maximization},
	pages = {108592},
	title = {Bayesian mixture of gaussian processes for data association problem},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322000735},
	volume = {127},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322000735},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108592}}

@article{WANG2019236,
	abstract = {Probabilistic latent semantic analysis (PLSA) is a popular data analysis method with the objective to discover the underlying semantic structure of input data. In this work, we describe a method for probabilistic topic analysis in image and text based on a new representation of graph-regularized PLSA (GPLSA). In GPLSA, data entities are mapped to an undirected graph, where similarities between topic compositions on the graph are measured by the divergence between discrete probabilities. Such divergence is essentially incorporated as a graph-regularizer that augments the original PLSA algorithm. Furthermore, we extend the GPLSA algorithms to multiple data modalities based on the connections between data entities of each modality. We propose efficient multiplicative iterative algorithms for GPLSA with three popular regularizers, namely 1, 2 and symmetric KL divergences. In each case, we derive simple efficient numerical solutions that require only matrix arithmetic operations during the optimization. Experimental results demonstrate the efficacy of GPLSA over state-of-the-art methods.},
	author = {Xin Wang and Ming-Ching Chang and Lan Wang and Siwei Lyu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.09.004},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Probabilistic latent semantic analysis, Graph regularization, Topic analysis, Clustering},
	pages = {236-247},
	title = {Efficient algorithms for graph regularized PLSA for probabilistic topic modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318303236},
	volume = {86},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318303236},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.09.004}}

@article{REN2020107297,
	abstract = {Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed.},
	author = {Yan Ren and Adams Wai Kin Kong and Licheng Jiao},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107297},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Image cosegmentation, Video cosegmentation},
	pages = {107297},
	title = {A survey on image and video cosegmentation: Methods, challenges and analyses},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301011},
	volume = {103},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320301011},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107297}}

@article{NGUYEN2018280,
	abstract = {We present VIGO, a novel online Bayesian classifier for both binary and multiclass problems. In our model, variational inference for multivariate distribution technique is exploited to approximate the class conditional probability density functions of data in an online manner. To handle concept drift that could arise in streaming data, we develop 2 new adaptive methods based on VIGO, which we called VIGOw and VIGOd. While VIGOw naturally adapts to any kind of changing environments, VIGOd maximises the benefit of a static environment as long as it does not detect any change. Extensive experiments on big/medium real-world/synthetic datasets demonstrate the superior performance of our algorithms over many state-of-the-art methods in the literature.},
	author = {Thi Thu Thuy Nguyen and Tien Thanh Nguyen and Alan Wee-Chung Liew and Shi-Lin Wang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.04.007},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Online learning, Variational inference, Bayesian classifier, Data stream, Concept drift},
	pages = {280-293},
	title = {Variational inference based bayes online classifiers with concept drift adaptation},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318301419},
	volume = {81},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318301419},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.04.007}}

@article{LI2021107824,
	abstract = {Targeting at boosting business revenue, purchase prediction based on user behavior is crucial to e-commerce. However, it is not a well-explored topic due to a lack of relevant datasets. Specifically, no public dataset provides both price and discount information varying on time, which play an essential role in the user's decision making. Besides, existing learn-to-rank methods cannot explicitly predict the purchase possibility for a specific user-item pair. In this paper, we propose a two-step graph-based model, where the graph model is applied in the first step to learn representations of both users and items over click-through data, and the second step is a classifier incorporating the price information of each transaction record. To evaluate the model performance, we propose a transaction-based framework focusing on the purchased items and their context clicks, which contain items that a user is interested in but fails to choose after comparison. Our experiments show that exploiting the price and discount information can significantly enhance prediction accuracy.},
	author = {Zongxi Li and Haoran Xie and Guandong Xu and Qing Li and Mingming Leng and Chi Zhou},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.107824},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Purchase prediction, Graph-based method, e-commerce, Transaction-level data},
	pages = {107824},
	title = {Towards purchase prediction: A transaction-based setting and a graph-based method leveraging price information},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032100011X},
	volume = {113},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032100011X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.107824}}

@article{REN201899,
	abstract = {Person re-identification problem is targeting to match people in the views of non-overlapped camera networks. It is an important task in the fields of computer vision and video surveillance. It shows great value in applications like surveillance and action recognition. Existing metric learning based methods measure the similarity of sample pairs by learning a metric space in which the positive pairs are closer than negative pairs. However, the appearance features undergo with drastic variation. Person re-identification is a typical small sample problem. It is hard to learn a robust projection of metric subspace that takes all the situations into consideration. The learned metric subspace is usually over-fitting to training dataset due to the strict metric learning constraint. And the hard pairs in training dataset will weaken the discrimination of matching pairs' similarity. To address these problems, a feedback mechanism based iterative metric learning method is proposed. The proposed method introduces a mean distance of multi-metric subspace to deal with the over-fitting problem. The joint discriminant optimal model on feedback top ranks matching pairs will enhance the discrimination of matching pairs' similarity. It is a robust and discriminative distance metric which measures the matching pairs similarity with distances of multiple metric projections learned by a set of training datasets. Aiming to learn the multi-metric subspace, the proposed method gives a feedback mechanism based approach which back propagates the top ranks identification results as pseudo training datasets. The effectiveness of proposed mean distance of multi-metric projection is analyzed and proved theoretically. And extensive experiments on three challenging datasets, VIPeR, GRID and CUHK01 are conducted. The results show that the proposed method achieves the best performance and improves the state-of-the-art rank-1 identification rates by 18.48%, 2.00% and 5.41% on three datasets respectively.},
	author = {Yutao Ren and Xuelong Li and Xiaoqiang Lu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.04.012},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Person re-identification, Metric learning, Feedback, Iteration},
	note = {Distance Metric Learning for Pattern Recognition},
	pages = {99-111},
	title = {Feedback mechanism based iterative metric learning for person re-identification},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317301620},
	volume = {75},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317301620},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.04.012}}

@article{HUANG2019174,
	abstract = {Datasets are often collected from different resources or comprised of multiple representations (i.e., views). Multi-view clustering aims to analyze the multi-view data in an unsupervised way. Owing to the efficiency of uncovering the hidden structures of data, graph-based approaches have been investigated widely for various multi-view learning tasks. However, similarity measurement in these methods is challenging since the construction of similarity graph is impacted by several factors such as the scale of data, neighborhood size, choice of similarity metric, noise and outliers. Moreover, nonlinear relationships usually exist in real-world datasets, which have not been considered by most existing methods. In order to address these challenges, a novel model which simultaneously performs multi-view clustering task and learns similarity relationships in kernel spaces is proposed in this paper. The target optimal graph can be directly partitioned into exact c connected components if there are c clusters. Furthermore, our model can assign ideal weight for each view automatically without additional parameters as previous methods do. Since the performance is often sensitive to the input kernel matrix, the proposed model is further extended with multiple kernel learning ability. With the proposed joint model, three subtasks including construct the most accurate similarity graph, automatically allocate optimal weight for each view and find the cluster indicator matrix can be simultaneously accomplished. By this joint learning, each subtask can be mutually enhanced. Experimental results on benchmark datasets demonstrate that our model outperforms other state-of-the-art multi-view clustering algorithms.},
	author = {Shudong Huang and Zhao Kang and Ivor W. Tsang and Zenglin Xu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.11.007},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Graph learning, Multi-view clustering, Multiple kernel learning, Auto-weighted strategy},
	pages = {174-184},
	title = {Auto-weighted multi-view clustering via kernelized graph learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318303959},
	volume = {88},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318303959},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.11.007}}

@article{TU2018203,
	abstract = {Discovering and tracking topics in a text stream has attracted the interests of many researchers. A limitation of most existing methods is that they organize topics in flat structures. Topic hierarchy could reveal the potential relations between topics, which can help to find high quality topics when analyzing the text stream. In this paper, a hierarchical online non-negative matrix factorization method (HONMF) is proposed to generate topic hierarchies from text streams. The proposed method can dynamically adjust the topic hierarchy to adapt to the emerging, evolving, and fading processes of the topics. In the experiment, HONMF is evaluated under a variety of metrics. Compared with the baseline methods, our method can achieve better performance with competitive time efficiency.},
	author = {Ding Tu and Ling Chen and Mingqi Lv and Hongyu Shi and Gencai Chen},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.11.002},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Topic modeling, Hierarchical matrix factorization, Online learning},
	pages = {203-214},
	title = {Hierarchical online NMF for detecting and tracking topic hierarchies in a text stream},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317304557},
	volume = {76},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317304557},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.11.002}}

@article{ZHOU2017548,
	abstract = {This paper proposes a semantic representation, pose lexicon, for action recognition. The lexicon is composed of a set of semantic poses, a set of visual poses and a probabilistic mapping between the visual and semantic poses. Specially, an action can be represented by a sequence of semantic poses extracted from an associated textual instruction. Visual frames of the action are considered to be generated from a sequence of hidden visual poses. To learn the lexicon, a visual pose model is learned from training samples by a Gaussian Mixture model to characterize the likelihood of an observed visual frame being generated by a visual pose. A pose lexicon model is also learned by an extended hidden Markov alignment model to encode the probabilistic mapping between hidden visual poses and semantic poses sequences. With the lexicon, action classification is formulated as a problem of finding the maximum posterior probability of a given sequence of visual frames that fits to a given sequence of semantic poses through the most likely visual pose and alignment sequences. The efficacy of the proposed method was evaluated on MSRC-12, WorkoutSU-10, WorkoutUOW-18, Combined-15 and Combined-17 action datasets using cross-subject, cross-dataset and zero-shot protocols.},
	author = {Lijuan Zhou and Wanqing Li and Philip Ogunbona and Zhengyou Zhang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.06.035},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Lexicon, Semantic pose, Visual pose, Action recognition},
	pages = {548-562},
	title = {Semantic action recognition by learning a pose lexicon},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317302595},
	volume = {72},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317302595},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.06.035}}

@article{WANG2020107479,
	abstract = {Multimodal hashing methods have gained considerable attention in recent years due to their effectiveness and efficiency for cross-modal similarity searches. Existing multimodal hashing methods either learn unified hash codes for different modalities or learn individual hash codes for each modality and then explore cross-correlations between them. Generally, learning unified hash codes tends to preserve the shared properties of multimodal data and learning individual hash codes tends to preserve the specific properties of each modality. There remains a crucial bottleneck regarding how to learn hash codes that simultaneously preserve the shared properties and specific properties of multimodal data. Therefore, we present a joint and individual matrix factorization hashing (JIMFH) method, which not only learns unified hash codes for multimodal data to preserve their common properties but also learns individual hash codes for each modality to retain its specific properties. The proposed JIMFH learns unified hash codes by joint matrix factorization, which jointly factorizes all modalities into a shared latent semantic space. In addition, JIMFH learns individual hash codes by individual matrix factorization, which separately factorizes each modality into a modal-specific latent semantic space. Finally, unified hash codes and individual hash codes are combined to obtain the final hash codes. In this way, hash codes learned by JIMFH can preserve both the shared properties and specific properties of multimodal data, and therefore the retrieval performance is enhanced. Comprehensive experiments show that the proposed JIMFH performs much better than many state-of-the-art methods on cross-modal retrieval applications.},
	author = {Di Wang and Quan Wang and Lihuo He and Xinbo Gao and Yumin Tian},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107479},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Hashing, Multimodal, Retrieval, Cross-modal, Matrix factorization},
	pages = {107479},
	title = {Joint and individual matrix factorization hashing for large-scale cross-modal retrieval},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032030282X},
	volume = {107},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032030282X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107479}}

@article{WANG2022108512,
	abstract = {Due to the huge commercial interests behind online reviews, a tremendous amount of spammers manufacture spam reviews for product reputation manipulation. To further enhance the influence of spam reviews, spammers often collaboratively post spam reviews within a short period of time, the activities of whom are called collective opinion spam campaign. The goals and members of the spam campaign activities change frequently, and some spammers also imitate normal purchases to conceal the identity, which makes the spammer detection challenging. In this paper, we propose an unsupervised network embedding-based approach to jointly exploiting different types of relations, e.g., direct common behavior relation, and indirect co-reviewed relation to effectively represent the relevances of users for detecting the collective opinion spammers. The average improvements of our method over the state-of-the-art solutions on dataset AmazonCn and YelpHotel are [14.09%,12.04%] and [16.25%,12.78%] in terms of AP and AUC, respectively.},
	author = {Ziyang Wang and Wei Wei and Xian-Ling Mao and Guibing Guo and Pan Zhou and Sheng Jiang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2021.108512},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Spam detection, Collective spammer, Network embedding, Signed network},
	pages = {108512},
	title = {User-based network embedding for opinion spammer detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321006889},
	volume = {125},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320321006889},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2021.108512}}

@article{LIU2017287,
	abstract = {Due to its ability to eliminate the visual ambiguities in single-shot algorithms, video-based person re-identification has received an increasing focus in computer vision. Visual ambiguities caused by variations in view angle, lighting, and occlusions make the re-identification problem extremely challenging. To overcome the ambiguities, most previous approaches often extract robust feature representations or learn a sophisticated feature transformation. However, most of these approaches ignore the effect of the impostors arising from annotation or tracking process. In this case, impostors are regarded as genuine and applied in training process, leading to the model drift problem. In order to reduce the risk of model drifting, we propose to automatically discover impostors in a multiple instance metric learning framework. Specifically, we propose a kNN based confidence score to evaluate how much an impostor invades the interested target and utilize it as a prior in the framework. In the meanwhile, we integrate an impostor rejection mechanism in the multiple instance metric learning framework to automatically discover impostors, and learn the semantical similarity metrics with the refined training set. Experiments show that the proposed system performs favorably against the state-of-the-art algorithms on two challenging datasets (iLIDS-VID and PRID 2011). We have improved the rank 1 recognition rate on iLIDS-VID and PRID 2011 dataset by 1.0% and 1.2%, respectively.},
	author = {Xiaokai Liu and Hongyu Wang and Jie Wang and Xiaorui Ma},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.02.015},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Person re-identification, Graphical model, Multiple instance metric learning, Impostor rejection},
	pages = {287-298},
	title = {Person re-identification by multiple instance metric learning with impostor rejection},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317300572},
	volume = {67},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317300572},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.02.015}}

@article{SONG2017368,
	abstract = {Multi-modal data modeling lately has been an active research area in pattern recognition community. Existing studies mainly focus on modeling the content of multi-modal documents, whilst the links amongst documents are commonly ignored. However, link information has shown being of key importance in many applications, such as document navigation, classification, and clustering. In this paper, we present a non-probabilistic formulation of Relational Topic Model (RTM), i.e., Sparse Relational Multi-Modal Topical Coding (SRMMTC), to model both multi-modal documents and the corresponding link information. SRMMTC has the following three appealing properties: i) It can effectively produce sparse latent representations via directly imposing sparsity-inducing regularizers. ii) It handles the imbalance issues on multi-modal data collections by introducing regularization parameters for positive and negative links, respectively; iii) It can be solved by an efficient coordinate descent algorithm. We also explore a generalized version of SRMMTC to find pairwise interactions amongst topics. Our methods are also capable of performing link prediction for documents, as well as the prediction of annotation words for attendant images in documents. Empirical studies on a set of benchmark datasets show that our proposed models significantly outperform many state-of-the-art methods.},
	author = {Lingyun Song and Jun Liu and Minnan Luo and Buyue Qian and Kuan Yang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.08.005},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Multi-modal data, Sparse latent representation, Image annotation, Link prediction},
	pages = {368-380},
	title = {Sparse Relational Topical Coding on multi-modal data},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317303102},
	volume = {72},
	year = {2017},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317303102},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.08.005}}

@article{ZHAO2018112,
	abstract = {Crowded scene analysis is a popular research topic due to its great application potentials, such as intelligent video surveillance and crowd density estimation. In this paper, we propose a novel approach to detecting crowd groups and learning semantic regions with a unified hierarchical clustering framework. According to the Gestalt laws of grouping, we propose three priors to define a unified similarity metric to measure the similarities of pairs of original tracklets and pairs of representative tracklets from different crowd groups, so that the short-term crowd groups and the long-term semantic paths commonly composed of several short-term crowd groups can be detected by a bottom-up hierarchical clustering algorithm simultaneously. In order to verify our method at the longer time duration video sequences in the crowded scene, we construct a new crowd database (CASIA crowd database 1) with various crowd densities in real scenes. Extensive experiments on our CASIA crowd database, Collective Motion Database and CUHK database are performed, and the results demonstrate that our approach is effective and reliable for crowd detection and semantic scene understanding in various crowd densities, especially for the crowd analysis in long temporal video clips.},
	author = {Weiqi Zhao and Zhang Zhang and Kaiqi Huang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2017.06.020},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Similarity measurement, Group detection, Semantic regions, Hierarchical clustering},
	note = {Distance Metric Learning for Pattern Recognition},
	pages = {112-127},
	title = {Gestalt laws based tracklets analysis for human crowd understanding},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320317302431},
	volume = {75},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320317302431},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2017.06.020}}

@article{TANG2022108787,
	abstract = {In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning, which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CAT's main components.},
	author = {Xudong Tang and Chao Dong and Wei Zhang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108787},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Text clustering, Contrastive learning, Representation learning},
	pages = {108787},
	title = {Contrastive author-aware text clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322002680},
	volume = {130},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320322002680},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108787}}

@article{REIHANIAN2018370,
	abstract = {Owing to advances in information technology, online communications between people living in different parts of the world have considerably increased. The subsequent emergence of social networks helped this kind of communications to be further organized. One of the most important issues considered when analyzing these kinds of networks is community detection, in which a majority of studies tend to detect disjoint communities through analyzing linkages of networks. What this paper aims to achieve is to obtain overlapping communities in which the members have the same topics of interest, and where the strengths of connections between them are the consequence of their communications' content analysis. Consequently, we have hereby proposed a generic framework for overlapping community detection in social networks with special focus on rating-based social networks. This framework considers the information shared by the users (ratings), as well as their topics of interest, for the sake of finding meaningful communities. This will lead us to topical communities in which members are interested in the same topics, and the strengths of their relationships are directly based on the rate of their viewpoints' unity. Quantitative evaluations also reveal that the framework presented in this study achieves favorable results which are quite superior to the results of 3 other relevant frameworks in the literature.},
	author = {Ali Reihanian and Mohammad-Reza Feizi-Derakhshi and Hadi S. Aghdasi},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.04.013},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Overlapping community detection, Content analysis, Topical community, Semantic network, Rating-based social networks},
	pages = {370-387},
	title = {Overlapping community detection in rating-based social networks through analyzing topics, ratings and links},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318301481},
	volume = {81},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318301481},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.04.013}}

@article{LIU2020107053,
	abstract = {Online activity recognition which aims to detect and recognize activity instantly from a continuous video stream is a key technology in human-robot interaction. However, the partial activity observation problem, mainly due to the incomplete sequence acquisition, makes it greatly challenging. This paper proposes a novel approach, named Multi-stage Adaptive Regression (MAR), for online activity recognition with the main focus on addressing the partial observation problem. Specifically, the MAR framework delicately assembles overlapped activity observations to improve its robustness against arbitrary activity segments. Then multiple score functions corresponding to each specific performance stage are collaboratively learned via a adaptive label strategy to enhance its power of discriminating similar partial activities. Moreover, the Online Human Interaction (OHI) database is constructed to evaluate the online activity recognition in human interaction scenarios. Extensive experimental evaluations on the Multi-Modal Action Detection (MAD) database and the OHI database show that the MAR method achieves an outstanding performance over the state-of-the-art approaches.},
	author = {Bangli Liu and Haibin Cai and Zhaojie Ju and Honghai Liu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2019.107053},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Online activity recognition, Interaction recognition, Partial observation, Adaptive regression},
	pages = {107053},
	title = {Multi-stage adaptive regression for online activity recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319303553},
	volume = {98},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320319303553},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2019.107053}}

@article{CHENG2018242,
	abstract = {In recent years, image annotation has attracted extensive attention due to the explosive growth of image data. With the capability of describing images at the semantic level, image annotation has many applications not only in image analysis and understanding but also in some relative disciplines, such as urban management and biomedical engineering. Because of the inherent weaknesses of manual image annotation, Automatic Image Annotation (AIA) has been raised since the late 1990s. In this paper, a deep review of state-of-the-art AIA methods is presented by synthesizing 138 literatures published during the past two decades. We classify AIA methods into five categories: 1) Generative model-based image annotation, 2) Nearest neighbor-based image annotation, 3) Discriminative model-based image annotation, and 4) Tag completion-based image annotation, 5) Deep Learning-based image annotation. Comparisons of the five types of AIA methods are made on the basis of the underlying idea, main contribution, model framework, computational complexity, computation time, and annotation accuracy. We also give an overview of five publicly available image datasets and four standard evaluation metrics commonly used as benchmarks for evaluating AIA methods. Then the performance of some typical or well-behaved models is assessed based on benchmark dataset and standard evaluation metrics. Finally, we share our viewpoints on the open issues and challenges in AIA as well as research trends in the future.},
	author = {Qimin Cheng and Qian Zhang and Peng Fu and Conghuan Tu and Sen Li},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.02.017},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Automatic image annotation, Generative model, Nearest-neighbor model, Discriminative model, Tag-completion, Deep learning},
	pages = {242-259},
	title = {A survey and analysis on automatic image annotation},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318300670},
	volume = {79},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318300670},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.02.017}}

@article{ZHANG2022108661,
	abstract = {Graph convolutional network (GCN) is an effective neural network model for graph representation learning. However, standard GCN suffers from three main limitations: (1) most real-world graphs have no regular connectivity and node degrees can range from one to hundreds or thousands, (2) neighboring nodes are aggregated with fixed weights, and (3) node features within a node feature vector are considered equally important. Several extensions have been proposed to tackle the limitations respectively. This paper focuses on tackling all the proposed limitations. Specifically, we propose a new node-feature convolutional (NFC) layer for GCN. The NFC layer first constructs a feature map using features selected and ordered from a fixed number of neighbors. It then performs a convolution operation on this feature map to learn the node representation. In this way, we can learn the usefulness of both individual nodes and individual features from a fixed-size neighborhood. Experiments on three benchmark datasets show that NFC-GCN consistently outperforms state-of-the-art methods in node classification.},
	author = {Li Zhang and Heda Song and Nikolaos Aletras and Haiping Lu},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2022.108661},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Graph, Representation learning, Graph convolutional networks, Convolutional neural networks},
	pages = {108661},
	title = {Node-Feature Convolution for Graph Convolutional Networks},
	url = {https://www.sciencedirect.com/science/article/pii/S003132032200142X},
	volume = {128},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S003132032200142X},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2022.108661}}

@article{LU2018129,
	abstract = {In large-scale visual recognition tasks, researchers are usually faced with some challenging problems, such as the extreme imbalance in the number of training data between classes or the lack of annotated data for some classes. In this paper, we propose a novel neural network architecture that automatically synthesizes pseudo feature representations for the classes in lack of annotated images. With the supply of semantic attributes for classes, the proposed Attribute-Based Synthetic Network (ABS-Net) can be applied to zero-shot learning (ZSL) scenario and conventional supervised learning (CSL) scenario as well. For ZSL tasks, the pseudo feature representations can be viewed as annotated feature-level instances for novel concepts, which facilitates the construction of unseen class predictor. For CSL tasks, the pseudo feature representations can be viewed as products of data augmentation on training set, which enriches the interpretation capacity of CSL systems. We demonstrate the effectiveness of the proposed ABS-Net in ZSL and CSL settings on a synthetic colored MNIST dataset (C-MNIST). For several popular ZSL benchmark datasets, our architecture also shows competitive results on zero-shot recognition task, especially leading to tremendous improvement to state-of-the-art mAP on zero-shot retrieval task.},
	author = {Jiang Lu and Jin Li and Ziang Yan and Fenghua Mei and Changshui Zhang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2018.03.006},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Pseudo feature representation, Zero-shot learning, Supervised learning, Data augmentation, Attribute learning},
	pages = {129-142},
	title = {Attribute-Based Synthetic Network (ABS-Net): Learning more from pseudo feature representations},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318300876},
	volume = {80},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320318300876},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2018.03.006}}

@article{WEI2021107636,
	abstract = {Sequence labeling is a fundamental task in natural language processing and has been widely studied. Recently, RNN-based sequence labeling models have increasingly gained attentions. Despite superior performance achieved by learning the long short-term (i.e., successive) dependencies, the way of sequentially processing inputs might limit the ability to capture the non-continuous relations over tokens within a sentence. To tackle the problem, we focus on how to effectively model successive and discrete dependencies of each token for enhancing the sequence labeling performance. Specifically, we propose an innovative attention-based model (called position-aware self-attention, i.e., PSA) as well as a well-designed self-attentional context fusion layer within a neural network architecture, to explore the positional information of an input sequence for capturing the latent relations among tokens. Extensive experiments on three classical tasks in sequence labeling domain, i.e.,  part-of-speech (POS) tagging, named entity recognition (NER) and phrase chunking, demonstrate our proposed model outperforms the state-of-the-arts without any external knowledge, in terms of various metrics.},
	author = {Wei Wei and Zanbo Wang and Xianling Mao and Guangyou Zhou and Pan Zhou and Sheng Jiang},
	date-added = {2022-10-23 18:19:46 +0200},
	date-modified = {2022-10-23 18:19:46 +0200},
	doi = {https://doi.org/10.1016/j.patcog.2020.107636},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Equence labeling, Self-attention, Discrete context dependency},
	pages = {107636},
	title = {Position-aware self-attention based neural sequence labeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320304398},
	volume = {110},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320320304398},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2020.107636}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>Decision Support Systems</string>
		<key>keys</key>
		<string>CHUA2019113142,HOLSAPPLE201832,NAM2019100,XU2021113525,POURNARAKIS201798,CHEN201896,KAZMAIER2020113304,WANG201887,EIRASFRANCO2019113141,DUTTA2022113662,CHENG2022113864,LI2022113863,BISWAS2022113651,ROEDER2022113770,SINGH201781,DEHGHAN2020113425,WANG2020113171,JUNG2019113074,LIU2021113609,ZHANG2020113288,IBRAHIM201937,KUNDU2020113164,GOLDBERG2022113751,YANG2022113813,KUMAR2022113792,XU2020113162,XU2021113467,ZHANG2019113064,LI2022113755,GARCIALOZANO201718,ZHANG2022113765,GUAN201958,ZHENG2020113369,WANG2021113465</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Expert Systems with Applications</string>
		<key>keys</key>
		<string>CHIEW20181,CUI201794,JOHN2017385,ALI2020113790,PORTUGAL2018205,KWON2017386,KARDAS2017343,ULIAN2021115341,YOO2020112965,MORISE20191,DU2021114791,REUBEN2022117027,PARADARAMI2017300,BHOPALE2020113441,LIU2021115752,XUE2022116057,SEONG2021113988,LOPEZMONROY2020112909,MIRTALAIE2018267,KUMARASWAMY2022118433,RUAS2019288,FIOK2021115771,ZHOU2022116560,HOOSHYAR2022116670,HU2017277,YOUSEFIAZAR201793,CARVALHO2017374,MATTHIES2018330,CEREZOCOSTAS201832,QORIB2023118715,PENSA201718,DU2018190,KWON2021114488,GUPTA201949,YANG2018206,HONG2019112813,NGUYEN201771,ZHU20188,SHAO2022118221,WANG2022117317,JANG2021114042,RINALDI2021114320,JAMES2017479,LI2018107,DAI2023118841,KHALID2022115926,AKKASI2021115162,KAUR2023118997,THIRUMOORTHY2021115040,MOJRIAN2021114555,DOKUZ2017113,SAEED2019115,BI2022118352,ZHU2023118364,IWATSUKI2022115840,LI2022116600,KAKISIM2019256,GAO2021114191,FRANCE2019456,LI2020112839,SCHOUTEN201968,ZAMIRI2021114657,RANA2017273,CALI2022118440,WANG2021114557,VERAS2019388,ROMERO2019522,ELBOUSHAKI2020112829,GUO201921,AGUADO2022118103,PATEL2019167,LI2018103,MARTINEZHUERTAS2021115621,SALAHIAN2022119051,ALTINEL2022118606,FICCADENTI2019127,MOIRANGTHEM2021113898,JIANG2021115537,CHOI201927,MOHSIN2021113808,ZANG2018250,YU2019365,SARKAR2021115026,NGUYEN2022117096,LATHABAI2022118317,PARK2019208,DU2018157,LIU2019246,HAN2022116472,BAMAKAN2019200,KAUR2018397,GUVEN2022116592,WANG2018163,DO2019272,ZHANG2022115826,ELKASSAS2021113679,ROSTAMI2019231,ZHANG2021115439,LO2017282,JOSHI2019200,LI2022118336,ZHOU2020113361,FAHFOUH2020113517,JEONG2022118375,BENSASSI2021115375,RAHIMI2022116518,LIANG2018322,TERAN201863,ALMUZAINI2022117384,GARCIAPABLOS2018127,CHRISTOPHE2021114831,LI2021114585,WEN201719,ARBANE2023118710,KIM2020113401,EFFROSYNIDIS2022117541,BASTANI2019256,TAVAKOLI2018186,FANG2021114306,HUSSAIN2022118119,CAO2020113465,BELFORD2018159,GROKLUMANN2019171,KIM2022117983,ZHOU2022116194,ZOTOVA2021114547,JEYARAJ2022115896,XU2018106,MIRONCZUK201836,JOSHI2022116846,MA2023118695,GREGORIADES2021115546,LIU2022116741,HARALABOPOULOS2021114769,DONG2018210,PAVLINEK201783,DARGAHINOBARI2021114303,VANDINTER2021115261,LEE2018121,BOUGIATIOTIS201886,MEILIAN2020113427,WAHID2022116562,FALLAHNEJAD2022116433,DAU2020112871,CATELLI2022118290,WU2019285,GOMEZ2022118400,TU201720,LEBENA2022117303,ALDUNATE2022118309,SRINIVASARAO2022116475,ZHENG2018244,CHEN2018516,SKRJANC2022117881,CHEN2022116574,ZHANG2022116882,KORFIATIS2019472,DOSSANTOS201834,ZHANG2022116717,KORENCIC2018357,ADLOUNI2019432,TERROSOSAENZ2020112892,XIE2019178,WANG2022115887,MA2019346,ZHENG2021115030,TANG2021115070,LYKOUSAS2021114808,DHEERAJ2021115265,RANI2022118461,ALSALEMI2018531,BELFORD2020113709,ZHANG2020113073,ERFANIAN2022116086,KIM2020113288,PHAM2019328,TANG2022118062,AMADORDOMINGUEZ2021115731,NUGUMANOVA2022117179,JORGEBOTANA201971,GOZUACIK2021115388,JOSHI2023118442,CAMPOS2022117510,ZIHAYAT2021114910,SHI2022116538,SAKSHI2023119028,SHAMS2017136,ALTINELGIRGIN2021114599,SINGHCHAUHAN2020113673,ZHAO2022118335,RAHIMI2020113770,ZHENG2018168,ANTONAKAKI2021114006,HACOHENKERNER2022117140,AYO2021114762,ZHU2019430,ALAMI2021114652,XIA2022118143,DUAN2020113540,CAO2022115977,DAHIR2021114909,KAUR2020113350,VIDANAGAMA2022117869,ELIGUZEL2022117433,JIA2022116405</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Information Sciences</string>
		<key>keys</key>
		<string>LI2020241,SHEN202063,KHAN202269,ZHANG2020306,BAI2022,DAU20201279,XIAO2021262,YANG2021185,ZHAO2021283,PRADHAN2021212,COSTA2021226,TANG2019190,YANG2019271,GULLO2017199,QIN202237,HOU2022215,BICALHO201766,CHAI20221029,WU2020100,ZHANG2017125,LI201883,ASGHARI2022184,CHEN2021343,WANG2018110,ZHOU20221030,KONG202039,LEI202098,DONG2020203,WANG2021762,CHAUDHURI2022,LIU2018219,ZHENG2022211,DAS2021279,PHAN2021243,NAKAMURA2021482,WANG2021136,FENG202279,DAI2018216,SONG201883,LIU2020227,ETEMADI2022,CHEN2018519,LIN2019483,LI201961,XUAN2017263,LU202087,KHODABAKHSH20221,LU201959,HUANG202018,TERRAGNI2020581,KIM201915,MAO2019269,AMPLAYO2018200,BONDIELLI201938,JUNIOR2021116,MA2017325,ZHONG2021178,LI20221023,MOURINOGARCIA201712,LIANG202194,FIGUEIREDO201965,LIU2021129,DING201912,CHEN201932,VANLIERDE2019212,STEIN2019216,PANG2017190,LU201754,HU2022239,HAN2020177,FROLOV2020595,DESMET201861,LI2022186,RONG2019158,YAN201715,HUYNHTHE2020112,SONG2020138,XIAO2017114,SUN2019456,LIU201713,QIU2017102,ZHENG201955,ZHOU2022809,BAEK2022235,ANNAMORADNEJAD2022144,GAO2022170,DAI2017198,YANG202241,XIAO20211,LIU2022395,XIAO2019526,WAN2020243,J201823</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Journal of Informetrics</string>
		<key>keys</key>
		<string>BALLESTER2022101224,XIE2021101201,ZHOU2021101162,ALSUDAIS2021101139,SHU20171080,HAJIBABAEI2022101275,XU2020101014,ZHANG2020101032,LEE2021101126,AMON2022101284,REHS2021101166,DOGAN2020101076,HUANG2021101145,WANG2021101214,EBADI2020101018,CITRON2018181,CHUNG2021101103,THELWALL2019149,JUNG2022101320,ZHANG20181099,JUNG2020101040,CHEN2022101281,HUANG2022101317,CASSI20171095,XU2019100983,KIM2022101242,BU2017810,MCLEVEY2017176,KIM2022101255,QIAN2020101047</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Knowledge-Based Systems</string>
		<key>keys</key>
		<string>WEI201741,ALI201927,ZHANG2021107135,YANG2021106687,BERNABEMORENO2020105236,PRADHAN2020105784,TANG2019426,ALP2019944,HE201711,MOHSIN2022107711,WANG2020106433,YAN2018149,XU2021106858,KHODABAKHSH20181,LENG2020105600,HOANG2019104788,DU2021107247,DESSI2022109945,GAO2020105418,JEONG2021106659,PARK2020104825,PENG2022109933,CHEN20171,VARSHNEY201766,ZHANG2019194,PESSUTTO2020105339,DAI2022107659,HU2017105,CHEN2021107521,WANG201768,LIU2021106660,LI2021106846,ZHOU2020105695,LI2018203,WU201866,CHEN20191,WANG2017153,AN2022107623,BEHPOUR2021106907,LENG20171,MARTIN2022109265,LIAO2022108665,GUI201734,WANDABWA2021107249,ZHANG201982,LI2021107163,PENTA2021107342,LIU2020106435,YUELIU2019104794,YANG2022108488,LI2021106827,QIN2021107160,QIAN2020105684,SEBASTIAN201766,XU2021107225,LI2020105436,ELAKROUCHI2021106650,WAN2022109551,ZHANG2017255,LIU2021106917,LIU2018158,LIU2022108495,WU2019736,FU201981,YE2022108699,ZHOU2020105458,PENG2019429,WANG2019104812,QIN2020105750,GANGAVARAPU2020105321,KAUR2022108014,YERA2022109216,XU201844,VENUGOPALAN2022108668,AYETIRAN2021106902,FACCHINETTI2022109266,ZHAO2022108550,GONZALEZSANTOS2021107113,ZENGINALP2018211,ALI2020106438,ZHU2021106511,HUANG2019104791,GUO2021107454,DECAMPOS2020105337,LIANG2022108050,SINOARA2019955,XU2020106391,ZHENG2018200,DONG2022108954,YIN201868,YANG2019106,ZOU2022107927,LIU2020105918,DHELIM2020106227,CHEN2020105546,LOPEZ2021107455,ASDAGHI2019198,JUNG2022110020,KUNDU2021106535,FU201843,REN2021107093,HE2020106228,NGUYEN2018313,DELCARMENRODRIGUEZHERNANDEZ2021106740,ABEBE2020104817,ZHANG201849,WEN2020106344,ABUALIGAH2022108833,QIN2018342,LI2021107359,LI2021106948,PONZA2020105051,DIGIROLAMO2021106563,CAO2020106114,KEIKHA201847,ZHANG2018143,ARAQUE2020105184,TOMER2022108108,CHEN2022110000,ORTEGABUENO2022107597,VILLEGAS2018173,HUANG201731,LIAO20179,PRADHAN2020106181,ORTEGAMENDOZA2018169,TELLEZ2018110,BING201838,WANG201885,DENHAM2020105114,BENABDERRAHMANE201895,LEE201770,YUE2020106206,WANG2017125,PANG2019104786,ZHANG2022108006,XU2022107839,YANG2020105214,CHI201788,ZHANG2018236,ZHU2022108741</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>Pattern Recognition</string>
		<key>keys</key>
		<string>JIAN2021108100,IJJINA2017504,CHENG2018474,TALAVERA2020107330,PATRO2021107586,ZHANG2022108217,HOU201766,JIA2018691,XIE2020107205,LU2018228,DENITTO2020107318,CHEN2018404,URICCHIO2017144,RUIZ2019298,LIU20191,WANG2022108230,GUO2022108334,KANDEMIR2018226,CHEN2022108849,ZHENG2020107126,BOSE2021107784,KURBAN2022108621,JOO2020107514,HU2021107734,SELOSSE2020107315,ZHANG2019196,JEON2022108592,WANG2019236,REN2020107297,NGUYEN2018280,LI2021107824,REN201899,HUANG2019174,TU2018203,ZHOU2017548,WANG2020107479,WANG2022108512,LIU2017287,SONG2017368,ZHAO2018112,TANG2022108787,REIHANIAN2018370,LIU2020107053,CHENG2018242,ZHANG2022108661,LU2018129,WEI2021107636</string>
	</dict>
</array>
</plist>
}}
