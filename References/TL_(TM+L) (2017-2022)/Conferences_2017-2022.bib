%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-10-27 21:31:11 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{10.1145/3077136.3080772,
	abstract = {With the advancement of mobile computing technology and cloud-based streaming music service, user-centered music retrieval has become increasingly important. User-specific information has a fundamental impact on personal music preferences and interests. However, existing research pays little attention to the modeling and integration of user-specific information in music retrieval algorithms/models to facilitate music search. In this paper, we propose a novel model, named User-Information-Aware Music Interest Topic (UIA-MIT) model. The model is able to effectively capture the influence of user-specific information on music preferences, and further associate users' music preferences and search terms under the same latent space. Based on this model, a user information aware retrieval system is developed, which can search and re-rank the results based on age- and/or gender-specific music preferences. A comprehensive experimental study demonstrates that our methods can significantly improve the search accuracy over existing text-based music retrieval methods.},
	address = {New York, NY, USA},
	author = {Cheng, Zhiyong and Shen, Jialie and Nie, Liqiang and Chua, Tat-Seng and Kankanhalli, Mohan},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3077136.3080772},
	isbn = {9781450350228},
	keywords = {topic model, user demographic information, re-ranking, semantic music retrieval},
	location = {Shinjuku, Tokyo, Japan},
	numpages = {10},
	pages = {655--664},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '17},
	title = {Exploring User-Specific Information in Music Retrieval},
	url = {https://doi.org/10.1145/3077136.3080772},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3077136.3080772}}

@inproceedings{10.1145/3077136.3080781,
	abstract = {One important way for people to make their voice heard is to comment on the articles they have read online, such as news reports and each other's posts. The user-generated comments together with the commented documents form a unique correspondence structure. Properly modeling the dependency in such data is thus vital for one to obtain accurate insight of people's opinions and attention.In this work, we develop a Commented Correspondence Topic Model to model correspondence in commented text data. We focus on two levels of correspondence. First, to capture topic-level correspondence, we treat the topic assignments in commented documents as the prior to their comments' topic proportions. This captures the thematic dependency between commented documents and their comments. Second, to capture word-level correspondence, we utilize the Dirichlet compound multinomial distribution to model topics. This captures the word repetition patterns within the commented data. By integrating these two aspects, our model demonstrated encouraging performance in capturing the correspondence sturcture, which provides improved results in modeling user-generated content, spam comment detection, and sentence-based comment retrieval compared with state-of-the-art topic model solutions for correspondence modeling.},
	address = {New York, NY, USA},
	author = {Cai, Renqin and Wang, Chi and Wang, Hongning},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3077136.3080781},
	isbn = {9781450350228},
	keywords = {user comments, topic models, social media, text correspondence modeling},
	location = {Shinjuku, Tokyo, Japan},
	numpages = {10},
	pages = {365--374},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '17},
	title = {Accounting for the Correspondence in Commented Data},
	url = {https://doi.org/10.1145/3077136.3080781},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3077136.3080781}}

@inproceedings{10.1145/3077136.3084138,
	abstract = {Tweets summarization aims to find a group of representative tweets for a specific topic. In recent times, there have been several research efforts toward devising a variety of techniques to summarize tweets in Twitter. However, these techniques are either not personal (i.e., consider only tweets in the timeline of a specific user) or are too expensive to be realized on a mobile device. Given that 80% of active Twitter users access the site on mobile devices, in this demonstration we present a lightweight, personalized, on-demand, topic modeling-based tweets summarization engine called TOTEM, designed for such devices. Specifically, TOTEM summarizes most recent tweets on a user's timeline and enables her to visualize and navigate representative topics and associated tweets in a user-friendly tap-and-swipe manner.},
	address = {New York, NY, USA},
	author = {Chin, Jin Yao and Bhowmick, Sourav S. and Jatowt, Adam},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3077136.3084138},
	isbn = {9781450350228},
	keywords = {mobile device, tweets, personal, topic modeling, summarization},
	location = {Shinjuku, Tokyo, Japan},
	numpages = {4},
	pages = {1305--1308},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '17},
	title = {TOTEM: Personal Tweets Summarization on Mobile Devices},
	url = {https://doi.org/10.1145/3077136.3084138},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3077136.3084138}}

@inproceedings{10.1145/3077136.3080703,
	abstract = {We propose a method to clarify the evolution of users' information needs related to a user's interests and actions based upon life events such as "childbirth." First, we extract topic transitions using dynamic topic models from blogs posted by users who have experienced life events. Next, we select the topics by computing the differences in topic probabilities before and after the life event. We evaluated our method based on three life events: "childbirth," "finding employment," and "marriage." Our method selected life event-relevant topics such as "child development," "working life," and "wedding ceremony." We found mothers' information needs such as "how to introduce baby food," employees' information needs such as "preparing an induction programme," and couples' information needs such as "wedding reception planning" in each topic.},
	address = {New York, NY, USA},
	author = {Takeda, Naoto and Seki, Yohei and Morishita, Mimpei and Inagaki, Yoichi},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3077136.3080703},
	isbn = {9781450350228},
	keywords = {dtms (dynamic topic models), life event, information needs},
	location = {Shinjuku, Tokyo, Japan},
	numpages = {4},
	pages = {1009--1012},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '17},
	title = {Evolution of Information Needs Based on Life Event Experiences with Topic Transition},
	url = {https://doi.org/10.1145/3077136.3080703},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3077136.3080703}}

@inproceedings{10.1145/3209978.3210046,
	abstract = {Email continues to be one of the most important means of online communication. People spend a significant amount of time sending, reading, searching and responding to email in order to manage tasks, exchange information, etc. In this paper, we focus on information exchange over enterprise email in the form of questions and answers. We study a large scale publicly available email dataset to characterize information exchange via questions and answers in enterprise email. We augment our analysis with a survey to gain insights on the types of questions exchanged, when and how do people get back to them and whether this behavior is adequately supported by existing email management and search functionality. We leverage this understanding to define the task of extracting question/answer pairs from threaded email conversations. We propose a neural network based approach that matches the question to the answer considering comparisons at different levels of granularity. We also show that we can improve the performance by leveraging external data of question and answer pairs. We test our approach using a manually labeled email data collected using a crowd-sourcing annotation study. Our findings have implications for designing email clients and intelligent agents that support question answering and information lookup in email.},
	address = {New York, NY, USA},
	author = {Yang, Xiao and Awadallah, Ahmed Hassan and Khabsa, Madian and Wang, Wei and Wang, Miaosen},
	booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3209978.3210046},
	isbn = {9781450356572},
	keywords = {question answering, email, information retrieval},
	location = {Ann Arbor, MI, USA},
	numpages = {10},
	pages = {345--354},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '18},
	title = {Characterizing and Supporting Question Answering in Human-to-Human Communication},
	url = {https://doi.org/10.1145/3209978.3210046},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3209978.3210046}}

@inproceedings{10.1145/3209978.3209984,
	abstract = {Sections are the building blocks of Wikipedia articles. They enhance readability and can be used as a structured entry point for creating and expanding articles. Structuring a new or already existing Wikipedia article with sections is a hard task for humans, especially for newcomers or less experienced editors, as it requires significant knowledge about how a well-written article looks for each possible topic. Inspired by this need, the present paper defines the problem of section recommendation for Wikipedia articles and proposes several approaches for tackling it. Our systems can help editors by recommending what sections to add to already existing or newly created Wikipedia articles. Our basic paradigm is to generate recommendations by sourcing sections from articles that are similar to the input article. We explore several ways of defining similarity for this purpose (based on topic modeling, collaborative filtering, and Wikipedia's category system). We use both automatic and human evaluation approaches for assessing the performance of our recommendation system, concluding that the category-based approach works best, achieving precision@10 of about 80% in the human evaluation.},
	address = {New York, NY, USA},
	author = {Piccardi, Tiziano and Catasta, Michele and Zia, Leila and West, Robert},
	booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3209978.3209984},
	isbn = {9781450356572},
	keywords = {recommender system, category network, wikipedia, sections},
	location = {Ann Arbor, MI, USA},
	numpages = {10},
	pages = {665--674},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '18},
	title = {Structuring Wikipedia Articles with Section Recommendations},
	url = {https://doi.org/10.1145/3209978.3209984},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3209978.3209984}}

@inproceedings{10.1145/3209978.3209985,
	abstract = {Learning to rank has been intensively studied and widely applied in information retrieval. Typically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature space. Inspired by the idea of pseudo relevance feedback where top ranked documents, which we refer as the local ranking context, can provide important information about the query's characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list. Specifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results. There are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods. Experimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpora.},
	address = {New York, NY, USA},
	author = {Ai, Qingyao and Bi, Keping and Guo, Jiafeng and Croft, W. Bruce},
	booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3209978.3209985},
	isbn = {9781450356572},
	keywords = {local ranking context, learning to rank, deep neural network},
	location = {Ann Arbor, MI, USA},
	numpages = {10},
	pages = {135--144},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '18},
	title = {Learning a Deep Listwise Context Model for Ranking Refinement},
	url = {https://doi.org/10.1145/3209978.3209985},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3209978.3209985}}

@inproceedings{10.1145/3209978.3210054,
	abstract = {Online streaming services are prevalent. Major service providers, such as Netflix (for movies) and Spotify (for music), usually have a large customer base. More often than not, users may share an account. This has attracted increasing attention recently, as account sharing not only compromises the service provider's financial interests but also impairs the performance of recommendation systems and consequently the quality of service provided to the users. To address this issue, this paper focuses on the problem of user identification in shared accounts. Our goal is three-fold: (1) Given an account, along with its historical session logs, we identify a set of users who share such account; (2) Given a new session issued by an account, we find the corresponding user among the identified users of such account; (3) We aim to boost the performance of item recommendation by user identification. While the mapping between users and accounts is unknown, we propose an unsupervised learning-based framework, Session-based Heterogeneous graph Embedding for User Identification (SHE-UI), to differentiate and model the preferences of users in an account, and to group sessions by these users. In SHE-UI, a heterogeneous graph is constructed to represent items such as songs and their available metadata such as artists, genres, and albums. An item-based session embedding technique is proposed using a normalized random walk in the heterogeneous graph. Our experiments conducted on two large-scale music streaming datasets, Last.fm and KKBOX, show that SHE-UI not only accurately identifies users, but also significantly improves the performance of item recommendation over the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Jiang, Jyun-Yu and Li, Cheng-Te and Chen, Yian and Wang, Wei},
	booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3209978.3210054},
	isbn = {9781450356572},
	keywords = {user session clustering, user identification, heterogeneous graph embedding, shared accounts, recommender systems},
	location = {Ann Arbor, MI, USA},
	numpages = {10},
	pages = {65--74},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '18},
	title = {Identifying Users behind Shared Accounts in Online Streaming Services},
	url = {https://doi.org/10.1145/3209978.3210054},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3209978.3210054}}

@inproceedings{10.1145/3331184.3331287,
	abstract = {Social emotion classification is to estimate the distribution of readers' emotion evoked by an article. In this paper, we design a new neural network model by encoding sentence syntactic dependency and document topical information into the document representation. We first use a dependency embedded recursive neural network to learn syntactic features for each sentence, and then use a gated recurrent unit to transform the sentences' vectors into a document vector. We also use a multi-layer perceptron to encode the topical information of a document into a topic vector. Finally, a gate layer is used to compose the document representation from the gated summation of the document vector and the topic vector. Experiment results on two public datasets indicate that our proposed model outperforms the state-of-the-art methods in terms of better average Pearson correlation coefficient and MicroF1 performance.},
	address = {New York, NY, USA},
	author = {Wang, Chang and Wang, Bang and Xiang, Wei and Xu, Minghua},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3331184.3331287},
	isbn = {9781450361729},
	keywords = {dependency embedding, topic model, social emotion classification, recursive neural network},
	location = {Paris, France},
	numpages = {4},
	pages = {881--884},
	publisher = {Association for Computing Machinery},
	series = {SIGIR'19},
	title = {Encoding Syntactic Dependency and Topical Information for Social Emotion Classification},
	url = {https://doi.org/10.1145/3331184.3331287},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3331184.3331287}}

@inproceedings{10.1145/3331184.3331367,
	abstract = {Targeted stance detection aims to classify the attitude of an opinionated text towards a pre-defined target. Previous methods mainly focus on in-target setting that models are trained and tested using data specific to the same target. In practical cases, the target we concern may have few or no labeled data, which restrains us from training a target-specific model. In this paper we study the problem of cross-target stance detection, utilizing labeled data of a source target to learn models that can be adapted to a destination target. To this end, we propose an effective method, the core intuition of which is to leverage shared latent topics between two targets as transferable knowledge to facilitate model adaptation. Our method acquires topic knowledge with neural variational inference, and further adopts adversarial training that encourages the model to learn target-invariant representations. Experimental results verify that our proposed method is superior to the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Wei, Penghui and Mao, Wenji},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3331184.3331367},
	isbn = {9781450361729},
	keywords = {cross-target stance detection, variational transfer network, transferable topics},
	location = {Paris, France},
	numpages = {4},
	pages = {1173--1176},
	publisher = {Association for Computing Machinery},
	series = {SIGIR'19},
	title = {Modeling Transferable Topics for Cross-Target Stance Detection},
	url = {https://doi.org/10.1145/3331184.3331367},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3331184.3331367}}

@inproceedings{10.1145/3331184.3331228,
	abstract = {Community-based question answering (CQA), which provides a platform for people with diverse backgrounds to share information and knowledge, has become increasingly popular. With the accumulation of site data, methods to detect duplicate questions in CQA sites have attracted considerable attention. Existing methods typically use only questions to complete the task. However, the paired answers may also provide valuable information. In this paper, we propose an answer information- enhanced adaptive multi-attention network (AMAN) to perform this task. AMAN takes full advantage of the semantic information in the paired answers while alleviating the noise problem caused by adding the answers. To evaluate the proposed method, we use a CQADupStack set and the Quora question-pair dataset expanded with paired answers. Experimental results demonstrate that the proposed model can achieve state-of-the-art performance on the above two data sets.},
	address = {New York, NY, USA},
	author = {Liang, Di and Zhang, Fubao and Zhang, Weidong and Zhang, Qi and Fu, Jinlan and Peng, Minlong and Gui, Tao and Huang, Xuanjing},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3331184.3331228},
	isbn = {9781450361729},
	keywords = {duplicate question detection, community-based question answering, adaptive multi-attention},
	location = {Paris, France},
	numpages = {10},
	pages = {95--104},
	publisher = {Association for Computing Machinery},
	series = {SIGIR'19},
	title = {Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection},
	url = {https://doi.org/10.1145/3331184.3331228},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3331184.3331228}}

@inproceedings{10.1145/3331184.3338062,
	abstract = {We improve the measurement accuracy of retrieval system performance by better modeling the noise present in test collection scores. Our technique draws its inspiration from two approaches: one, which exploits the variable measurement accuracy of topics; the other, which randomly splits document collections into shards. We describe and theoretically analyze an ANOVA model able to capture the effects of topics, systems, and document shards as well as their interactions. Using multiple TREC collections, we empirically confirm theoretical results in terms of improved estimation accuracy and robustness of found significant differences. The improvements compared to widely used test collection measurement techniques are substantial. We speculate that our technique works because we do not assume that the topics of a test collection measure performance equally.},
	address = {New York, NY, USA},
	author = {Ferro, Nicola and Sanderson, Mark},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3331184.3338062},
	isbn = {9781450361729},
	keywords = {anova, multiple comparison, effectiveness model},
	location = {Paris, France},
	numpages = {10},
	pages = {805--814},
	publisher = {Association for Computing Machinery},
	series = {SIGIR'19},
	title = {Improving the Accuracy of System Performance Estimation by Using Shards},
	url = {https://doi.org/10.1145/3331184.3338062},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3331184.3338062}}

@inproceedings{10.1145/3331184.3331216,
	abstract = {User reviews contain rich semantics towards the preference of users to features of items. Recently, many deep learning based solutions have been proposed by exploiting reviews for recommendation. The attention mechanism is mainly adopted in these works to identify words or aspects that are important for rating prediction. However, it is still hard to understand whether a user likes or dislikes an aspect of an item according to what viewpoint the user holds and to what extent, without examining the review details. Here, we consider a pair of a viewpoint held by a user and an aspect of an item as a logic unit. Reasoning a rating behavior by discovering the informative logic units from the reviews and resolving their corresponding sentiments could enable a better rating prediction with explanation.To this end, in this paper, we propose a capsule network based model for rating prediction with user reviews, named CARP. For each user-item pair, CARP is devised to extract the informative logic units from the reviews and infer their corresponding sentiments. The model firstly extracts the viewpoints and aspects from the user and item review documents respectively. Then we derive the representation of each logic unit based on its constituent viewpoint and aspect. A sentiment capsule architecture with a novel Routing by Bi-Agreement mechanism is proposed to identify the informative logic unit and the sentiment based representations in user-item level for rating prediction. Extensive experiments are conducted over seven real-world datasets with diverse characteristics. Our results demonstrate that the proposed CARP obtains substantial performance gain over recently proposed state-of-the-art models in terms of prediction accuracy. Further analysis shows that our model can successfully discover the interpretable reasons at a finer level of granularity.},
	address = {New York, NY, USA},
	author = {Li, Chenliang and Quan, Cong and Peng, Li and Qi, Yunwei and Deng, Yuming and Wu, Libing},
	booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3331184.3331216},
	isbn = {9781450361729},
	keywords = {deep learning, user reviews, recommender system},
	location = {Paris, France},
	numpages = {10},
	pages = {275--284},
	publisher = {Association for Computing Machinery},
	series = {SIGIR'19},
	title = {A Capsule Network for Recommendation and Explaining What You Like and Dislike},
	url = {https://doi.org/10.1145/3331184.3331216},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3331184.3331216}}

@inproceedings{10.1145/3397271.3401168,
	abstract = {Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1)the presence of metadataâ€¯: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2)label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into the same semantic space to encode heterogeneous signals. Then, based on the same generative process, we synthesize training samples to address the bottleneck of label scarcity. We conduct a thorough evaluation on a wide range of datasets. Experimental results prove the effectiveness of MetaCat over many competitive baselines.},
	address = {New York, NY, USA},
	author = {Zhang, Yu and Meng, Yu and Huang, Jiaxin and Xu, Frank F. and Wang, Xuan and Han, Jiawei},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401168},
	isbn = {9781450380164},
	keywords = {weak supervision, text classification, metadata},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {1231--1240},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Minimally Supervised Categorization of Text with Metadata},
	url = {https://doi.org/10.1145/3397271.3401168},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401168}}

@inproceedings{10.1145/3397271.3401179,
	abstract = {Aspect-based sentiment analysis is a substantial step towards text understanding which benefits numerous applications. Since most existing algorithms require a large amount of labeled data or substantial external language resources, applying them on a new domain or a new language is usually expensive and time-consuming. We aim to build an aspect-based sentiment analysis model from an unlabeled corpus with minimal guidance from users, i.e., only a small set of seed words for each aspect class and each sentiment class. We employ an autoencoder structure with attention to learn two dictionary matrices for aspect and sentiment respectively where each row of the dictionary serves as an embedding vector for an aspect or a sentiment class. We propose to utilize the user-given seed words to regularize the dictionary learning. In addition, we improve the model by joining the aspect and sentiment encoder in the reconstruction of sentiment in sentences. The joint structure enables sentiment embeddings in the dictionary to be tuned towards the aspect-specific sentiment words for each aspect, which benefits the classification performance. We conduct experiments on two real data sets to verify the effectiveness of our models.},
	address = {New York, NY, USA},
	author = {Zhuang, Honglei and Guo, Fang and Zhang, Chao and Liu, Liyuan and Han, Jiawei},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401179},
	isbn = {9781450380164},
	keywords = {aspect-based sentiment analysis, autoencoder, weakly-supervised},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {1241--1250},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Joint Aspect-Sentiment Analysis with Minimal User Guidance},
	url = {https://doi.org/10.1145/3397271.3401179},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401179}}

@inproceedings{10.1145/3397271.3401185,
	abstract = {Topic modelling is a popular unsupervised method for identifying the underlying themes in document collections that has many applications in information retrieval. A topic is usually represented by a list of terms ranked by their probability but, since these can be difficult to interpret, various approaches have been developed to assign descriptive labels to topics. Previous work on the automatic assignment of labels to topics has relied on a two-stage approach: (1) candidate labels are retrieved from a large pool (e.g. Wikipedia article titles); and then (2) re-ranked based on their semantic similarity to the topic terms. However, these extractive approaches can only assign candidate labels from a restricted set that may not include any suitable ones. This paper proposes using a sequence-to-sequence neural-based approach to generate labels that does not suffer from this limitation. The model is trained over a new large synthetic dataset created using distant supervision. The method is evaluated by comparing the labels it generates to ones rated by humans.},
	address = {New York, NY, USA},
	author = {Alokaili, Areej and Aletras, Nikolaos and Stevenson, Mark},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401185},
	isbn = {9781450380164},
	keywords = {topic representation, neural network, topic modeling},
	location = {Virtual Event, China},
	numpages = {4},
	pages = {1965--1968},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Automatic Generation of Topic Labels},
	url = {https://doi.org/10.1145/3397271.3401185},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401185}}

@inproceedings{10.1145/3397271.3401269,
	abstract = {Manually extracting relevant aspects and opinions from large volumes of user-generated text is a time-consuming process. Summaries, on the other hand, help readers with limited time budgets to quickly consume the key ideas from the data. State-of-the-art approaches for multi-document summarization, however, do not consider user preferences while generating summaries. In this work, we argue the need and propose a solution for generating personalized aspect-based opinion summaries from large collections of online tourist reviews. We let our readers decide and control several attributes of the summary such as the length and specific aspects of interest among others. Specifically, we take an unsupervised approach to extract coherent aspects from tourist reviews posted onTripAdvisor. We then propose an Integer Linear Programming (ILP) based extractive technique to select an informative subset of opinions around the identified aspects while respecting the user-specified values for various control parameters. Finally, we evaluate and compare our summaries using crowdsourcing and ROUGE-based metrics and obtain competitive results.},
	address = {New York, NY, USA},
	author = {Mukherjee, Rajdeep and Peruri, Hari Chandana and Vishnu, Uppada and Goyal, Pawan and Bhattacharya, Sourangshu and Ganguly, Niloy},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401269},
	isbn = {9781450380164},
	keywords = {personalization, controllable summarization, aspect-based opinion mining, tourism, unsupervised extractive opinion summarization},
	location = {Virtual Event, China},
	numpages = {4},
	pages = {1825--1828},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Read What You Need: Controllable Aspect-Based Opinion Summarization of Tourist Reviews},
	url = {https://doi.org/10.1145/3397271.3401269},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401269}}

@inproceedings{10.1145/3397271.3401128,
	abstract = {Many sellers on e-commerce platforms offer buyers product bundles, which package together two or more different items. The identification of such bundles is a necessary step to support a variety of related services, from recommendation to dynamic pricing. In this work, we present a comprehensive study of bundle identification on a large e-commerce website. Our analysis of bundle compared to non-bundle listed items reveals several key differentiating characteristics, spanning the listing's title, image, and attributes. Following, we experiment with a multi-modal classifier, which takes advantage of these characteristics as features. Our analysis also shows that a bundle indicator input by sellers tends to be highly noisy and carries only a weak signal. The bundle identification task therefore faces the challenge of having a small set of manually-labeled clean examples and a larger set of noisy-labeled examples, in conjunction with class imbalance due to the relative scarcity of bundles.Our experiments with basic supervised classifiers, using the manually-labeled and/or the noisy-labeled data for training, demonstrates only moderate performance. We therefore turn to a semisupervised approach and propose GREED, a self-training ensemblebased algorithm with a greedy model selection. Our evaluation over two different meta-categories shows a superior performance of semi-supervised approaches for the bundle identification task, with GREED outperforming several semi-supervised alternatives. The combination of textual, image, and some metadata features is shown to yield the best performance, reaching an AUC of 0.89 and 0.92 for the two meta-categories, respectively},
	address = {New York, NY, USA},
	author = {Tzaban, Hen and Guy, Ido and Greenstein-Messica, Asnat and Dagan, Arnon and Rokach, Lior and Shapira, Bracha},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401128},
	isbn = {9781450380164},
	keywords = {product bundling, semi-supervised learning, self-training, electronic commerce, ensemble learning},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {791--800},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Product Bundle Identification Using Semi-Supervised Learning},
	url = {https://doi.org/10.1145/3397271.3401128},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401128}}

@inproceedings{10.1145/3397271.3401150,
	abstract = {In this work, we aim to investigate the practical task of flexible fashion search with attribute manipulation, where users can retrieve the target fashion items by replacing the unwanted attributes of an available query image with the desired ones (e.g., changing the collar attribute from v-neck to round). Although several pioneer efforts have been dedicated to fulfilling the task, they mainly ignore the potential of generative models in enhancing the visual understanding of target fashion items. To this end, we propose an end-to-end generative attribute manipulation scheme, which consists of a generator and a discriminator. The generator works on producing the prototype image that meets the user's requirement of attribute manipulation over the query image with the regularization of visual-semantic consistency and pixel-wise consistency. Besides, the discriminator aims to jointly fulfill the semantic learning towards correct attribute manipulation and adversarial metric learning for fashion search. Pertaining to the adversarial metric learning, we provide two general paradigms: the pair-based scheme and the triplet-based scheme, where the fake generated prototype images that closely resemble the ground truth images of target items are incorporated as hard negative samples to boost the model performance. Extensive experiments on two real-world datasets verify the effectiveness of our scheme.},
	address = {New York, NY, USA},
	author = {Yang, Xin and Song, Xuemeng and Han, Xianjing and Wen, Haokun and Nie, Jie and Nie, Liqiang},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401150},
	isbn = {9781450380164},
	keywords = {attribute manipulation, fashion search, deep metric learning, generative adversarial networks},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {941--950},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Generative Attribute Manipulation Scheme for Flexible Fashion Search},
	url = {https://doi.org/10.1145/3397271.3401150},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401150}}

@inproceedings{10.1145/3397271.3401047,
	abstract = {Recent years have witnessed a growing trend of fashion compatibility modeling, which scores the matching degree of the given outfit and then provides people with some dressing advice. Existing methods have primarily solved this problem by analyzing the discrete interaction among multiple complementary items. However, the fashion items would present certain occlusion and deformation when they are worn on the body. Therefore, the discrete item interaction cannot capture the fashion compatibility in a combined manner due to the neglect of a crucial factor: the overall try-on appearance. In light of this, we propose a multi-modal try-on-guided compatibility modeling scheme to jointly characterize the discrete interaction and try-on appearance of the outfit. In particular, we first propose a multi-modal try-on template generator to automatically generate a try-on template from the visual and textual information of the outfit, depicting the overall look of its composing fashion items. Then, we introduce a new compatibility modeling scheme which integrates the outfit try-on appearance into the traditional discrete item interaction modeling. To fulfill the proposal, we construct a large-scale real-world dataset from SSENSE, named FOTOS, consisting of 11,000 well-matched outfits and their corresponding realistic try-on images. Extensive experiments have demonstrated its superiority to state-of-the-arts.},
	address = {New York, NY, USA},
	author = {Dong, Xue and Wu, Jianlong and Song, Xuemeng and Dai, Hongjun and Nie, Liqiang},
	booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3397271.3401047},
	isbn = {9781450380164},
	keywords = {fashion analysis, compatibility modeling, try-on-guided scheme},
	location = {Virtual Event, China},
	numpages = {10},
	pages = {771--780},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '20},
	title = {Fashion Compatibility Modeling through a Multi-Modal Try-on-Guided Scheme},
	url = {https://doi.org/10.1145/3397271.3401047},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3397271.3401047}}

@inproceedings{10.1145/3404835.3463080,
	abstract = {We propose VADEC, a multi-task framework that exploits the correlation between the categorical and dimensional models of emotion representation for better subjectivity analysis. Focusing primarily on the effective detection of emotions from tweets, we jointly train multi-label emotion classification and multi-dimensional emotion regression, thereby utilizing the inter-relatedness between the tasks. Co-training especially helps in improving the performance of the classification task as we outperform the strongest baselines with 3.4%, 11%, and 3.9% gains in Jaccard Accuracy, Macro-F1, and Micro-F1 scores respectively on the AIT dataset [17]. We also achieve state-of-the-art results with 11.3% gains averaged over six different metrics on the SenWave dataset [27]. For the regression task, VADEC, when trained with SenWave, achieves 7.6% and 16.5% gains in Pearson Correlation scores over the current state-of-the-art on the EMOBANK dataset [5] for the Valence (V) and Dominance (D) affect dimensions respectively. We conclude our work with a case study on COVID-19 tweets posted by Indians that further helps in establishing the efficacy of our proposed solution.},
	address = {New York, NY, USA},
	author = {Mukherjee, Rajdeep and Naik, Atharva and Poddar, Sriyash and Dasgupta, Soham and Ganguly, Niloy},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3404835.3463080},
	isbn = {9781450380379},
	keywords = {multi-task learning, COVID, coarse-grained emotion analysis, twitter, fine-grained emotion analysis, valence-arousal-dominance},
	location = {Virtual Event, Canada},
	numpages = {5},
	pages = {2303--2307},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {Understanding the Role of Affect Dimensions in Detecting Emotions from Tweets: A Multi-Task Approach},
	url = {https://doi.org/10.1145/3404835.3463080},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3463080}}

@inproceedings{10.1145/3404835.3462975,
	abstract = {Understanding how knowledge is technically transferred across academic disciplines is very relevant for understanding and facilitating innovation. There are two challenges for this purpose, namely the semantic ambiguity and the asymmetric influence across disciplines. In this paper we investigate knowledge propagation and characterize semantic correlations for cross discipline paper recommendation. We adopt a generative model to represent a paper content as the probabilistic association with an existing hierarchically classified discipline to reduce the ambiguity of word semantics. The semantic correlation across disciplines is represented by an influence function, a correlation metric and a ranking mechanism. Then a user interest is represented as a probabilistic distribution over the target domain semantics and the correlated papers are recommended. Experimental results on real datasets show the effectiveness of our methods. We also discuss the intrinsic factors of results in an interpretable way. Compared with traditional word embedding based methods, our approach supports the evolution of domain semantics that accordingly lead to the update of semantic correlation. Another advantage of our approach is its flexibility and uniformity in supporting user interest specifications by either a list of papers or a query of key words, which is suited for practical scenarios.},
	address = {New York, NY, USA},
	author = {Xie, Yi and Sun, Yuqing and Bertino, Elisa},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3404835.3462975},
	isbn = {9781450380379},
	keywords = {academic paper, recommendation, semantic correlation},
	location = {Virtual Event, Canada},
	numpages = {10},
	pages = {706--715},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {Learning Domain Semantics and Cross-Domain Correlations for Paper Recommendation},
	url = {https://doi.org/10.1145/3404835.3462975},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3462975}}

@inproceedings{10.1145/3404835.3462938,
	abstract = {Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines.},
	address = {New York, NY, USA},
	author = {Zogan, Hamad and Razzak, Imran and Jameel, Shoaib and Xu, Guandong},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3404835.3462938},
	isbn = {9781450380379},
	keywords = {machine learning, deep learning, text summarization, social network, depression detection},
	location = {Virtual Event, Canada},
	numpages = {10},
	pages = {133--142},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {DepressionNet: Learning Multi-Modalities with User Post Summarization for Depression Detection on Social Media},
	url = {https://doi.org/10.1145/3404835.3462938},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3462938}}

@inproceedings{10.1145/3404835.3463100,
	abstract = {We propose AutoName, an unsupervised framework that extracts a name for a set of query entities from a large-scale text corpus. Entity-set naming is useful in many tasks related to natural language processing and information retrieval such as session-based and conversational information seeking. Previous studies mainly extract set names from knowledge bases which provide highly reliable entity relations, but suffer from limited coverage of entities and set names that represent broad semantic classes. To address these problems, AutoName generates hypernym-anchored candidate phrases via probing a pre-trained language model and the entities' context in documents. Phrases are then clustered to identify ones that describe common concepts among query entities. Finally, AutoName ranks refined phrases based on the co-occurrences of their words with query entities and the conceptual integrity of their respective clusters. We built a new benchmark dataset for this task, consisting of 130 entity sets with name labels. Experimental results show that AutoName generates coherent and meaningful set names and significantly outperforms all baselines.},
	address = {New York, NY, USA},
	author = {Huang, Zhiqi and Rahimi, Razieh and Yu, Puxuan and Shang, Jingbo and Allan, James},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3404835.3463100},
	isbn = {9781450380379},
	keywords = {conceptual clustering, entity set naming, language model probing},
	location = {Virtual Event, Canada},
	numpages = {5},
	pages = {2101--2105},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {AutoName: A Corpus-Based Set Naming Framework},
	url = {https://doi.org/10.1145/3404835.3463100},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3463100}}

@inproceedings{10.1145/3404835.3462798,
	abstract = {We present the IR Anthology, a corpus of information retrieval publications accessible via a metadata browser and a full-text search engine. Following the example of the well-known ACL Anthology, the IR Anthology serves as a hub for researchers interested in information retrieval. Our search engine ChatNoir indexes the publications' full texts, enabling a focused search and linking users to the respective publisher's site for personal access. Listing more than 40,000 publications at the time of writing, the IR Anthology can be freely accessed at https://IR.webis.de.},
	address = {New York, NY, USA},
	author = {Potthast, Martin and G\"{u}nther, Sebastian and Bevendorff, Janek and Bittner, Jan Philipp and Bondarenko, Alexander and Fr\"{o}be, Maik and Kahmann, Christian and Niekler, Andreas and V\"{o}lske, Michael and Stein, Benno and Hagen, Matthias},
	booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3404835.3462798},
	isbn = {9781450380379},
	keywords = {bibliography, scientific literature analysis, scholarly search},
	location = {Virtual Event, Canada},
	numpages = {6},
	pages = {2550--2555},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '21},
	title = {The Information Retrieval Anthology},
	url = {https://doi.org/10.1145/3404835.3462798},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3404835.3462798}}

@inproceedings{10.1145/3477495.3531877,
	abstract = {We present a novel semantic context prior-based venue recommendation system that uses only the title and the abstract of a paper. Based on the intuition that the text in the title and abstract have both semantic and syntactic components, we demonstrate that a joint training of a semantic feature extractor and syntactic feature extractor collaboratively leverages meaningful information that helps to provide venues for papers. The proposed methodology that we call DeSCoVeR at first elicits these semantic and syntactic features using a Neural Topic Model and text classifier respectively. The model then executes a transfer learning optimization procedure to perform a contextual transfer between the feature distributions of the Neural Topic Model and the text classifier during the training phase. DeSCoVeR also mitigates the document-level label bias using a Causal back-door path criterion and a sentence-level keyword bias removal technique. Experiments on the DBLP dataset show that DeSCoVeR outperforms the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Rajanala, Sailaja and Pal, Arghya and Singh, Manish and Phan, Rapha\"{e}l C.-W. and Wong, KokSheik},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3477495.3531877},
	isbn = {9781450387323},
	keywords = {document classification, joint learning, mutual transfer, causal debiasing, topic modeling},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {2456--2461},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {DeSCoVeR: Debiased Semantic Context Prior for Venue Recommendation},
	url = {https://doi.org/10.1145/3477495.3531877},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531877}}

@inproceedings{10.1145/3477495.3531812,
	abstract = {When searching the web for answers to health questions, people can make incorrect decisions that have a negative effect on their lives if the search results contain misinformation. To reduce health misinformation in search results, we need to be able to detect documents with correct answers and promote them over documents containing misinformation. Determining the correct answer has been a difficult hurdle to overcome for participants in the TREC Health Misinformation Track. In the 2021 track, automatic runs were not allowed to use the known answer to a topic's health question, and as a result, the top automatic run had a compatibility-difference score of 0.043 while the top manual run, which used the known answer, had a score of 0.259. The compatibility-difference measures the ability of methods to rank correct and credible documents before incorrect and non-credible documents. By using an existing set of health questions and their known answers, we show it is possible to learn which web hosts are trustworthy, from which we can predict the correct answers to the 2021 health questions with an accuracy of 76%. Using our predicted answers, we can promote documents that we predict contain this answer and achieve a compatibility-difference score of 0.129, which is a three-fold increase in performance over the best previous automatic method.},
	address = {New York, NY, USA},
	author = {Zhang, Dake and Vakili Tahami, Amir and Abualsaud, Mustafa and Smucker, Mark D.},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3477495.3531812},
	isbn = {9781450387323},
	keywords = {stance detection, web search, health misinformation},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {2099--2104},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {Learning Trustworthy Web Sources to Derive Correct Answers and Reduce Health Misinformation in Search},
	url = {https://doi.org/10.1145/3477495.3531812},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531812}}

@inproceedings{10.1145/3477495.3531817,
	abstract = {Dialogue topic segmentation is a challenging task in which dialogues are split into segments with pre-defined topics. Existing works on topic segmentation adopt a two-stage paradigm, including text segmentation and segment labeling. However, such methods tend to focus on the local context in segmentation, and the inter-segment dependency is not well captured. Besides, the ambiguity and labeling noise in dialogue segment bounds bring further challenges to existing models. In this work, we propose the Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to address the above issues. Specifically, we propose the parallel extraction network to perform segment extractions, optimizing the bipartite matching cost of segments to capture inter-segment dependency. Furthermore, we propose neighbor smoothing to handle the segment-bound noise and ambiguity. Experiments on a dialogue-based and a document-based topic segmentation dataset show that PEN-NS outperforms state-the-of-art models significantly.},
	address = {New York, NY, USA},
	author = {Xia, Jinxiong and Liu, Cao and Chen, Jiansong and Li, Yuchen and Yang, Fan and Cai, Xunliang and Wan, Guanglu and Wang, Houfeng},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3477495.3531817},
	isbn = {9781450387323},
	keywords = {dialogue topic segmentation, neighbor smoothing., boundary ambiguity, parallel extraction, data noise},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {2126--2131},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing},
	url = {https://doi.org/10.1145/3477495.3531817},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531817}}

@inproceedings{10.1145/3477495.3531784,
	abstract = {Previous studies about event-level sentiment analysis (SA) usually model the event as a topic, a category or target terms, while the structured arguments (e.g., subject, object, time and location) that have potential effects on the sentiment are not well studied. In this paper, we redefine the task as structured event-level SA and propose an End-to-End Event-level Sentiment Analysis (E3SA) approach to solve this issue. Specifically, we explicitly extract and model the event structure information for enhancing event-level SA. Extensive experiments demonstrate the great advantages of our proposed approach over the state-of-the-art methods. Noting the lack of the dataset, we also release a large-scale real-world dataset with event arguments and sentiment labelling for promoting more researches.},
	address = {New York, NY, USA},
	author = {Zhang, Qi and Zhou, Jie and Chen, Qin and Bai, Qingchun and He, Liang},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3477495.3531784},
	isbn = {9781450387323},
	keywords = {datasets, structured, event-level sentiment analysis},
	location = {Madrid, Spain},
	numpages = {6},
	pages = {1944--1949},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {Enhancing Event-Level Sentiment Analysis with Structured Arguments},
	url = {https://doi.org/10.1145/3477495.3531784},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3531784}}

@inproceedings{10.1145/3477495.3532084,
	abstract = {Community question answering (CQA) becomes increasingly prevalent in recent years, providing platforms for users with various backgrounds to obtain information and share knowledge. However, the redundancy and lengthiness issues of crowd-sourced answers limit the performance of answer selection, thus leading to difficulties in reading or even misunderstandings for community users. To solve these problems, we propose the dual graph question-answer attention networks (DGQAN) for answer selection task. Aims to fully understand the internal structure of the question and the corresponding answer, firstly, we construct a dual-CQA concept graph with graph convolution networks using the original question and answer text. Specifically, our CQA concept graph exploits the correlation information between question-answer pairs to construct two sub-graphs (QSubject-Answer and QBody-Answer), respectively. Further, a novel dual attention mechanism is incorporated to model both the internal and external semantic relations among questions and answers. More importantly, we conduct experiment to investigate the impact of each layer in the BERT model. The experimental results show that DGQAN model achieves state-of-the-art performance on three datasets (SemEval-2015, 2016, and 2017), outperforming all the baseline models.},
	address = {New York, NY, USA},
	author = {Yang, Haitian and Zhao, Xuan and Wang, Yan and Li, Min and Chen, Wei and Huang, Weiqing},
	booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	date-added = {2022-10-27 21:30:53 +0200},
	date-modified = {2022-10-27 21:30:53 +0200},
	doi = {10.1145/3477495.3532084},
	isbn = {9781450387323},
	keywords = {dual graph attention, community question answering, answer selection},
	location = {Madrid, Spain},
	numpages = {10},
	pages = {1230--1239},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '22},
	title = {DGQAN: Dual Graph Question-Answer Attention Networks for Answer Selection},
	url = {https://doi.org/10.1145/3477495.3532084},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3477495.3532084}}

@proceedings{naacl-2018-2018,
	address = {New Orleans, Louisiana},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	publisher = {Association for Computational Linguistics},
	title = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	url = {https://aclanthology.org/N18-1000},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1000},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1}}

@inproceedings{habernal-etal-2018-name,
	abstract = {Arguing without committing a fallacy is one of the main requirements of an ideal debate. But even when debating rules are strictly enforced and fallacious arguments punished, arguers often lapse into attacking the opponent by an ad hominem argument. As existing research lacks solid empirical investigation of the typology of ad hominem arguments as well as their potential causes, this paper fills this gap by (1) performing several large-scale annotation studies, (2) experimenting with various neural architectures and validating our working hypotheses, such as controversy or reasonableness, and (3) providing linguistic insights into triggers of ad hominem using explainable neural network architectures.},
	address = {New Orleans, Louisiana},
	author = {Habernal, Ivan and Wachsmuth, Henning and Gurevych, Iryna and Stein, Benno},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1036},
	month = jun,
	pages = {386--396},
	publisher = {Association for Computational Linguistics},
	title = {Before Name-Calling: Dynamics and Triggers of Ad Hominem Fallacies in Web Argumentation},
	url = {https://aclanthology.org/N18-1036},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1036},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1036}}

@inproceedings{wu-tsai-2018-cross,
	abstract = {Cross-language article linking (CLAL) is the task of finding corresponding article pairs of different languages across encyclopedias. This task is a difficult disambiguation problem in which one article must be selected among several candidate articles with similar titles and contents. Existing works focus on engineering text-based or link-based features for this task, which is a time-consuming job, and some of these features are only applicable within the same encyclopedia. In this paper, we address these problems by proposing cross-encyclopedia entity embedding. Unlike other works, our proposed method does not rely on known cross-language pairs. We apply our method to CLAL between English Wikipedia and Chinese Baidu Baike. Our features improve performance relative to the baseline by 29.62{\%}. Tested 30 times, our system achieved an average improvement of 2.76{\%} over the current best system (26.86{\%} over baseline), a statistically significant result.},
	address = {New Orleans, Louisiana},
	author = {Wu, Chun-Kai and Tsai, Richard Tzong-Han},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-2054},
	month = jun,
	pages = {334--339},
	publisher = {Association for Computational Linguistics},
	title = {Cross-language Article Linking Using Cross-Encyclopedia Entity Embedding},
	url = {https://aclanthology.org/N18-2054},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-2054},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-2054}}

@inproceedings{chambers-etal-2018-detecting,
	abstract = {This paper describes a novel application of NLP models to detect denial of service attacks using only social media as evidence. Individual networks are often slow in reporting attacks, so a detection system from public data could better assist a response to a broad attack across multiple services. We explore NLP methods to use social media as an indirect measure of network service status. We describe two learning frameworks for this task: a feed-forward neural network and a partially labeled LDA model. Both models outperform previous work by significant margins (20{\%} F1 score). We further show that the topic-based model enables the first fine-grained analysis of how the public reacts to ongoing network attacks, discovering multiple {``}stages{''} of observation. This is the first model that both detects network attacks (with best performance) and provides an analysis of when and how the public interprets service outages. We describe the models, present experiments on the largest twitter DDoS corpus to date, and conclude with an analysis of public reactions based on the learned model{'}s output.},
	address = {New Orleans, Louisiana},
	author = {Chambers, Nathanael and Fry, Ben and McMasters, James},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1147},
	month = jun,
	pages = {1626--1635},
	publisher = {Association for Computational Linguistics},
	title = {Detecting Denial-of-Service Attacks from Social Media Text: Applying {NLP} to Computer Security},
	url = {https://aclanthology.org/N18-1147},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1147},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1147}}

@inproceedings{ambroselli-etal-2018-prediction,
	abstract = {The overwhelming success of the Web and mobile technologies has enabled millions to share their opinions publicly at any time. But the same success also endangers this freedom of speech due to closing down of participatory sites misused by individuals or interest groups. We propose to support manual moderation by proactively drawing the attention of our moderators to article discussions that most likely need their intervention. To this end, we predict which articles will receive a high number of comments. In contrast to existing work, we enrich the article with metadata, extract semantic and linguistic features, and exploit annotated data from a foreign language corpus. Our logistic regression model improves F1-scores by over 80{\%} in comparison to state-of-the-art approaches.},
	address = {New Orleans - Louisiana},
	author = {Ambroselli, Carl and Risch, Julian and Krestel, Ralf and Loos, Andreas},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-3024},
	month = jun,
	pages = {193--199},
	publisher = {Association for Computational Linguistics},
	title = {Prediction for the Newsroom: Which Articles Will Get the Most Comments?},
	url = {https://aclanthology.org/N18-3024},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-3024},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-3024}}

@inproceedings{hessel-etal-2018-quantifying,
	abstract = {Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.},
	address = {New Orleans, Louisiana},
	author = {Hessel, Jack and Mimno, David and Lee, Lillian},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1199},
	month = jun,
	pages = {2194--2205},
	publisher = {Association for Computational Linguistics},
	title = {Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets},
	url = {https://aclanthology.org/N18-1199},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1199}}

@inproceedings{benton-dredze-2018-deep,
	abstract = {Dirichlet Multinomial Regression (DMR) and other supervised topic models can incorporate arbitrary document-level features to inform topic priors. However, their ability to model corpora are limited by the representation and selection of these features {--} a choice the topic modeler must make. Instead, we seek models that can learn the feature representations upon which to condition topic selection. We present deep Dirichlet Multinomial Regression (dDMR), a generative topic model that simultaneously learns document feature representations and topics. We evaluate dDMR on three datasets: New York Times articles with fine-grained tags, Amazon product reviews with product images, and Reddit posts with subreddit identity. dDMR learns representations that outperform DMR and LDA according to heldout perplexity and are more effective at downstream predictive tasks as the number of topics grows. Additionally, human subjects judge dDMR topics as being more representative of associated document features. Finally, we find that supervision leads to faster convergence as compared to an LDA baseline and that dDMR{'}s model fit is less sensitive to training parameters than DMR.},
	address = {New Orleans, Louisiana},
	author = {Benton, Adrian and Dredze, Mark},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1034},
	month = jun,
	pages = {365--374},
	publisher = {Association for Computational Linguistics},
	title = {Deep {D}irichlet Multinomial Regression},
	url = {https://aclanthology.org/N18-1034},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1034},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1034}}

@inproceedings{gupta-etal-2018-deep-temporal,
	abstract = {Dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents. We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps. We account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time, using distributional estimators with temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNNRSM shows better generalization, topic interpretation, evolution and trends. We also introduce a metric (named as SPAN) to quantify the capability of dynamic topic model to capture word evolution in topics over time.},
	address = {New Orleans, Louisiana},
	author = {Gupta, Pankaj and Rajaram, Subburam and Sch{\"u}tze, Hinrich and Andrassy, Bernt},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1098},
	month = jun,
	pages = {1079--1089},
	publisher = {Association for Computational Linguistics},
	title = {Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time},
	url = {https://aclanthology.org/N18-1098},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1098},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1098}}

@inproceedings{jin-etal-2018-combining,
	abstract = {With the rise of e-commerce, people are accustomed to writing their reviews after receiving the goods. These comments are so important that a bad review can have a direct impact on others buying. Besides, the abundant information within user reviews is very useful for extracting user preferences and item properties. In this paper, we investigate the approach to effectively utilize review information for recommender systems. The proposed model is named LSTM-Topic matrix factorization (LTMF) which integrates both LSTM and Topic Modeling for review understanding. In the experiments on popular review dataset Amazon , our LTMF model outperforms previous proposed HFT model and ConvMF model in rating prediction. Furthermore, LTMF shows the better ability on making topic clustering than traditional topic model based method, which implies integrating the information from deep learning and topic modeling is a meaningful approach to make a better understanding of reviews.},
	address = {New Orleans, Louisiana},
	author = {Jin, Mingmin and Luo, Xin and Zhu, Huiling and Zhuo, Hankz Hankui},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N18-1145},
	month = jun,
	pages = {1605--1614},
	publisher = {Association for Computational Linguistics},
	title = {Combining Deep Learning and Topic Modeling for Review Understanding in Context-Aware Recommendation},
	url = {https://aclanthology.org/N18-1145},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1145},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1145}}

@inproceedings{demszky-etal-2019-analyzing,
	abstract = {We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms {``}terrorist{''} and {``}crazy{''}, that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them.},
	address = {Minneapolis, Minnesota},
	author = {Demszky, Dorottya and Garg, Nikhil and Voigt, Rob and Zou, James and Shapiro, Jesse and Gentzkow, Matthew and Jurafsky, Dan},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N19-1304},
	month = jun,
	pages = {2970--3005},
	publisher = {Association for Computational Linguistics},
	title = {Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings},
	url = {https://aclanthology.org/N19-1304},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1304},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1304}}

@inproceedings{nguyen-etal-2019-capsule,
	abstract = {In this paper, we introduce an embedding model, named CapsE, exploring a capsule network to model relationship triples (subject, relation, object). Our CapsE represents each triple as a 3-column matrix where each column vector represents the embedding of an element in the triple. This 3-column matrix is then fed to a convolution layer where multiple filters are operated to generate different feature maps. These feature maps are reconstructed into corresponding capsules which are then routed to another capsule to produce a continuous vector. The length of this vector is used to measure the plausibility score of the triple. Our proposed CapsE obtains better performance than previous state-of-the-art embedding models for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17.},
	address = {Minneapolis, Minnesota},
	author = {Nguyen, Dai Quoc and Vu, Thanh and Nguyen, Tu Dinh and Nguyen, Dat Quoc and Phung, Dinh},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N19-1226},
	month = jun,
	pages = {2180--2189},
	publisher = {Association for Computational Linguistics},
	title = {A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization},
	url = {https://aclanthology.org/N19-1226},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1226},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1226}}

@inproceedings{wang-etal-2019-microblog,
	abstract = {Automatic hashtag annotation plays an important role in content understanding for microblog posts. To date, progress made in this field has been restricted to phrase selection from limited candidates, or word-level hashtag discovery using topic models. Different from previous work considering hashtags to be inseparable, our work is the first effort to annotate hashtags with a novel sequence generation framework via viewing the hashtag as a short sequence of words. Moreover, to address the data sparsity issue in processing short microblog posts, we propose to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention. Extensive experimental results on two large-scale datasets, newly collected from English Twitter and Chinese Weibo, show that our model significantly outperforms state-of-the-art models based on classification. Further studies demonstrate our ability to effectively generate rare and even unseen hashtags, which is however not possible for most existing methods.},
	address = {Minneapolis, Minnesota},
	author = {Wang, Yue and Li, Jing and King, Irwin and Lyu, Michael R. and Shi, Shuming},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N19-1164},
	month = jun,
	pages = {1624--1633},
	publisher = {Association for Computational Linguistics},
	title = {Microblog Hashtag Generation via Encoding Conversation Contexts},
	url = {https://aclanthology.org/N19-1164},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1164},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1164}}

@inproceedings{zeng-etal-2019-variational,
	abstract = {In this paper, we propose a variational approach to weakly supervised document-level multi-aspect sentiment classification. Instead of using user-generated ratings or annotations provided by domain experts, we use target-opinion word pairs as {``}supervision.{''} These word pairs can be extracted by using dependency parsers and simple rules. Our objective is to predict an opinion word given a target word while our ultimate goal is to learn a sentiment polarity classifier to predict the sentiment polarity of each aspect given a document. By introducing a latent variable, i.e., the sentiment polarity, to the objective function, we can inject the sentiment polarity classifier to the objective via the variational lower bound. We can learn a sentiment polarity classifier by optimizing the lower bound. We show that our method can outperform weakly supervised baselines on TripAdvisor and BeerAdvocate datasets and can be comparable to the state-of-the-art supervised method with hundreds of labels per aspect.},
	address = {Minneapolis, Minnesota},
	author = {Zeng, Ziqian and Zhou, Wenxuan and Liu, Xin and Song, Yangqiu},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N19-1036},
	month = jun,
	pages = {386--396},
	publisher = {Association for Computational Linguistics},
	title = {A Variational Approach to Weakly Supervised Document-Level Multi-Aspect Sentiment Classification},
	url = {https://aclanthology.org/N19-1036},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1036},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1036}}

@inproceedings{chitkara-etal-2019-topic,
	abstract = {Success of deep learning techniques have renewed the interest in development of dialogue systems. However, current systems struggle to have consistent long term conversations with the users and fail to build rapport. Topic spotting, the task of automatically inferring the topic of a conversation, has been shown to be helpful in making dialog system more engaging and efficient. We propose a hierarchical model with self attention for topic spotting. Experiments on the Switchboard corpus show the superior performance of our model over previously proposed techniques for topic spotting and deep models for text classification. Additionally, in contrast to offline processing of dialog, we also analyze the performance of our model in a more realistic setting i.e. in an online setting where the topic is identified in real time as the dialog progresses. Results show that our model is able to generalize even with limited information in the online setting.},
	address = {Minneapolis, Minnesota},
	author = {Chitkara, Pooja and Modi, Ashutosh and Avvaru, Pravalika and Janghorbani, Sepehr and Kapadia, Mubbasir},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N19-1376},
	month = jun,
	pages = {3755--3761},
	publisher = {Association for Computational Linguistics},
	title = {Topic Spotting using Hierarchical Networks with Self Attention},
	url = {https://aclanthology.org/N19-1376},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1376},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1376}}

@inproceedings{hao-paul-2019-analyzing,
	abstract = {We introduce a theoretical analysis of crosslingual transfer in probabilistic topic models. By formulating posterior inference through Gibbs sampling as a process of language transfer, we propose a new measure that quantifies the loss of knowledge across languages during this process. This measure enables us to derive a PAC-Bayesian bound that elucidates the factors affecting model quality, both during training and in downstream applications. We provide experimental validation of the analysis on a diverse set of five languages, and discuss best practices for data collection and model design based on our analysis.},
	address = {Minneapolis, Minnesota},
	author = {Hao, Shudong and Paul, Michael J.},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/N19-1158},
	month = jun,
	pages = {1551--1565},
	publisher = {Association for Computational Linguistics},
	title = {Analyzing {B}ayesian Crosslingual Transfer in Topic Models},
	url = {https://aclanthology.org/N19-1158},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1158},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1158}}

@inproceedings{subramanian-etal-2021-spanpredict,
	abstract = {In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.},
	address = {Online},
	author = {Subramanian, Vivek and Engelhard, Matthew and Berchuck, Sam and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.413},
	month = jun,
	pages = {5234--5258},
	publisher = {Association for Computational Linguistics},
	title = {{S}pan{P}redict: Extraction of Predictive Document Spans with Neural Attention},
	url = {https://aclanthology.org/2021.naacl-main.413},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.413},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.413}}

@inproceedings{ji-etal-2021-discrete,
	abstract = {In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations.},
	address = {Online},
	author = {Ji, Lu and Wei, Zhongyu and Li, Jing and Zhang, Qi and Huang, Xuanjing},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.431},
	month = jun,
	pages = {5467--5478},
	publisher = {Association for Computational Linguistics},
	title = {Discrete Argument Representation Learning for Interactive Argument Pair Identification},
	url = {https://aclanthology.org/2021.naacl-main.431},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.431},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.431}}

@inproceedings{schiller-etal-2021-aspect,
	abstract = {We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.},
	address = {Online},
	author = {Schiller, Benjamin and Daxenberger, Johannes and Gurevych, Iryna},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.34},
	month = jun,
	pages = {380--396},
	publisher = {Association for Computational Linguistics},
	title = {Aspect-Controlled Neural Argument Generation},
	url = {https://aclanthology.org/2021.naacl-main.34},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.34},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.34}}

@inproceedings{cui-hu-2021-sliding,
	abstract = {Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from.},
	address = {Online},
	author = {Cui, Peng and Hu, Le},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.470},
	month = jun,
	pages = {5881--5891},
	publisher = {Association for Computational Linguistics},
	title = {Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents},
	url = {https://aclanthology.org/2021.naacl-main.470},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.470},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.470}}

@inproceedings{he-etal-2021-model,
	abstract = {Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.},
	address = {Online},
	author = {He, Xuanli and Lyu, Lingjuan and Sun, Lichao and Xu, Qiongkai},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.161},
	month = jun,
	pages = {2006--2012},
	publisher = {Association for Computational Linguistics},
	title = {Model Extraction and Adversarial Transferability, Your {BERT} is Vulnerable!},
	url = {https://aclanthology.org/2021.naacl-main.161},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.161},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.161}}

@inproceedings{zeng-nie-2021-simple,
	abstract = {Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer {--} conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.},
	address = {Online},
	author = {Zeng, Yan and Nie, Jian-Yun},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.392},
	month = jun,
	pages = {4927--4939},
	publisher = {Association for Computational Linguistics},
	title = {A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation},
	url = {https://aclanthology.org/2021.naacl-main.392},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.392},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.392}}

@inproceedings{zhan-etal-2021-augmenting,
	abstract = {Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pre-training a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pre-trained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledge-grounded dialogue benchmarks indicate that our model achieves better performance over baseline models.},
	address = {Online},
	author = {Zhan, Haolan and Zhang, Hainan and Chen, Hongshen and Ding, Zhuoye and Bao, Yongjun and Lan, Yanyan},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.446},
	month = jun,
	pages = {5621--5630},
	publisher = {Association for Computational Linguistics},
	title = {Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition},
	url = {https://aclanthology.org/2021.naacl-main.446},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.446},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.446}}

@inproceedings{khanehzar-etal-2021-framing,
	abstract = {Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semi-supervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation.},
	address = {Online},
	author = {Khanehzar, Shima and Cohn, Trevor and Mikolajczak, Gosia and Turpin, Andrew and Frermann, Lea},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.174},
	month = jun,
	pages = {2154--2166},
	publisher = {Association for Computational Linguistics},
	title = {Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames},
	url = {https://aclanthology.org/2021.naacl-main.174},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.174},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.174}}

@inproceedings{iida-etal-2021-tabbie,
	abstract = {Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.},
	address = {Online},
	author = {Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.270},
	month = jun,
	pages = {3446--3456},
	publisher = {Association for Computational Linguistics},
	title = {{TABBIE}: Pretrained Representations of Tabular Data},
	url = {https://aclanthology.org/2021.naacl-main.270},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.270},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.270}}

@inproceedings{shen-etal-2021-taxoclass,
	abstract = {Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its {``}core classes{''}, and then check core classes{'} ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document{'}s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25{\%}.},
	address = {Online},
	author = {Shen, Jiaming and Qiu, Wenda and Meng, Yu and Shang, Jingbo and Ren, Xiang and Han, Jiawei},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.335},
	month = jun,
	pages = {4239--4249},
	publisher = {Association for Computational Linguistics},
	title = {{T}axo{C}lass: Hierarchical Multi-Label Text Classification Using Only Class Names},
	url = {https://aclanthology.org/2021.naacl-main.335},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.335},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.335}}

@inproceedings{sun-etal-2021-tita,
	abstract = {In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topic-aware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at above-mentioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of {``}topic-aware{''}, we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of {``}two-stage interaction{''}, we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system.},
	address = {Online},
	author = {Sun, Xingwu and Cui, Yanling and Tang, Hongyin and Zhu, Qiuyu and Zhang, Fuzheng and Jin, Beihong},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.428},
	month = jun,
	pages = {5431--5440},
	publisher = {Association for Computational Linguistics},
	title = {{TITA}: A Two-stage Interaction and Topic-Aware Text Matching Model},
	url = {https://aclanthology.org/2021.naacl-main.428},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.428},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.428}}

@inproceedings{mueller-dredze-2021-fine,
	abstract = {Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer.},
	address = {Online},
	author = {Mueller, Aaron and Dredze, Mark},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.243},
	month = jun,
	pages = {3054--3068},
	publisher = {Association for Computational Linguistics},
	title = {Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling},
	url = {https://aclanthology.org/2021.naacl-main.243},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.243},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.243}}

@inproceedings{xie-etal-2021-inductive,
	abstract = {Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global word-level graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents.},
	address = {Online},
	author = {Xie, Qianqian and Huang, Jimin and Du, Pan and Peng, Min and Nie, Jian-Yun},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.333},
	month = jun,
	pages = {4218--4227},
	publisher = {Association for Computational Linguistics},
	title = {Inductive Topic Variational Graph Auto-Encoder for Text Classification},
	url = {https://aclanthology.org/2021.naacl-main.333},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.333},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.333}}

@inproceedings{gupta-etal-2021-multi,
	abstract = {Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.},
	address = {Online},
	author = {Gupta, Pankaj and Chaudhary, Yatin and Sch{\"u}tze, Hinrich},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.332},
	month = jun,
	pages = {4205--4217},
	publisher = {Association for Computational Linguistics},
	title = {Multi-source Neural Topic Modeling in Multi-view Embedding Spaces},
	url = {https://aclanthology.org/2021.naacl-main.332},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.332},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.332}}

@inproceedings{pergola-etal-2021-disentangled,
	abstract = {The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers{'} subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.},
	address = {Online},
	author = {Pergola, Gabriele and Gui, Lin and He, Yulan},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.228},
	month = jun,
	pages = {2870--2883},
	publisher = {Association for Computational Linguistics},
	title = {A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews},
	url = {https://aclanthology.org/2021.naacl-main.228},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.228},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.228}}

@inproceedings{doogan-buntine-2021-topic,
	abstract = {When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models{'} generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.},
	address = {Online},
	author = {Doogan, Caitlin and Buntine, Wray},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2021.naacl-main.300},
	month = jun,
	pages = {3824--3848},
	publisher = {Association for Computational Linguistics},
	title = {Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures},
	url = {https://aclanthology.org/2021.naacl-main.300},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.300},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.300}}

@inproceedings{zhu-etal-2022-disentangled,
	abstract = {Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering.},
	address = {Seattle, United States},
	author = {Zhu, Lixing and Fang, Zheng and Pergola, Gabriele and Procter, Robert and He, Yulan},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.112},
	month = jul,
	pages = {1566--1580},
	publisher = {Association for Computational Linguistics},
	title = {Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media},
	url = {https://aclanthology.org/2022.naacl-main.112},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.112},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.112}}

@inproceedings{kulkarni-etal-2022-ctm,
	abstract = {Automatically associating social media posts with topics is an important prerequisite for effective search and recommendation on many social media platforms. However, topic classification of such posts is quite challenging because of (a) a large topic space (b) short text with weak topical cues, and (c) multiple topic associations per post. In contrast to most prior work which only focuses on post-classification into a small number of topics ($10-20$), we consider the task of large-scale topic classification in the context of Twitter where the topic space is 10 times larger with potentially multiple topic associations per Tweet. We address the challenges above and propose a novel neural model, that (a) supports a large topic space of 300 topics (b) takes a holistic approach to tweet content modeling {--} leveraging multi-modal content, author context, and deeper semantic cues in the Tweet. Our method offers an effective way to classify Tweets into topics at scale by yielding superior performance to other approaches (a relative lift of $\mathbf{20}\%$ in median average precision score) and has been successfully deployed in production at Twitter.},
	address = {Hybrid: Seattle, Washington + Online},
	author = {Kulkarni, Vivek and Leung, Kenny and Haghighi, Aria},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-industry.28},
	month = jul,
	pages = {247--258},
	publisher = {Association for Computational Linguistics},
	title = {{CTM} - A Model for Large-Scale Multi-View Tweet Topic Classification},
	url = {https://aclanthology.org/2022.naacl-industry.28},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-industry.28},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-industry.28}}

@inproceedings{li-etal-2022-corwa,
	abstract = {Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the {``}Related Work{''} section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has focused on the sentence as the basic unit of generation, neglecting the fact that related work sections consist of variable length text fragments derived from different information sources. As a first step toward a linguistically-motivated related work generation framework, we present a Citation Oriented Related Work Annotation (CORWA) dataset that labels different types of citation text fragments from different information sources. We train a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts. We further suggest a novel framework for human-in-the-loop, iterative, abstractive related work generation.},
	address = {Seattle, United States},
	author = {Li, Xiangci and Mandal, Biswadip and Ouyang, Jessica},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.397},
	month = jul,
	pages = {5426--5440},
	publisher = {Association for Computational Linguistics},
	title = {{CORWA}: A Citation-Oriented Related Work Annotation Dataset},
	url = {https://aclanthology.org/2022.naacl-main.397},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.397},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.397}}

@inproceedings{bhattacharjee-etal-2022-users,
	abstract = {Users often leave feedback on a myriad of aspects of a product which, if leveraged successfully, can help yield useful insights that can lead to further improvements down the line. Detecting actionable insights can be challenging owing to large amounts of data as well as the absence of labels in real-world scenarios. In this work, we present an aggregation and graph-based ranking strategy for unsupervised detection of these insights from real-world, noisy, user-generated feedback. Our proposed approach significantly outperforms strong baselines on two real-world user feedback datasets and one academic dataset.},
	address = {Hybrid: Seattle, Washington + Online},
	author = {Bhattacharjee, Kasturi and Gangadharaiah, Rashmi and McKeown, Kathleen and Roth, Dan},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-industry.27},
	month = jul,
	pages = {239--246},
	publisher = {Association for Computational Linguistics},
	title = {What Do Users Care About? Detecting Actionable Insights from User Feedback},
	url = {https://aclanthology.org/2022.naacl-industry.27},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-industry.27},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-industry.27}}

@inproceedings{spangher-etal-2022-newsedits,
	abstract = {News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.},
	address = {Seattle, United States},
	author = {Spangher, Alexander and Ren, Xiang and May, Jonathan and Peng, Nanyun},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.10},
	month = jul,
	pages = {127--157},
	publisher = {Association for Computational Linguistics},
	title = {{N}ews{E}dits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge},
	url = {https://aclanthology.org/2022.naacl-main.10},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.10},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.10}}

@inproceedings{zhang-etal-2022-seed,
	abstract = {Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users{'} particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.},
	address = {Seattle, United States},
	author = {Zhang, Yu and Meng, Yu and Wang, Xuan and Wang, Sheng and Han, Jiawei},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.21},
	month = jul,
	pages = {279--290},
	publisher = {Association for Computational Linguistics},
	title = {Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds},
	url = {https://aclanthology.org/2022.naacl-main.21},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.21},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.21}}

@inproceedings{sircar-etal-2022-distantly,
	abstract = {Product aspect extraction from reviews is a critical task for e-commerce services to understand customer preferences and pain points. While aspect phrases extraction and sentiment analysis have received a lot of attention, clustering of aspect phrases and assigning human readable names to clusters in e-commerce reviews is an extremely important and challenging problem due to the scale of the reviews that makes human review infeasible. In this paper, we propose fully automated methods for clustering aspect words and generating human readable names for the clusters without any manually labeled data. We train transformer based sentence embeddings that are aware of unique e-commerce language characteristics (eg. incomplete sentences, spelling and grammar errors, vernacular etc.). We also train transformer based sequence to sequence models to generate human readable aspect names from clusters. Both the models are trained using heuristic based distant supervision. Additionally, the models are used to improve each other. Extensive empirical testing showed that the clustering model improves the Silhouette Score by 64{\%} when compared to the state-of-the-art baseline and the aspect naming model achieves a high ROUGE-L score of 0.79.},
	address = {Hybrid: Seattle, Washington + Online},
	author = {Sircar, Prateek and Chakrabarti, Aniket and Gupta, Deepak and Majumdar, Anirban},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-industry.12},
	month = jul,
	pages = {94--102},
	publisher = {Association for Computational Linguistics},
	title = {Distantly Supervised Aspect Clustering And Naming For {E}-Commerce Reviews},
	url = {https://aclanthology.org/2022.naacl-industry.12},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-industry.12},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-industry.12}}

@inproceedings{liu-etal-2022-hiure,
	abstract = {Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.},
	address = {Seattle, United States},
	author = {Liu, Shuliang and Hu, Xuming and Zhang, Chenwei and Li, Shu{'}ang and Wen, Lijie and Yu, Philip},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.437},
	month = jul,
	pages = {5970--5980},
	publisher = {Association for Computational Linguistics},
	title = {{H}i{URE}: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction},
	url = {https://aclanthology.org/2022.naacl-main.437},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.437},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.437}}

@inproceedings{voigt-etal-2022-survey,
	abstract = {Natural language as a modality of interaction is becoming increasingly popular in the field of visualization. In addition to the popular query interfaces, other language-based interactions such as annotations, recommendations, explanations, or documentation experience growing interest. In this survey, we provide an overview of natural language-based interaction in the research area of visualization. We discuss a renowned taxonomy of visualization tasks and classify 119 related works to illustrate the state-of-the-art of how current natural language interfaces support their performance. We examine applied NLP methods and discuss human-machine dialogue structures with a focus on initiative, duration, and communicative functions in recent visualization-oriented dialogue interfaces. Based on this overview, we point out interesting areas for the future application of NLP methods in the field of visualization.},
	address = {Seattle, United States},
	author = {Voigt, Henrik and Alacam, Ozge and Meuschke, Monique and Lawonn, Kai and Zarrie{\ss}, Sina},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.27},
	month = jul,
	pages = {348--374},
	publisher = {Association for Computational Linguistics},
	title = {The Why and The How: A Survey on Natural Language Interaction in Visualization},
	url = {https://aclanthology.org/2022.naacl-main.27},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.27},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.27}}

@inproceedings{zhang-etal-2022-kcd,
	abstract = {Political perspective detection has become an increasingly important task that can help combat echo chambers and political polarization. Previous approaches generally focus on leveraging textual content to identify stances, while they fail to reason with background knowledge or leverage the rich semantic and syntactic textual labels in news articles. In light of these limitations, we propose KCD, a political perspective detection approach to enable multi-hop knowledge reasoning and incorporate textual cues as paragraph-level labels. Specifically, we firstly generate random walks on external knowledge graphs and infuse them with news text representations. We then construct a heterogeneous information network to jointly model news content as well as semantic, syntactic and entity cues in news articles. Finally, we adopt relational graph neural networks for graph-level representation learning and conduct political perspective detection. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on two benchmark datasets. We further examine the effect of knowledge walks and textual cues and how they contribute to our approach{'}s data efficiency.},
	address = {Seattle, United States},
	author = {Zhang, Wenqian and Feng, Shangbin and Chen, Zilong and Lei, Zhenyu and Li, Jundong and Luo, Minnan},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-10-27 21:30:43 +0200},
	date-modified = {2022-10-27 21:30:43 +0200},
	doi = {10.18653/v1/2022.naacl-main.304},
	month = jul,
	pages = {4129--4140},
	publisher = {Association for Computational Linguistics},
	title = {{KCD}: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media},
	url = {https://aclanthology.org/2022.naacl-main.304},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.304},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.304}}

@inproceedings{10.1145/3097983.3098067,
	abstract = {While exploring human mobility can benefit many applications such as smart transportation, city planning, and urban economics, there are two key questions that need to be answered: (i) What is the nature of the spatial diffusion of human mobility across regions with different urban functions? (ii) How to spot and trace the trip purposes of human mobility trajectories? To answer these questions, we study large-scale and city-wide taxi trajectories; and furtherly organize them as arrival sequences according to the chronological arrival time. We figure out an important property across different regions from the arrival sequences, namely human mobility synchronization effect, which can be exploited to explain the phenomenon that two regions have similar arrival patterns in particular time periods if they share similar urban functions. In addition, the arrival sequences are mixed by arrival events with distinct trip purposes, which can be revealed by the regional environment of both the origins and destinations. To that end, in this paper, we develop a joint model that integrates Mixture of Hawkes Process (MHP) with a hierarchical topic model to capture the arrival sequences with mixed trip purposes. Essentially, the human mobility synchronization effect is encoded as a synchronization rate in the MHP; while the regional environment is modeled by introducing latent Trip Purpose and POI Topic to generate the Point of Interests (POIs) in the regions. Moreover, we provide an effective inference algorithm for parameter learning. Finally, we conduct intensive experiments on synthetic data and real-world data, and the experimental results have demonstrated the effectiveness of the proposed model.},
	address = {New York, NY, USA},
	author = {Wang, Pengfei and Fu, Yanjie and Liu, Guannan and Hu, Wenqing and Aggarwal, Charu},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3097983.3098067},
	isbn = {9781450348874},
	keywords = {hawkes process, human mobility, synchronization, trip purpose, variational inference},
	location = {Halifax, NS, Canada},
	numpages = {9},
	pages = {495--503},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {Human Mobility Synchronization and Trip Purpose Detection with Mixture of Hawkes Processes},
	url = {https://doi.org/10.1145/3097983.3098067},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3097983.3098067}}

@inproceedings{10.1145/3097983.3098122,
	abstract = {Spatial item recommendation has become an important means to help people discover interesting locations, especially when people pay a visit to unfamiliar regions. Some current researches are focusing on modelling individual and collective geographical preferences for spatial item recommendation based on users' check-in records, but they fail to explore the phenomenon of user interest drift across geographical regions, i.e., users would show different interests when they travel to different regions. Besides, they ignore the influence of public comments for subsequent users' check-in behaviors. Specifically, it is intuitive that users would refuse to check in to a spatial item whose historical reviews seem negative overall, even though it might fit their interests. Therefore, it is necessary to recommend the right item to the right user at the right location. In this paper, we propose a latent probabilistic generative model called LSARS to mimic the decision-making process of users' check-in activities both in home-town and out-of-town scenarios by adapting to user interest drift and crowd sentiments, which can learn location-aware and sentiment-aware individual interests from the contents of spatial items and user reviews. Due to the sparsity of user activities in out-of-town regions, LSARS is further designed to incorporate the public preferences learned from local users' check-in behaviors. Finally, we deploy LSARS into two practical application scenes: spatial item recommendation and target user discovery. Extensive experiments on two large-scale location-based social networks (LBSNs) datasets show that LSARS achieves better performance than existing state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Wang, Hao and Fu, Yanmei and Wang, Qinyong and Yin, Hongzhi and Du, Changying and Xiong, Hui},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3097983.3098122},
	isbn = {9781450348874},
	keywords = {check-in behavior, recommendation, crowd sentiment, user interest drift},
	location = {Halifax, NS, Canada},
	numpages = {9},
	pages = {1135--1143},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {A Location-Sentiment-Aware Recommender System for Both Home-Town and Out-of-Town Users},
	url = {https://doi.org/10.1145/3097983.3098122},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3097983.3098122}}

@inproceedings{10.1145/3097983.3098074,
	abstract = {Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.},
	address = {New York, NY, USA},
	author = {He, Junxian and Hu, Zhiting and Berg-Kirkpatrick, Taylor and Huang, Ying and Xing, Eric P.},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3097983.3098074},
	isbn = {9781450348874},
	keywords = {scalability, correlated topic models, topic embedding},
	location = {Halifax, NS, Canada},
	numpages = {9},
	pages = {225--233},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {Efficient Correlated Topic Modeling with Topic Embedding},
	url = {https://doi.org/10.1145/3097983.3098074},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3097983.3098074}}

@inproceedings{10.1145/3097983.3098017,
	abstract = {Autoencoders have been successful in learning meaningful representations from image datasets. However, their performance on text datasets has not been widely studied. Traditional autoencoders tend to learn possibly trivial representations of text documents due to their confoundin properties such as high-dimensionality, sparsity and power-law word distributions. In this paper, we propose a novel k-competitive autoencoder, called KATE, for text documents. Due to the competition between the neurons in the hidden layer, each neuron becomes specialized in recognizing specific data patterns, and overall the model can learn meaningful representations of textual data. A comprehensive set of experiments show that KATE can learn better representations than traditional autoencoders including denoising, contractive, variational, and k-sparse autoencoders. Our model also outperforms deep generative models, probabilistic topic models, and even word representation models (e.g., Word2Vec) in terms of several downstream tasks such as document classification, regression, and retrieval.},
	address = {New York, NY, USA},
	author = {Chen, Yu and Zaki, Mohammed J.},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3097983.3098017},
	isbn = {9781450348874},
	keywords = {representation learning, competitive learning, text analytics, autoencoders},
	location = {Halifax, NS, Canada},
	numpages = {10},
	pages = {85--94},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {KATE: K-Competitive Autoencoder for Text},
	url = {https://doi.org/10.1145/3097983.3098017},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3097983.3098017}}

@inproceedings{10.1145/3097983.3098110,
	abstract = {We propose a new model selection criterion based on the minimum description length principle in a name of the decomposed normalized maximum likelihood criterion. Our criterion can be applied to a large class of hierarchical latent variable models, such as the Naive Bayes models, stochastic block models and latent Dirichlet allocations, for which many conventional information criteria cannot be straightforwardly applied due to irregularity of latent variable models. Our method also has an advantage that it can be exactly evaluated without asymptotic approximation with small time complexity. Our experiments using synthetic and real data demonstrated validity of our method in terms of computational efficiency and model selection accuracy, while our criterion especially dominated the other criteria when sample size is small and when data are noisy.},
	address = {New York, NY, USA},
	author = {Wu, Tianyi and Sugawara, Shinya and Yamanishi, Kenji},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3097983.3098110},
	isbn = {9781450348874},
	keywords = {topic and latent variable models, model selection, clustering},
	location = {Halifax, NS, Canada},
	numpages = {10},
	pages = {1165--1174},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {Decomposed Normalized Maximum Likelihood Codelength Criterion for Selecting Hierarchical Latent Variable Models},
	url = {https://doi.org/10.1145/3097983.3098110},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3097983.3098110}}

@inproceedings{10.1145/3097983.3098068,
	abstract = {Motifs are a powerful tool for analyzing physiological waveform data. Standard motif methods, however, ignore important contextual information (e.g., what the patient was doing at the time the data were collected). We hypothesize that these additional contextual data could increase the utility of motifs. Thus, we propose an extension to motifs, contextual motifs, that incorporates context. Recognizing that, oftentimes, context may be unobserved or unavailable, we focus on methods to jointly infer motifs and context. Applied to both simulated and real physiological data, our proposed approach improves upon existing motif methods in terms of the discriminative utility of the discovered motifs. In particular, we discovered contextual motifs in continuous glucose monitor (CGM) data collected from patients with type 1 diabetes. Compared to their contextless counterparts, these contextual motifs led to better predictions of hypo- and hyperglycemic events. Our results suggest that even when inferred, context is useful in both a long- and short-term prediction horizon when processing and interpreting physiological waveform data.},
	address = {New York, NY, USA},
	author = {Fox, Ian and Ang, Lynn and Jaiswal, Mamta and Pop-Busui, Rodica and Wiens, Jenna},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3097983.3098068},
	isbn = {9781450348874},
	keywords = {contextual motifs, motif discovery, blood glucose},
	location = {Halifax, NS, Canada},
	numpages = {10},
	pages = {155--164},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {Contextual Motifs: Increasing the Utility of Motifs Using Contextual Data},
	url = {https://doi.org/10.1145/3097983.3098068},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3097983.3098068}}

@inproceedings{10.1145/3219819.3219929,
	abstract = {Cultural activity is an inherent aspect of urban life and the success of a modern city is largely determined by its capacity to offer generous cultural entertainment to its citizens. To this end, the optimal allocation of cultural establishments and related resources across urban regions becomes of vital importance, as it can reduce financial costs in terms of planning and improve quality of life in the city, more generally. In this paper, we make use of a large longitudinal dataset of user location check-ins from the online social network WeChat to develop a data-driven framework for cultural planning in the city of Beijing. We exploit rich spatio-temporal representations on user activity at cultural venues and use a novel extended version of the traditional latent Dirichlet allocation model that incorporates temporal information to identify latent patterns of urban cultural interactions. Using the characteristic typologies of mobile user cultural activities emitted by the model, we determine the levels of demand for different types of cultural resources across urban areas. We then compare those with the corresponding levels of supply as driven by the presence and spatial reach of cultural venues in local areas to obtain high resolution maps that indicate urban regions with lack of cultural resources, and thus give suggestions for further urban cultural planning and investment optimisation.},
	address = {New York, NY, USA},
	author = {Zhou, Xiao and Noulas, Anastasios and Mascolo, Cecilia and Zhao, Zhongxiang},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3219819.3219929},
	isbn = {9781450355520},
	keywords = {topic modeling, pattern mining, urban computing, spatial accessibility, spatio-temporal analysis},
	location = {London, United Kingdom},
	numpages = {10},
	pages = {1069--1078},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {Discovering Latent Patterns of Urban Cultural Interactions in WeChat for Modern City Planning},
	url = {https://doi.org/10.1145/3219819.3219929},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3219819.3219929}}

@inproceedings{10.1145/3219819.3220064,
	abstract = {Taxonomy construction is not only a fundamental task for semantic analysis of text corpora, but also an important step for applications such as information filtering, recommendation, and Web search. Existing pattern-based methods extract hypernym-hyponym term pairs and then organize these pairs into a taxonomy. However, by considering each term as an independent concept node, they overlook the topical proximity and the semantic correlations among terms. In this paper, we propose a method for constructing topic taxonomies, wherein every node represents a conceptual topic and is defined as a cluster of semantically coherent concept terms. Our method, TaxoGen, uses term embeddings and hierarchical clustering to construct a topic taxonomy in a recursive fashion. To ensure the quality of the recursive process, it consists of: (1) an adaptive spherical clustering module for allocating terms to proper levels when splitting a coarse topic into fine-grained ones; (2) a local embedding module for learning term embeddings that maintain strong discriminative power at different levels of the taxonomy. Our experiments on two real datasets demonstrate the effectiveness of TaxoGen compared with baseline methods.},
	address = {New York, NY, USA},
	author = {Zhang, Chao and Tao, Fangbo and Chen, Xiusi and Shen, Jiaming and Jiang, Meng and Sadler, Brian and Vanni, Michelle and Han, Jiawei},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3219819.3220064},
	isbn = {9781450355520},
	keywords = {taxonomy construction, text mining, word embedding},
	location = {London, United Kingdom},
	numpages = {9},
	pages = {2701--2709},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering},
	url = {https://doi.org/10.1145/3219819.3220064},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3219819.3220064}}

@inproceedings{10.1145/3219819.3219964,
	abstract = {Stock comments from analysts contain important consulting information for investors to foresee stock volatility and market trends. Existing studies on stock comments usually focused on capturing coarse-grained opinion polarities or understanding market fundamentals. However, investors are often overwhelmed and confused by massive comments with huge noises and ambiguous opinions. Therefore, it is an emerging need to have a fine-grained stock comment analysis tool to identify more reliable stock comments. To this end, this paper provides a solution called StockAssIstant for modeling the reliability of stock comments by considering multiple factors, such as stock price trends, comment content, and the performances of analysts, in a holistic manner. Specifically, we first analyze the pattern of analysts' opinion dynamics from historical comments. Then, we extract key features from the time-series constructed by using the semantic information in comment text, stock prices and the historical behaviors of analysts. Based on these features, we propose an ensemble learning based approach for measuring the reliability of comments. Finally, we conduct extensive experiments and provide a trading simulation on real-world stock data. The experimental results and the profit achieved by the simulated trading in 12-month period clearly validate the effectiveness of our approach for modeling the reliability of stock comments.},
	address = {New York, NY, USA},
	author = {Zhang, Chen and Wang, Yijun and Chen, Can and Du, Changying and Yin, Hongzhi and Wang, Hao},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3219819.3219964},
	isbn = {9781450355520},
	keywords = {stock comment, reliability modeling, time-series},
	location = {London, United Kingdom},
	numpages = {10},
	pages = {2710--2719},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {StockAssIstant: A Stock AI Assistant for Reliability Modeling of Stock Comments},
	url = {https://doi.org/10.1145/3219819.3219964},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3219819.3219964}}

@inproceedings{10.1145/3219819.3219827,
	abstract = {The automatic extraction of breaking news events from natural language text is a valuable capability for decision support systems. Traditional systems tend to focus on extracting events from a single media source and often ignore cross-media references. Here, we describe a large-scale automated system for extracting natural disasters and critical events from both newswire text and social media. We outline a comprehensive architecture that can identify, categorize and summarize seven different event types - namely floods, storms, fires, armed conflict, terrorism, infrastructure breakdown, and labour unavailability. The system comprises fourteen modules and is equipped with a novel coreference mechanism, capable of linking events extracted from the two complementary data sources. Additionally, the system is easily extensible to accommodate new event types. Our experimental evaluation demonstrates the effectiveness of the system.},
	address = {New York, NY, USA},
	author = {Petroni, Fabio and Raman, Natraj and Nugent, Tim and Nourbakhsh, Armineh and Pani\'{c}, \v{Z}arko and Shah, Sameena and Leidner, Jochen L.},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3219819.3219827},
	isbn = {9781450355520},
	keywords = {event extraction, news analytics, information extraction, event coreference, first story detection},
	location = {London, United Kingdom},
	numpages = {10},
	pages = {626--635},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {An Extensible Event Extraction System With Cross-Media Event Resolution},
	url = {https://doi.org/10.1145/3219819.3219827},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3219819.3219827}}

@inproceedings{10.1145/3292500.3330706,
	abstract = {In talent recruitment, the job interview aims at selecting the right candidates for the right jobs through assessing their skills and experiences in relation to the job positions. While tremendous efforts have been made in improving job interviews, a long-standing challenge is how to design appropriate interview questions for comprehensively assessing the competencies that may be deemed relevant and representative for person-job fit. To this end, in this research, we focus on the development of a personalized question recommender system, namely DuerQuiz, for enhancing the job interview assessment. DuerQuiz is a fully deployed system, in which a knowledge graph of job skills, Skill-Graph, has been built for comprehensively modeling the relevant competencies that should be assessed in the job interview. Specifically, we first develop a novel skill entity extraction approach based on a bidirectional Long Short-Term Memory (LSTM) with a Conditional Random Field (CRF) layer (LSTM-CRF) neural network enhanced with adapted gate mechanism. In particular, to improve the reliability of extracted skill entities, we design a label propagation method based on more than 10 billion click-through data from the large-scale Baidu query logs. Furthermore, we discover the hypernym-hyponym relations between skill entities and construct the Skill-Graph by leveraging the classifier trained with extensive contextual features. Finally, we design a personalized question recommendation algorithm based on the Skill-Graph for improving the efficiency and effectiveness of job interview assessment. Extensive experiments on real-world recruitment data clearly validate the effectiveness of DuerQuiz, which had been deployed for generating written exercises in the 2018 Baidu campus recruitment event and received remarkable performances in terms of efficiency and effectiveness for selecting outstanding talents compared with a traditional non-personalized human-only assessment approach.},
	address = {New York, NY, USA},
	author = {Qin, Chuan and Zhu, Hengshu and Zhu, Chen and Xu, Tong and Zhuang, Fuzhen and Ma, Chao and Zhang, Jingshuai and Xiong, Hui},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3292500.3330706},
	isbn = {9781450362016},
	keywords = {intelligence interview system, question recommendation},
	location = {Anchorage, AK, USA},
	numpages = {9},
	pages = {2165--2173},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {DuerQuiz: A Personalized Question Recommender System for Intelligent Job Interview},
	url = {https://doi.org/10.1145/3292500.3330706},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3292500.3330706}}

@inproceedings{10.1145/3292500.3330737,
	abstract = {Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtained data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text snippets from users narratives. Specifically, using users' posts, we trained a binary classifier which predicts a user's transitions from casual drug discussion forums to drug recovery forums. We also proposed a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.},
	address = {New York, NY, USA},
	author = {Lu, John and Sridhar, Sumati and Pandey, Ritika and Hasan, Mohammad Al and Mohler, Georege},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3292500.3330737},
	isbn = {9781450362016},
	keywords = {text mining, reddit forum, drug addiction and recovery, cox regression},
	location = {Anchorage, AK, USA},
	numpages = {9},
	pages = {2367--2375},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {Investigate Transitions into Drug Addiction through Text Mining of Reddit Data},
	url = {https://doi.org/10.1145/3292500.3330737},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3292500.3330737}}

@inproceedings{10.1145/3292500.3330698,
	abstract = {Tags of a Point of Interest (POI) can facilitate location-based services from many aspects like location search and place recommendation. However, many POI tags are often incomplete or imprecise, which may lead to performance degradation of tag-dependent applications. In this paper, we study the POI tag refinement problem which aims to automatically fill in the missing tags as well as correct noisy tags for POIs. We propose a tri-adaptive collaborative learning framework to search for an optimal POI-tag score matrix. The framework integrates three components to collaboratively (i) model the similarity matching between POI and tag, (ii) recover the POI-tag pattern via matrix factorization and (iii) learn to infer the most possible tags by maximum likelihood estimation. We devise an adaptively joint training process to optimize the model and regularize each component simultaneously. And the final refinement results are the consensus of multiple views from different components. We also discuss how to utilize various data sources to construct features for tag refinement, including user profile data, query data on Baidu Maps and basic properties of POIs. Finally, we conduct extensive experiments to demonstrate the effectiveness of our framework. And we further present a case study of the deployment of our framework on Baidu Maps.},
	address = {New York, NY, USA},
	author = {Zhou, Jingbo and Gou, Shan and Hu, Renjun and Zhang, Dongxiang and Xu, Jin and Jiang, Airong and Li, Ying and Xiong, Hui},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3292500.3330698},
	isbn = {9781450362016},
	keywords = {collaborative learning, tag refinement, tag mining, location based service, point of interest},
	location = {Anchorage, AK, USA},
	numpages = {10},
	pages = {1752--1761},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {A Collaborative Learning Framework to Tag Refinement for Points of Interest},
	url = {https://doi.org/10.1145/3292500.3330698},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3292500.3330698}}

@inproceedings{10.1145/3292500.3330924,
	abstract = {Recently, network embedding (NE) has achieved great successes in learning low dimensional representations for network nodes and has been increasingly applied to various network analytic tasks. In this paper, we consider the representation learning problem for content-rich networks whose nodes are associated with rich content information. Content-rich network embedding is challenging in fusing the complex structural dependencies and the rich contents. To tackle the challenges, we propose a generative model, Network-to-Network Network Embedding (Net2Net-NE) model, which can effectively fuse the structure and content information into one continuous embedding vector for each node. Specifically, we regard the content-rich network as a pair of networks with different modalities, i.e., content network and node network. By exploiting the strong correlation between the focal node and the nodes to whom it is connected to, a multilayer recursively composable encoder is proposed to fuse the structure and content information of the entire ego network into the egocentric node embedding. Moreover, a cross-modal decoder is deployed to mapping the egocentric node embeddings into node identities in an interconnected network. By learning the identity of each node according to its content, the mapping from content network to node network is learned in a generative manner. Hence the latent encoding vectors learned by the Net2Net-NE can be used as effective node embeddings. Extensive experimental results on three real-world networks demonstrate the superiority of Net2Net-NE over state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {He, Zhicheng and Liu, Jie and Li, Na and Huang, Yalou},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3292500.3330924},
	isbn = {9781450362016},
	keywords = {network representation learning, egocentric embedding, network embedding, network to network},
	location = {Anchorage, AK, USA},
	numpages = {9},
	pages = {1037--1045},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {Learning Network-to-Network Model for Content-Rich Network Embedding},
	url = {https://doi.org/10.1145/3292500.3330924},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3292500.3330924}}

@inproceedings{10.1145/3292500.3330721,
	abstract = {A large payment network contains millions of merchants and billions of transactions, and the merchants are described in a large number of attributes with incomplete values. Understanding its community structures is crucial to ensure its sustainable and long lasting. Knowing a merchant's community is also important from many applications - risk management, compliance, legal and marketing. To detect communities, an algorithm has to take advances from both attribute and topological information. Further, the method has to be able to handle incomplete and complex attributes. In this paper, we propose a framework named AGGMMR to effectively address the challenges come from scalability, mixed attributes, and incomplete value. We evaluate our proposed framework on four benchmark datasets against five strong baselines. More importantly, we provide a case study of running AGGMMR on a large network from PayPal which contains $100 million$ merchants with $1.5 billion$ transactions. The results demonstrate AGGMMR's effectiveness and practicability.},
	address = {New York, NY, USA},
	author = {Zhe, Chen and Sun, Aixin and Xiao, Xiaokui},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3292500.3330721},
	isbn = {9781450362016},
	keywords = {large attributed network, community detection, complex attributes},
	location = {Anchorage, AK, USA},
	numpages = {9},
	pages = {2041--2049},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {Community Detection on Large Complex Attribute Network},
	url = {https://doi.org/10.1145/3292500.3330721},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3292500.3330721}}

@inproceedings{10.1145/3394486.3403242,
	abstract = {Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.},
	address = {New York, NY, USA},
	author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Zhang, Yu and Zhang, Chao and Han, Jiawei},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3394486.3403242},
	isbn = {9781450379984},
	keywords = {topic hierarchy, tree embedding, text embedding, topic mining},
	location = {Virtual Event, CA, USA},
	numpages = {10},
	pages = {1908--1917},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding},
	url = {https://doi.org/10.1145/3394486.3403242},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3403242}}

@inproceedings{10.1145/3394486.3403244,
	abstract = {Taxonomy is not only a fundamental form of knowledge representation, but also crucial to vast knowledge-rich applications, such as question answering and web search. Most existing taxonomy construction methods extract hypernym-hyponym entity pairs to organize a "universal" taxonomy. However, these generic taxonomies cannot satisfy user's specific interest in certain areas and relations. Moreover, the nature of instance taxonomy treats each node as a single word, which has low semantic coverage for people to fully understand. In this paper, we propose a method for seed-guided topical taxonomy construction, which takes a corpus and a seed taxonomy described by concept names as input, and constructs a more complete taxonomy based on user's interest, wherein each node is represented by a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill this goal. A relation transferring module learns and transfers the user's interested relation along multiple paths to expand the seed taxonomy structure in width and depth. A concept learning module enriches the semantics of each concept node by jointly embedding the taxonomy and text. Comprehensive experiments conducted on real-world datasets show that CoRel generates high-quality topical taxonomies and outperforms all the baselines significantly.},
	address = {New York, NY, USA},
	author = {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Zhang, Yunyi and Han, Jiawei},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3394486.3403244},
	isbn = {9781450379984},
	keywords = {semantic computing, relation extraction, taxonomy construction, topic discovery},
	location = {Virtual Event, CA, USA},
	numpages = {9},
	pages = {1928--1936},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring},
	url = {https://doi.org/10.1145/3394486.3403244},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3403244}}

@inproceedings{10.1145/3394486.3403179,
	abstract = {The competitive relationship of Points of Interest (POIs) refers to the degree of competition between two POIs for business opportunities from third parties in an urban area. Existing studies for competitive analysis usually focus on mining competitive relationships of entities, such as companies or products, from textual data. However, there are few studies which have a focus on competitive analysis for POIs. Indeed, the growing availability of user behavior data about POIs, such as POI reviews and human mobility data, enables a new paradigm for understanding the competitive relationships among POIs. To this end, in this paper, we study how to predict the POI competitive relationship. Along this line, a very first challenge is how to integrate heterogeneous user behavior data with the spatial features of POIs. As a solution, we first build a heterogeneous POI information network (HPIN) from POI reviews and map search data. Then, we develop a graph neural network-based deep learning framework, named DeepR, for POI competitive relationship prediction based on HPIN. Specifically, DeepR contains two components: a spatial adaptive graph neural network (SA-GNN) and a POI pairwise knowledge extraction learning (PKE) model. The SA-GNN is a novel GNN architecture with incorporating POI's spatial information and location distribution by a specially designed spatial oriented aggregation layer and spatial-dependency attentive propagation mechanism. In addition, PKE is devised to distill the POI pairwise knowledge in HPIN being useful for relationship prediction into condensate vectors with relational graph convolution and cross attention. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of our method.},
	address = {New York, NY, USA},
	author = {Li, Shuangli and Zhou, Jingbo and Xu, Tong and Liu, Hao and Lu, Xinjiang and Xiong, Hui},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3394486.3403179},
	isbn = {9781450379984},
	keywords = {graph neural networks, competitive analysis, point of interest, heterogeneous information network},
	location = {Virtual Event, CA, USA},
	numpages = {10},
	pages = {1265--1274},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {Competitive Analysis for Points of Interest},
	url = {https://doi.org/10.1145/3394486.3403179},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3394486.3403179}}

@inproceedings{10.1145/3447548.3467410,
	abstract = {Long story generation (LSG) is one of the coveted goals in natural language processing. Different from most text generation tasks, LSG requires to output a long story of rich content based on a much shorter text input, and often suffers from information sparsity. In this paper, we propose TopNet to alleviate this problem, by leveraging the recent advances in neural topic modeling to obtain high-quality skeleton words to complement the short input. In particular, instead of directly generating a story, we first learn to map the short text input to a low-dimensional topic distribution (which is pre-assigned by a topic model). Based on this latent topic distribution, we can use the reconstruction decoder of the topic model to sample a sequence of inter-related words as a skeleton for the story. Experiments on two benchmark datasets show that our proposed framework is highly effective in skeleton word selection and significantly outperforms the state-of-the-art models in both automatic evaluation and human evaluation.},
	address = {New York, NY, USA},
	author = {Yang, Yazheng and Pan, Boyuan and Cai, Deng and Sun, Huan},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3447548.3467410},
	isbn = {9781450383325},
	keywords = {deep learning, long story generation, topic model, natural language processing, story telling},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {1997--2005},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {TopNet: Learning from Neural Topic Model to Generate Long Stories},
	url = {https://doi.org/10.1145/3447548.3467410},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467410}}

@inproceedings{10.1145/3447548.3467426,
	abstract = {Extreme Multi-label Learning (XML) involves assigning the subset of most relevant labels to a data point from millions of label choices. A hitherto unaddressed challenge in XML is that of predicting unseen labels with no training points. These form a significant fraction of total labels and contain fresh and personalized information desired by end users. Most existing extreme classifiers are not equipped for zero-shot label prediction and hence fail to leverage unseen labels. As a remedy, this paper proposes a novel approach called ZestXML for the task of Generalized Zero-shot XML (GZXML) where relevant labels have to be chosen from all available seen and unseen labels. ZestXML learns to project a data point's features close to the features of its relevant labels through a highly sparsified linear transform. This L0-constrained linear map between the two high-dimensional feature vectors is tractably recovered through a novel optimizer based on Hard Thresholding. By effectively leveraging the sparsities in features, labels and the learnt model, ZestXML achieves higher accuracy and smaller model size than existing XML approaches while also promoting efficient training &amp; prediction, real-time label update as well as explainable prediction.Experiments on large-scale GZXML datasets demonstrated that ZestXML can be up to 14% and 10% more accurate than state-of-the-art extreme classifiers and leading BERT-based dense retrievers respectively, while having 10x smaller model size. ZestXML trains on largest dataset with 31M labels in just 30 hours on a single core of a commodity desktop. When added to an large ensemble of existing models in Bing Sponsored Search Advertising, ZestXML significantly improved click yield of IR based system by 17% and unseen query coverage by 3.4% respectively. ZestXML's source code and benchmark datasets for GZXML will be publically released for research purposes here.},
	address = {New York, NY, USA},
	author = {Gupta, Nilesh and Bohra, Sakina and Prabhu, Yashoteja and Purohit, Saurabh and Varma, Manik},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3447548.3467426},
	isbn = {9781450383325},
	keywords = {label metadata, sponsored search advertising, extreme multi-label classification, zero-shot learning},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {527--535},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {Generalized Zero-Shot Extreme Multi-Label Learning},
	url = {https://doi.org/10.1145/3447548.3467426},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467426}}

@inproceedings{10.1145/3447548.3467390,
	abstract = {What should a malicious user write next to fool a detection model? Identifying malicious users is critical to ensure the safety and integrity of internet platforms. Several deep learning based detection models have been created. However, malicious users can evade deep detection models by manipulating their behavior, rendering these models of little use. The vulnerability of such deep detection models against adversarial attacks is unknown. Here we create a novel adversarial attack model against deep user sequence embedding-based classification models, which use the sequence of user posts to generate user embeddings and detect malicious users. In the attack, the adversary generates a new post to fool the classifier. We propose a novel end-to-end Personalized Text Generation Attack model, called PETGEN, that simultaneously reduces the efficacy of the detection model and generates posts that have several key desirable properties. Specifically, PETGEN generates posts that are personalized to the user's writing style, have knowledge about a given target context, are aware of the user's historical posts on the target context, and encapsulate the user's recent topical interests. We conduct extensive experiments on two real-world datasets (Yelp and Wikipedia, both with ground-truth of malicious users) to show that PETGEN significantly reduces the performance of popular deep user sequence embedding-based classification models. PETGEN outperforms five attack baselines in terms of text quality and attack efficacy in both white-box and black-box classifier settings. Overall, this work paves the path towards the next generation of adversary-aware sequence classification models.},
	address = {New York, NY, USA},
	author = {He, Bing and Ahamad, Mustaque and Kumar, Srijan},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3447548.3467390},
	isbn = {9781450383325},
	keywords = {deep learning, attack, user classification, sequence classification, adversarial text generation},
	location = {Virtual Event, Singapore},
	numpages = {10},
	pages = {575--584},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-Based Classification Models},
	url = {https://doi.org/10.1145/3447548.3467390},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467390}}

@inproceedings{10.1145/3447548.3467302,
	abstract = {Treatment effect estimation from observational data is a critical research topic across many domains. The foremost challenge in treatment effect estimation is how to capture hidden confounders. Recently, the growing availability of networked observational data offers a new opportunity to deal with the issue of hidden confounders. Unlike networked data in traditional graph learning tasks, such as node classification and link detection, the networked data under the causal inference problem has its particularity, i.e., imbalanced network structure. In this paper, we propose a Graph Infomax Adversarial Learning (GIAL) model for treatment effect estimation, which makes full use of the network structure to capture more information by recognizing the imbalance in network structure. We evaluate the performance of our GIAL model on two benchmark datasets, and the results demonstrate superiority over the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Chu, Zhixuan and Rathbun, Stephen L. and Li, Sheng},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3447548.3467302},
	isbn = {9781450383325},
	keywords = {graph mining, social network analysis, causal inference},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {176--184},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data},
	url = {https://doi.org/10.1145/3447548.3467302},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3447548.3467302}}

@inproceedings{10.1145/3534678.3542675,
	abstract = {Electronic health records (EHRs) provide rich clinical information and the opportunities to extract epidemiological patterns to understand and predict patient disease risks with suitable machine learning methods such as topic models. However, existing topic models do not generate identifiable topics each predicting a unique phenotype. One promising direction is to use known phenotype concepts to guide topic inference. We present a seed-guided Bayesian topic model called MixEHR-Seed with 3 contributions: (1) for each phenotype, we infer a dual-form of topic distribution: a seed-topic distribution over a small set of key EHR codes and a regular topic distribution over the entire EHR vocabulary; (2) we model age-dependent disease progression as Markovian dynamic topic priors; (3) we infer seed-guided multi-modal topics over distinct EHR data types. For inference, we developed a variational inference algorithm. Using MixEHR-Seed, we inferred 1569 PheCode-guided phenotype topics from an EHR database in Quebec, Canada covering 1.3 million patients for up to 20-year follow-up with 122 million records for 8539 and 1126 unique diagnostic and drug codes, respectively. We observed (1) accurate phenotype prediction by the guided topics, (2) clinically relevant PheCode-guided disease topics, (3) meaningful age-dependent disease prevalence. Source code is available at GitHub: https://github.com/li-lab-mcgill/MixEHR-Seed.},
	address = {New York, NY, USA},
	author = {Song, Ziyang and Hu, Yuanyi and Verma, Aman and Buckeridge, David L. and Li, Yue},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3534678.3542675},
	isbn = {9781450393850},
	keywords = {variational autoencoder, predictive healthcare, topic modeling, electronic health records},
	location = {Washington DC, USA},
	numpages = {11},
	pages = {4713--4723},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Automatic Phenotyping by a Seed-Guided Topic Model},
	url = {https://doi.org/10.1145/3534678.3542675},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3534678.3542675}}

@inproceedings{10.1145/3534678.3539310,
	abstract = {While Variational Graph Auto-Encoder (VGAE) has presented promising ability to learn representations for documents, most existing VGAE methods do not model a latent topic structure and therefore lack semantic interpretability. Exploring hidden topics within documents and discovering key words associated with each topic allow us to develop a semantic interpretation of the corpus. Moreover, documents are usually associated with authors. For example, news reports have journalists specializing in writing certain type of events, academic papers have authors with expertise in certain research topics, etc. Modeling authorship information could benefit topic modeling, since documents by the same authors tend to reveal similar semantics. This observation also holds for documents published on the same venues. However, most topic models ignore the auxiliary authorship and publication venues. Given above two challenges, we propose a Variational Graph Author Topic Model for documents to integrate both semantic interpretability and authorship and venue modeling into a unified VGAE framework. For authorship and venue modeling, we construct a hierarchical multi-layered document graph with both intra- and cross-layer topic propagation. For semantic interpretability, three word relations (contextual, syntactic, semantic) are modeled and constitute three word sub-layers in the document graph. We further propose three alternatives for variational divergence. Experiments verify the effectiveness of our model on supervised and unsupervised tasks.},
	address = {New York, NY, USA},
	author = {Zhang, Delvin Ce and Lauw, Hady W.},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3534678.3539310},
	isbn = {9781450393850},
	keywords = {author topic modeling, graph neural networks, text mining, variational graph auto-encoder},
	location = {Washington DC, USA},
	numpages = {10},
	pages = {2429--2438},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Variational Graph Author Topic Modeling},
	url = {https://doi.org/10.1145/3534678.3539310},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3534678.3539310}}

@inproceedings{10.1145/3534678.3539107,
	abstract = {Online education, which educates students that cannot be present at school, has become an important supplement to traditional education. Without the direct supervision and instruction of teachers, online education is always concerned with potential distractions and misunderstandings. Learning Style Classification (LSC) is proposed to analyze the learning behavior patterns of online learning users, based on which personalized learning paths are generated to help them learn and maintain their interests.Existing LSC studies rely on expert-labored labeling, which is infeasible in large-scale applications, so we resort to unsupervised classification techniques. However, current unsupervised classification methods are not applicable due to two important challenges: C1) the unawareness of the LSC problem formulation and pedagogy domain knowledge; C2) the absence of any supervision signals. In this paper, we give a formal definition of the unsupervised LSC problem and summarize the domain knowledge into problem-solving heuristics (which addresses C1). A rule-based approach is first designed to provide a tentative solution in a principled manner (which addresses C2). On top of that, a novel Deep Unsupervised Classifier with domain Knowledge (DUCK) is proposed to convert the discovered conclusions and domain knowledge into learnable model components (which addresses both C1 and C2), which significantly improves the effectiveness, efficiency, and robustness. Extensive offline experiments on both public and industrial datasets demonstrate the superiority of our proposed methods. Moreover, the proposed methods are now deployed in the Huawei Education Center, and the ongoing A/B testing results verify the effectiveness of the methods.},
	address = {New York, NY, USA},
	author = {He, Zhicheng and Xia, Wei and Dong, Kai and Guo, Huifeng and Tang, Ruiming and Xia, Dingyin and Zhang, Rui},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	date-added = {2022-10-27 21:30:33 +0200},
	date-modified = {2022-10-27 21:30:33 +0200},
	doi = {10.1145/3534678.3539107},
	isbn = {9781450393850},
	keywords = {user behavior analysis, unsupervised classification, educational data mining, deep clustering, learning style classification},
	location = {Washington DC, USA},
	numpages = {10},
	pages = {2997--3006},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Unsupervised Learning Style Classification for Learning Path Generation in Online Education Platforms},
	url = {https://doi.org/10.1145/3534678.3539107},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3534678.3539107}}

@inproceedings{morales-zhai-2017-identifying,
	abstract = {We study the problem of automatically identifying humorous text from a new kind of text data, i.e., online reviews. We propose a generative language model, based on the theory of incongruity, to model humorous text, which allows us to leverage background text sources, such as Wikipedia entry descriptions, and enables construction of multiple features for identifying humorous reviews. Evaluation of these features using supervised learning for classifying reviews into humorous and non-humorous reviews shows that the features constructed based on the proposed generative model are much more effective than the major features proposed in the existing literature, allowing us to achieve almost 86{\%} accuracy. These humorous review predictions can also supply good indicators for identifying helpful reviews.},
	address = {Copenhagen, Denmark},
	author = {Morales, Alex and Zhai, Chengxiang},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1051},
	month = sep,
	pages = {492--501},
	publisher = {Association for Computational Linguistics},
	title = {Identifying Humor in Reviews using Background Text Sources},
	url = {https://aclanthology.org/D17-1051},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1051},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1051}}

@inproceedings{sterckx-etal-2017-break,
	abstract = {Comprehending lyrics, as found in songs and poems, can pose a challenge to human and machine readers alike. This motivates the need for systems that can understand the ambiguity and jargon found in such creative texts, and provide commentary to aid readers in reaching the correct interpretation. We introduce the task of automated lyric annotation (ALA). Like text simplification, a goal of ALA is to rephrase the original text in a more easily understandable manner. However, in ALA the system must often include additional information to clarify niche terminology and abstract concepts. To stimulate research on this task, we release a large collection of crowdsourced annotations for song lyrics. We analyze the performance of translation and retrieval models on this task, measuring performance with both automated and human evaluation. We find that each model captures a unique type of information important to the task.},
	address = {Copenhagen, Denmark},
	author = {Sterckx, Lucas and Naradowsky, Jason and Byrne, Bill and Demeester, Thomas and Develder, Chris},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1220},
	month = sep,
	pages = {2074--2080},
	publisher = {Association for Computational Linguistics},
	title = {Break it Down for Me: A Study in Automated Lyric Annotation},
	url = {https://aclanthology.org/D17-1220},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1220},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1220}}

@inproceedings{serban-etal-2017-piecewise,
	abstract = {Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.},
	address = {Copenhagen, Denmark},
	author = {Serban, Iulian Vlad and Ororbia, Alexander G. and Pineau, Joelle and Courville, Aaron},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1043},
	month = sep,
	pages = {422--432},
	publisher = {Association for Computational Linguistics},
	title = {Piecewise Latent Variables for Neural Variational Text Processing},
	url = {https://aclanthology.org/D17-1043},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1043},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1043}}

@inproceedings{wang-etal-2017-learning-fine,
	abstract = {User generated categories (UGCs) are short texts that reflect how people describe and organize entities, expressing rich semantic relations implicitly. While most methods on UGC relation extraction are based on pattern matching in English circumstances, learning relations from Chinese UGCs poses different challenges due to the flexibility of expressions. In this paper, we present a weakly supervised learning framework to harvest relations from Chinese UGCs. We identify is-a relations via word embedding based projection and inference, extract non-taxonomic relations and their category patterns by graph mining. We conduct experiments on Chinese Wikipedia and achieve high accuracy, outperforming state-of-the-art methods.},
	address = {Copenhagen, Denmark},
	author = {Wang, Chengyu and Fan, Yan and He, Xiaofeng and Zhou, Aoying},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1273},
	month = sep,
	pages = {2577--2587},
	publisher = {Association for Computational Linguistics},
	title = {Learning Fine-grained Relations from {C}hinese User Generated Categories},
	url = {https://aclanthology.org/D17-1273},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1273},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1273}}

@inproceedings{rahimi-etal-2017-continuous,
	abstract = {We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.},
	address = {Copenhagen, Denmark},
	author = {Rahimi, Afshin and Baldwin, Timothy and Cohn, Trevor},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1016},
	month = sep,
	pages = {167--176},
	publisher = {Association for Computational Linguistics},
	title = {Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks},
	url = {https://aclanthology.org/D17-1016},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1016},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1016}}

@inproceedings{mysore-sathyendra-etal-2017-identifying,
	abstract = {Websites{'} and mobile apps{'} privacy policies, written in natural language, tend to be long and difficult to understand. Information privacy revolves around the fundamental principle of Notice and choice, namely the idea that users should be able to make informed decisions about what information about them can be collected and how it can be used. Internet users want control over their privacy, but their choices are often hidden in long and convoluted privacy policy texts. Moreover, little (if any) prior work has been done to detect the provision of choices in text. We address this challenge of enabling user choice by automatically identifying and extracting pertinent choice language in privacy policies. In particular, we present a two-stage architecture of classification models to identify opt-out choices in privacy policy text, labelling common varieties of choices with a mean F1 score of 0.735. Our techniques enable the creation of systems to help Internet users to learn about their choices, thereby effectuating notice and choice and improving Internet privacy.},
	address = {Copenhagen, Denmark},
	author = {Mysore Sathyendra, Kanthashree and Wilson, Shomir and Schaub, Florian and Zimmeck, Sebastian and Sadeh, Norman},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1294},
	month = sep,
	pages = {2774--2779},
	publisher = {Association for Computational Linguistics},
	title = {Identifying the Provision of Choices in Privacy Policy Text},
	url = {https://aclanthology.org/D17-1294},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1294},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1294}}

@inproceedings{wang-zhang-2017-opinion,
	abstract = {We present opinion recommendation, a novel task of jointly generating a review with a rating score that a certain user would give to a certain product which is unreviewed by the user, given existing reviews to the product by other users, and the reviews that the user has given to other products. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning. We use a single neural network to model users and products, generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp{'}s own ratings. our methods give better results compared to several pipelines baselines.},
	address = {Copenhagen, Denmark},
	author = {Wang, Zhongqing and Zhang, Yue},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1170},
	month = sep,
	pages = {1626--1637},
	publisher = {Association for Computational Linguistics},
	title = {Opinion Recommendation Using A Neural Model},
	url = {https://aclanthology.org/D17-1170},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1170},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1170}}

@inproceedings{gui-etal-2017-question,
	abstract = {Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01{\%} in F-measure.},
	address = {Copenhagen, Denmark},
	author = {Gui, Lin and Hu, Jiannan and He, Yulan and Xu, Ruifeng and Lu, Qin and Du, Jiachen},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1167},
	month = sep,
	pages = {1593--1602},
	publisher = {Association for Computational Linguistics},
	title = {A Question Answering Approach for Emotion Cause Extraction},
	url = {https://aclanthology.org/D17-1167},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1167},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1167}}

@inproceedings{wang-etal-2017-sentiment,
	abstract = {Although many sentiment lexicons in different languages exist, most are not comprehensive. In a recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words in social media. This prompted us to make a new attempt to study sentiment lexicon expansion. This paper first poses the problem as a PU learning problem, which is a new formulation. It then proposes a new PU learning method suitable for our problem using a neural network. The results are enhanced further with a new dictionary-based technique and a novel polarity classification technique. Experimental results show that the proposed approach outperforms baseline methods greatly.},
	address = {Copenhagen, Denmark},
	author = {Wang, Yasheng and Zhang, Yang and Liu, Bing},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1059},
	month = sep,
	pages = {553--563},
	publisher = {Association for Computational Linguistics},
	title = {Sentiment Lexicon Expansion Based on Neural {PU} Learning, Double Dictionary Lookup, and Polarity Association},
	url = {https://aclanthology.org/D17-1059},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1059},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1059}}

@inproceedings{yin-etal-2017-document,
	abstract = {Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper, we model the task as a machine comprehension problem where pseudo question-answer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines. We will release our code and data for the method replicability.},
	address = {Copenhagen, Denmark},
	author = {Yin, Yichun and Song, Yangqiu and Zhang, Ming},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1217},
	month = sep,
	pages = {2044--2054},
	publisher = {Association for Computational Linguistics},
	title = {Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension},
	url = {https://aclanthology.org/D17-1217},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1217},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1217}}

@inproceedings{zhuang-etal-2017-identifying,
	abstract = {A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many applications, such as screening health records for medical mistakes. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a generative model to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135{\%} improvement over baselines in terms of recall at top-1{\%} of the outlier ranking.},
	address = {Copenhagen, Denmark},
	author = {Zhuang, Honglei and Wang, Chi and Tao, Fangbo and Kaplan, Lance and Han, Jiawei},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1291},
	month = sep,
	pages = {2748--2757},
	publisher = {Association for Computational Linguistics},
	title = {Identifying Semantically Deviating Outlier Documents},
	url = {https://aclanthology.org/D17-1291},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1291},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1291}}

@inproceedings{vilares-he-2017-detecting,
	abstract = {We explore how to detect people{'}s perspectives that occupy a certain proposition. We propose a Bayesian modelling approach where topics (or propositions) and their associated perspectives (or viewpoints) are modeled as latent variables. Words associated with topics or perspectives follow different generative routes. Based on the extracted perspectives, we can extract the top associated sentences from text to generate a succinct summary which allows a quick glimpse of the main viewpoints in a document. The model is evaluated on debates from the House of Commons of the UK Parliament, revealing perspectives from the debates without the use of labelled data and obtaining better results than previous related solutions under a variety of evaluations.},
	address = {Copenhagen, Denmark},
	author = {Vilares, David and He, Yulan},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1165},
	month = sep,
	pages = {1573--1582},
	publisher = {Association for Computational Linguistics},
	title = {Detecting Perspectives in Political Debates},
	url = {https://aclanthology.org/D17-1165},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1165},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1165}}

@inproceedings{mekala-etal-2017-scdv,
	abstract = {We present a feature vector formation technique for documents - Sparse Composite Document Vector (SCDV) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outperform the previous state-of-the-art method, NTSG. We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval. Moreover, we achieve a significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds - better performance with lower time and space complexity.},
	address = {Copenhagen, Denmark},
	author = {Mekala, Dheeraj and Gupta, Vivek and Paranjape, Bhargavi and Karnick, Harish},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1069},
	month = sep,
	pages = {659--669},
	publisher = {Association for Computational Linguistics},
	title = {{SCDV} : Sparse Composite Document Vectors using soft clustering over distributional representations},
	url = {https://aclanthology.org/D17-1069},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1069},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1069}}

@inproceedings{yang-etal-2017-identifying,
	abstract = {We study the problem of identifying the topics and sentiments and tracking their shifts from social media texts in different geographical regions during emergencies and disasters. We propose a location-based dynamic sentiment-topic model (LDST) which can jointly model topic, sentiment, time and Geolocation information. The experimental results demonstrate that LDST performs very well at discovering topics and sentiments from social media and tracking their shifts in different geographical regions during emergencies and disasters. We will release the data and source code after this work is published.},
	address = {Copenhagen, Denmark},
	author = {Yang, Min and Mei, Jincheng and Ji, Heng and Zhao, Wei and Zhao, Zhou and Chen, Xiaojun},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1055},
	month = sep,
	pages = {527--533},
	publisher = {Association for Computational Linguistics},
	title = {Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters},
	url = {https://aclanthology.org/D17-1055},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1055},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1055}}

@inproceedings{yang-etal-2017-adapting,
	abstract = {Models work best when they are optimized taking into account the evaluation criteria that people care about. For topic models, people often care about interpretability, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting extrinsic performance.},
	address = {Copenhagen, Denmark},
	author = {Yang, Weiwei and Boyd-Graber, Jordan and Resnik, Philip},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D17-1203},
	month = sep,
	pages = {1901--1906},
	publisher = {Association for Computational Linguistics},
	title = {Adapting Topic Models using Lexical Associations with Tree Priors},
	url = {https://aclanthology.org/D17-1203},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/D17-1203},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D17-1203}}

@inproceedings{wang-jurgens-2018-going,
	abstract = {People use online platforms to seek out support for their informational and emotional needs. Here, we ask what effect does revealing one{'}s gender have on receiving support. To answer this, we create (i) a new dataset and method for identifying supportive replies and (ii) new methods for inferring gender from text and name. We apply these methods to create a new massive corpus of 102M online interactions with gender-labeled users, each rated by degree of supportiveness. Our analysis shows wide-spread and consistent disparity in support: identifying as a woman is associated with higher rates of support - but also higher rates of disparagement.},
	address = {Brussels, Belgium},
	author = {Wang, Zijian and Jurgens, David},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1004},
	month = oct # {-} # nov,
	pages = {33--45},
	publisher = {Association for Computational Linguistics},
	title = {It{'}s going to be okay: Measuring Access to Support in Online Communities},
	url = {https://aclanthology.org/D18-1004},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1004},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1004}}

@inproceedings{chen-etal-2018-iterative,
	abstract = {In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.},
	address = {Brussels, Belgium},
	author = {Chen, Xiuying and Gao, Shen and Tao, Chongyang and Song, Yan and Zhao, Dongyan and Yan, Rui},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1442},
	month = oct # {-} # nov,
	pages = {4088--4097},
	publisher = {Association for Computational Linguistics},
	title = {Iterative Document Representation Learning Towards Summarization with Polishing},
	url = {https://aclanthology.org/D18-1442},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1442},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1442}}

@inproceedings{baheti-etal-2018-generating,
	abstract = {Neural conversation models tend to generate safe, generic responses for most inputs. This is due to the limitations of likelihood-based decoding objectives in generation tasks with diverse outputs, such as conversation. To address this challenge, we propose a simple yet effective approach for incorporating side information in the form of distributional constraints over the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics (Griffiths et al., 2005) and semantic similarity (Arora et al., 2016). We evaluate our approach against a variety of competitive baselines, using both automatic metrics and human judgments, showing that our proposed approach generates responses that are much less generic without sacrificing plausibility. A working demo of our code can be found at \url{https://github.com/abaheti95/DC-NeuralConversation}.},
	address = {Brussels, Belgium},
	author = {Baheti, Ashutosh and Ritter, Alan and Li, Jiwei and Dolan, Bill},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1431},
	month = oct # {-} # nov,
	pages = {3970--3980},
	publisher = {Association for Computational Linguistics},
	title = {Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints},
	url = {https://aclanthology.org/D18-1431},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1431},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1431}}

@inproceedings{qi-etal-2018-cross,
	abstract = {Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many NLP tasks. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict sememes for words in other languages. We propose a novel framework to model correlations between sememes and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at \url{https://github.com/thunlp/CL-SP}.},
	address = {Brussels, Belgium},
	author = {Qi, Fanchao and Lin, Yankai and Sun, Maosong and Zhu, Hao and Xie, Ruobing and Liu, Zhiyuan},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1033},
	month = oct # {-} # nov,
	pages = {358--368},
	publisher = {Association for Computational Linguistics},
	title = {Cross-lingual Lexical Sememe Prediction},
	url = {https://aclanthology.org/D18-1033},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1033},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1033}}

@inproceedings{luo-etal-2018-extra,
	abstract = {Many existing systems for analyzing and summarizing customer reviews about products or service are based on a number of prominent review aspects. Conventionally, the prominent review aspects of a product type are determined manually. This costly approach cannot scale to large and cross-domain services such as Amazon.com, Taobao.com or Yelp.com where there are a large number of product types and new products emerge almost every day. In this paper, we propose a novel framework, for extracting the most prominent aspects of a given product type from textual reviews. The proposed framework, ExtRA, extracts K most prominent aspect terms or phrases which do not overlap semantically automatically without supervision. Extensive experiments show that ExtRA is effective and achieves the state-of-the-art performance on a dataset consisting of different product types.},
	address = {Brussels, Belgium},
	author = {Luo, Zhiyi and Huang, Shanshan and Xu, Frank F. and Lin, Bill Yuchen and Shi, Hanyuan and Zhu, Kenny},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1384},
	month = oct # {-} # nov,
	pages = {3477--3486},
	publisher = {Association for Computational Linguistics},
	title = {{E}xt{RA}: Extracting Prominent Review Aspects from Customer Feedback},
	url = {https://aclanthology.org/D18-1384},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1384},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1384}}

@inproceedings{liu-etal-2018-automatic,
	abstract = {Identifying the salience (i.e. importance) of discourse units is an important task in language understanding. While events play important roles in text documents, little research exists on analyzing their saliency status. This paper empirically studies Event Salience and proposes two salience detection models based on discourse relations. The first is a feature based salience model that incorporates cohesion among discourse units. The second is a neural model that captures more complex interactions between discourse units. In our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).},
	address = {Brussels, Belgium},
	author = {Liu, Zhengzhong and Xiong, Chenyan and Mitamura, Teruko and Hovy, Eduard},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1154},
	month = oct # {-} # nov,
	pages = {1226--1236},
	publisher = {Association for Computational Linguistics},
	title = {Automatic Event Salience Identification},
	url = {https://aclanthology.org/D18-1154},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1154},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1154}}

@inproceedings{srivatsan-etal-2018-modeling,
	abstract = {In this paper, we propose a deep, globally normalized topic model that incorporates structural relationships connecting documents in socially generated corpora, such as online forums. Our model (1) captures discursive interactions along observed reply links in addition to traditional topic information, and (2) incorporates latent distributed representations arranged in a deep architecture, which enables a GPU-based mean-field inference procedure that scales efficiently to large data. We apply our model to a new social media dataset consisting of 13M comments mined from the popular internet forum Reddit, a domain that poses significant challenges to models that do not account for relationships connecting user comments. We evaluate against existing methods across multiple metrics including perplexity and metadata prediction, and qualitatively analyze the learned interaction patterns.},
	address = {Brussels, Belgium},
	author = {Srivatsan, Nikita and Wojtowicz, Zachary and Berg-Kirkpatrick, Taylor},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1496},
	month = oct # {-} # nov,
	pages = {4673--4682},
	publisher = {Association for Computational Linguistics},
	title = {Modeling Online Discourse with Coupled Distributed Topics},
	url = {https://aclanthology.org/D18-1496},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1496},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1496}}

@inproceedings{yang-etal-2018-interpretable,
	abstract = {Text might express or evoke multiple emotions with varying intensities. As such, it is crucial to predict and rank multiple relevant emotions by their intensities. Moreover, as emotions might be evoked by hidden topics, it is important to unveil and incorporate such topical information to understand how the emotions are evoked. We proposed a novel interpretable neural network approach for relevant emotion ranking. Specifically, motivated by transfer learning, the neural network is initialized to make the hidden layer approximate the behavior of topic models. Moreover, a novel error function is defined to optimize the whole neural network for relevant emotion ranking. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label learning methods. Moreover, the extracted emotion-associated topic words indeed represent emotion-evoking events and are in line with our common-sense knowledge.},
	address = {Brussels, Belgium},
	author = {Yang, Yang and Zhou, Deyu and He, Yulan},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1379},
	month = oct # {-} # nov,
	pages = {3423--3432},
	publisher = {Association for Computational Linguistics},
	title = {An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking},
	url = {https://aclanthology.org/D18-1379},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1379},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1379}}

@inproceedings{lund-etal-2018-labeled,
	abstract = {We propose Labeled Anchors, an interactive and supervised topic model based on the anchor words algorithm (Arora et al., 2013). Labeled Anchors is similar to Supervised Anchors (Nguyen et al., 2014) in that it extends the vector-space representation of words to include document labels. However, our formulation also admits a classifier which requires no training beyond inferring topics, which means our approach is also fast enough to be interactive. We run a small user study that demonstrates that untrained users can interactively update topics in order to improve classification accuracy.},
	address = {Brussels, Belgium},
	author = {Lund, Jeffrey and Cowley, Stephen and Fearn, Wilson and Hales, Emily and Seppi, Kevin},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1095},
	month = oct # {-} # nov,
	pages = {824--829},
	publisher = {Association for Computational Linguistics},
	title = {Labeled Anchors and a Scalable, Transparent, and Interactive Classifier},
	url = {https://aclanthology.org/D18-1095},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1095},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1095}}

@inproceedings{zeng-etal-2018-topic,
	abstract = {Many classification models work poorly on short texts due to data sparsity. To address this issue, we propose topic memory networks for short text classification with a novel topic memory mechanism to encode latent topic representations indicative of class labels. Different from most prior work that focuses on extending features with external knowledge or pre-trained topics, our model jointly explores topic inference and text classification with memory networks in an end-to-end manner. Experimental results on four benchmark datasets show that our model outperforms state-of-the-art models on short text classification, meanwhile generates coherent topics.},
	address = {Brussels, Belgium},
	author = {Zeng, Jichuan and Li, Jing and Song, Yan and Gao, Cuiyun and Lyu, Michael R. and King, Irwin},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1351},
	month = oct # {-} # nov,
	pages = {3120--3131},
	publisher = {Association for Computational Linguistics},
	title = {Topic Memory Networks for Short Text Classification},
	url = {https://aclanthology.org/D18-1351},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1351},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1351}}

@inproceedings{huang-etal-2018-siamese,
	abstract = {Label-specific topics can be widely used for supporting personality psychology, aspect-level sentiment analysis, and cross-domain sentiment classification. To generate label-specific topics, several supervised topic models which adopt likelihood-driven objective functions have been proposed. However, it is hard for them to get a precise estimation on both topic discovery and supervised learning. In this study, we propose a supervised topic model based on the Siamese network, which can trade off label-specific word distributions with document-specific label distributions in a uniform framework. Experiments on real-world datasets validate that our model performs competitive in topic discovery quantitatively and qualitatively. Furthermore, the proposed model can effectively predict categorical or real-valued labels for new documents by generating word embeddings from a label-specific topical space.},
	address = {Brussels, Belgium},
	author = {Huang, Minghui and Rao, Yanghui and Liu, Yuwei and Xie, Haoran and Wang, Fu Lee},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D18-1494},
	month = oct # {-} # nov,
	pages = {4652--4662},
	publisher = {Association for Computational Linguistics},
	title = {{S}iamese Network-Based Supervised Topic Modeling},
	url = {https://aclanthology.org/D18-1494},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1494},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1494}}

@inproceedings{desai-etal-2019-adaptive,
	abstract = {Insightful findings in political science often require researchers to analyze documents of a certain subject or type, yet these documents are usually contained in large corpora that do not distinguish between pertinent and non-pertinent documents. In contrast, we can find corpora that label relevant documents but have limitations (e.g., from a single source or era), preventing their use for political science research. To bridge this gap, we present adaptive ensembling, an unsupervised domain adaptation framework, equipped with a novel text classification model and time-aware training to ensure our methods work well with diachronic corpora. Experiments on an expert-annotated dataset show that our framework outperforms strong benchmarks. Further analysis indicates that our methods are more stable, learn better representations, and extract cleaner corpora for fine-grained analysis.},
	address = {Hong Kong, China},
	author = {Desai, Shrey and Sinno, Barea and Rosenfeld, Alex and Li, Junyi Jessy},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1478},
	month = nov,
	pages = {4718--4730},
	publisher = {Association for Computational Linguistics},
	title = {Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis},
	url = {https://aclanthology.org/D19-1478},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1478},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1478}}

@inproceedings{tang-etal-2019-topic,
	abstract = {Text generation is among the most fundamental tasks in natural language processing. In this paper, we propose a text generation model that learns semantics and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on Gaussian distribution to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the model further adapts the encoder of the topic modeling component for a discriminator. The results of experiments over several datasets demonstrate that our model outperforms several states of the art models in terms of text perplexity and topic coherence. Moreover, the latent representations learned by our model is superior to others in a text classification task. Finally, given the input texts, our model can generate meaningful texts which hold similar structures but under different topics.},
	address = {Hong Kong, China},
	author = {Tang, Hongyin and Li, Miao and Jin, Beihong},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1513},
	month = nov,
	pages = {5090--5099},
	publisher = {Association for Computational Linguistics},
	title = {A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features},
	url = {https://aclanthology.org/D19-1513},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1513},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1513}}

@inproceedings{yin-etal-2019-benchmarking,
	abstract = {Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the {``}topic{''} aspect includes {``}sports{''} and {``}politics{''} as labels; the {``}emotion{''} aspect includes {``}joy{''} and {``}anger{''}; the {``}situation{''} aspect includes {``}medical assistance{''} and {``}water shortage{''}. ii) We extend the existing evaluation setup (label-partially-unseen) {--} given a dataset, train on some labels, test on all labels {--} to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way.},
	address = {Hong Kong, China},
	author = {Yin, Wenpeng and Hay, Jamaal and Roth, Dan},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1404},
	month = nov,
	pages = {3914--3923},
	publisher = {Association for Computational Linguistics},
	title = {Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach},
	url = {https://aclanthology.org/D19-1404},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1404},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1404}}

@inproceedings{zhang-singh-2019-leveraging,
	abstract = {Opinionated text often involves attributes such as authorship and location that influence the sentiments expressed for different aspects. We posit that structural and semantic correspondence is both prevalent in opinionated text, especially when associated with attributes, and crucial in accurately revealing its latent aspect and sentiment structure. However, it is not recognized by existing approaches. We propose Trait, an unsupervised probabilistic model that discovers aspects and sentiments from text and associates them with different attributes. To this end, Trait infers and leverages structural and semantic correspondence using a Markov Random Field. We show empirically that by incorporating attributes explicitly Trait significantly outperforms state-of-the-art baselines both by generating attribute profiles that accord with our intuitions, as shown via visualization, and yielding topics of greater semantic cohesion.},
	address = {Hong Kong, China},
	author = {Zhang, Zhe and Singh, Munindar},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1555},
	month = nov,
	pages = {5528--5538},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery},
	url = {https://aclanthology.org/D19-1555},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1555},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1555}}

@inproceedings{chaudhary-etal-2019-little,
	abstract = {Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now many proposed solutions to this problem involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. In this paper, we ask the question: given this recent progress, and some amount of human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we settle on a recipe of starting with a cross-lingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data.},
	address = {Hong Kong, China},
	author = {Chaudhary, Aditi and Xie, Jiateng and Sheikh, Zaid and Neubig, Graham and Carbonell, Jaime},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1520},
	month = nov,
	pages = {5164--5174},
	publisher = {Association for Computational Linguistics},
	title = {A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers},
	url = {https://aclanthology.org/D19-1520},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1520},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1520}}

@inproceedings{liao-etal-2019-coupling,
	abstract = {Aspect words, indicating opinion targets, are essential in expressing and understanding human opinions. To identify aspects, most previous efforts focus on using sequence tagging models trained on human-annotated data. This work studies unsupervised aspect extraction and explores how words appear in global context (on sentence level) and local context (conveyed by neighboring words). We propose a novel neural model, capable of coupling global and local representation to discover aspect words. Experimental results on two benchmarks, laptop and restaurant reviews, show that our model significantly outperforms the state-of-the-art models from previous studies evaluated with varying metrics. Analysis on model output show our ability to learn meaningful and coherent aspect representations. We further investigate how words distribute in global and local context, and find that aspect and non-aspect words do exhibit different context, interpreting our superiority in unsupervised aspect extraction.},
	address = {Hong Kong, China},
	author = {Liao, Ming and Li, Jing and Zhang, Haisong and Wang, Lingzhi and Wu, Xixin and Wong, Kam-Fai},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1465},
	month = nov,
	pages = {4579--4589},
	publisher = {Association for Computational Linguistics},
	title = {Coupling Global and Local Context for Unsupervised Aspect Extraction},
	url = {https://aclanthology.org/D19-1465},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1465},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1465}}

@inproceedings{karamanolakis-etal-2019-leveraging,
	abstract = {User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches fail to leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.},
	address = {Hong Kong, China},
	author = {Karamanolakis, Giannis and Hsu, Daniel and Gravano, Luis},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1468},
	month = nov,
	pages = {4611--4621},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training},
	url = {https://aclanthology.org/D19-1468},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1468},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1468}}

@inproceedings{linmei-etal-2019-heterogeneous,
	abstract = {Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.},
	address = {Hong Kong, China},
	author = {Linmei, Hu and Yang, Tianchi and Shi, Chuan and Ji, Houye and Li, Xiaoli},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1488},
	month = nov,
	pages = {4821--4830},
	publisher = {Association for Computational Linguistics},
	title = {Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification},
	url = {https://aclanthology.org/D19-1488},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1488},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1488}}

@inproceedings{caragea-etal-2019-myth,
	abstract = {The review and selection process for scientific paper publication is essential for the quality of scholarly publications in a scientific field. The double-blind review system, which enforces author anonymity during the review period, is widely used by prestigious conferences and journals to ensure the integrity of this process. Although the notion of anonymity in the double-blind review has been questioned before, the availability of full text paper collections brings new opportunities for exploring the question: Is the double-blind review process really double-blind? We study this question on the ACL and EMNLP paper collections and present an analysis on how well deep learning techniques can infer the authors of a paper. Specifically, we explore Convolutional Neural Networks trained on various aspects of a paper, e.g., content, style features, and references, to understand the extent to which we can infer the authors of a paper and what aspects contribute the most. Our results show that the authors of a paper can be inferred with accuracy as high as 87{\%} on ACL and 78{\%} on EMNLP for the top 100 most prolific authors.},
	address = {Hong Kong, China},
	author = {Caragea, Cornelia and Uban, Ana and Dinu, Liviu P.},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1236},
	month = nov,
	pages = {2317--2327},
	publisher = {Association for Computational Linguistics},
	title = {The Myth of Double-Blind Review Revisited: {ACL} vs. {EMNLP}},
	url = {https://aclanthology.org/D19-1236},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1236},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1236}}

@inproceedings{ruckle-etal-2019-neural,
	abstract = {Supervised training of neural models to duplicate question detection in community Question Answering (CQA) requires large amounts of labeled question pairs, which can be costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methods{---}weak supervision using the title and body of a question, and the automatic generation of duplicate questions{---}and show that both can achieve improved performances even though they do not require any labeled data. We provide a comparison of popular training strategies and show that our proposed approaches are more effective in many cases because they can utilize larger amounts of data from the CQA forums. Finally, we show that weak supervision with question title and body information is also an effective method to train CQA answer selection models without direct answer supervision.},
	address = {Hong Kong, China},
	author = {R{\"u}ckl{\'e}, Andreas and Moosavi, Nafise Sadat and Gurevych, Iryna},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1171},
	month = nov,
	pages = {1607--1617},
	publisher = {Association for Computational Linguistics},
	title = {Neural Duplicate Question Detection without Labeled Training Data},
	url = {https://aclanthology.org/D19-1171},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1171},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1171}}

@inproceedings{pethe-skiena-2019-trumpiest,
	abstract = {The sequence of documents produced by any given author varies in style and content, but some documents are more typical or representative of the source than others. We quantify the extent to which a given short text is characteristic of a specific person, using a dataset of tweets from fifteen celebrities. Such analysis is useful for generating excerpts of high-volume Twitter profiles, and understanding how representativeness relates to tweet popularity. We first consider the related task of binary author detection (is x the author of text T?), and report a test accuracy of 90.37{\%} for the best of five approaches to this problem. We then use these models to compute characterization scores among all of an author{'}s texts. A user study shows human evaluators agree with our characterization model for all 15 celebrities in our dataset, each with p-value {\textless} 0.05. We use these classifiers to show surprisingly strong correlations between characterization scores and the popularity of the associated texts. Indeed, we demonstrate a statistically significant correlation between this score and tweet popularity (likes/replies/retweets) for 13 of the 15 celebrities in our study.},
	address = {Hong Kong, China},
	author = {Pethe, Charuta and Skiena, Steve},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1175},
	month = nov,
	pages = {1653--1663},
	publisher = {Association for Computational Linguistics},
	title = {The Trumpiest Trump? Identifying a Subject{'}s Most Characteristic Tweets},
	url = {https://aclanthology.org/D19-1175},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1175},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1175}}

@inproceedings{balashankar-etal-2019-identifying,
	abstract = {We propose a new framework to uncover the relationship between news events and real world phenomena. We present the Predictive Causal Graph (PCG) which allows to detect latent relationships between events mentioned in news streams. This graph is constructed by measuring how the occurrence of a word in the news influences the occurrence of another (set of) word(s) in the future. We show that PCG can be used to extract latent features from news streams, outperforming other graph-based methods in prediction error of 10 stock price time series for 12 months. We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5{\%} and 5{\%} for about 4 years. We then manually validated PCG, finding that 67{\%} of the causation semantic frame arguments present in the news corpus were directly connected in the PCG, the remaining being connected through a semantically relevant intermediate node.},
	address = {Hong Kong, China},
	author = {Balashankar, Ananth and Chakraborty, Sunandan and Fraiberger, Samuel and Subramanian, Lakshminarayanan},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1238},
	month = nov,
	pages = {2338--2348},
	publisher = {Association for Computational Linguistics},
	title = {Identifying Predictive Causal Factors from News Streams},
	url = {https://aclanthology.org/D19-1238},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1238},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1238}}

@inproceedings{cao-etal-2019-latent,
	abstract = {Despite detection of suicidal ideation on social media has made great progress in recent years, people{'}s implicitly and anti-real contrarily expressed posts still remain as an obstacle, constraining the detectors to acquire higher satisfactory performance. Enlightened by the hidden {``}tree holes{''} phenomenon on microblog, where people at suicide risk tend to disclose their inner real feelings and thoughts to the microblog space whose authors have committed suicide, we explore the use of tree holes to enhance microblog-based suicide risk detection from the following two perspectives. (1) We build suicide-oriented word embeddings based on tree hole contents to strength the sensibility of suicide-related lexicons and context based on tree hole contents. (2) A two-layered attention mechanism is deployed to grasp intermittently changing points from individual{'}s open blog streams, revealing one{'}s inner emotional world more or less. Our experimental results show that with suicide-oriented word embeddings and attention, microblog-based suicide risk detection can achieve over 91{\%} accuracy. A large-scale well-labelled suicide data set is also reported in the paper.},
	address = {Hong Kong, China},
	author = {Cao, Lei and Zhang, Huijun and Feng, Ling and Wei, Zihan and Wang, Xin and Li, Ningyun and He, Xiaohao},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/D19-1181},
	month = nov,
	pages = {1718--1728},
	publisher = {Association for Computational Linguistics},
	title = {Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention},
	url = {https://aclanthology.org/D19-1181},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1181},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1181}}

@inproceedings{zhou-jurgens-2020-condolence,
	abstract = {Offering condolence is a natural reaction to hearing someone{'}s distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal{---}trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.},
	address = {Online},
	author = {Zhou, Naitian and Jurgens, David},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.45},
	month = nov,
	pages = {609--626},
	publisher = {Association for Computational Linguistics},
	title = {Condolence and Empathy in Online Communities},
	url = {https://aclanthology.org/2020.emnlp-main.45},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.45},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.45}}

@inproceedings{sawhney-etal-2020-time,
	abstract = {Social media{'}s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user{'}s historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.},
	address = {Online},
	author = {Sawhney, Ramit and Joshi, Harshit and Gandhi, Saumya and Shah, Rajiv Ratn},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.619},
	month = nov,
	pages = {7685--7697},
	publisher = {Association for Computational Linguistics},
	title = {A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media},
	url = {https://aclanthology.org/2020.emnlp-main.619},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.619},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.619}}

@inproceedings{pei-jurgens-2020-quantifying,
	abstract = {Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.},
	address = {Online},
	author = {Pei, Jiaxin and Jurgens, David},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.428},
	month = nov,
	pages = {5307--5326},
	publisher = {Association for Computational Linguistics},
	title = {Quantifying Intimacy in Language},
	url = {https://aclanthology.org/2020.emnlp-main.428},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.428},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.428}}

@inproceedings{field-tsvetkov-2020-unsupervised,
	abstract = {Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.},
	address = {Online},
	author = {Field, Anjalie and Tsvetkov, Yulia},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.44},
	month = nov,
	pages = {596--608},
	publisher = {Association for Computational Linguistics},
	title = {Unsupervised Discovery of Implicit Gender Bias},
	url = {https://aclanthology.org/2020.emnlp-main.44},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.44},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.44}}

@inproceedings{wang-etal-2020-continuity,
	abstract = {Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn{'}s existing contents. Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.},
	address = {Online},
	author = {Wang, Lingzhi and Li, Jing and Zeng, Xingshan and Zhang, Haisong and Wong, Kam-Fai},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.538},
	month = nov,
	pages = {6640--6650},
	publisher = {Association for Computational Linguistics},
	title = {Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations},
	url = {https://aclanthology.org/2020.emnlp-main.538},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.538},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.538}}

@inproceedings{roy-goldwasser-2020-weakly,
	abstract = {In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.},
	address = {Online},
	author = {Roy, Shamik and Goldwasser, Dan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.620},
	month = nov,
	pages = {7698--7716},
	publisher = {Association for Computational Linguistics},
	title = {Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media},
	url = {https://aclanthology.org/2020.emnlp-main.620},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.620},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.620}}

@inproceedings{zhao-chang-2020-logan,
	abstract = {Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.},
	address = {Online},
	author = {Zhao, Jieyu and Chang, Kai-Wei},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.155},
	month = nov,
	pages = {1968--1977},
	publisher = {Association for Computational Linguistics},
	title = {{LOGAN}: Local Group Bias Detection by Clustering},
	url = {https://aclanthology.org/2020.emnlp-main.155},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.155},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.155}}

@inproceedings{bommasani-cardie-2020-intrinsic,
	abstract = {High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.},
	address = {Online},
	author = {Bommasani, Rishi and Cardie, Claire},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.649},
	month = nov,
	pages = {8075--8096},
	publisher = {Association for Computational Linguistics},
	title = {Intrinsic Evaluation of Summarization Datasets},
	url = {https://aclanthology.org/2020.emnlp-main.649},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.649},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.649}}

@inproceedings{hoyle-etal-2020-improving,
	abstract = {Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.},
	address = {Online},
	author = {Hoyle, Alexander Miserlis and Goel, Pranav and Resnik, Philip},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.137},
	month = nov,
	pages = {1752--1771},
	publisher = {Association for Computational Linguistics},
	title = {{I}mproving {N}eural {T}opic {M}odels using {K}nowledge {D}istillation},
	url = {https://aclanthology.org/2020.emnlp-main.137},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.137},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.137}}

@inproceedings{meng-etal-2020-text,
	abstract = {Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90{\%} accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.},
	address = {Online},
	author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Xiong, Chenyan and Ji, Heng and Zhang, Chao and Han, Jiawei},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.724},
	month = nov,
	pages = {9006--9017},
	publisher = {Association for Computational Linguistics},
	title = {Text Classification Using Label Names Only: A Language Model Self-Training Approach},
	url = {https://aclanthology.org/2020.emnlp-main.724},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.724},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.724}}

@inproceedings{hu-etal-2020-neural,
	abstract = {Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.},
	address = {Online},
	author = {Hu, Xuemeng and Wang, Rui and Zhou, Deyu and Xiong, Yuxuan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.725},
	month = nov,
	pages = {9018--9030},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Modeling with Cycle-Consistent Adversarial Training},
	url = {https://aclanthology.org/2020.emnlp-main.725},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.725},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.725}}

@inproceedings{ousidhoum-etal-2020-comparative,
	abstract = {Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.},
	address = {Online},
	author = {Ousidhoum, Nedjma and Song, Yangqiu and Yeung, Dit-Yan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.199},
	month = nov,
	pages = {2532--2542},
	publisher = {Association for Computational Linguistics},
	title = {Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets},
	url = {https://aclanthology.org/2020.emnlp-main.199},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.199}}

@inproceedings{liu-etal-2020-cross-lingual-spoken,
	abstract = {Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3{\%} of the target language training data, achieves comparable performance to the supervised training with all the training data.},
	address = {Online},
	author = {Liu, Zihan and Winata, Genta Indra and Xu, Peng and Lin, Zhaojiang and Fung, Pascale},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.587},
	month = nov,
	pages = {7241--7251},
	publisher = {Association for Computational Linguistics},
	title = {Cross-lingual Spoken Language Understanding with Regularized Representation Alignment},
	url = {https://aclanthology.org/2020.emnlp-main.587},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.587},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.587}}

@inproceedings{gao-gormley-2020-training,
	abstract = {Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.},
	address = {Online},
	author = {Gao, Sida and Gormley, Matthew R.},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.406},
	month = nov,
	pages = {4999--5011},
	publisher = {Association for Computational Linguistics},
	title = {Training for {G}ibbs Sampling on Conditional Random Fields with Neural Scoring Factors},
	url = {https://aclanthology.org/2020.emnlp-main.406},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.406},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.406}}

@inproceedings{tian-etal-2020-learning,
	abstract = {The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.},
	address = {Online},
	author = {Tian, Runzhi and Mao, Yongyi and Zhang, Richong},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.101},
	month = nov,
	pages = {1315--1325},
	publisher = {Association for Computational Linguistics},
	title = {Learning {VAE}-{LDA} Models with Rounded Reparameterization Trick},
	url = {https://aclanthology.org/2020.emnlp-main.101},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.101},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.101}}

@inproceedings{spell-etal-2020-embedding,
	abstract = {Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators{'} political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators{'} attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.},
	address = {Online},
	author = {Spell, Gregory and Guay, Brian and Hillygus, Sunshine and Carin, Lawrence},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2020.emnlp-main.46},
	month = nov,
	pages = {627--641},
	publisher = {Association for Computational Linguistics},
	title = {An {E}mbedding {M}odel for {E}stimating {L}egislative {P}references from the {F}requency and {S}entiment of {T}weets},
	url = {https://aclanthology.org/2020.emnlp-main.46},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.46},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.46}}

@inproceedings{jin-etal-2021-neural,
	abstract = {Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Jin, Yuan and Zhao, He and Liu, Ming and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.80},
	month = nov,
	pages = {1042--1052},
	publisher = {Association for Computational Linguistics},
	title = {Neural Attention-Aware Hierarchical Topic Model},
	url = {https://aclanthology.org/2021.emnlp-main.80},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.80},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.80}}

@inproceedings{ye-etal-2021-beyond,
	abstract = {Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels{'} hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Ye, Chenchen and Zhang, Linhai and He, Yulan and Zhou, Deyu and Wu, Jie},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.253},
	month = nov,
	pages = {3162--3171},
	publisher = {Association for Computational Linguistics},
	title = {Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs},
	url = {https://aclanthology.org/2021.emnlp-main.253},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.253},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.253}}

@inproceedings{zhang-etal-2021-howyoutagtweets,
	abstract = {Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user{'}s hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized that one{'}s interests in a hashtag are related with what they said before (user history) and the existing posts present the hashtag (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via multitask learning. In this way, user interests learned from the past can be customized to match future hashtags, which is beyond the capability of existing methods assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our model significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Zhang, Yuji and Zhang, Yubo and Xu, Chunpu and Li, Jing and Jiang, Ziyan and Peng, Baolin},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.616},
	month = nov,
	pages = {7811--7820},
	publisher = {Association for Computational Linguistics},
	title = {{\#}{H}ow{Y}ou{T}ag{T}weets: Learning User Hashtagging Preferences via Personalized Topic Attention},
	url = {https://aclanthology.org/2021.emnlp-main.616},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.616},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.616}}

@inproceedings{wang-etal-2021-phrase,
	abstract = {Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Wang, Shufan and Thompson, Laure and Iyyer, Mohit},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.846},
	month = nov,
	pages = {10837--10851},
	publisher = {Association for Computational Linguistics},
	title = {Phrase-{BERT}: Improved Phrase Embeddings from {BERT} with an Application to Corpus Exploration},
	url = {https://aclanthology.org/2021.emnlp-main.846},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.846},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.846}}

@inproceedings{kang-etal-2021-leveraging,
	abstract = {Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to address the inter-dependency among tags when they are ranked, the autoregressive approach fails to take orderlessness into account because it is designed to utilize sequential relations among tokens. We propose a sequence-oblivious generation method for tag recommendation, in which the next tag to be generated is independent of the order of the generated tags and the order of the ground truth tags occurring in training data. Empirical results on two different domains, Instagram and Stack Overflow, show that our method is significantly superior to the previous approaches.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Kang, Junmo and Kim, Jeonghwan and Shin, Suwon and Myaeng, Sung-Hyon},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.279},
	month = nov,
	pages = {3464--3476},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Order-Free Tag Relations for Context-Aware Recommendation},
	url = {https://aclanthology.org/2021.emnlp-main.279},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.279},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.279}}

@inproceedings{liu-etal-2021-leveraging,
	abstract = {Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with medical literature hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from medical literature and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and dynamic routing, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with medical literature hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with medical literature than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the medical literature, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Liu, Xin and Chen, Qingcai and Chen, Junying and Zhou, Wenxiu and Liu, Tingyu and Yang, Xinlan and Peng, Weihua},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.285},
	month = nov,
	pages = {3518--3532},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically},
	url = {https://aclanthology.org/2021.emnlp-main.285},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.285},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.285}}

@inproceedings{situ-etal-2021-lifelong,
	abstract = {Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher {--} an arbitrary explanation algorithm {--} on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10{\^{}}2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Situ, Xuelin and Maruf, Sameen and Zukerman, Ingrid and Paris, Cecile and Haffari, Gholamreza},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.233},
	month = nov,
	pages = {2933--2940},
	publisher = {Association for Computational Linguistics},
	title = {Lifelong Explainer for Lifelong Learners},
	url = {https://aclanthology.org/2021.emnlp-main.233},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.233},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.233}}

@inproceedings{saxon-etal-2021-modeling,
	abstract = {Broader disclosive transparency{---}truth and clarity in communication regarding the function of AI systems{---}is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where {``}too much information{''} clouds a reader{'}s understanding of what a system description means. Disclosive transparency{'}s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between transparency, confusion, and user perceptions in a corpus of real NLP system descriptions.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Saxon, Michael and Levy, Sharon and Wang, Xinyi and Albalak, Alon and Wang, William Yang},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.153},
	month = nov,
	pages = {2023--2037},
	publisher = {Association for Computational Linguistics},
	title = {Modeling Disclosive Transparency in {NLP} Application Descriptions},
	url = {https://aclanthology.org/2021.emnlp-main.153},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.153},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.153}}

@inproceedings{manchanda-karypis-2021-evaluating,
	abstract = {Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content of the publications. This limits their ability to provide rigorous quality-related metrics because they cannot account for the reasons that led to a citation. We present approaches to estimate content-aware bibliometrics to quantitatively measure the scholarly impact of a publication. Our approaches assess the impact of a cited publication by the extent to which the cited publication informs the citing publication. We introduce a new metric, called {``}Content Informed Index{''} (CII), that uses the content of the paper as a source of distant-supervision, to quantify how much the cited-node informs the citing-node. We evaluate the weights estimated by our approach on three manually annotated datasets, where the annotations quantify the extent of information in the citation. Particularly, we evaluate how well the ranking imposed by our approach associates with the ranking imposed by the manual annotations. CII achieves up to 103{\%} improvement in performance as compared to the second-best performing approach.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Manchanda, Saurav and Karypis, George},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.488},
	month = nov,
	pages = {6041--6053},
	publisher = {Association for Computational Linguistics},
	title = {Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics},
	url = {https://aclanthology.org/2021.emnlp-main.488},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.488},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.488}}

@inproceedings{yu-etal-2021-exophoric,
	abstract = {Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Yu, Xintong and Zhang, Hongming and Song, Yangqiu and Zhang, Changshui and Xu, Kun and Yu, Dong},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.311},
	month = nov,
	pages = {3832--3845},
	publisher = {Association for Computational Linguistics},
	title = {Exophoric Pronoun Resolution in Dialogues with Topic Regularization},
	url = {https://aclanthology.org/2021.emnlp-main.311},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.311},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.311}}

@inproceedings{lin-etal-2021-csds,
	abstract = {Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer{'}s issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers{'} viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Lin, Haitao and Ma, Liqun and Zhu, Junnan and Xiang, Lu and Zhou, Yu and Zhang, Jiajun and Zong, Chengqing},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.365},
	month = nov,
	pages = {4436--4451},
	publisher = {Association for Computational Linguistics},
	title = {{CSDS}: A Fine-Grained {C}hinese Dataset for Customer Service Dialogue Summarization},
	url = {https://aclanthology.org/2021.emnlp-main.365},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.365},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.365}}

@inproceedings{milbauer-etal-2021-aligning,
	abstract = {The Internet is home to thousands of communities, each with their own unique worldview and associated ideological differences. With new communities constantly emerging and serving as ideological birthplaces, battlegrounds, and bunkers, it is critical to develop a framework for understanding worldviews and ideological distinction. Most existing work, however, takes a predetermined view based on political polarization: the {``}right vs. left{''} dichotomy of U.S. politics. In reality, both political polarization {--} and worldviews more broadly {--} transcend one-dimensional difference, and deserve a more complete analysis. Extending the ability of word embedding models to capture the semantic and cultural characteristics of their training corpora, we propose a novel method for discovering the multifaceted ideological and worldview characteristics of communities. Using over 1B comments collected from the largest communities on Reddit.com representing {\textasciitilde}40{\%} of Reddit activity, we demonstrate the efficacy of this approach to uncover complex ideological differences across multiple axes of polarization.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Milbauer, Jeremiah and Mathew, Adarsh and Evans, James},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.396},
	month = nov,
	pages = {4832--4845},
	publisher = {Association for Computational Linguistics},
	title = {Aligning Multidimensional Worldviews and Discovering Ideological Differences},
	url = {https://aclanthology.org/2021.emnlp-main.396},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.396},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.396}}

@inproceedings{li-etal-2021-detecting,
	abstract = {Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based prediction model for identifying health advice in research publications. We annotated a corpus of 6,000 sentences extracted from structured abstracts in PubMed publications as {`}{``}strong advice{''}, {``}weak advice{''}, or {``}no advice{''}, and developed a BERT-based model that can predict, with a macro-averaged F1-score of 0.93, whether a sentence gives strong advice, weak advice, or not. The prediction model generalized well to sentences in both unstructured abstracts and discussion sections, where health advice normally appears. We also conducted a case study that applied this prediction model to retrieve specific health advice on COVID-19 treatments from LitCovid, a large COVID research literature portal, demonstrating the usefulness of retrieving health advice sentences as an advanced research literature navigation function for health researchers and the general public.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Li, Yingya and Wang, Jun and Yu, Bei},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.486},
	month = nov,
	pages = {6018--6029},
	publisher = {Association for Computational Linguistics},
	title = {Detecting Health Advice in Medical Research Literature},
	url = {https://aclanthology.org/2021.emnlp-main.486},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.486},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.486}}

@inproceedings{dodge-etal-2021-documenting,
	abstract = {Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-10-27 21:30:20 +0200},
	date-modified = {2022-10-27 21:30:20 +0200},
	doi = {10.18653/v1/2021.emnlp-main.98},
	month = nov,
	pages = {1286--1305},
	publisher = {Association for Computational Linguistics},
	title = {Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus},
	url = {https://aclanthology.org/2021.emnlp-main.98},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.98},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.98}}

@incollection{Burkhardt_2017,
	author = {Sophie Burkhardt and Stefan Kramer},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-319-71246-8_12},
	pages = {189--204},
	publisher = {Springer International Publishing},
	title = {Online Sparse Collapsed Hybrid Variational-Gibbs Algorithm for Hierarchical Dirichlet Process Topic Models},
	url = {https://doi.org/10.1007%2F978-3-319-71246-8_12},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-71246-8_12},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-71246-8_12}}

@incollection{Melvin_2017,
	author = {Sara Melvin and Wenchao Yu and Peng Ju and Sean Young and Wei Wang},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-319-71273-4_8},
	pages = {89--101},
	publisher = {Springer International Publishing},
	title = {Event Detection and Summarization Using Phrase Network},
	url = {https://doi.org/10.1007%2F978-3-319-71273-4_8},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-71273-4_8},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-71273-4_8}}

@incollection{Saha_2017,
	author = {Tanay Kumar Saha and Shafiq Joty and Mohammad Al Hasan},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-319-71249-9_45},
	pages = {753--769},
	publisher = {Springer International Publishing},
	title = {Con-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec},
	url = {https://doi.org/10.1007%2F978-3-319-71249-9_45},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-71249-9_45},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-71249-9_45}}

@incollection{Barbieri_2017,
	author = {Nicola Barbieri and Giuseppe Manco and Ettore Ritacco},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-319-71249-9_41},
	pages = {684--700},
	publisher = {Springer International Publishing},
	title = {Survival Factorization on Diffusion Networks},
	url = {https://doi.org/10.1007%2F978-3-319-71249-9_41},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-71249-9_41},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-71249-9_41}}

@incollection{Wenzel_2017,
	author = {Florian Wenzel and Th{\'{e}}o Galy-Fajou and Matth{\"a}us Deutsch and Marius Kloft},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-319-71249-9_19},
	pages = {307--322},
	publisher = {Springer International Publishing},
	title = {Bayesian Nonlinear Support Vector Machines for Big Data},
	url = {https://doi.org/10.1007%2F978-3-319-71249-9_19},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-71249-9_19},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-71249-9_19}}

@incollection{Lan_2019,
	author = {Andrew S. Lan and Jonathan C. Spencer and Ziqi Chen and Christopher G. Brinton and Mung Chiang},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-10928-8_43},
	pages = {725--740},
	publisher = {Springer International Publishing},
	title = {Personalized Thread Recommendation for {MOOC} Discussion Forums},
	url = {https://doi.org/10.1007%2F978-3-030-10928-8_43},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-10928-8_43},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-10928-8_43}}

@incollection{Hu_2019,
	author = {Wangsu Hu and Zijun Yao and Sen Yang and Shuhong Chen and Peter J. Jin},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-10928-8_6},
	pages = {88--104},
	publisher = {Springer International Publishing},
	title = {Discovering Urban Travel Demands Through Dynamic Zone Correlation in Location-Based Social Networks},
	url = {https://doi.org/10.1007%2F978-3-030-10928-8_6},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-10928-8_6},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-10928-8_6}}

@incollection{Appel_2019,
	author = {Ana Paula Appel and Renato L. F. Cunha and Charu C. Aggarwal and Marcela Megumi Terakado},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-10928-8_1},
	pages = {3--18},
	publisher = {Springer International Publishing},
	title = {Temporally Evolving Community Detection and Prediction in Content-Centric Networks},
	url = {https://doi.org/10.1007%2F978-3-030-10928-8_1},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-10928-8_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-10928-8_1}}

@incollection{Lim_2019,
	author = {Kwan Hui Lim and Sachini Jayasekara and Shanika Karunasekera and Aaron Harwood and Lucia Falzon and John Dunn and Glenn Burgess},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-10997-4_44},
	pages = {649--653},
	publisher = {Springer International Publishing},
	title = {{RAPID}: Real-time Analytics Platform for Interactive Data Mining},
	url = {https://doi.org/10.1007%2F978-3-030-10997-4_44},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-10997-4_44},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-10997-4_44}}

@incollection{Chalapathy_2019,
	author = {Raghavendra Chalapathy and Edward Toth and Sanjay Chawla},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-10925-7_11},
	pages = {173--189},
	publisher = {Springer International Publishing},
	title = {Group Anomaly Detection Using Deep Generative Models},
	url = {https://doi.org/10.1007%2F978-3-030-10925-7_11},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-10925-7_11},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-10925-7_11}}

@incollection{Liu_2019,
	author = {Peng Liu and Lemei Zhang and Jon Atle Gulla},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-10928-8_41},
	pages = {691--708},
	publisher = {Springer International Publishing},
	title = {Learning Multi-granularity Dynamic Network Representations for Social Recommendation},
	url = {https://doi.org/10.1007%2F978-3-030-10928-8_41},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-10928-8_41},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-10928-8_41}}

@inproceedings{10.1007/978-3-030-46133-1_33,
	abstract = {Although the majority of news articles are only viewed for days or weeks, there are a small fraction of news articles that are read across years, thus named as evergreen news articles. Because evergreen articles maintain a timeless quality and are consistently of interests to the public, understanding their characteristics better has huge implications for news outlets and platforms yet there are few studies that have explicitly investigated on evergreen articles. Addressing this gap, in this paper, we first propose a flexible parameterized definition of evergreen articles to capture their long-term high traffic patterns. Using a real dataset from the Washington Post, then, we unearth several distinctive characteristics of evergreen articles and build an early prediction model with encouraging results. Although less than {\$}{\$}1{\backslash}{\%}{\$}{\$} of news articles were identified as evergreen, our model achieves 0.961 in ROC AUC and 0.172 in PR AUC in 10-fold cross validation.},
	address = {Cham},
	author = {Liao, Yiming and Wang, Shuguang and Han, Eui-Hong (Sam) and Lee, Jongwuk and Lee, Dongwon},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
	isbn = {978-3-030-46133-1},
	pages = {552--568},
	publisher = {Springer International Publishing},
	title = {Characterization and Early Detection of Evergreen News Articles},
	year = {2020},
	bdsk-url-1 = {https://link.springer.com/chapter/10.1007/978-3-030-46133-1_33}}

@incollection{Ferner_2020,
	author = {Cornelia Ferner and Stefan Wegenkittl},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-46147-8_42},
	pages = {697--710},
	publisher = {Springer International Publishing},
	title = {A Semi-discriminative Approach for Sub-sentence Level Topic Classification on a Small Dataset},
	url = {https://doi.org/10.1007%2F978-3-030-46147-8_42},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-46147-8_42},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-46147-8_42}}

@incollection{Madrid_2020,
	author = {Jorge G. Madrid and Hugo Jair Escalante and Eduardo Morales},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-43823-4_6},
	pages = {57--67},
	publisher = {Springer International Publishing},
	title = {Meta-learning of Textual Representations},
	url = {https://doi.org/10.1007%2F978-3-030-43823-4_6},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-43823-4_6},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-43823-4_6}}

@incollection{Farruque_2020,
	author = {Nawshad Farruque and Osmar Zaiane and Randy Goebel},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-46133-1_22},
	pages = {359--375},
	publisher = {Springer International Publishing},
	title = {Augmenting Semantic Representation of Depressive Language: From Forums to Microblogs},
	url = {https://doi.org/10.1007%2F978-3-030-46133-1_22},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-46133-1_22},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-46133-1_22}}

@incollection{Audebert_2020,
	author = {Nicolas Audebert and Catherine Herold and Kuider Slimani and C{\'{e}}dric Vidal},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-43823-4_35},
	pages = {427--443},
	publisher = {Springer International Publishing},
	title = {Multimodal Deep Networks for Text and Image-Based Document Classification},
	url = {https://doi.org/10.1007%2F978-3-030-43823-4_35},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-43823-4_35},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-43823-4_35}}

@incollection{Zhang_2021,
	author = {Jason (Jiasheng) Zhang and Dongwon Lee},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-67658-2_15},
	pages = {249--265},
	publisher = {Springer International Publishing},
	title = {{PROMO} for Interpretable Personalized Social Emotion Mining},
	url = {https://doi.org/10.1007%2F978-3-030-67658-2_15},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67658-2_15},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67658-2_15}}

@incollection{Bai_2021,
	author = {Zilong Bai and S. S. Ravi and Ian Davidson},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-67664-3_3},
	pages = {37--53},
	publisher = {Springer International Publishing},
	title = {Towards Description of Block Model on Graph},
	url = {https://doi.org/10.1007%2F978-3-030-67664-3_3},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67664-3_3},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67664-3_3}}

@incollection{Harada_2021,
	author = {Shonosuke Harada and Hisashi Kashima},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-67658-2_31},
	pages = {542--558},
	publisher = {Springer International Publishing},
	title = {Counterfactual Propagation for Semi-supervised Individual Treatment Effect Estimation},
	url = {https://doi.org/10.1007%2F978-3-030-67658-2_31},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67658-2_31},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67658-2_31}}

@incollection{Khandelwal_2021,
	author = {Kanishka Khandelwal and Devendra Dhaka and Vivek Barsopia},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-67658-2_36},
	pages = {628--643},
	publisher = {Springer International Publishing},
	title = {Predicting Future Classifiers for Evolving Non-linear Decision Boundaries},
	url = {https://doi.org/10.1007%2F978-3-030-67658-2_36},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67658-2_36},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67658-2_36}}

@incollection{Yang_2021,
	author = {Haitian Yang and Weiqing Huang and Xuan Zhao and Yan Wang and Yuyan Chen and Bin Lv and Rui Mao and Ning Li},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-10-27 21:30:12 +0200},
	date-modified = {2022-10-27 21:30:12 +0200},
	doi = {10.1007/978-3-030-67664-3_35},
	pages = {584--599},
	publisher = {Springer International Publishing},
	title = {{AMQAN}: Adaptive Multi-Attention Question-Answer Networks for Answer Selection},
	url = {https://doi.org/10.1007%2F978-3-030-67664-3_35},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-67664-3_35},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-67664-3_35}}

@incollection{Aletras_2017,
	author = {Nikolaos Aletras and Arpit Mittal},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-56608-5_40},
	pages = {500--505},
	publisher = {Springer International Publishing},
	title = {Labeling Topics with Images Using a Neural Network},
	url = {https://doi.org/10.1007%2F978-3-319-56608-5_40},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-56608-5_40},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-56608-5_40}}

@incollection{Azarbonyad_2017,
	author = {Hosein Azarbonyad and Mostafa Dehghani and Tom Kenter and Maarten Marx and Jaap Kamps and Maarten de Rijke},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-56608-5_6},
	pages = {68--81},
	publisher = {Springer International Publishing},
	title = {Hierarchical Re-estimation of Topic Models for Measuring Topical Diversity},
	url = {https://doi.org/10.1007%2F978-3-319-56608-5_6},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-56608-5_6},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-56608-5_6}}

@incollection{Fang_2017,
	author = {Anjie Fang and Craig Macdonald and Iadh Ounis and Philip Habel and Xiao Yang},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-56608-5_20},
	pages = {252--265},
	publisher = {Springer International Publishing},
	title = {Exploring Time-Sensitive Variational Bayesian Inference {LDA} for Social Media Data},
	url = {https://doi.org/10.1007%2F978-3-319-56608-5_20},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-56608-5_20},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-56608-5_20}}

@incollection{Vu_2017,
	author = {Thanh Vu and Dat Quoc Nguyen and Mark Johnson and Dawei Song and Alistair Willis},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-56608-5_54},
	pages = {598--604},
	publisher = {Springer International Publishing},
	title = {Search Personalization with Embeddings},
	url = {https://doi.org/10.1007%2F978-3-319-56608-5_54},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-56608-5_54},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-56608-5_54}}

@incollection{Mourad_2017,
	author = {Ahmed Mourad and Falk Scholer and Mark Sanderson},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-56608-5_26},
	pages = {331--342},
	publisher = {Springer International Publishing},
	title = {Language Influences on Tweeter Geolocation},
	url = {https://doi.org/10.1007%2F978-3-319-56608-5_26},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-56608-5_26},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-56608-5_26}}

@incollection{Spitz_2018,
	author = {Andreas Spitz and Michael Gertz},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-76941-7_1},
	pages = {3--15},
	publisher = {Springer International Publishing},
	title = {Entity-Centric Topic Extraction and Exploration: A Network-Based Approach},
	url = {https://doi.org/10.1007%2F978-3-319-76941-7_1},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-76941-7_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-76941-7_1}}

@incollection{Bahrainian_2018,
	author = {Seyed Ali Bahrainian and Ida Mele and Fabio Crestani},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-76941-7_2},
	pages = {16--28},
	publisher = {Springer International Publishing},
	title = {Predicting Topics in Scholarly Papers},
	url = {https://doi.org/10.1007%2F978-3-319-76941-7_2},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-76941-7_2},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-76941-7_2}}

@incollection{Badjatiya_2018,
	author = {Pinkesh Badjatiya and Litton J. Kurisinkel and Manish Gupta and Vasudeva Varma},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-76941-7_14},
	pages = {180--193},
	publisher = {Springer International Publishing},
	title = {Attention-Based Neural Text Segmentation},
	url = {https://doi.org/10.1007%2F978-3-319-76941-7_14},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-76941-7_14},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-76941-7_14}}

@incollection{Zhang_2018,
	author = {Ruqing Zhang and Jiafeng Guo and Yanyan Lan and Jun Xu and Xueqi Cheng},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-76941-7_23},
	pages = {303--315},
	publisher = {Springer International Publishing},
	title = {Aggregating Neural Word Embeddings for Document Representation},
	url = {https://doi.org/10.1007%2F978-3-319-76941-7_23},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-76941-7_23},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-76941-7_23}}

@incollection{Sumikawa_2018,
	author = {Yasunobu Sumikawa and Adam Jatowt},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-76941-7_69},
	pages = {729--736},
	publisher = {Springer International Publishing},
	title = {Classifying Short Descriptions of Past Events},
	url = {https://doi.org/10.1007%2F978-3-319-76941-7_69},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-76941-7_69},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-76941-7_69}}

@incollection{Meladianos_2018,
	author = {Polykarpos Meladianos and Christos Xypolopoulos and Giannis Nikolentzos and Michalis Vazirgiannis},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-319-76941-7_36},
	pages = {481--493},
	publisher = {Springer International Publishing},
	title = {An Optimization Approach for Sub-event Detection and Summarization in Twitter},
	url = {https://doi.org/10.1007%2F978-3-319-76941-7_36},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-76941-7_36},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-76941-7_36}}

@incollection{Wang_2019,
	author = {Xi Wang and Anjie Fang and Iadh Ounis and Craig Macdonald},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_54},
	pages = {787--794},
	publisher = {Springer International Publishing},
	title = {Evaluating Similarity Metrics for Latent Twitter Topics},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_54},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_54},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_54}}

@incollection{Bahrainian_2019,
	author = {Seyed Ali Bahrainian and Fattane Zarrinkalam and Ida Mele and Fabio Crestani},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_17},
	pages = {261--275},
	publisher = {Springer International Publishing},
	title = {Predicting the Topic of Your Next Query for Just-In-Time {IR}},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_17},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_17},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_17}}

@incollection{Potha_2019,
	author = {Nektaria Potha and Efstathios Stamatatos},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_7},
	pages = {102--115},
	publisher = {Springer International Publishing},
	title = {Dynamic Ensemble Selection for Author Verification},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_7},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_7},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_7}}

@incollection{Bi_2019,
	author = {Keping Bi and Qingyao Ai and W. Bruce Croft},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_36},
	pages = {558--572},
	publisher = {Springer International Publishing},
	title = {Iterative Relevance Feedback for Answer Passage Retrieval with Passage-Level Semantic Match},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_36},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_36},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_36}}

@incollection{Fisher_2019,
	author = {Mark Fisher and Dyaa Albakour and Udo Kruschwitz and Miguel Martinez},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_5},
	pages = {69--85},
	publisher = {Springer International Publishing},
	title = {Recognising Summary Articles},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_5},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_5},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_5}}

@incollection{Mullick_2019,
	author = {Ankan Mullick and Sayan Ghosh and Ritam Dutt and Avijit Ghosh and Abhijnan Chakraborty},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15719-7_23},
	pages = {180--187},
	publisher = {Springer International Publishing},
	title = {Public Sphere 2.0: Targeted Commenting in Online News Media},
	url = {https://doi.org/10.1007%2F978-3-030-15719-7_23},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15719-7_23},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15719-7_23}}

@incollection{Pritsos_2019,
	author = {Dimitrios Pritsos and Anderson Rocha and Efstathios Stamatatos},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15719-7_1},
	pages = {3--11},
	publisher = {Springer International Publishing},
	title = {Open-Set Web Genre Identification Using Distributional Features and Nearest Neighbors Distance Ratio},
	url = {https://doi.org/10.1007%2F978-3-030-15719-7_1},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15719-7_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15719-7_1}}

@incollection{Almasian_2019,
	author = {Satya Almasian and Andreas Spitz and Michael Gertz},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_20},
	pages = {307--322},
	publisher = {Springer International Publishing},
	title = {Word Embeddings for Entity-Annotated Texts},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_20},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_20},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_20}}

@incollection{Bhattacharya_2019,
	author = {Paheli Bhattacharya and Kaustubh Hiware and Subham Rajgaria and Nilay Pochhi and Kripabandhu Ghosh and Saptarshi Ghosh},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_27},
	pages = {413--428},
	publisher = {Springer International Publishing},
	title = {A Comparative Study of Summarization Algorithms Applied to Legal Case Judgments},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_27},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_27},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_27}}

@incollection{Ghadery_2019,
	author = {Erfan Ghadery and Sajad Movahedi and Masoud Jalili Sabet and Heshaam Faili and Azadeh Shakery},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-15712-8_37},
	pages = {575--589},
	publisher = {Springer International Publishing},
	title = {{LICD}: A Language-Independent Approach for Aspect Category Detection},
	url = {https://doi.org/10.1007%2F978-3-030-15712-8_37},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-15712-8_37},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-15712-8_37}}

@incollection{Fard_2020,
	author = {Mazar Moradi Fard and Thibaut Thonet and Eric Gaussier},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45439-5_1},
	pages = {3--16},
	publisher = {Springer International Publishing},
	title = {Seed-Guided Deep Document Clustering},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_1},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_1}}

@incollection{Brochier_2020,
	author = {Robin Brochier and Adrien Guille and Julien Velcin},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45439-5_22},
	pages = {326--340},
	publisher = {Springer International Publishing},
	title = {Inductive Document Network Embedding with Topic-Word Attention},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_22},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_22},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_22}}

@incollection{Kovalchuk_2020,
	author = {Pavlo Kovalchuk and Diogo Proen{\c{c}}a and Jos{\'{e}} Borbinha and Rui Henriques},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45439-5_19},
	pages = {281--295},
	publisher = {Springer International Publishing},
	title = {Moving from Formal Towards Coherent Concept Analysis: Why, When and How},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_19},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_19},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_19}}

@incollection{Montazeralghaem_2020,
	author = {Ali Montazeralghaem and Razieh Rahimi and James Allan},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45439-5_30},
	pages = {446--460},
	publisher = {Springer International Publishing},
	title = {Relevance Ranking Based on Query-Aware Context Analysis},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_30},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_30},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_30}}

@incollection{Batra_2020,
	author = {Vishwash Batra and Aparajita Haldar and Yulan He and Hakan Ferhatosmanoglu and George Vogiatzis and Tanaya Guha},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45439-5_4},
	pages = {50--64},
	publisher = {Springer International Publishing},
	title = {Variational Recurrent Sequence-to-Sequence Retrieval for Stepwise Illustration},
	url = {https://doi.org/10.1007%2F978-3-030-45439-5_4},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45439-5_4},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45439-5_4}}

@incollection{Chelliah_2020,
	author = {Muthusamy Chelliah and Manish Shrivastava and Jaidam Ram Tej},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45442-5_88},
	pages = {663--668},
	publisher = {Springer International Publishing},
	title = {Principle-to-Program: Neural Methods for Similar Question Retrieval in Online Communities},
	url = {https://doi.org/10.1007%2F978-3-030-45442-5_88},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45442-5_88},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45442-5_88}}

@incollection{Ishigaki_2020,
	author = {Tatsuya Ishigaki and Kazuya Machida and Hayato Kobayashi and Hiroya Takamura and Manabu Okumura},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-45442-5_23},
	pages = {182--189},
	publisher = {Springer International Publishing},
	title = {Distant Supervision for Extractive Question Summarization},
	url = {https://doi.org/10.1007%2F978-3-030-45442-5_23},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-45442-5_23},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-45442-5_23}}

@incollection{_ahinu__2021,
	author = {Furkan {\c{S}}ahinu{\c{c}} and Cagri Toraman},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72240-1_50},
	pages = {471--478},
	publisher = {Springer International Publishing},
	title = {Tweet Length Matters: A Comparative Analysis on Topic Detection in Microblogs},
	url = {https://doi.org/10.1007%2F978-3-030-72240-1_50},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72240-1_50},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72240-1_50}}

@incollection{Jatowt_2021,
	author = {Adam Jatowt and I-Chen Hung and Michael F{\"a}rber and Ricardo Campos and Masatoshi Yoshikawa},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72113-8_17},
	pages = {254--269},
	publisher = {Springer International Publishing},
	title = {Exploding {TV} Sets and Disappointing Laptops: Suggesting Interesting Content in News Archives Based on Surprise Estimation},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_17},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_17},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_17}}

@incollection{Kuzi_2021,
	author = {Saar Kuzi and ChengXiang Zhai},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72113-8_19},
	pages = {284--297},
	publisher = {Springer International Publishing},
	title = {A Study of Distributed Representations for Figures of Research Articles},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_19},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_19},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_19}}

@incollection{Lugo_2021,
	author = {Luis Lugo and Jose G. Moreno and Gilles Hubert},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72113-8_27},
	pages = {405--418},
	publisher = {Springer International Publishing},
	title = {Modeling User Search Tasks with a Language-Agnostic Unsupervised Approach},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_27},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_27},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_27}}

@incollection{Abazari_Kia_2021,
	author = {Mahsa Abazari Kia},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72240-1_78},
	pages = {667--671},
	publisher = {Springer International Publishing},
	title = {Automated Multi-document Text Summarization from Heterogeneous Data Sources},
	url = {https://doi.org/10.1007%2F978-3-030-72240-1_78},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72240-1_78},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72240-1_78}}

@incollection{Meng_2021,
	author = {Rui Meng and Zhen Yue and Alyssa Glass},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72113-8_29},
	pages = {433--450},
	publisher = {Springer International Publishing},
	title = {Predicting User Engagement Status for~Online Evaluation of Intelligent Assistants},
	url = {https://doi.org/10.1007%2F978-3-030-72113-8_29},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72113-8_29},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72113-8_29}}

@incollection{Bondarenko_2021,
	author = {Alexander Bondarenko and Lukas Gienapp and Maik Fr{\"o}be and Meriem Beloucif and Yamen Ajjour and Alexander Panchenko and Chris Biemann and Benno Stein and Henning Wachsmuth and Martin Potthast and Matthias Hagen},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-72240-1_67},
	pages = {574--582},
	publisher = {Springer International Publishing},
	title = {Overview of Touch{\'{e}} 2021: Argument Retrieval},
	url = {https://doi.org/10.1007%2F978-3-030-72240-1_67},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-72240-1_67},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-72240-1_67}}

@incollection{Zosa_2022,
	author = {Elaine Zosa and Lidia Pivovarova and Michele Boggia and Sardana Ivanova},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-99739-7_29},
	pages = {248--256},
	publisher = {Springer International Publishing},
	title = {Multilingual Topic Labelling of News Topics Using Ontological Mapping},
	url = {https://doi.org/10.1007%2F978-3-030-99739-7_29},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99739-7_29},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99739-7_29}}

@incollection{Valero_2022,
	author = {Francisco B. Valero and Marion Baranes and Elena V. Epure},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-99736-6_32},
	pages = {472--486},
	publisher = {Springer International Publishing},
	title = {Topic Modeling on Podcast Short-Text Metadata},
	url = {https://doi.org/10.1007%2F978-3-030-99736-6_32},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99736-6_32},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99736-6_32}}

@incollection{Palencia_Olivar_2022,
	author = {Miguel Palencia-Olivar},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-99739-7_64},
	pages = {520--527},
	publisher = {Springer International Publishing},
	title = {A Topical Approach to Capturing Customer Insight Dynamics in Social Media},
	url = {https://doi.org/10.1007%2F978-3-030-99739-7_64},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99739-7_64},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99739-7_64}}

@incollection{Esuli_2022,
	author = {Andrea Esuli and Alejandro Moreo and Fabrizio Sebastiani},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-99739-7_47},
	pages = {374--381},
	publisher = {Springer International Publishing},
	title = {{LeQua}@{CLEF}2022: Learning to Quantify},
	url = {https://doi.org/10.1007%2F978-3-030-99739-7_47},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99739-7_47},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99739-7_47}}

@incollection{Wang_2022,
	author = {Xi Wang and Iadh Ounis and Craig Macdonald},
	booktitle = {Lecture Notes in Computer Science},
	date-added = {2022-10-27 21:29:37 +0200},
	date-modified = {2022-10-27 21:29:37 +0200},
	doi = {10.1007/978-3-030-99736-6_33},
	pages = {487--501},
	publisher = {Springer International Publishing},
	title = {Effective Rating Prediction Using an Attention-Based User Review Sentiment Model},
	url = {https://doi.org/10.1007%2F978-3-030-99736-6_33},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-99736-6_33},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-99736-6_33}}

@inproceedings{sari-etal-2017-continuous,
	abstract = {This paper presents work on using continuous representations for authorship attribution. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for n-gram features via a neural network jointly with the classification layer. Experimental results demonstrate that the proposed model outperforms the state-of-the-art on two datasets, while producing comparable results on the remaining two.},
	address = {Valencia, Spain},
	author = {Sari, Yunita and Vlachos, Andreas and Stevenson, Mark},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {267--273},
	publisher = {Association for Computational Linguistics},
	title = {Continuous N-gram Representations for Authorship Attribution},
	url = {https://aclanthology.org/E17-2043},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-2043}}

@inproceedings{bonadiman-etal-2017-effective,
	abstract = {An important asset of using Deep Neural Networks (DNNs) for text applications is their ability to automatically engineering features. Unfortunately, DNNs usually require a lot of training data, especially for highly semantic tasks such as community Question Answering (cQA). In this paper, we tackle the problem of data scarcity by learning the target DNN together with two auxiliary tasks in a multitask learning setting. We exploit the strong semantic connection between selection of comments relevant to (i) new questions and (ii) forum questions. This enables a global representation for comments, new and previous questions. The experiments of our model on a SemEval challenge dataset for cQA show a 20{\%} of relative improvement over standard DNNs.},
	address = {Valencia, Spain},
	author = {Bonadiman, Daniele and Uva, Antonio and Moschitti, Alessandro},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {726--732},
	publisher = {Association for Computational Linguistics},
	title = {Effective shared representations with Multitask Learning for Community Question Answering},
	url = {https://aclanthology.org/E17-2115},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-2115}}

@inproceedings{muzny-etal-2017-two,
	abstract = {We present a deterministic sieve-based system for attributing quotations in literary text and a new dataset: QuoteLi3. Quote attribution, determining who said what in a given text, is important for tasks like creating dialogue systems, and in newer areas like computational literary studies, where it creates opportunities to analyze novels at scale rather than only a few at a time. We release QuoteLi3, which contains more than 6,000 annotations linking quotes to speaker mentions and quotes to speaker entities, and introduce a new algorithm for quote attribution. Our two-stage algorithm first links quotes to mentions, then mentions to entities. Using two stages encapsulates difficult sub-problems and improves system performance. The modular design allows us to tune for overall performance or higher precision, which is useful for many real-world use cases. Our system achieves an average F-score of 87.5 across three novels, outperforming previous systems, and can be tuned for precision of 90.4 at a recall of 65.1.},
	address = {Valencia, Spain},
	author = {Muzny, Grace and Fang, Michael and Chang, Angel and Jurafsky, Dan},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {460--470},
	publisher = {Association for Computational Linguistics},
	title = {A Two-stage Sieve Approach for Quote Attribution},
	url = {https://aclanthology.org/E17-1044},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-1044}}

@inproceedings{brychcin-kral-2017-unsupervised,
	abstract = {This paper introduces a new unsupervised approach for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue. Utterances are represented as real-valued vectors encoding their meaning. We model the dialogue as Hidden Markov model with emission probabilities estimated by Gaussian mixtures. We use Gibbs sampling for posterior inference. We present the results on the standard Switchboard-DAMSL corpus. Our algorithm achieves promising results compared with strong supervised baselines and outperforms other unsupervised algorithms.},
	address = {Valencia, Spain},
	author = {Brychc{\'\i}n, Tom{\'a}{\v{s}} and Kr{\'a}l, Pavel},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {485--490},
	publisher = {Association for Computational Linguistics},
	title = {Unsupervised Dialogue Act Induction using {G}aussian Mixtures},
	url = {https://aclanthology.org/E17-2078},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-2078}}

@inproceedings{sato-etal-2017-distributed,
	abstract = {Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper, we present a descriptive clustering approach that employs a distributed representation model, namely the paragraph vector model, to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e., a co-embedding) to automatically select a descriptive phrase that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them.},
	address = {Valencia, Spain},
	author = {Sato, Motoki and Brockmeier, Austin J. and Kontonatsios, Georgios and Mu, Tingting and Goulermas, John Y. and Tsujii, Jun{'}ichi and Ananiadou, Sophia},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {991--1001},
	publisher = {Association for Computational Linguistics},
	title = {Distributed Document and Phrase Co-embeddings for Descriptive Clustering},
	url = {https://aclanthology.org/E17-1093},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-1093}}

@inproceedings{ramrakhiyani-etal-2017-measuring,
	abstract = {Measuring topic quality is essential for scoring the learned topics and their subsequent use in Information Retrieval and Text classification. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single coherent theme, in turn indicating high topic coherence. TBuckets uses word embeddings of topic words and employs singular value decomposition (SVD) and Integer Linear Programming based optimization to create coherent word buckets. TBuckets outperforms the state-of-the-art techniques when evaluated using 3 publicly available datasets and on another one proposed in this paper.},
	address = {Valencia, Spain},
	author = {Ramrakhiyani, Nitin and Pawar, Sachin and Hingmire, Swapnil and Palshikar, Girish},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {437--442},
	publisher = {Association for Computational Linguistics},
	title = {Measuring Topic Coherence through Optimal Word Buckets},
	url = {https://aclanthology.org/E17-2070},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-2070}}

@inproceedings{mukherjee-etal-2017-creating,
	abstract = {Part of speech (POS) taggers and dependency parsers tend to work well on homogeneous datasets but their performance suffers on datasets containing data from different genres. In our current work, we investigate how to create POS tagging and dependency parsing experts for heterogeneous data by employing topic modeling. We create topic models (using Latent Dirichlet Allocation) to determine genres from a heterogeneous dataset and then train an expert for each of the genres. Our results show that the topic modeling experts reach substantial improvements when compared to the general versions. For dependency parsing, the improvement reaches 2 percent points over the full training baseline when we use two topics.},
	address = {Valencia, Spain},
	author = {Mukherjee, Atreyee and K{\"u}bler, Sandra and Scheutz, Matthias},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {347--355},
	publisher = {Association for Computational Linguistics},
	title = {Creating {POS} Tagging and Dependency Parsing Experts via Topic Modeling},
	url = {https://aclanthology.org/E17-1033},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-1033}}

@inproceedings{sorodoc-etal-2017-multimodal,
	abstract = {Topics generated by topic models are typically presented as a list of topic terms. Automatic topic labelling is the task of generating a succinct label that summarises the theme or subject of a topic, with the intention of reducing the cognitive load of end-users when interpreting these topics. Traditionally, topic label systems focus on a single label modality, e.g. textual labels. In this work we propose a multimodal approach to topic labelling using a simple feedforward neural network. Given a topic and a candidate image or textual label, our method automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems.},
	address = {Valencia, Spain},
	author = {Sorodoc, Ionut and Lau, Jey Han and Aletras, Nikolaos and Baldwin, Timothy},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	month = apr,
	pages = {701--706},
	publisher = {Association for Computational Linguistics},
	title = {Multimodal Topic Labelling},
	url = {https://aclanthology.org/E17-2111},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-2111}}

@inproceedings{sawhney-etal-2021-phase,
	abstract = {Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Contextualizing the build-up of such ideation is critical for the identification of users at risk. In this work, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users{'} historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user{'}s historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users{'} historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming state-of-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection. We further discuss practical and ethical considerations.},
	address = {Online},
	author = {Sawhney, Ramit and Joshi, Harshit and Flek, Lucie and Shah, Rajiv Ratn},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.205},
	month = apr,
	pages = {2415--2428},
	publisher = {Association for Computational Linguistics},
	title = {{PHASE}: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media},
	url = {https://aclanthology.org/2021.eacl-main.205},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.205},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.205}}

@inproceedings{shen-etal-2021-source,
	abstract = {While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with self-training and by increasing the amount of target side monolingual data.},
	address = {Online},
	author = {Shen, Jiajun and Chen, Peng-Jen and Le, Matthew and He, Junxian and Gu, Jiatao and Ott, Myle and Auli, Michael and Ranzato, Marc{'}Aurelio},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.130},
	month = apr,
	pages = {1519--1533},
	publisher = {Association for Computational Linguistics},
	title = {The Source-Target Domain Mismatch Problem in Machine Translation},
	url = {https://aclanthology.org/2021.eacl-main.130},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.130},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.130}}

@inproceedings{zhou-etal-2021-challenges,
	abstract = {Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.},
	address = {Online},
	author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.274},
	month = apr,
	pages = {3143--3155},
	publisher = {Association for Computational Linguistics},
	title = {Challenges in Automated Debiasing for Toxic Language Detection},
	url = {https://aclanthology.org/2021.eacl-main.274},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.274},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.274}}

@inproceedings{saravanakumar-etal-2021-event,
	abstract = {We propose a method for online news stream clustering that is a variant of the non-parametric streaming K-means algorithm. Our model uses a combination of sparse and dense document representations, aggregates document-cluster similarity along these multiple representations and makes the clustering decision using a neural classifier. The weighted document-cluster similarity model is learned using a novel adaptation of the triplet loss into a linear classification objective. We show that the use of a suitable fine-tuning objective and external knowledge in pre-trained transformer models yields significant improvements in the effectiveness of contextual embeddings for clustering. Our model achieves a new state-of-the-art on a standard stream clustering dataset of English documents.},
	address = {Online},
	author = {Saravanakumar, Kailash Karthik and Ballesteros, Miguel and Chandrasekaran, Muthu Kumar and McKeown, Kathleen},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.198},
	month = apr,
	pages = {2330--2340},
	publisher = {Association for Computational Linguistics},
	title = {Event-Driven News Stream Clustering using Entity-Aware Contextual Embeddings},
	url = {https://aclanthology.org/2021.eacl-main.198},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.198},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.198}}

@inproceedings{zehe-etal-2021-detecting,
	abstract = {This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The corpus we describe consists of German-language dime novels (550k tokens) that have been annotated in parallel, achieving an inter-annotator agreement of gamma = 0.7. Baseline experiments using BERT achieve an F1 score of 24{\%}, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like coreference resolution.},
	address = {Online},
	author = {Zehe, Albin and Konle, Leonard and D{\"u}mpelmann, Lea Katharina and Gius, Evelyn and Hotho, Andreas and Jannidis, Fotis and Kaufmann, Lucas and Krug, Markus and Puppe, Frank and Reiter, Nils and Schreiber, Annekea and Wiedmer, Nathalie},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.276},
	month = apr,
	pages = {3167--3177},
	publisher = {Association for Computational Linguistics},
	title = {Detecting Scenes in Fiction: A new Segmentation Task},
	url = {https://aclanthology.org/2021.eacl-main.276},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.276},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.276}}

@inproceedings{paul-etal-2021-multi,
	abstract = {Universal schema (USchema) assumes that two sentence patterns that share the same entity pairs are similar to each other. This assumption is widely adopted for solving various types of relation extraction (RE) tasks. Nevertheless, each sentence pattern could contain multiple facets, and not every facet is similar to all the facets of another sentence pattern co-occurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they co-occur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings significantly outperform their single-facet embedding counterpart, compositional universal schema (CUSchema) (Verga et al., 2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available.},
	address = {Online},
	author = {Paul, Rohan and Chang, Haw-Shiuan and McCallum, Andrew},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.77},
	month = apr,
	pages = {909--919},
	publisher = {Association for Computational Linguistics},
	title = {Multi-facet Universal Schema},
	url = {https://aclanthology.org/2021.eacl-main.77},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.77},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.77}}

@inproceedings{zhao-etal-2021-adversarial,
	abstract = {In this paper, we propose the Brand-Topic Model (BTM) which aims to detect brand-associated polarity-bearing topics from product reviews. Different from existing models for sentiment-topic extraction which assume topics are grouped under discrete sentiment categories such as {`}positive{'}, {`}negative{'} and {`}neural{'}, BTM is able to automatically infer real-valued brand-associated sentiment scores and generate fine-grained sentiment-topics in which we can observe continuous changes of words under a certain topic (e.g., {`}shaver{'} or {`}cream{'}) while its associated sentiment gradually varies from negative to positive. BTM is built on the Poisson factorisation model with the incorporation of adversarial learning. It has been evaluated on a dataset constructed from Amazon reviews. Experimental results show that BTM outperforms a number of competitive baselines in brand ranking, achieving a better balance of topic coherence and unique-ness, and extracting better-separated polarity-bearing topics.},
	address = {Online},
	author = {Zhao, Runcong and Gui, Lin and Pergola, Gabriele and He, Yulan},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.199},
	month = apr,
	pages = {2341--2351},
	publisher = {Association for Computational Linguistics},
	title = {Adversarial Learning of {P}oisson Factorisation Model for Gauging Brand Sentiment in User Reviews},
	url = {https://aclanthology.org/2021.eacl-main.199},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.199}}

@inproceedings{popa-rebedea-2021-bart,
	abstract = {We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pre-trained BART models on a large number of potential labels generated by state of the art non-neural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.},
	address = {Online},
	author = {Popa, Cristian and Rebedea, Traian},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-10-27 21:29:28 +0200},
	date-modified = {2022-10-27 21:29:28 +0200},
	doi = {10.18653/v1/2021.eacl-main.121},
	month = apr,
	pages = {1418--1425},
	publisher = {Association for Computational Linguistics},
	title = {{BART}-{TL}: Weakly-Supervised Topic Label Generation},
	url = {https://aclanthology.org/2021.eacl-main.121},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.121},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.121}}

@inproceedings{hanselowski-etal-2018-retrospective,
	abstract = {The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance classification task as a crucial first step towards detecting fake news. To date, there is no in-depth analysis paper to critically discuss FNC-1{'}s experimental setup, reproduce the results, and draw conclusions for next-generation stance classification methods. In this paper, we provide such an in-depth analysis for the three top-performing systems. We first find that FNC-1{'}s proposed evaluation metric favors the majority class, which can be easily classified, and thus overestimates the true discriminative power of the methods. Therefore, we propose a new F1-based metric yielding a changed system ranking. Next, we compare the features and architectures used, which leads to a novel feature-rich stacked LSTM model that performs on par with the best systems, but is superior in predicting minority classes. To understand the methods{'} ability to generalize, we derive a new dataset and perform both in-domain and cross-domain experiments. Our qualitative and quantitative study helps interpreting the original FNC-1 scores and understand which features help improving performance and why. Our new dataset and all source code used during the reproduction study are publicly available for future research.},
	address = {Santa Fe, New Mexico, USA},
	author = {Hanselowski, Andreas and PVS, Avinesh and Schiller, Benjamin and Caspelherr, Felix and Chaudhuri, Debanjan and Meyer, Christian M. and Gurevych, Iryna},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {1859--1874},
	publisher = {Association for Computational Linguistics},
	title = {A Retrospective Analysis of the Fake News Challenge Stance-Detection Task},
	url = {https://aclanthology.org/C18-1158},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1158}}

@inproceedings{you-etal-2018-attribute,
	abstract = {Spam detection has long been a research topic in both academic and industry due to its wide applications. Previous studies are mainly focused on extracting linguistic or behavior features to distinguish the spam and legitimate reviews. Such features are either ineffective or take long time to collect and thus are hard to be applied to cold-start spam review detection tasks. Recent advance leveraged the neural network to encode the textual and behavior features for the cold-start problem. However, the abundant attribute information are largely neglected by the existing framework. In this paper, we propose a novel deep learning architecture for incorporating entities and their inherent attributes from various domains into a unified framework. Specifically, our model not only encodes the entities of reviewer, item, and review, but also their attributes such as location, date, price ranges. Furthermore, we present a domain classifier to adapt the knowledge from one domain to the other. With the abundant attributes in existing entities and knowledge in other domains, we successfully solve the problem of data scarcity in the cold-start settings. Experimental results on two Yelp datasets prove that our proposed framework significantly outperforms the state-of-the-art methods.},
	address = {Santa Fe, New Mexico, USA},
	author = {You, Zhenni and Qian, Tieyun and Liu, Bing},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {1884--1895},
	publisher = {Association for Computational Linguistics},
	title = {An Attribute Enhanced Domain Adaptive Model for Cold-Start Spam Review Detection},
	url = {https://aclanthology.org/C18-1160},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1160}}

@inproceedings{yang-etal-2018-aspect,
	abstract = {Review text has been widely studied in traditional tasks such as sentiment analysis and aspect extraction. However, to date, no work is towards the abstractive review summarization that is essential for business organizations and individual consumers to make informed decisions. This work takes the lead to study the aspect/sentiment-aware abstractive review summarization by exploring multi-factor attentions. Specifically, we propose an interactive attention mechanism to interactively learns the representations of context words, sentiment words and aspect words within the reviews, acted as an encoder. The learned sentiment and aspect representations are incorporated into the decoder to generate aspect/sentiment-aware review summaries via an attention fusion network. In addition, the abstractive summarizer is jointly trained with the text categorization task, which helps learn a category-specific text encoder, locating salient aspect information and exploring the variations of style and wording of content with respect to different text categories. The experimental results on a real-life dataset demonstrate that our model achieves impressive results compared to other strong competitors.},
	address = {Santa Fe, New Mexico, USA},
	author = {Yang, Min and Qu, Qiang and Shen, Ying and Liu, Qiao and Zhao, Wei and Zhu, Jia},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {1110--1120},
	publisher = {Association for Computational Linguistics},
	title = {Aspect and Sentiment Aware Abstractive Review Summarization},
	url = {https://aclanthology.org/C18-1095},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1095}}

@inproceedings{barnes-etal-2018-projecting,
	abstract = {Domain adaptation for sentiment analysis is challenging due to the fact that supervised classifiers are very sensitive to changes in domain. The two most prominent approaches to this problem are structural correspondence learning and autoencoders. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform domain adaptation experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our model performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at https://github.com/jbarnesspain/domain{\_}blse},
	address = {Santa Fe, New Mexico, USA},
	author = {Barnes, Jeremy and Klinger, Roman and Schulte im Walde, Sabine},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {818--830},
	publisher = {Association for Computational Linguistics},
	title = {Projecting Embeddings for Domain Adaption: Joint Modeling of Sentiment Analysis in Diverse Domains},
	url = {https://aclanthology.org/C18-1070},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1070}}

@inproceedings{gupta-etal-2018-semantic,
	abstract = {Technical support problems are very complex. In contrast to regular web queries (that contain few keywords) or factoid questions (which are a few sentences), these problems usually include attributes like a detailed description of what is failing (symptom), steps taken in an effort to remediate the failure (activity), and sometimes a specific request or ask (intent). Automating support is the task of automatically providing answers to these problems given a corpus of solution documents. Traditional approaches to this task rely on information retrieval and are keyword based; looking for keyword overlap between the question and solution documents and ignoring these attributes. We present an approach for semantic parsing of technical questions that uses grammatical structure to extract these attributes as a baseline, and a CRF based model that can improve performance considerably in the presence of annotated data for training. We also demonstrate that combined with reasoning, these attributes help outperform retrieval baselines.},
	address = {Santa Fe, New Mexico, USA},
	author = {Gupta, Abhirut and Ray, Anupama and Dasgupta, Gargi and Singh, Gautam and Aggarwal, Pooja and Mohapatra, Prateeti},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {3251--3259},
	publisher = {Association for Computational Linguistics},
	title = {Semantic Parsing for Technical Support Questions},
	url = {https://aclanthology.org/C18-1275},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1275}}

@inproceedings{sari-etal-2018-topic,
	abstract = {Approaches to authorship attribution, the task of identifying the author of a document, are based on analysis of individuals{'} writing style and/or preferred topics. Although the problem has been widely explored, no previous studies have analysed the relationship between dataset characteristics and effectiveness of different types of features. This study carries out an analysis of four widely used datasets to explore how different types of features affect authorship attribution accuracy under varying conditions. The results of the analysis are applied to authorship attribution models based on both discrete and continuous representations. We apply the conclusions from our analysis to an extension of an existing approach to authorship attribution and outperform the prior state-of-the-art on two out of the four datasets used.},
	address = {Santa Fe, New Mexico, USA},
	author = {Sari, Yunita and Stevenson, Mark and Vlachos, Andreas},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {343--353},
	publisher = {Association for Computational Linguistics},
	title = {Topic or Style? Exploring the Most Useful Features for Authorship Attribution},
	url = {https://aclanthology.org/C18-1029},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1029}}

@inproceedings{an-etal-2018-model,
	abstract = {Word composition is a promising technique for representation learning of large linguistic units (e.g., phrases, sentences and documents). However, most of the current composition models do not take the ambiguity of words and the context outside of a linguistic unit into consideration for learning representations, and consequently suffer from the inaccurate representation of semantics. To address this issue, we propose a model-free context-aware word composition model, which employs the latent semantic information as global context for learning representations. The proposed model attempts to resolve the word sense disambiguation and word composition in a unified framework. Extensive evaluation shows consistent improvements over various strong word representation/composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method.},
	address = {Santa Fe, New Mexico, USA},
	author = {An, Bo and Han, Xianpei and Sun, Le},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {2834--2845},
	publisher = {Association for Computational Linguistics},
	title = {Model-Free Context-Aware Word Composition},
	url = {https://aclanthology.org/C18-1240},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1240}}

@inproceedings{li-etal-2018-document,
	abstract = {Document-level multi-aspect sentiment classification aims to predict user{'}s sentiment polarities for different aspects of a product in a review. Existing approaches mainly focus on text information. However, the authors (i.e. users) and overall ratings of reviews are ignored, both of which are proved to be significant on interpreting the sentiments of different aspects in this paper. Therefore, we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consider user preference and overall ratings jointly. Specifically, HUARN adopts a hierarchical architecture to encode word, sentence, and document level information. Then, user attention and aspect attention are introduced into building sentence and document level representation. The document representation is combined with user and overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances.},
	address = {Santa Fe, New Mexico, USA},
	author = {Li, Junjie and Yang, Haitong and Zong, Chengqing},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {925--936},
	publisher = {Association for Computational Linguistics},
	title = {Document-level Multi-aspect Sentiment Classification by Jointly Modeling Users, Aspects, and Overall Ratings},
	url = {https://aclanthology.org/C18-1079},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1079}}

@inproceedings{jiang-etal-2018-identifying,
	abstract = {Identifying emergent research trends is a key issue for both primary researchers as well as secondary research managers. Such processes can uncover the historical development of an area, and yield insight on developing topics. We propose an embedded trend detection framework for this task which incorporates our bijunctive hypothesis that important phrases are written by important authors within a field and vice versa. By ranking both author and phrase information in a multigraph, our method jointly determines key phrases and authoritative authors. We represent this intermediate output as phrasal embeddings, and feed this to a recurrent neural network (RNN) to compute trend scores that identify research trends. Over two large datasets of scientific articles, we demonstrate that our approach successfully detects past trends from the field, outperforming baselines based solely on text centrality or citation.},
	address = {Santa Fe, New Mexico, USA},
	author = {Jiang, Shenhao and Prasad, Animesh and Kan, Min-Yen and Sugiyama, Kazunari},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {259--269},
	publisher = {Association for Computational Linguistics},
	title = {Identifying Emergent Research Trends by Key Authors and Phrases},
	url = {https://aclanthology.org/C18-1022},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1022}}

@inproceedings{hao-paul-2018-learning,
	abstract = {Multilingual topic models enable crosslingual tasks by extracting consistent topics from multilingual corpora. Most models require parallel or comparable training corpora, which limits their ability to generalize. In this paper, we first demystify the knowledge transfer mechanism behind multilingual topic models by defining an alternative but equivalent formulation. Based on this analysis, we then relax the assumption of training data required by most existing models, creating a model that only requires a dictionary for training. Experiments show that our new method effectively learns coherent multilingual topics from partially and fully incomparable corpora with limited amounts of dictionary resources.},
	address = {Santa Fe, New Mexico, USA},
	author = {Hao, Shudong and Paul, Michael J.},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {2595--2609},
	publisher = {Association for Computational Linguistics},
	title = {Learning Multilingual Topics from Incomparable Corpora},
	url = {https://aclanthology.org/C18-1220},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1220}}

@inproceedings{li-yang-2018-pseudo,
	abstract = {Traditional supervised text classifiers require a large number of manually labeled documents, which are often expensive to obtain. Recently, dataless text classification has attracted more attention, since it only requires very few seed words of categories that are much cheaper. In this paper, we develop a pseudo-label based dataless Naive Bayes (PL-DNB) classifier with seed words. We initialize pseudo-labels for each document using seed word occurrences, and employ the expectation maximization algorithm to train PL-DNB in a semi-supervised manner. The pseudo-labels are iteratively updated using a mixture of seed word occurrences and estimations of label posteriors. To avoid noisy pseudo-labels, we also consider the information of nearest neighboring documents in the pseudo-label update step, i.e., preserving local neighborhood structure of documents. We empirically show that PL-DNB outperforms traditional dataless text classification algorithms with seed words. Especially, PL-DNB performs well on the imbalanced dataset.},
	address = {Santa Fe, New Mexico, USA},
	author = {Li, Ximing and Yang, Bo},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = aug,
	pages = {1908--1917},
	publisher = {Association for Computational Linguistics},
	title = {A Pseudo Label based Dataless Naive {B}ayes Algorithm for Text Classification with Seed Words},
	url = {https://aclanthology.org/C18-1162},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1162}}

@inproceedings{nouri-etal-2020-mining,
	abstract = {Crowdsourcing is used in academia and industry to solve tasks that are easy for humans but hard for computers, in natural language processing mostly to annotate data. The quality of annotations is affected by problems in the task design, task operation, and task evaluation that workers face with requesters in crowdsourcing processes. To learn about the major problems, we provide a short but comprehensive survey based on two complementary studies: (1) a literature review where we collect and organize problems known from interviews with workers, and (2) an empirical data analysis where we use topic modeling to mine workers{'} complaints from a new English corpus of workers{'} forum discussions. While literature covers all process phases, problems in the task evaluation are prevalent, including unfair rejections, late payments, and unjustified blockings of workers. According to the data, however, poor task design in terms of malfunctioning environments, bad workload estimation, and privacy violations seems to bother the workers most. Our findings form the basis for future research on how to improve crowdsourcing processes.},
	address = {Barcelona, Spain (Online)},
	author = {Nouri, Zahra and Wachsmuth, Henning and Engels, Gregor},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.551},
	month = dec,
	pages = {6264--6276},
	publisher = {International Committee on Computational Linguistics},
	title = {Mining Crowdsourcing Problems from Discussion Forums of Workers},
	url = {https://aclanthology.org/2020.coling-main.551},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.551},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.551}}

@inproceedings{srinivasa-desikan-etal-2020-comp,
	abstract = {Popular approaches to natural language processing create word embeddings based on textual co-occurrence patterns, but often ignore embodied, sensory aspects of language. Here, we introduce the Python package comp-syn, which provides grounded word embeddings based on the perceptually uniform color distributions of Google Image search results. We demonstrate that comp-syn significantly enriches models of distributional semantics. In particular, we show that(1) comp-syn predicts human judgments of word concreteness with greater accuracy and in a more interpretable fashion than word2vec using low-dimensional word{--}color embeddings ,and (2) comp-syn performs comparably to word2vec on a metaphorical vs. literal word-pair classification task. comp-syn is open-source on PyPi and is compatible with mainstream machine-learning Python packages. Our package release includes word{--}color embeddings forover 40,000 English words, each associated with crowd-sourced word concreteness judgments.},
	address = {Barcelona, Spain (Online)},
	author = {Srinivasa Desikan, Bhargav and Hull, Tasker and Nadler, Ethan and Guilbeault, Douglas and Abubakar Kar, Aabir and Chu, Mark and Lo Sardo, Donald Ruggiero},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.154},
	month = dec,
	pages = {1744--1751},
	publisher = {International Committee on Computational Linguistics},
	title = {comp-syn: Perceptually Grounded Word Embeddings with Color},
	url = {https://aclanthology.org/2020.coling-main.154},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.154},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.154}}

@inproceedings{pham-le-2020-auto,
	abstract = {Visualization and topic modeling are widely used approaches for text analysis. Traditional visualization methods find low-dimensional representations of documents in the visualization space (typically 2D or 3D) that can be displayed using a scatterplot. In contrast, topic modeling aims to discover topics from text, but for visualization, one needs to perform a post-hoc embedding using dimensionality reduction methods. Recent approaches propose using a generative model to jointly find topics and visualization, allowing the semantics to be infused in the visualization space for a meaningful interpretation. A major challenge that prevents these methods from being used practically is the scalability of their inference algorithms. We present, to the best of our knowledge, the first fast Auto-Encoding Variational Bayes based inference method for jointly inferring topics and visualization. Since our method is black box, it can handle model changes efficiently with little mathematical rederivation effort. We demonstrate the efficiency and effectiveness of our method on real-world large datasets and compare it with existing baselines.},
	address = {Barcelona, Spain (Online)},
	author = {Pham, Dang and Le, Tuan},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.458},
	month = dec,
	pages = {5223--5234},
	publisher = {International Committee on Computational Linguistics},
	title = {Auto-Encoding Variational {B}ayes for Inferring Topics and Visualization},
	url = {https://aclanthology.org/2020.coling-main.458},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.458},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.458}}

@inproceedings{du-etal-2020-pointing,
	abstract = {Recurrent neural networks (RNNs) suffer from well-known limitations and complications which include slow inference and vanishing gradients when processing long sequences in text classification. Recent studies have attempted to accelerate RNNs via various ad hoc mechanisms to skip irrelevant words in the input. However, word skipping approaches proposed to date effectively stop at each or a given time step to decide whether or not a given input word should be skipped, breaking the coherence of input processing in RNNs. Furthermore, current methods cannot change skip rates during inference and are consequently unable to support different skip rates in demanding real-world conditions. To overcome these limitations, we propose Pointer- LSTM, a novel LSTM framework which relies on a pointer network to select important words for target prediction. The model maintains a coherent input process for the LSTM modules and makes it possible to change the skip rate during inference. Our evaluation on four public data sets demonstrates that Pointer-LSTM (a) is 1.1xâˆ¼3.5x faster than the standard LSTM architecture; (b) is more accurate than Leap-LSTM (the state-of-the-art LSTM skipping model) at high skip rates; and (c) reaches robust accuracy levels even when the skip rate is changed during inference.},
	address = {Barcelona, Spain (Online)},
	author = {Du, Jinhua and Huang, Yan and Moilanen, Karo},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.544},
	month = dec,
	pages = {6184--6193},
	publisher = {International Committee on Computational Linguistics},
	title = {Pointing to Select: A Fast Pointer-{LSTM} for Long Text Classification},
	url = {https://aclanthology.org/2020.coling-main.544},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.544},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.544}}

@inproceedings{zhao-etal-2020-improving,
	abstract = {Recently, people have been beginning paying more attention to the abstractive dialogue summarization task. Since the information flows are exchanged between at least two interlocutors and key elements about a certain event are often spanned across multiple utterances, it is necessary for researchers to explore the inherent relations and structures of dialogue contents. However, the existing approaches often process the dialogue with sequence-based models, which are hard to capture long-distance inter-sentence relations. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network to model the dialogue as an interaction graph according to the topic word information. A masked graph self-attention mechanism is used to integrate cross-sentence information flows and focus more on the related utterances, which makes it better to understand the dialogue. Moreover, the topic word features are introduced to assist the decoding process. We evaluate our model on the SAMSum Corpus and Automobile Master Corpus. The experimental results show that our method outperforms most of the baselines.},
	address = {Barcelona, Spain (Online)},
	author = {Zhao, Lulu and Xu, Weiran and Guo, Jun},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.39},
	month = dec,
	pages = {437--449},
	publisher = {International Committee on Computational Linguistics},
	title = {Improving Abstractive Dialogue Summarization with Graph Structures and Topic Words},
	url = {https://aclanthology.org/2020.coling-main.39},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.39},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.39}}

@inproceedings{bai-etal-2020-pre,
	abstract = {Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria from the pre-trained language model to measure instances and help select more effective instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances.},
	address = {Barcelona, Spain (Online)},
	author = {Bai, Guirong and He, Shizhu and Liu, Kang and Zhao, Jun and Nie, Zaiqing},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.130},
	month = dec,
	pages = {1495--1504},
	publisher = {International Committee on Computational Linguistics},
	title = {Pre-trained Language Model Based Active Learning for Sentence Matching},
	url = {https://aclanthology.org/2020.coling-main.130},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.130},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.130}}

@inproceedings{nevezhin-etal-2020-topic,
	abstract = {Online advertising is one of the most widespread ways to reach and increase a target audience for those selling products. Usually having a form of a banner, advertising engages users into visiting a corresponding webpage. Professional generation of banners requires creative and writing skills and a basic understanding of target products. The great variety of goods presented in the online market enforce professionals to spend more and more time creating new advertisements different from existing ones. In this paper, we propose a neural network-based approach for the automatic generation of online advertising using texts from given webpages as sources. The important part of the approach is training on open data available online, which allows avoiding costly procedures of manual labeling. Collected open data consist of multiple subdomains with high data heterogeneity. The subdomains belong to different topics and vary in used vocabularies, phrases, styles that lead to reduced quality in adverts generation. We try to solve the problem of identifying existed subdomains and proposing a new ensemble approach based on exploiting multiple instances of a seq2seq model. Our experimental study on a dataset in the Russian language shows that our approach can significantly improve the quality of adverts generation.},
	address = {Barcelona, Spain (Online)},
	author = {Nevezhin, Egor and Butakov, Nikolay and Khodorchenko, Maria and Petrov, Maxim and Nasonov, Denis},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.206},
	month = dec,
	pages = {2273--2283},
	publisher = {International Committee on Computational Linguistics},
	title = {Topic-driven Ensemble for Online Advertising Generation},
	url = {https://aclanthology.org/2020.coling-main.206},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.206},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.206}}

@inproceedings{li-etal-2020-neural,
	abstract = {Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. Collecting sufficient labelled data from expert annotators to train a high-performance coreference resolution system is time-consuming and expensive. Crowdsourcing makes it possible to obtain the required amounts of data rapidly and cost-effectively. However, crowd-sourced labels can be noisy. To ensure high-quality data, it is crucial to infer the correct labels by aggregating the noisy labels. In this paper, we split the aggregation into two subtasks, i.e, mention classification and coreference chain inference. Firstly, we predict the general class of each mention using an autoencoder, which incorporates contextual information about each mention, while at the same time taking into account the mention{'}s annotation complexity and annotators{'} reliability at different levels. Secondly, to determine the coreference chain of each mention, we use weighted voting which takes into account the learned reliability in the first subtask. Experimental results demonstrate the effectiveness of our method in predicting the correct labels. We also illustrate our model{'}s interpretability through a comprehensive analysis of experimental results.},
	address = {Barcelona, Spain (Online)},
	author = {Li, Maolin and Takamura, Hiroya and Ananiadou, Sophia},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.507},
	month = dec,
	pages = {5760--5773},
	publisher = {International Committee on Computational Linguistics},
	title = {A Neural Model for Aggregating Coreference Annotation in Crowdsourcing},
	url = {https://aclanthology.org/2020.coling-main.507},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.507},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.507}}

@inproceedings{zhang-etal-2020-topic,
	abstract = {Conventional neural generative models tend to generate safe and generic responses which have little connection with previous utterances semantically and would disengage users in a dialog system. To generate relevant responses, we propose a method that employs two types of constraints - topical constraint and semantic constraint. Under the hypothesis that a response and its context have higher relevance when they share the same topics, the topical constraint encourages the topics of a response to match its context by conditioning response decoding on topic words{'} embeddings. The semantic constraint, which encourages a response to be semantically related to its context by regularizing the decoding objective function with semantic distance, is proposed. Optimal transport is applied to compute a weighted semantic distance between the representation of a response and the context. Generated responses are evaluated by automatic metrics, as well as human judgment, showing that the proposed method can generate more topic-relevant and content-rich responses than conventional models.},
	address = {Barcelona, Spain (Online)},
	author = {Zhang, Shuying and Zhao, Tianyu and Kawahara, Tatsuya},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.359},
	month = dec,
	pages = {4067--4077},
	publisher = {International Committee on Computational Linguistics},
	title = {Topic-relevant Response Generation using Optimal Transport for an Open-domain Dialog System},
	url = {https://aclanthology.org/2020.coling-main.359},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.359},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.359}}

@inproceedings{shang-etal-2020-speaker,
	abstract = {Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.},
	address = {Barcelona, Spain (Online)},
	author = {Shang, Guokan and Tixier, Antoine and Vazirgiannis, Michalis and Lorr{\'e}, Jean-Pierre},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.40},
	month = dec,
	pages = {450--464},
	publisher = {International Committee on Computational Linguistics},
	title = {Speaker-change Aware {CRF} for Dialogue Act Classification},
	url = {https://aclanthology.org/2020.coling-main.40},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.40},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.40}}

@inproceedings{takatsu-etal-2020-sentiment,
	abstract = {As smart speakers and conversational robots become ubiquitous, the demand for expressive speech synthesis has increased. In this paper, to control the emotional parameters of the speech synthesis according to certain dialogue contents, we construct a news dataset with emotion labels ({``}positive,{''} {``}negative,{''} or {``}neutral{''}) annotated for each sentence. We then propose a method to identify emotion labels using a model combining BERT and BiLSTM-CRF, and evaluate its effectiveness using the constructed dataset. The results showed that the classification model performance can be efficiently improved by preferentially annotating news articles with low confidence in the human-in-the-loop machine learning framework.},
	address = {Barcelona, Spain (Online)},
	author = {Takatsu, Hiroaki and Ando, Ryota and Matsuyama, Yoichi and Kobayashi, Tetsunori},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.440},
	month = dec,
	pages = {5013--5025},
	publisher = {International Committee on Computational Linguistics},
	title = {Sentiment Analysis for Emotional Speech Synthesis in a News Dialogue System},
	url = {https://aclanthology.org/2020.coling-main.440},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.440},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.440}}

@inproceedings{an-etal-2020-multimodal,
	abstract = {From the perspective of health psychology, human beings with long-term and sustained negativity are highly possible to be diagnosed with depression. Inspired by this, we argue that the global topic information derived from user-generated contents (e.g., texts and images) is crucial to boost the performance of the depression detection task, though this information has been neglected by almost all previous studies on depression detection. To this end, we propose a new Multimodal Topic-enriched Auxiliary Learning (MTAL) approach, aiming at capturing the topic information inside different modalities (i.e., texts and images) for depression detection. Especially, in our approach, a modality-agnostic topic model is proposed to be capable of mining the topical clues from either the discrete textual signals or the continuous visual signals. On this basis, the topic modeling w.r.t. the two modalities are cast as two auxiliary tasks for improving the performance of the primary task (i.e., depression detection). Finally, the detailed evaluation demonstrates the great advantage of our MTAL approach to depression detection over the state-of-the-art baselines. This justifies the importance of the multimodal topic information to depression detection and the effectiveness of our approach in capturing such information.},
	address = {Barcelona, Spain (Online)},
	author = {An, Minghui and Wang, Jingjing and Li, Shoushan and Zhou, Guodong},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	doi = {10.18653/v1/2020.coling-main.94},
	month = dec,
	pages = {1078--1089},
	publisher = {International Committee on Computational Linguistics},
	title = {Multimodal Topic-Enriched Auxiliary Learning for Depression Detection},
	url = {https://aclanthology.org/2020.coling-main.94},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.coling-main.94},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.94}}

@inproceedings{keh-etal-2022-pineapple,
	abstract = {A personification is a figure of speech that endows inanimate entities with properties and actions typically seen as requiring animacy. In this paper, we explore the task of personification generation. To this end, we propose PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification data for Learning Enhanced generation. We curate a corpus of personifications called PersonifCorp, together with automatically generated de-personified literalizations of these personifications. We demonstrate the usefulness of this parallel corpus by training a seq2seq model to personify a given literal input. Both automatic and human evaluations show that fine-tuning with PersonifCorp leads to significant gains in personification-related qualities such as animacy and interestingness. A detailed qualitative analysis also highlights key strengths and imperfections of PINEAPPLE over baselines, demonstrating a strong ability to generate diverse and creative personifications that enhance the overall appeal of a sentence.},
	address = {Gyeongju, Republic of Korea},
	author = {Keh, Sedrick Scott and Lu, Kevin and Gangal, Varun and Feng, Steven Y. and Jhamtani, Harsh and Alikhani, Malihe and Hovy, Eduard},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {6270--6284},
	publisher = {International Committee on Computational Linguistics},
	title = {{PINEAPPLE}: Personifying {IN}animate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation},
	url = {https://aclanthology.org/2022.coling-1.547},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.547}}

@inproceedings{huynh-etal-2022-vinli,
	abstract = {Over a decade, the research field of computational linguistics has witnessed the growth of corpora and models for natural language inference (NLI) for rich-resource languages such as English and Chinese. A large-scale and high-quality corpus is necessary for studies on NLI for Vietnamese, which can be considered a low-resource language. In this paper, we introduce ViNLI (Vietnamese Natural Language Inference), an open-domain and high-quality corpus for evaluating Vietnamese NLI models, which is created and evaluated with a strict process of quality control. ViNLI comprises over 30,000 human-annotated premise-hypothesis sentence pairs extracted from more than 800 online news articles on 13 distinct topics. In this paper, we introduce the guidelines for corpus creation which take the specific characteristics of the Vietnamese language in expressing entailment and contradiction into account. To evaluate the challenging level of our corpus, we conduct experiments with state-of-the-art deep neural networks and pre-trained models on our dataset. The best system performance is still far from human performance (a 14.20{\%} gap in accuracy). The ViNLI corpus is a challenging corpus to accelerate progress in Vietnamese computational linguistics. Our corpus is available publicly for research purposes.},
	address = {Gyeongju, Republic of Korea},
	author = {Huynh, Tin Van and Nguyen, Kiet Van and Nguyen, Ngan Luu-Thuy},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {3858--3872},
	publisher = {International Committee on Computational Linguistics},
	title = {{V}i{NLI}: A {V}ietnamese Corpus for Studies on Open-Domain Natural Language Inference},
	url = {https://aclanthology.org/2022.coling-1.339},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.339}}

@inproceedings{yin-etal-2022-improving,
	abstract = {Driven by recent advances in neural networks, various Deep Embedding Clustering (DEC) based short text clustering models are being developed. In these works, latent representation learning and text clustering are performed simultaneously. Although these methods are becoming increasingly popular, they use pure cluster-oriented objectives, which can produce meaningless representations. To alleviate this problem, several improvements have been developed to introduce additional learning objectives in the clustering process, such as models based on contrastive learning. However, existing efforts rely heavily on learning meaningful representations at the instance level. They have limited focus on learning global representations, which are necessary to capture the overall data structure at the cluster level. In this paper, we propose a novel DEC model, which we named the deep embedded clustering model with cluster-level representation learning (DECCRL) to jointly learn cluster and instance level representations. Here, we extend the embedded topic modelling approach to introduce reconstruction constraints to help learn cluster-level representations. Experimental results on real-world short text datasets demonstrate that our model produces meaningful clusters.},
	address = {Gyeongju, Republic of Korea},
	author = {Yin, Qing and Wang, Zhihua and Song, Yunya and Xu, Yida and Niu, Shuai and Bai, Liang and Guo, Yike and Yang, Xian},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {2226--2236},
	publisher = {International Committee on Computational Linguistics},
	title = {Improving Deep Embedded Clustering via Learning Cluster-level Representations},
	url = {https://aclanthology.org/2022.coling-1.195},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.195}}

@inproceedings{amplayo-etal-2022-attribute,
	abstract = {Metadata attributes (e.g., user and product IDs from reviews) can be incorporated as additional inputs to neural-based NLP models, by expanding the architecture of the models to improve performance. However, recent models rely on pretrained language models (PLMs), in which previously used techniques for attribute injection are either nontrivial or cost-ineffective. In this paper, we introduce a benchmark for evaluating attribute injection models, which comprises eight datasets across a diverse range of tasks and domains and six synthetically sparsified ones. We also propose a lightweight and memory-efficient method to inject attributes into PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules, to include attributes both independently of or jointly with the text. We use approximation techniques to parameterize the model efficiently for domains with large attribute vocabularies, and training mechanisms to handle multi-labeled and sparse attributes. Extensive experiments and analyses show that our method outperforms previous attribute injection methods and achieves state-of-the-art performance on all datasets.},
	address = {Gyeongju, Republic of Korea},
	author = {Amplayo, Reinald Kim and Yoo, Kang Min and Lee, Sang-Woo},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {1051--1064},
	publisher = {International Committee on Computational Linguistics},
	title = {Attribute Injection for Pretrained Language Models: A New Benchmark and an Efficient Method},
	url = {https://aclanthology.org/2022.coling-1.88},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.88}}

@inproceedings{wang-etal-2022-imci,
	abstract = {With the rapid development of automatic fake news detection technology, fact extraction and verification (FEVER) has been attracting more attention. The task aims to extract the most related fact evidences from millions of open-domain Wikipedia documents and then verify the credibility of corresponding claims. Although several strong models have been proposed for the task and they have made great process, we argue that they fail to utilize multi-view contextual information and thus cannot obtain better performance. In this paper, we propose to integrate multi-view contextual information (IMCI) for fact extraction and verification. For each evidence sentence, we define two kinds of context, i.e. intra-document context and inter-document context. Intra-document context consists of the document title and all the other sentences from the same document. Inter-document context consists of all other evidences which may come from different documents. Then we integrate the multi-view contextual information to encode the evidence sentences to handle the task. Our experimental results on FEVER 1.0 shared task show that our IMCI framework makes great progress on both fact extraction and verification, and achieves state-of-the-art performance with a winning FEVER score of 73.96{\%} and label accuracy of 77.25{\%} on the online blind test set. We also conduct ablation study to detect the impact of multi-view contextual information.},
	address = {Gyeongju, Republic of Korea},
	author = {Wang, Hao and Li, Yangguang and Huang, Zhen and Dou, Yong},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {1412--1421},
	publisher = {International Committee on Computational Linguistics},
	title = {{IMCI}: Integrate Multi-view Contextual Information for Fact Extraction and Verification},
	url = {https://aclanthology.org/2022.coling-1.121},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.121}}

@inproceedings{xie-etal-2022-gretel,
	abstract = {Recently, neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs), to capture the global semantic information for text summarization. However, in these methods, there remain limitations in the way they capture and integrate the global semantic information. In this paper, we propose a novel model, the graph contrastive topic enhanced language model (GRETEL), that incorporates the graph contrastive topic model with the pre-trained language model, to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs, the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics. Experimental results on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods.},
	address = {Gyeongju, Republic of Korea},
	author = {Xie, Qianqian and Huang, Jimin and Saha, Tulika and Ananiadou, Sophia},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {6259--6269},
	publisher = {International Committee on Computational Linguistics},
	title = {{GRETEL}: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization},
	url = {https://aclanthology.org/2022.coling-1.546},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.546}}

@inproceedings{liu-etal-2022-bert,
	abstract = {Multi-label Text Classification (MLTC) is the task of categorizing documents into one or more topics. Considering the large volumes of data and varying domains of such tasks, fully supervised learning requires manually fully annotated datasets which is costly and time-consuming. In this paper, we propose BERT-Flow-VAE (BFV), a Weakly-Supervised Multi-Label Text Classification (WSMLTC) model that reduces the need for full supervision. This new model (1) produces BERT sentence embeddings and calibrates them using a flow model, (2) generates an initial topic-document matrix by averaging results of a seeded sparse topic model and a textual entailment model which only require surface name of topics and 4-6 seed words per topic, and (3) adopts a VAE framework to reconstruct the embeddings under the guidance of the topic-document matrix. Finally, (4) it uses the means produced by the encoder model in the VAE architecture as predictions for MLTC. Experimental results on 6 multi-label datasets show that BFV can substantially outperform other baseline WSMLTC models in key metrics and achieve approximately 84{\%} performance of a fully-supervised model.},
	address = {Gyeongju, Republic of Korea},
	author = {Liu, Ziwen and Grau-Bove, Josep and Orr, Scott Allan Allan},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {1203--1220},
	publisher = {International Committee on Computational Linguistics},
	title = {{BERT}-Flow-{VAE}: A Weakly-supervised Model for Multi-Label Text Classification},
	url = {https://aclanthology.org/2022.coling-1.104},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.104}}

@inproceedings{antypas-etal-2022-twitter,
	abstract = {Social media platforms host discussions about a wide variety of topics that arise everyday. Making sense of all the content and organising it into categories is an arduous task. A common way to deal with this issue is relying on topic modeling, but topics discovered using this technique are difficult to interpret and can differ from corpus to corpus. In this paper, we present a new task based on tweet topic classification and release two associated datasets. Given a wide range of topics covering the most important discussion points in social media, we provide training and testing data from recent time periods that can be used to evaluate tweet classification models. Moreover, we perform a quantitative evaluation and analysis of current general- and domain-specific language models on the task, which provide more insights on the challenges and nature of the task.},
	address = {Gyeongju, Republic of Korea},
	author = {Antypas, Dimosthenis and Ushio, Asahi and Camacho-Collados, Jose and Silva, Vitor and Neves, Leonardo and Barbieri, Francesco},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {3386--3400},
	publisher = {International Committee on Computational Linguistics},
	title = {{T}witter Topic Classification},
	url = {https://aclanthology.org/2022.coling-1.299},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.299}}

@inproceedings{austin-etal-2022-community,
	abstract = {We present our novel, hyperparameter-free topic modelling algorithm, Community Topic. Our algorithm is based on mining communities from term co-occurrence networks. We empirically evaluate and compare Community Topic with Latent Dirichlet Allocation and the recently developed top2vec algorithm. We find that Community Topic runs faster than the competitors and produces topics that achieve higher coherence scores. Community Topic can discover coherent topics at various scales. The network representation used by Community Topic results in a natural relationship between topics and a topic hierarchy. This allows sub- and super-topics to be found on demand. These features make Community Topic the ideal tool for downstream applications such as applied research and conversational agents.},
	address = {Gyeongju, Republic of Korea},
	author = {Austin, Eric and Za{\"\i}ane, Osmar R. and Largeron, Christine},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-10-27 21:29:18 +0200},
	date-modified = {2022-10-27 21:29:18 +0200},
	month = oct,
	pages = {971--983},
	publisher = {International Committee on Computational Linguistics},
	title = {Community Topic: Topic Model Inference by Consecutive Word Community Discovery},
	url = {https://aclanthology.org/2022.coling-1.81},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.81}}

@inproceedings{10.1145/3132847.3132942,
	abstract = {Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA.},
	address = {New York, NY, USA},
	author = {Jiang, Haixin and Zhou, Rui and Zhang, Limeng and Wang, Hua and Zhang, Yanchun},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132942},
	isbn = {9781450349185},
	keywords = {topic coherence, topic model, text classification, statistical testing, poisson decomposition},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1489--1498},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {A Topic Model Based on Poisson Decomposition},
	url = {https://doi.org/10.1145/3132847.3132942},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132942}}

@inproceedings{10.1145/3132847.3133181,
	abstract = {Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially. We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching.},
	address = {New York, NY, USA},
	author = {Le, Tuan M. V. and Lauw, Hady W.},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133181},
	isbn = {9781450349185},
	keywords = {interactive topical analysis, topic model, semantic visualization},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2487--2490},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {SemVis: Semantic Visualization for Interactive Topical Analysis},
	url = {https://doi.org/10.1145/3132847.3133181},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133181}}

@inproceedings{10.1145/3132847.3133162,
	abstract = {Semantic similarity based retrieval is playing an increasingly important role in many IR systems such as modern web search, question-answering, similar document retrieval etc. Improvements in retrieval of semantically similar content are very significant to applications like Quora, Stack Overflow, Siri etc. We propose a novel unsupervised model for semantic similarity based content retrieval, where we construct semantic flow graphs for each query, and introduce the concept of "soft seeding" in graph based semi-supervised learning (SSL) to convert this into an unsupervised model.We demonstrate the effectiveness of our model on an equivalent question retrieval problem on the Stack Exchange QA dataset, where our unsupervised approach significantly outperforms the state-of-the-art unsupervised models, and produces comparable results to the best supervised models. Our research provides a method to tackle semantic similarity based retrieval without any training data, and allows seamless extension to different domain QA communities, as well as to other semantic equivalence tasks.},
	address = {New York, NY, USA},
	author = {Srivastava, Avikalp and Datt, Madhav},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133162},
	isbn = {9781450349185},
	keywords = {topic model application, semantic similarity, similar question retrieval, soft seeded semi-supervised learning graphs, document representation},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2315--2318},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Soft Seeded SSL Graphs for Unsupervised Semantic Similarity-Based Retrieval},
	url = {https://doi.org/10.1145/3132847.3133162},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133162}}

@inproceedings{10.1145/3132847.3133145,
	abstract = {People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification.},
	address = {New York, NY, USA},
	author = {Lin, Junjie and Mao, Wenji and Zhang, Yuhao},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133145},
	isbn = {9781450349185},
	keywords = {Multiple stance identification, constrained Nonnegative Matrix Factorization, topic modeling},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2167--2170},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {An Enhanced Topic Modeling Approach to Multiple Stance Identification},
	url = {https://doi.org/10.1145/3132847.3133145},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133145}}

@inproceedings{10.1145/3132847.3133109,
	abstract = {Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics.},
	address = {New York, NY, USA},
	author = {Wang, Yiren and Seyler, Dominic and Santu, Shubhra Kanti Karmaker and Zhai, ChengXiang},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133109},
	isbn = {9781450349185},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2347--2350},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {A Study of Feature Construction for Text-Based Forecasting of Time Series Variables},
	url = {https://doi.org/10.1145/3132847.3133109},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133109}}

@inproceedings{10.1145/3132847.3133011,
	abstract = {Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences. For solving these tasks, bag-of-word based representation has been used for a long time. In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations. However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context. In this paper, we first characterize two types of contexts depending on their scope and utility. We then propose two approaches to incorporate contextual information into content-based models. We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors. Experimental results demonstrate that our proposed models outshine existing models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.},
	address = {New York, NY, USA},
	author = {Saha, Tanay Kumar and Joty, Shafiq and Hassan, Naeemul and Hasan, Mohammad Al},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133011},
	isbn = {9781450349185},
	keywords = {ranking, feature learning, sen2vec, retrofitting, distributed representation of sentences, discourse, clustering, classification},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {547--556},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Regularized and Retrofitted Models for Learning Sentence Representation with Context},
	url = {https://doi.org/10.1145/3132847.3133011},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133011}}

@inproceedings{10.1145/3132847.3132864,
	abstract = {Emotion analysis of online customer service conservation is important for good user experience and customer satisfaction. However, conventional metrics do not fit this application scenario. In this work, by collecting and labeling online conversations of customer service on Twitter, we identify 8 new metrics, named as tones, to describe emotional information. To better interpret each tone, we extend the Latent Dirichlet Allocation (LDA) model to Tone LDA (T-LDA). In T-LDA, each latent topic is explicitly associated with one of three semantic categories, i.e., tone-related, domain-specific and auxiliary. By integrating tone label into learning, T-LDA can interfere the original unsupervised training process and thus is able to identify representative tone-related words. In evaluation, T-LDA shows better performance than baselines in predicting tone intensity. Also, a case study is conducted to analyze each tone via T-LDA output.},
	address = {New York, NY, USA},
	author = {Yin, Peifeng and Liu, Zhe and Xu, Anbang and Nakamura, Taiga},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132864},
	isbn = {9781450349185},
	keywords = {tone, topic modeling, emotion, online customer service},
	location = {Singapore, Singapore},
	numpages = {9},
	pages = {1887--1895},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Tone Analyzer for Online Customer Service: An Unsupervised Model with Interfered Training},
	url = {https://doi.org/10.1145/3132847.3132864},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132864}}

@inproceedings{10.1145/3132847.3132968,
	abstract = {Twitter provides us a convenient channel to get access to the immediate information about major events. However, it is challenging to acquire a clean and complete set of event-related data due to the characteristics of tweets, eg short and noisy. In this paper, we propose a semi-supervised method to obtain high quality event-related tweets from Twitter stream, in terms of precision and recall. Specifically, candidate event-related tweets are selected based on a set of keywords. We propose to generate and update these keywords dynamically along the event development. To be included in this keyword set, words are evaluated based on single word properties, property based on co-occurred words, and changes of word importance over time. Our solution is capable of capturing keywords of emerging aspects or aspects with increasing importance along event evolvement. By leveraging keyword importance information and a few labeled tweets, we propose a semi-supervised expectation maximization process to identify event-related tweets. This process significantly reduces human effort in acquiring high quality tweets. Experiments on three real world datasets show that our solution outperforms state-of-the-art approaches by up to 10% in F1 measure.},
	address = {New York, NY, USA},
	author = {Zheng, Xin and Sun, Aixin and Wang, Sibo and Han, Jialong},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132968},
	isbn = {9781450349185},
	keywords = {event-related tweet identification, dynamic keyword generation},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1619--1628},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Semi-Supervised Event-Related Tweet Identification with Dynamic Keyword Generation},
	url = {https://doi.org/10.1145/3132847.3132968},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132968}}

@inproceedings{10.1145/3132847.3132988,
	abstract = {Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams.Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event.},
	address = {New York, NY, USA},
	author = {Mele, Ida and Bahrainian, Seyed Ali and Crestani, Fabio},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132988},
	isbn = {9781450349185},
	keywords = {event mining, temporal analysis, dynamic topic modeling, news streams},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {767--776},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Linking News across Multiple Streams for Timeliness Analysis},
	url = {https://doi.org/10.1145/3132847.3132988},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132988}}

@inproceedings{10.1145/3132847.3133071,
	abstract = {Automatic tagging techniques are important for many applications such as searching and recommendation, which has attracted many researchers' attention in recent years. Existing methods mainly rely on users' tagging behavior or items' content information for tagging, yet users' consuming behavior is ignored. In this paper, we propose to leverage such information and introduce a probabilistic model called joint-tagging LDA to improve tagging accuracy. An effective algorithm based on Zero-Order Collapsed Variational Bayes is developed. Experiments conducted on a real dataset demonstrate that joint-tagging LDA outperforms existing competing methods.},
	address = {New York, NY, USA},
	author = {Liu, Shen and Liu, Hongyan},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133071},
	isbn = {9781450349185},
	keywords = {generative model, user behavior modeling, tag recommendation},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2175--2178},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Exploiting User Consuming Behavior for Effective Item Tagging},
	url = {https://doi.org/10.1145/3132847.3133071},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133071}}

@inproceedings{10.1145/3132847.3133063,
	abstract = {This paper addresses the task of cross-domain social emotion classification of online documents. The cross-domain task is formulated as using abundant labeled documents from a source domain and a small amount of labeled documents from a target domain, to predict the emotion of unlabeled documents in the target domain. Although several cross-domain emotion classification algorithms have been proposed, they require that feature distributions of different domains share a sufficient overlapping, which is hard to meet in practical applications. This paper proposes a novel framework, which uses the emotion distribution of training documents at the cluster level, to alleviate the aforementioned issue. Experimental results on two datasets show the effectiveness of our proposed model on cross-domain social emotion classification.},
	address = {New York, NY, USA},
	author = {Zhu, Endong and Rao, Yanghui and Xie, Haoran and Liu, Yuwei and Yin, Jian and Wang, Fu Lee},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133063},
	isbn = {9781450349185},
	keywords = {emotion detection, clustering, cross-domain classification},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2435--2438},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Cluster-Level Emotion Pattern Matching for Cross-Domain Social Emotion Classification},
	url = {https://doi.org/10.1145/3132847.3133063},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133063}}

@inproceedings{10.1145/3132847.3133150,
	abstract = {Fact-checking political discussions has become an essential clog in computational journalism. This task encompasses an important sub-task---identifying the set of statements with 'check-worthy' claims. Previous work has treated this as a simple text classification problem discounting the nuances involved in determining what makes statements check-worthy. We introduce a dataset of political debates from the 2016 US Presidential election campaign annotated using all major fact-checking media outlets and show that there is a need to model conversation context, debate dynamics and implicit world knowledge. We design a multi-classifier system TATHYA, that models latent groupings in data and improves state-of-art systems in detecting check-worthy statements by 19.5% in F1-score on a held-out test set, gaining primarily gaining in Recall.},
	address = {New York, NY, USA},
	author = {Patwari, Ayush and Goldwasser, Dan and Bagchi, Saurabh},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133150},
	isbn = {9781450349185},
	keywords = {clustering, computational journalism, natural language processing},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2259--2262},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {TATHYA: A Multi-Classifier System for Detecting Check-Worthy Statements in Political Debates},
	url = {https://doi.org/10.1145/3132847.3133150},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133150}}

@inproceedings{10.1145/3132847.3132967,
	abstract = {Graph clustering aims to discovercommunity structures in networks, the task being fundamentally challenging mainly because the topology structure and the content of the graphs are difficult to represent for clustering analysis. Recently, graph clustering has moved from traditional shallow methods to deep learning approaches, thanks to the unique feature representation learning capability of deep learning. However, existing deep approaches for graph clustering can only exploit the structure information, while ignoring the content information associated with the nodes in a graph. In this paper, we propose a novel marginalized graph autoencoder (MGAE) algorithm for graph clustering. The key innovation of MGAE is that it advances the autoencoder to the graph domain, so graph representation learning can be carried out not only in a purely unsupervised setting by leveraging structure and content information, it can also be stacked in a deep fashion to learn effective representation. From a technical viewpoint, we propose a marginalized graph convolutional network to corrupt network node content, allowing node content to interact with network features, and marginalizes the corrupted features in a graph autoencoder context to learn graph feature representations. The learned features are fed into the spectral clustering algorithm for graph clustering. Experimental results on benchmark datasets demonstrate the superior performance of MGAE, compared to numerous baselines.},
	address = {New York, NY, USA},
	author = {Wang, Chun and Pan, Shirui and Long, Guodong and Zhu, Xingquan and Jiang, Jing},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132967},
	isbn = {9781450349185},
	keywords = {network representation, graph convolutional network, graph clustering, graph autoencoder, autoencoder},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {889--898},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {MGAE: Marginalized Graph Autoencoder for Graph Clustering},
	url = {https://doi.org/10.1145/3132847.3132967},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132967}}

@inproceedings{10.1145/3132847.3133123,
	abstract = {An entity on the web can be referred by numerous morphs that are always ambiguous, implicit and informal, which makes it challenging to accurately identify all the morphs corresponding to a specific entity. In this paper, we introduce a novel method based on knowledge graph, which takes advantage of both knowledge reasoning and statistic learning. First, we present a model to build a knowledge graph for the given entity. The knowledge graph integrates the fragmented knowledge on how humans create morphs. Then, the candidate morphs are generated based on the rules summarized from the knowledge graph. At last, we use a classification method to filter the useless candidates and identify the target morphs. The experiments conducted on real world dataset demonstrate efficiency of our proposed method in terms of precision and recall.},
	address = {New York, NY, USA},
	author = {Huang, Longtao and Zhao, Lin and Lv, Shangwen and Lu, Fangzhou and Zhai, Yue and Hu, Songlin},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133123},
	isbn = {9781450349185},
	keywords = {entity morphs, language understanding, web mining, knowledge graph},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2111--2114},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {KIEM: A Knowledge Graph Based Method to Identify Entity Morphs},
	url = {https://doi.org/10.1145/3132847.3133123},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133123}}

@inproceedings{10.1145/3132847.3132906,
	abstract = {Which venue is a tweet posted from? We referred this as fine-grained geolocation. To solve this problem effectively, we develop novel techniques to exploit each posting user's content history. This is motivated by our finding that most users do not share their visitation history, but have ample content history from tweet posts. We formulate fine-grained geolocation as a ranking problem whereby given a test tweet, we rank candidate venues. We propose several models that leverage on three types of signals from locations, users and peers. Firstly, the location signals are words that are indicative of venues. We propose a location-indicative weighting scheme to capture this. Next we exploit user signals from each user's content history to enrich the very limited content of their tweets which have been targeted for geolocation. The intuition is that the user's other tweets may have been from the test venue or related venues, thus providing informative words. In this regard, we propose query expansion as the enrichment approach. Finally, we exploit the signals from peer users who have similar content history and thus potentially similar visitation behavior as the users of the test tweets. This suggests collaborative filtering where visitation information is propagated via content similarities. We proposed several models incorporating different combinations of the three signals. Our experiments show that the best model incorporates all three signals. It performs 6% to 40% better than the baselines depending on the metric and dataset.},
	address = {New York, NY, USA},
	author = {Chong, Wen-Haw and Lim, Ee-Peng},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132906},
	isbn = {9781450349185},
	keywords = {tweet geolocation, query expansion, collaborative filtering},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1279--1288},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Tweet Geolocation: Leveraging Location, User and Peer Signals},
	url = {https://doi.org/10.1145/3132847.3132906},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132906}}

@inproceedings{10.1145/3132847.3132971,
	abstract = {Quality control is one of the major problems in crowdsourcing. One of the primary approaches to rectify this issue is to assign the same task to different workers and then aggregate their answers to obtain a reliable answer. In addition to simple aggregation approaches such as majority voting, various sophisticated probabilistic models have been proposed. However, given that most of the existing methods operate by strengthening the opinions of the majority, these models often fail when the tasks require highly specialized knowledge and the ability of a large majority of the workers is inadequate. In this paper, we focus on an important class of answer aggregation problems in which majority voting fails and propose the concept of hyper questions to devise effective aggregation methods. A hyper question is a set of single questions, and our key idea is that experts are more likely to provide correct answers to all of the single questions included in a hyper question than non-experts. Thus, experts are more likely to reach consensus on the hyper questions than non-experts, which strengthen their influences. We incorporate the concept of hyper questions into existing answer aggregation methods. The results of our experiments conducted using both synthetic datasets and real datasets demonstrate that our simple and easily usable approach works effectively in cases where only a few experts are available.},
	address = {New York, NY, USA},
	author = {Li, Jiyi and Baba, Yukino and Kashima, Hisashi},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132971},
	isbn = {9781450349185},
	keywords = {hyper question, answer aggregation, crowdsourcing, heterogeneous-answer multiple-choice questions},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1069--1078},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Hyper Questions: Unsupervised Targeting of a Few Experts in Crowdsourcing},
	url = {https://doi.org/10.1145/3132847.3132971},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132971}}

@inproceedings{10.1145/3132847.3133007,
	abstract = {Recommending lifestyle articles is of immediate interest to the e-commerce industry and is beginning to attract research attention. Often followed strategies, such as recommending popular items are inadequate for this vertical because of two reasons. Firstly, users have their own personal preference over items, referred to as personal styles, which lead to the long-tail phenomenon. Secondly, each user displays multiple personas, each persona has a preference over items which could be dictated by a particular occasion, e.g. dressing for a party would be different from dressing to go to office. Recommendation in this vertical is crucially dependent on discovering styles for each of the multiple personas. There is no literature which addresses this problem.We posit a generative model which describes each user by a Simplex Over PERsona, SOPER, where a persona is described as the individuals preferences over prevailing styles modelled as topics over items. The choice of simplex and the long-tail nature necessitates the use of stick-breaking process. The main technical contribution is an efficient collapsed Gibbs sampling based algorithm for solving the attendant inference problem.Trained on large-scale interaction logs spanning more than half-a-million sessions collected from an e-commerce portal, SOPER outperforms previous baselines such as [9] by a large margin of 35% in identifying persona. Consequently it outperforms several competitive baselines comprehensively on the task of recommending from a catalogue of roughly 150 thousand lifestyle articles, by improving the recommendation quality as measured by AUC by a staggering 12.23%, in addition to aiding the interpretability of uncovered personal and fashionable styles thus advancing our precise understanding of the underlying phenomena.},
	address = {New York, NY, USA},
	author = {Dhakad, Lucky and Das, Mrinal and Bhattacharyya, Chiranjib and Datta, Samik and Kale, Mihir and Mehta, Vivek},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133007},
	isbn = {9781450349185},
	keywords = {topic models, stick-breaking process, lifestyle, fashion, bayesian nonparametrics},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1609--1618},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {SOPER: Discovering the Influence of Fashion and the Many Faces of User from Session Logs Using Stick Breaking Process},
	url = {https://doi.org/10.1145/3132847.3133007},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133007}}

@inproceedings{10.1145/3132847.3132965,
	abstract = {The recorded student activities in Massive Open Online Course (MOOC) provide us a unique opportunity to model their learning behaviors, identify their particular learning intents, and enable personalized assistance and guidance in online education. In this work, based on a thorough qualitative study of students' behaviors recorded in two MOOC courses with large student enrollments, we develop a non-parametric Bayesian model to capture students' sequential learning activities in a generative manner. Homogeneity of students' learning behaviors is captured by clustering them into latent student groups, where shared model structure characterizes the transitional patterns, intensity and temporal distribution of their learning activities. In the meanwhile, heterogeneity is captured by clustering students into different groups. Both qualitative and quantitative studies on those two MOOC courses confirmed the effectiveness of the proposed model in identifying students' learning behavior patterns and clustering them into related groups for predictive analysis. The identified student groups accurately predict student retention, course satisfaction and demographics.},
	address = {New York, NY, USA},
	author = {Shi, Yuling and Peng, Zhiyong and Wang, Hongning},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132965},
	isbn = {9781450349185},
	keywords = {moocs, behavior modeling, sequential data mining, probabilistic modeling},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {979--988},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Modeling Student Learning Styles in MOOCs},
	url = {https://doi.org/10.1145/3132847.3132965},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132965}}

@inproceedings{10.1145/3132847.3133023,
	abstract = {This paper studies the automated categorization and extraction of scientific concepts from titles of scientific articles, in order to gain a deeper understanding of their key contributions and facilitate the construction of a generic academic knowledgebase. Towards this goal, we propose an unsupervised, domain-independent, and scalable two-phase algorithm to type and extract key concept mentions into aspects of interest (e.g., Techniques, Applications, etc.). In the first phase of our algorithm we proposePhraseType, a probabilistic generative model which exploits textual features and limited POS tags to broadly segment text snippets into aspect-typed phrases. We extend this model to simultaneously learn aspect-specific features and identify academic domains in multi-domain corpora, since the two tasks mutually enhance each other. In the second phase, we propose an approach based on adaptor grammars to extract fine grained concept mentions from the aspect-typed phrases without the need for any external resources or human effort, in a purely data-driven manner. We apply our technique to study literature from diverse scientific domains and show significant gains over state-of-the-art concept extraction techniques. We also present a qualitative analysis of the results obtained.},
	address = {New York, NY, USA},
	author = {Krishnan, Adit and Sankar, Aravind and Zhi, Shi and Han, Jiawei},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3133023},
	isbn = {9781450349185},
	keywords = {probabilistic model, adaptor grammar, concept extraction},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1339--1348},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Unsupervised Concept Categorization and Extraction from Scientific Document Titles},
	url = {https://doi.org/10.1145/3132847.3133023},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3133023}}

@inproceedings{10.1145/3132847.3132963,
	abstract = {Entity disambiguation, also known as entity linking, is the task of mapping mentions in text to the corresponding entities in a given knowledge base, e.g. Wikipedia. Two key challenges are making use of mention's context to disambiguate (i.e. local objective), and promoting coherence of all the linked entities (i.e. global objective). In this paper, we propose a deep neural network model to effectively measure the semantic matching between mention's context and target entity. We are the first to employ the long short-term memory (LSTM) and attention mechanism for entity disambiguation. We also propose Pair-Linking, a simple but effective and significantly fast linking algorithm. Pair-Linking iteratively identifies and resolves pairs of mentions, starting from the most confident pair. It finishes linking all mentions in a document by scanning the pairs of mentions at most once. Our neural network model combined with Pair-Linking, named NeuPL, outperforms state-of-the-art systems over different types of documents including news, RSS, and tweets.},
	address = {New York, NY, USA},
	author = {Phan, Minh C. and Sun, Aixin and Tay, Yi and Han, Jialong and Li, Chenliang},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3132847.3132963},
	isbn = {9781450349185},
	keywords = {semantic matching, pair-linking, entity disambiguation},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1667--1676},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {NeuPL: Attention-Based Semantic Matching and Pair-Linking for Entity Disambiguation},
	url = {https://doi.org/10.1145/3132847.3132963},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132963}}

@inproceedings{10.1145/3269206.3271671,
	abstract = {Recently, dataless text classification has attracted increasing attention. It trains a classifier using seed words of categories, rather than labeled documents that are expensive to obtain. However, a small set of seed words may provide very limited and noisy supervision information, because many documents contain no seed words or only irrelevant seed words. In this paper, we address these issues using document manifold, assuming that neighboring documents tend to be assigned to a same category label. Following this idea, we propose a novel Laplacian seed word topic model (LapSWTM). In LapSWTM, we model each document as a mixture of hidden category topics, each of which corresponds to a distinctive category. Also, we assume that neighboring documents tend to have similar category topic distributions. This is achieved by incorporating a manifold regularizer into the log-likelihood function of the model, and then maximizing this regularized objective. Experimental results show that our LapSWTM significantly outperforms the existing dataless text classification algorithms and is even competitive with supervised algorithms to some extent. More importantly, it performs extremely well when the seed words are scarce.},
	address = {New York, NY, USA},
	author = {Li, Ximing and Li, Changchun and Chi, Jinjin and Ouyang, Jihong and Li, Chenliang},
	booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3269206.3271671},
	isbn = {9781450360142},
	keywords = {topic modeling, seed word, dataless text classification, document manifold},
	location = {Torino, Italy},
	numpages = {10},
	pages = {973--982},
	publisher = {Association for Computing Machinery},
	series = {CIKM '18},
	title = {Dataless Text Classification: A Topic Modeling Approach with Document Manifold},
	url = {https://doi.org/10.1145/3269206.3271671},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3269206.3271671}}

@inproceedings{10.1145/3269206.3269309,
	abstract = {Topic detection and tracking in document streams is a critical task in many important applications, hence has been attracting research interest in recent decades. With the large size of data streams, there have been a number of works from different approaches that propose automatic methods for the task. However, there is only a few small benchmark datasets that are publicly available for evaluating the proposed methods. The lack of large datasets with fine-grained groundtruth implicitly restrains the development of more advanced methods. In this work, we address this issue by collecting and publishing W2E - a large dataset consisting of news articles from more than 50 prominent mass media channels worldwide. The articles cover a large set of popular events within a full year. W2E is more than 15 times larger than TREC's TDT2 dataset, which is widely used in prior work. We further conduct exploratory analysis to examine the dynamics and diversity of W2E and propose potential uses of the dataset in other research.},
	address = {New York, NY, USA},
	author = {Hoang, Tuan-Anh and Vo, Khoi Duy and Nejdl, Wolfgang},
	booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3269206.3269309},
	isbn = {9781450360142},
	keywords = {topic detection, benchmark dataset, topic tracking},
	location = {Torino, Italy},
	numpages = {4},
	pages = {1847--1850},
	publisher = {Association for Computing Machinery},
	series = {CIKM '18},
	title = {W2E: A Worldwide-Event Benchmark Dataset for Topic Detection and Tracking},
	url = {https://doi.org/10.1145/3269206.3269309},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3269206.3269309}}

@inproceedings{10.1145/3269206.3271737,
	abstract = {Deep neural networks are gaining increasing popularity for the classic text classification task, due to their strong expressive power and less requirement for feature engineering. Despite such attractiveness, neural text classification models suffer from the lack of training data in many real-world applications. Although many semi-supervised and weakly-supervised text classification models exist, they cannot be easily applied to deep neural models and meanwhile support limited supervision types. In this paper, we propose a weakly-supervised method that addresses the lack of training data in neural text classification. Our method consists of two modules: (1) a pseudo-document generator that leverages seed information to generate pseudo-labeled documents for model pre-training, and (2) a self-training module that bootstraps on real unlabeled data for model refinement. Our method has the flexibility to handle different types of weak supervision and can be easily integrated into existing deep neural models for text classification. We have performed extensive experiments on three real-world datasets from different domains. The results demonstrate that our proposed method achieves inspiring performance without requiring excessive training data and outperforms baseline methods significantly.},
	address = {New York, NY, USA},
	author = {Meng, Yu and Shen, Jiaming and Zhang, Chao and Han, Jiawei},
	booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3269206.3271737},
	isbn = {9781450360142},
	keywords = {pseudo document generation, text classification, weakly-supervised learning, neural classification model},
	location = {Torino, Italy},
	numpages = {10},
	pages = {983--992},
	publisher = {Association for Computing Machinery},
	series = {CIKM '18},
	title = {Weakly-Supervised Neural Text Classification},
	url = {https://doi.org/10.1145/3269206.3271737},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3269206.3271737}}

@inproceedings{10.1145/3269206.3272011,
	abstract = {We consider the problem of predicting the success of startup companies at their early development stages. We formulate the task as predicting whether a company that has already secured initial (seed or angel) funding will attract a further round of investment in a given period of time. Previous work on this task has mostly been restricted to mining structured data sources, such as databases of the startup ecosystem consisting of investors, incubators and startups. Instead, we investigate the potential of using web-based open sources for the startup success prediction task and model the task using a very rich set of signals from such sources. In particular, we enrich structured data about the startup ecosystem with information from a business- and employment-oriented social networking service and from the web in general. Using these signals, we train a robust machine learning pipeline encompassing multiple base models using gradient boosting. We show that utilizing companies' mentions on the Web yields a substantial performance boost in comparison to only using structured data about the startup ecosystem. We also provide a thorough analysis of the obtained model that allows one to obtain insights into both the types of useful signals discoverable on the Web and market mechanisms underlying the funding process.},
	address = {New York, NY, USA},
	author = {Sharchilev, Boris and Roizner, Michael and Rumyantsev, Andrey and Ozornin, Denis and Serdyukov, Pavel and de Rijke, Maarten},
	booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3269206.3272011},
	isbn = {9781450360142},
	keywords = {heterogeneous web data, gradient boosting, predictive modeling, mining open sources},
	location = {Torino, Italy},
	numpages = {9},
	pages = {2283--2291},
	publisher = {Association for Computing Machinery},
	series = {CIKM '18},
	title = {Web-Based Startup Success Prediction},
	url = {https://doi.org/10.1145/3269206.3272011},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3269206.3272011}}

@inproceedings{10.1145/3269206.3269273,
	abstract = {This study takes the lead to study the aspect/sentiment-aware abstractive review summarization in domain adaptation scenario. The proposed model CASAS (neural attentive model for Cross-domain Aspect/Sentiment-aware Abstractive review Summarization) leverages domain classification task, working on datasets of both source and target domains, to recognize the domain information of texts and transfer knowledge from source domains to target domains. The extensive experiments on Amazon reviews demonstrate that CASAS outperforms the compared methods in both out-of-domain and in-domain setups.},
	address = {New York, NY, USA},
	author = {Yang, Min and Qu, Qiang and Zhu, Jia and Shen, Ying and Zhao, Zhou},
	booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3269206.3269273},
	isbn = {9781450360142},
	keywords = {abstractive review summarization, domain adaptation, topic modeling},
	location = {Torino, Italy},
	numpages = {4},
	pages = {1531--1534},
	publisher = {Association for Computing Machinery},
	series = {CIKM '18},
	title = {Cross-Domain Aspect/Sentiment-Aware Abstractive Review Summarization},
	url = {https://doi.org/10.1145/3269206.3269273},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3269206.3269273}}

@inproceedings{10.1145/3357384.3357941,
	abstract = {The rapid development of social media services has facilitated the communication of opinions through online news, blogs, microblogs, instant-messages, and so on. This article concentrates on the mining of readers' social sentiments evoked by social media materials. Existing methods are only applicable to a minority of social media like news portals with emotional voting information, while ignore the emotional contagion between writers and readers. However, incorporating such factors is challenging since the learned hidden variables would be very fuzzy (because of the short and noisy text in social networks). In this paper, we try to solve this problem by introducing a high-order network structure, i.e. communities. We first propose a new generative model called Community-Enhanced Social Sentiment Mining (CESSM), which 1) considers the emotional contagion between writers and readers to capture precise social sentiment, and 2) incorporates network communities to capture coherent topics. We then derive an inference algorithm based on Gibbs sampling. Empirical results show that, CESSM achieves significantly superior performance against the state-of-the-art techniques for text sentiment classification and interestingness in social sentiment mining.},
	address = {New York, NY, USA},
	author = {Wang, Xiaobao and Jin, Di and Liu, Mengquan and He, Dongxiao and Musial, Katarzyna and Dang, Jianwu},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3357384.3357941},
	isbn = {9781450369763},
	keywords = {social sentiment, community, emotional contagion, social network},
	location = {Beijing, China},
	numpages = {10},
	pages = {1763--1772},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Emotional Contagion-Based Social Sentiment Mining in Social Networks by Introducing Network Communities},
	url = {https://doi.org/10.1145/3357384.3357941},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3357384.3357941}}

@inproceedings{10.1145/3357384.3357828,
	abstract = {In the era of big data, online doctor review platforms, which enable patients to give feedback to their doctors, have become one of the most important components in healthcare systems. On one hand, they help patients to choose their doctors based on the experience of others. On the other hand, they help doctors to improve the quality of their service. Moreover, they provide important sources for us to discover common concerns of patients and existing problems in clinics, which potentially improve current healthcare systems. In this paper, we systematically investigate the dataset from one of such review platform, namely, ratemds.com, where each review for a doctor comes with an overall rating and ratings of four different aspects. A comprehensive statistical analysis is conducted first for reviews, ratings, and doctors. Then, we explore the content of reviews by extracting latent topics related to different aspects with unsupervised topic modeling techniques. As the core component of this paper, we propose a multi-task learning framework for the document-level multi-aspect sentiment classification. This task helps us to not only recover missing aspect-level ratings and detect inconsistent rating scores but also identify aspect-keywords for a given review based on ratings. The proposed model takes both features of doctors and aspect-keywords into consideration. Extensive experiments have been conducted on two subsets of ratemds dataset to demonstrate the effectiveness of the proposed model.},
	address = {New York, NY, USA},
	author = {Shi, Tian and Rakesh, Vineeth and Wang, Suhang and Reddy, Chandan K.},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3357384.3357828},
	isbn = {9781450369763},
	keywords = {sentiment classification, multi-aspect, multi-task learning, attention mechanism, online reviews},
	location = {Beijing, China},
	numpages = {9},
	pages = {2723--2731},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Document-Level Multi-Aspect Sentiment Classification for Online Reviews of Medical Experts},
	url = {https://doi.org/10.1145/3357384.3357828},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3357384.3357828}}

@inproceedings{10.1145/3357384.3358020,
	abstract = {High-impact catastrophic events (bomb attacks, shootings) trigger posting of large volume of information on social media platforms such as Twitter. Recent works have proposed content-aware systems for summarizing this information, thereby facilitating post-disaster services. However, a significant proportion of the posted content is unverified, which restricts the practical usage of the existing summarization systems. In this paper, we work on the novel task of generating verified summaries of information posted on Twitter during disasters. We first jointly learn representations of content-classes and expression-classes of tweets posted during disasters using a novel LDA-based generative model. These representations of content &amp; expression classes are used in conjunction with pre-disaster user behavior and temporal signals (replies) for training a Tree-LSTM based tweet-verification model. The model infers tweet verification probabilities which are used, besides information content of tweets, in an Integer Linear Programming (ILP) framework for generating the desired verified summaries. The summaries are fine-tuned using the class information of the tweets as obtained from the LDA-based generative model. Extensive experiments are performed on a publicly-available labeled dataset of man-made disasters which demonstrate the effectiveness of our tweet-verification (3-13% gain over baselines) and summarization (12-48% gain in verified content proportion, 8-13% gain in ROUGE-score over state-of-the-art) systems. We make implementations of our various modules available online.},
	address = {New York, NY, USA},
	author = {Sharma, Ashish and Rudra, Koustav and Ganguly, Niloy},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3357384.3358020},
	isbn = {9781450369763},
	keywords = {summarization, disaster, unverified information, microblogs},
	location = {Beijing, China},
	numpages = {10},
	pages = {921--930},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Going Beyond Content Richness: Verified Information Aware Summarization of Crisis-Related Microblogs},
	url = {https://doi.org/10.1145/3357384.3358020},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3357384.3358020}}

@inproceedings{10.1145/3357384.3358048,
	abstract = {Identifying the topic (domain) of each user's utterance in open-domain conversational systems is a crucial step for all subsequent language understanding and response tasks. In particular, for complex domains, an utterance is often routed to a single component responsible for that domain. Thus, correctly mapping a user utterance to the right domain is critical. This is a challenging task: users could mention entities like actors, singers or locations to implicitly indicate the domain, which requires extensive domain knowledge to interpret. To address this problem, we introduce ConCET: a Concurrent Entity-aware conversational Topic classifier, which incorporates entity type information together with the utterance content features. Specifically, ConCET utilizes entity information to enrich the utterance representation, combining character, word, and entity type embeddings into a single representation. However, for rich domains with millions of available entities, unrealistic amounts of labeled training data would be required. To complement our model, we propose a simple and effective method for generating synthetic training data, to augment the typically limited amounts of labeled training data, using commonly available knowledge bases as to generate additional labeled utterances. We extensively evaluate ConCET and our proposed training method first on an openly available human-human conversational dataset called Self-Dialogue, to calibrate our approach against previous state-of-the-art methods; second, we evaluate ConCET on a large dataset of human-machine conversations with real users, collected as part of the Amazon Alexa Prize. Our results show that ConCET significantly improves topic classification performance on both datasets, reaching 8-10% improvements compared to state-of-the-art deep learning methods. We complement our quantitative results with detailed analysis of system performance, which could be used for further improvements of conversational agents.},
	address = {New York, NY, USA},
	author = {Ahmadvand, Ali and Sahijwani, Harshita and Choi, Jason Ingyu and Agichtein, Eugene},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3357384.3358048},
	isbn = {9781450369763},
	keywords = {conversational topic classification, entity-aware conversation domain classification, open-domain conversational agents},
	location = {Beijing, China},
	numpages = {10},
	pages = {1371--1380},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {ConCET: Entity-Aware Topic Classification for Open-Domain Conversational Agents},
	url = {https://doi.org/10.1145/3357384.3358048},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3357384.3358048}}

@inproceedings{10.1145/3357384.3358099,
	abstract = {Information-seeking conversation system aims at satisfying the information needs of users through conversations. Text matching between a user query and a pre-collected question is an important part of the information-seeking conversation in E-commerce. In the practical scenario, a sort of questions always correspond to a same answer. Naturally, these questions can form a bag. Learning the matching between user query and bag directly may improve the conversation performance, denoted as query-bag matching. Inspired by such opinion, we propose a query-bag matching model which mainly utilizes the mutual coverage between query and bag and measures the degree of the content in the query mentioned by the bag, and vice verse. In addition, the learned bag representation in word level helps find the main points of a bag in a fine grade and promotes the query-bag matching performance. Experiments on two datasets show the effectiveness of our model.},
	address = {New York, NY, USA},
	author = {Fu, Zhenxin and Ji, Feng and Hu, Wenpeng and Zhou, Wei and Zhao, Dongyan and Chen, Haiqing and Yan, Rui},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3357384.3358099},
	isbn = {9781450369763},
	keywords = {ranking, coverage, bag, matching, e-commerce},
	location = {Beijing, China},
	numpages = {4},
	pages = {2337--2340},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Query-Bag Matching with Mutual Coverage for Information-Seeking Conversations in E-Commerce},
	url = {https://doi.org/10.1145/3357384.3358099},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3357384.3358099}}

@inproceedings{10.1145/3357384.3358017,
	abstract = {Interpretability of recommender systems has caused increasing attention due to its promotion of the effectiveness and persuasiveness of recommendation decision, and thus user satisfaction. Most existing methods, such as Matrix Factorization (MF), tend to be black-box machine learning models that lack interpretability and do not provide a straightforward explanation for their outputs. In this paper, we focus on probabilistic factorization model and further assume the absence of any auxiliary information, such as item content or user review. We propose an influence mechanism to evaluate the importance of the users' historical data, so that the most related users and items can be selected to explain each predicted rating. The proposed method is thus called Influencebased Interpretable Recommendation model (In2Rec). To further enhance the recommendation accuracy, we address the important issue of missing not at random, i.e., missing ratings are not independent from the observed and other unobserved ratings, because users tend to only interact what they like. In2Rec models the generative process for both observed and missing data, and integrates the influence mechanism in a Bayesian graphical model. A learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to maximum a posteriori estimation for In2Rec. A series of experiments on four real-world datasets (Movielens 10M, Netflix, Epinions, and Yelp) have been conducted. By comparing with the state-of-the-art recommendation methods, the experimental results have shown that In2Rec can consistently benefit the recommendation system in both rating prediction and ranking estimation tasks, and friendly interpret the recommendation results with the aid of the proposed influence mechanism.},
	address = {New York, NY, USA},
	author = {Liu, Huafeng and Wen, Jingxuan and Jing, Liping and Yu, Jian and Zhang, Xiangliang and Zhang, Min},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3357384.3358017},
	isbn = {9781450369763},
	keywords = {collaborative filtering, recommendation system, interpretable recommendation, probabilistic matrix factorization},
	location = {Beijing, China},
	numpages = {10},
	pages = {1803--1812},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {In2Rec: Influence-Based Interpretable Recommendation},
	url = {https://doi.org/10.1145/3357384.3358017},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3357384.3358017}}

@inproceedings{10.1145/3340531.3412050,
	abstract = {The task of session search focuses on using interaction data to improve relevance for the user's next query at the session level. In this paper, we formulate session search as a personalization task under the framework of learning to rank. Personalization approaches re-rank results to match a user model. Such user models are usually accumulated over time based on the user's browsing behaviour. We use a pre-computed and transparent set of user models based on concepts from the social science literature. Interaction data are used to map each session to these user models. Novel features are then estimated based on such models as well as sessions' interaction data. Extensive experiments on test collections from the TREC session track show statistically significant improvements over current session search algorithms.},
	address = {New York, NY, USA},
	author = {Aloteibi, Saad and Clark, Stephen},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3340531.3412050},
	isbn = {9781450368599},
	keywords = {personalization, session search, retrieval model, user models},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {15--24},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {Learning to Personalize for Web Search Sessions},
	url = {https://doi.org/10.1145/3340531.3412050},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3412050}}

@inproceedings{10.1145/3340531.3412878,
	abstract = {There are many existing retrieval and question answering datasets. However, most of them either focus on ranked list evaluation or single-candidate question answering. This divide makes it challenging to properly evaluate approaches concerned with ranking documents and providing snippets or answers for a given query. In this work, we present FiRA: a novel dataset of Fine-Grained Relevance Annotations. We extend the ranked retrieval annotations of the Deep Learning track of TREC 2019 with passage and word level graded relevance annotations for all relevant documents. We use our newly created data to study the distribution of relevance in long documents, as well as the attention of annotators to specific positions of the text. As an example, we evaluate the recently introduced TKL document ranking model. We find that although TKL exhibits state-of-the-art retrieval results for long documents, it misses many relevant passages.},
	address = {New York, NY, USA},
	author = {Hofst\"{a}tter, Sebastian and Zlabinger, Markus and Sertkan, Mete and Schr\"{o}der, Michael and Hanbury, Allan},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3340531.3412878},
	isbn = {9781450368599},
	keywords = {relevance distribution, position bias, word-level relevance, fine-grained annotations},
	location = {Virtual Event, Ireland},
	numpages = {8},
	pages = {3031--3038},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering},
	url = {https://doi.org/10.1145/3340531.3412878},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3412878}}

@inproceedings{10.1145/3340531.3411932,
	abstract = {News recommendation systems? purpose is to tackle the immense amount of news and offer personalized recommendations to users. A major issue in news recommendation is to capture the precise news representations for the efficacy of recommended items. Commonly, news contents are filled with well-known entities of different types. However, existing recommendation systems overlook exploiting external knowledge about entities and topical relatedness among the news. To cope with the above problem, in this paper, we propose Topic-Enriched Knowledge Graph Recommendation System(TEKGR). Three encoders in TEKGR handle news titles in two perspectives to obtain news representation embedding: (1) to extract meaning of news words without considering latent knowledge features in the news and (2) to extract semantic knowledge of news through topic information and contextual information from a knowledge graph. After obtaining news representation vectors, an attention network compares clicked news to the candidate news in order to get the user's final embedding. Our TEKGR model is superior to existing news recommendation methods by manipulating topical relations among entities and contextual features of entities. Experimental results on two public datasets show that our approach outperforms state-of-the-art deep recommendation approaches.},
	address = {New York, NY, USA},
	author = {Lee, Dongho and Oh, Byungkook and Seo, Seungmin and Lee, Kyong-Ho},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3340531.3411932},
	isbn = {9781450368599},
	keywords = {recommendation system, knowledge graphs, neural networks, news recommendation},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {695--704},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {News Recommendation with Topic-Enriched Knowledge Graphs},
	url = {https://doi.org/10.1145/3340531.3411932},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411932}}

@inproceedings{10.1145/3340531.3411933,
	abstract = {Online health communities (OHCs) provide a popular channel for users to seek information, suggestions and support during their medical treatment and recovery processes. To help users find relevant information easily, we present CLIR, an effective system for recommending relevant discussion threads to users in OHCs. We identify that thread content and user interests can be categorized in two dimensions: topics and concepts. CLIR leverages Latent Dirichlet Allocation model to summarize the topic dimension and uses Convolutional Neural Network to encode the concept dimension. It then builds a thread neural network to capture thread characteristics and builds a user neural network to capture user interests by integrating these two dimensions and their interactions. Finally, it matches the target thread's characteristics with candidate users' interests to make recommendations. Experimental evaluation with multiple OHC datasets demonstrates the performance advantage of CLIR over the state-of-the-art recommender systems on various evaluation metrics.},
	address = {New York, NY, USA},
	author = {Li, Mingda and Gao, Weiting and Chen, Yi},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3340531.3411933},
	isbn = {9781450368599},
	keywords = {online health community, discussion forum, thread recommendation, recommender systems, neural network, latent dirichlet allocation},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {765--774},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {A Topic and Concept Integrated Model for Thread Recommendation in Online Health Communities},
	url = {https://doi.org/10.1145/3340531.3411933},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411933}}

@inproceedings{10.1145/3340531.3411906,
	abstract = {E-Commerce marketplaces support millions of daily transactions, and some disagreements between buyers and sellers are unavoidable. Resolving disputes in an accurate, fast, and fair manner is of great importance for maintaining a trustworthy platform. Simple cases can be automated, but intricate cases are not sufficiently addressed by hard-coded rules, and therefore most disputes are currently resolved by people. In this work we take a first step towards automatically assisting human agents in dispute resolution at scale. We construct a large dataset of disputes from the eBay online marketplace, and identify several interesting behavioral and linguistic patterns. We then train classifiers to predict dispute outcomes with high accuracy. We explore the model and the dataset, reporting interesting correlations, important features, and insights.},
	address = {New York, NY, USA},
	author = {Tsurel, David and Doron, Michael and Nus, Alexander and Dagan, Arnon and Guy, Ido and Shahaf, Dafna},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3340531.3411906},
	isbn = {9781450368599},
	keywords = {online transactions, dispute resolution, e-commerce},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {1465--1474},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {E-Commerce Dispute Resolution Prediction},
	url = {https://doi.org/10.1145/3340531.3411906},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411906}}

@inproceedings{10.1145/3340531.3411890,
	abstract = {False information detection on social media is challenging as it commonly requires tedious evidence-collecting but lacks available comparative information. Clues mined from user comments, as the wisdom of crowds, could be of considerable benefit to this task. However, it is non-trivial to capture the complex semantics from the contents and comments in consideration of their implicit correlations. Although deep neural networks have good expressive power, one major drawback is the lack of explainability. In this paper, we focus on how to learn from the post contents and related comments in social media to understand and detect the false information more effectively, with explainability. We thus propose a Quantum-probability based Signed Attention Network (QSAN) that integrates the quantum-driven text encoding and a novel signed attention mechanism in a unified framework. QSAN is not only able to distinguish important comments from the others, but also can exploit the conflicting social viewpoints in the comments to facilitate the detection. Moreover, QSAN is advantageous with its explainability in terms of transparency due to quantum physics meanings and the attention weights. Extensive experiments on real-world datasets show that our approach outperforms state-of-the-art baselines and can provide different kinds of user comments to explain why a piece of information is detected as false.},
	address = {New York, NY, USA},
	author = {Tian, Tian and Liu, Yudong and Yang, Xiaoyu and Lyu, Yuefei and Zhang, Xi and Fang, Binxing},
	booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3340531.3411890},
	isbn = {9781450368599},
	keywords = {quantum probability, false information detection, explainable AI, stance detection},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {1445--1454},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {QSAN: A Quantum-Probability Based Signed Attention Network for Explainable False Information Detection},
	url = {https://doi.org/10.1145/3340531.3411890},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3340531.3411890}}

@inproceedings{10.1145/3459637.3482398,
	abstract = {As a well-established probabilistic method, topic models seek to uncover latent semantics from plain text. In addition to having textual content, we observe that documents are usually compared in listwise rankings based on their content. For instance, world-wide countries are compared in an international ranking in terms of electricity production based on their national reports. Such document comparisons constitute additional information that reveal documents' relative similarities. Incorporating them into topic modeling could yield comparative topics that help to differentiate and rank documents. Furthermore, based on different comparison criteria, the observed document comparisons usually cover multiple aspects, each expressing a distinct ranked list. For example, a country may be ranked higher in terms of electricity production, but fall behind others in terms of life expectancy or government budget. Each comparison criterion, or aspect, observes a distinct ranking. Considering such multiple aspects of comparisons based on different ranking criteria allows us to derive one set of topics that inform heterogeneous document similarities. We propose a generative topic model aimed at learning topics that are well aligned to multi-aspect listwise comparisons. Experiments on public datasets demonstrate the advantage of the proposed method in jointly modeling topics and ranked lists against baselines comprehensively.},
	address = {New York, NY, USA},
	author = {Zhang, Delvin Ce and Lauw, Hady W.},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3459637.3482398},
	isbn = {9781450384469},
	keywords = {text mining, generative topic model, comparative documents},
	location = {Virtual Event, Queensland, Australia},
	numpages = {10},
	pages = {2507--2516},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Topic Modeling for Multi-Aspect Listwise Comparisons},
	url = {https://doi.org/10.1145/3459637.3482398},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482398}}

@inproceedings{10.1145/3459637.3482253,
	abstract = {Identifying the dynamic functions of different urban zones enables a variety of smart city applications, such as intelligent urban planning, real-time traffic scheduling, and community precision management. Traditional urban function research using government administrative zoning systems is often conducted in a coarse resolution with fixed split, and ignore the reshaping of zones by city growth. To solve this problem, we propose a two-stage framework in order to represent the high-definition distribution of urban function across the city, by analyzing continuous human traces extracted from the dense, widespread, and full-time cellular data. At the representation stage, we embed the locations of base stations by modeling the user movements with staying and transfer events, along with the consideration of dynamic trip purposes in continuous human traces. At the annotation stage, we first divide the city into the finest unit zones and each covers at least one base station. By clustering the base stations, we further group the unit zones into functional zones. Last, we annotate functional zones based on the local point-of-interest (POI) information. In experiments, we evaluate the proposed high-definition function study in two tasks: (i) in-zone crowd flow prediction, and (ii) zone-enhanced POI recommendation. The results demonstrate the advantage of the proposed method with both the effectiveness of city split and the high-quality function annotation.},
	address = {New York, NY, USA},
	author = {Liu, Chunyu and Yang, Yongjian and Yao, Zijun and Xu, Yuanbo and Chen, Weitong and Yue, Lin and Wu, Haomeng},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3459637.3482253},
	isbn = {9781450384469},
	keywords = {mobile trajectory, zone embedding, signaling data, fine-grained functional zone, urban computing},
	location = {Virtual Event, Queensland, Australia},
	numpages = {10},
	pages = {1048--1057},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Discovering Urban Functions of High-Definition Zoning with Continuous Human Traces},
	url = {https://doi.org/10.1145/3459637.3482253},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482253}}

@inproceedings{10.1145/3459637.3482075,
	abstract = {The economic policy uncertainty (EPU) index is one of the important text-based indexes in finance and economics fields. The EPU indexes of more than 26 countries have been constructed to reflect the policy uncertainty on country-level economic environments and serve as an important economic leading indicator. The EPU indexes are calculated based on the number of news articles with some manually-selected keywords related to economic, uncertainty, and policy. We find that the keyword-based EPU indexes contain noise, which will influence their explainability and predictability. In our experimental dataset, over 40% of news articles with the selected keywords are not related to the EPU. Instead of using keywords only, our proposed models take contextual information into account and get good performance on identifying the articles unrelated to EPU. The noise free EPU index performs better than the keyword-based EPU index in both explainability and predictability.},
	address = {New York, NY, USA},
	author = {Chen, Chung-Chi and Huang, Hen-Hsen and Huang, Yu-Lieh and Chen, Hsin-Hsi},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3459637.3482075},
	isbn = {9781450384469},
	keywords = {denoise, economic index, document filtering, economic policy uncertainty},
	location = {Virtual Event, Queensland, Australia},
	numpages = {5},
	pages = {2915--2919},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Constructing Noise Free Economic Policy Uncertainty Index},
	url = {https://doi.org/10.1145/3459637.3482075},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482075}}

@inproceedings{10.1145/3459637.3482450,
	abstract = {Neural text matching models have been widely used in community question answering, information retrieval, and dialogue. However, these models designed for short texts cannot well address the long-form text matching problem, because there are many contexts in long-form texts can not be directly aligned with each other, and it is difficult for existing models to capture the key matching signals from such noisy data. Besides, these models are computationally expensive for simply use all textual data indiscriminately. To tackle the effectiveness and efficiency problem, we propose a novel hierarchical noise filtering model, namely Match-Ignition. The main idea is to plug the well-known PageRank algorithm into the Transformer, to identify and filter both sentence and word level noisy information in the matching process. Noisy sentences are usually easy to detect because previous work has shown that their similarity can be explicitly evaluated by the word overlapping, so we directly use PageRank to filter such information based on a sentence similarity graph. Unlike sentences, words rely on their contexts to express concrete meanings, so we propose to jointly learn the filtering and matching process, to well capture the critical word-level matching signals. Specifically, a word graph is first built based on the attention scores in each self-attention block of Transformer, and key words are then selected by applying PageRank on this graph. In this way, noisy words will be filtered out layer by layer in the matching process. Experimental results show that Match-Ignition outperforms both SOTA short text matching models and recent long-form text matching models. We also conduct detailed analysis to show that Match-Ignition efficiently captures important sentences and words, to facilitate the long-form text matching process.},
	address = {New York, NY, USA},
	author = {Pang, Liang and Lan, Yanyan and Cheng, Xueqi},
	booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3459637.3482450},
	isbn = {9781450384469},
	keywords = {text matching, pagerank algorithm, long-form text},
	location = {Virtual Event, Queensland, Australia},
	numpages = {10},
	pages = {1396--1405},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {Match-Ignition: Plugging PageRank into Transformer for Long-Form Text Matching},
	url = {https://doi.org/10.1145/3459637.3482450},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3459637.3482450}}

@inproceedings{10.1145/3511808.3557410,
	abstract = {Two general approaches are common for evaluating automatically generated labels in topic modeling: direct human assessment; or performance metrics that can be calculated without, but still correlate with, human assessment. However, both approaches implicitly assume that the quality of a topic label is single-dimensional. In contrast, this paper provides evidence that human assessments about the quality of topic labels consist of multiple latent dimensions. This evidence comes from human assessments of four simple labeling techniques. For each label, study participants responded to several items asking them to assess each label according to a variety of different criteria. Exploratory factor analysis shows that these human assessments of labeling quality have a two-factor latent structure. Subsequent analysis demonstrates that this multi-item, two-factor assessment can reveal nuances that would be missed using either a single-item human assessment of perceived label quality or established performance metrics. The paper concludes by suggesting future directions for the development of human-centered approaches to evaluating NLP and ML systems more broadly.},
	address = {New York, NY, USA},
	author = {Hosseiny Marani, Amin and Levine, Joshua and Baumer, Eric P.S.},
	booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3511808.3557410},
	isbn = {9781450392365},
	keywords = {topic labeling, topic modeling, human assessment, performance metrics, exploratory factor analysis},
	location = {Atlanta, GA, USA},
	numpages = {12},
	pages = {768--779},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {One Rating to Rule Them All? Evidence of Multidimensionality in Human Assessment of Topic Labeling Quality},
	url = {https://doi.org/10.1145/3511808.3557410},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3511808.3557410}}

@inproceedings{10.1145/3511808.3557561,
	abstract = {In this paper, we present a large Chinese news article dataset with 4.4 million articles. These articles are obtained from different news channels and sources. They are labeled with multi-level topic categories, and some of them also have summaries. This is the first Chinese news dataset that has both hierarchical topic labels and article full texts. And it is also the largest Chinese news topic dataset. We describe the data collection, annotation and quality evaluation process. The basic statistics of the dataset, comparison with other datasets and benchmark experiments are also presented.},
	address = {New York, NY, USA},
	author = {Li, Quanzhi and Liu, Yingchi and Chao, Yang},
	booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3511808.3557561},
	isbn = {9781450392365},
	keywords = {hierarchical topic classification, chinese news dataset, news summary, news topic},
	location = {Atlanta, GA, USA},
	numpages = {6},
	pages = {4193--4198},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {CNewsTS - A Large-Scale Chinese News Dataset with Hierarchical Topic Category and Summary},
	url = {https://doi.org/10.1145/3511808.3557561},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3511808.3557561}}

@inproceedings{10.1145/3511808.3557329,
	abstract = {State-of-the-art Graph Neural Networks (GNNs) have achieved tremendous success in social event detection tasks when restricted to a closed set of events. However, considering the large amount of data needed for training and the limited ability of a neural network in handling previously unknown data, it is hard for existing GNN-based methods to operate in an open set setting. To address this problem, we design a Quality-aware Self-improving Graph Neural Network (QSGNN) which extends the knowledge from known to unknown by leveraging the best of known samples and reliable knowledge transfer. Specifically, to fully exploit the labeled data, we propose a novel supervised pairwise loss with an additional orthogonal inter-class relation constraint to train the backbone GNN encoder. The learnt, already-known events further serve as strong reference bases for the unknown ones, which greatly prompts knowledge acquisition and transfer. When the model is generalized to unknown data, to ensure the effectiveness and reliability, we further leverage the reference similarity distribution vectors for pseudo pairwise label generation, selection and quality assessment. Following the diversity principle of active learning, our method selects diverse pair samples with the generated pseudo labels to fine-tune the GNN encoder. Besides, we propose a novel quality-guided optimization in which the contributions of pseudo labels are weighted based on consistency. Experimental results validate that our model achieves state-of-the-art results and extends well to unknown events.},
	address = {New York, NY, USA},
	author = {Ren, Jiaqian and Jiang, Lei and Peng, Hao and Cao, Yuwei and Wu, Jia and Yu, Philip S. and He, Lifang},
	booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
	date-added = {2022-10-27 21:29:08 +0200},
	date-modified = {2022-10-27 21:29:08 +0200},
	doi = {10.1145/3511808.3557329},
	isbn = {9781450392365},
	keywords = {contrastive learning, social event detection, graph neural network, active learning},
	location = {Atlanta, GA, USA},
	numpages = {10},
	pages = {1696--1705},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {From Known to Unknown: Quality-Aware Self-Improving Graph Neural Network For Open Set Social Event Detection},
	url = {https://doi.org/10.1145/3511808.3557329},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3511808.3557329}}

@inproceedings{lund-etal-2017-tandem,
	abstract = {Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms{---}an approach we call {``}Tandem Anchors{''}. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches.},
	address = {Vancouver, Canada},
	author = {Lund, Jeffrey and Cook, Connor and Seppi, Kevin and Boyd-Graber, Jordan},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1083},
	month = jul,
	pages = {896--905},
	publisher = {Association for Computational Linguistics},
	title = {Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling},
	url = {https://aclanthology.org/P17-1083},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1083},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1083}}

@inproceedings{lau-etal-2017-topically,
	abstract = {Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.},
	address = {Vancouver, Canada},
	author = {Lau, Jey Han and Baldwin, Timothy and Cohn, Trevor},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1033},
	month = jul,
	pages = {355--365},
	publisher = {Association for Computational Linguistics},
	title = {Topically Driven Neural Language Model},
	url = {https://aclanthology.org/P17-1033},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1033},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1033}}

@inproceedings{shu-etal-2017-lifelong,
	abstract = {This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.},
	address = {Vancouver, Canada},
	author = {Shu, Lei and Xu, Hu and Liu, Bing},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-2023},
	month = jul,
	pages = {148--154},
	publisher = {Association for Computational Linguistics},
	title = {Lifelong Learning {CRF} for Supervised Aspect Extraction},
	url = {https://aclanthology.org/P17-2023},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-2023},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-2023}}

@inproceedings{zhan-etal-2017-network,
	abstract = {This paper focuses on the task of noisy label aggregation in social media, where users with different social or culture backgrounds may annotate invalid or malicious tags for documents. To aggregate noisy labels at a small cost, a network framework is proposed by calculating the matching degree of a document{'}s topics and the annotators{'} meta-data. Unlike using the back-propagation algorithm, a probabilistic inference approach is adopted to estimate network parameters. Finally, a new simulation method is designed for validating the effectiveness of the proposed framework in aggregating noisy labels.},
	address = {Vancouver, Canada},
	author = {Zhan, Xueying and Wang, Yaowei and Rao, Yanghui and Xie, Haoran and Li, Qing and Wang, Fu Lee and Wong, Tak-Lam},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-2077},
	month = jul,
	pages = {484--490},
	publisher = {Association for Computational Linguistics},
	title = {A Network Framework for Noisy Label Aggregation in Social Media},
	url = {https://aclanthology.org/P17-2077},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-2077},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-2077}}

@inproceedings{he-etal-2017-unsupervised,
	abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
	address = {Vancouver, Canada},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1036},
	month = jul,
	pages = {388--397},
	publisher = {Association for Computational Linguistics},
	title = {An Unsupervised Neural Attention Model for Aspect Extraction},
	url = {https://aclanthology.org/P17-1036},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1036},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1036}}

@inproceedings{amoualian-etal-2017-topical,
	abstract = {This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.},
	address = {Vancouver, Canada},
	author = {Amoualian, Hesam and Lu, Wei and Gaussier, Eric and Balikas, Georgios and Amini, Massih R. and Clausel, Marianne},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1165},
	month = jul,
	pages = {1799--1809},
	publisher = {Association for Computational Linguistics},
	title = {Topical Coherence in {LDA}-based Models through Induced Segmentation},
	url = {https://aclanthology.org/P17-1165},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1165},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1165}}

@inproceedings{malmasi-etal-2017-unsupervised,
	abstract = {Most work on segmenting text does so on the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation.},
	address = {Vancouver, Canada},
	author = {Malmasi, Shervin and Dras, Mark and Johnson, Mark and Du, Lan and Wolska, Magdalena},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1134},
	month = jul,
	pages = {1457--1469},
	publisher = {Association for Computational Linguistics},
	title = {Unsupervised Text Segmentation Based on Native Language Characteristics},
	url = {https://aclanthology.org/P17-1134},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1134},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1134}}

@inproceedings{zhang-etal-2017-exploiting,
	abstract = {A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.},
	address = {Vancouver, Canada},
	author = {Zhang, Ye and Lease, Matthew and Wallace, Byron C.},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-2024},
	month = jul,
	pages = {155--160},
	publisher = {Association for Computational Linguistics},
	title = {Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization},
	url = {https://aclanthology.org/P17-2024},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-2024},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-2024}}

@inproceedings{ji-smith-2017-neural,
	abstract = {We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.},
	address = {Vancouver, Canada},
	author = {Ji, Yangfeng and Smith, Noah A.},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1092},
	month = jul,
	pages = {996--1005},
	publisher = {Association for Computational Linguistics},
	title = {Neural Discourse Structure for Text Categorization},
	url = {https://aclanthology.org/P17-1092},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1092},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1092}}

@inproceedings{cagan-etal-2017-data,
	abstract = {Opinionated Natural Language Generation (ONLG) is a new, challenging, task that aims to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users{'} agendas, consisting of topics and sentiments, and based on wide-coverage automatically-acquired generative grammars. We compare three types of grammatical representations that we design for ONLG, which interleave different layers of linguistic information and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima{'}an, 2008) inspired grammar gets better language model scores than lexicalized grammars {`}a la Collins (2003), and that the latter gets better human-evaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.},
	address = {Vancouver, Canada},
	author = {Cagan, Tomer and Frank, Stefan L. and Tsarfaty, Reut},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1122},
	month = jul,
	pages = {1331--1341},
	publisher = {Association for Computational Linguistics},
	title = {Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation ({ONLG})},
	url = {https://aclanthology.org/P17-1122},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1122},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1122}}

@inproceedings{johnson-etal-2017-leveraging,
	abstract = {Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.},
	address = {Vancouver, Canada},
	author = {Johnson, Kristen and Jin, Di and Goldwasser, Dan},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P17-1069},
	month = jul,
	pages = {741--752},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on {T}witter},
	url = {https://aclanthology.org/P17-1069},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1069},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1069}}

@inproceedings{card-etal-2018-neural,
	abstract = {Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.},
	address = {Melbourne, Australia},
	author = {Card, Dallas and Tan, Chenhao and Smith, Noah A.},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1189},
	month = jul,
	pages = {2031--2040},
	publisher = {Association for Computational Linguistics},
	title = {Neural Models for Documents with Metadata},
	url = {https://aclanthology.org/P18-1189},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1189},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1189}}

@inproceedings{huang-2018-phrasectm,
	abstract = {Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these models are lack of the ability to capture the correlation structure among the discovered numerous topics. We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coherent. In the second stage, we generate the correlation of topics from PhraseCTM. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.},
	address = {Melbourne, Australia},
	author = {Huang, Weijing},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-2083},
	month = jul,
	pages = {521--526},
	publisher = {Association for Computational Linguistics},
	title = {{P}hrase{CTM}: Correlated Topic Modeling on Phrases within {M}arkov Random Fields},
	url = {https://aclanthology.org/P18-2083},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-2083},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2083}}

@inproceedings{peng-etal-2018-neural,
	abstract = {Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts, which is critical to many scientific and engineering applications. However, the extensions of these models require carefully tailored graphical models and re-deduced inference algorithms, limiting their variations and applications. We propose a novel sparsity-enhanced topic model, Neural Sparse Topical Coding (NSTC) base on a sparsity-enhanced topic model called Sparse Topical Coding (STC). It focuses on replacing the complex inference process with the back propagation, which makes the model easy to explore extensions. Moreover, the external semantic information of words in word embeddings is incorporated to improve the representation of short texts. To illustrate the flexibility offered by the neural network based framework, we present three extensions base on NSTC without re-deduced inference algorithms. Experiments on Web Snippet and 20Newsgroups datasets demonstrate that our models outperform existing methods.},
	address = {Melbourne, Australia},
	author = {Peng, Min and Xie, Qianqian and Zhang, Yanchun and Wang, Hua and Zhang, Xiuzhen and Huang, Jimin and Tian, Gang},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1217},
	month = jul,
	pages = {2332--2340},
	publisher = {Association for Computational Linguistics},
	title = {Neural Sparse Topical Coding},
	url = {https://aclanthology.org/P18-1217},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1217},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1217}}

@inproceedings{hancock-etal-2018-training,
	abstract = {Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.},
	address = {Melbourne, Australia},
	author = {Hancock, Braden and Varma, Paroma and Wang, Stephanie and Bringmann, Martin and Liang, Percy and R{\'e}, Christopher},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1175},
	month = jul,
	pages = {1884--1895},
	publisher = {Association for Computational Linguistics},
	title = {Training Classifiers with Natural Language Explanations},
	url = {https://aclanthology.org/P18-1175},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1175},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1175}}

@inproceedings{uva-etal-2018-injecting,
	abstract = {Effectively using full syntactic parsing information in Neural Networks (NNs) for solving relational tasks, e.g., question similarity, is still an open problem. In this paper, we propose to inject structural representations in NNs by (i) learning a model with Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus. The results on Quora and SemEval question similarity datasets show that NNs using our approach can learn more accurate models, especially after fine tuning on GS.},
	address = {Melbourne, Australia},
	author = {Uva, Antonio and Bonadiman, Daniele and Moschitti, Alessandro},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-2046},
	month = jul,
	pages = {285--291},
	publisher = {Association for Computational Linguistics},
	title = {Injecting Relational Structural Representation in Neural Networks for Question Similarity},
	url = {https://aclanthology.org/P18-2046},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-2046},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2046}}

@inproceedings{fabbri-etal-2018-tutorialbank,
	abstract = {The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.},
	address = {Melbourne, Australia},
	author = {Fabbri, Alexander and Li, Irene and Trairatvorakul, Prawat and He, Yijiao and Ting, Weitai and Tung, Robert and Westerfield, Caitlin and Radev, Dragomir},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1057},
	month = jul,
	pages = {611--620},
	publisher = {Association for Computational Linguistics},
	title = {{T}utorial{B}ank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation},
	url = {https://aclanthology.org/P18-1057},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1057},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1057}}

@inproceedings{li-etal-2018-deep,
	abstract = {In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.},
	address = {Melbourne, Australia},
	author = {Li, Chenliang and Zhou, Wei and Ji, Feng and Duan, Yu and Chen, Haiqing},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1214},
	month = jul,
	pages = {2300--2310},
	publisher = {Association for Computational Linguistics},
	title = {A Deep Relevance Model for Zero-Shot Document Filtering},
	url = {https://aclanthology.org/P18-1214},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1214},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1214}}

@inproceedings{xu-etal-2018-double,
	abstract = {One key task of fine-grained sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using deep learning. Unlike other highly sophisticated supervised deep learning models, this paper proposes a novel and yet simple CNN model employing two types of pre-trained embeddings for aspect extraction: general-purpose embeddings and domain-specific embeddings. Without using any additional supervision, this model achieves surprisingly good results, outperforming state-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results.},
	address = {Melbourne, Australia},
	author = {Xu, Hu and Liu, Bing and Shu, Lei and Yu, Philip S.},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-2094},
	month = jul,
	pages = {592--598},
	publisher = {Association for Computational Linguistics},
	title = {Double Embeddings and {CNN}-based Sequence Labeling for Aspect Extraction},
	url = {https://aclanthology.org/P18-2094},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-2094},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2094}}

@inproceedings{wu-etal-2018-question,
	abstract = {Answer selection is an important subtask of community question answering (CQA). In a real-world CQA forum, a question is often represented as two parts: a subject that summarizes the main points of the question, and a body that elaborates on the subject in detail. Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation. In this paper, we propose the Question Condensing Networks (QCN) to make use of the subject-body relationship of community questions. In our model, the question subject is the primary part of the question representation, and the question body information is aggregated based on similarity and disparity with the question subject. Experimental results show that QCN outperforms all existing models on two CQA datasets.},
	address = {Melbourne, Australia},
	author = {Wu, Wei and Sun, Xu and Wang, Houfeng},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1162},
	month = jul,
	pages = {1746--1755},
	publisher = {Association for Computational Linguistics},
	title = {Question Condensing Networks for Answer Selection in Community Question Answering},
	url = {https://aclanthology.org/P18-1162},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1162},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1162}}

@inproceedings{shen-etal-2018-nash,
	abstract = {Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled \textit{ad-hoc}. In this paper, we present an \textit{end-to-end} Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as \textit{Bernoulli} latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw the connections between proposed method and \textit{rate-distortion theory}, which provides a theoretical foundation for the effectiveness of our framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both \textit{unsupervised} and \textit{supervised} scenarios.},
	address = {Melbourne, Australia},
	author = {Shen, Dinghan and Su, Qinliang and Chapfuwa, Paidamoyo and Wang, Wenlin and Wang, Guoyin and Henao, Ricardo and Carin, Lawrence},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1190},
	month = jul,
	pages = {2041--2050},
	publisher = {Association for Computational Linguistics},
	title = {{NASH}: Toward End-to-End Neural Architecture for Generative Semantic Hashing},
	url = {https://aclanthology.org/P18-1190},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1190},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1190}}

@inproceedings{ustalov-etal-2018-unsupervised-semantic,
	abstract = {We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task.},
	address = {Melbourne, Australia},
	author = {Ustalov, Dmitry and Panchenko, Alexander and Kutuzov, Andrey and Biemann, Chris and Ponzetto, Simone Paolo},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-2010},
	month = jul,
	pages = {55--62},
	publisher = {Association for Computational Linguistics},
	title = {Unsupervised Semantic Frame Induction using Triclustering},
	url = {https://aclanthology.org/P18-2010},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-2010},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2010}}

@inproceedings{dong-de-melo-2018-helping,
	abstract = {Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages.},
	address = {Melbourne, Australia},
	author = {Dong, Xin and de Melo, Gerard},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1235},
	month = jul,
	pages = {2524--2534},
	publisher = {Association for Computational Linguistics},
	title = {A Helping Hand: Transfer Learning for Deep Sentiment Analysis},
	url = {https://aclanthology.org/P18-1235},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1235},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1235}}

@inproceedings{blodgett-etal-2018-twitter,
	abstract = {Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods{'} impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at: \url{http://slanglab.cs.umass.edu/TwitterAAE/}.},
	address = {Melbourne, Australia},
	author = {Blodgett, Su Lin and Wei, Johnny and O{'}Connor, Brendan},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P18-1131},
	month = jul,
	pages = {1415--1425},
	publisher = {Association for Computational Linguistics},
	title = {{T}witter {U}niversal {D}ependency Parsing for {A}frican-{A}merican and Mainstream {A}merican {E}nglish},
	url = {https://aclanthology.org/P18-1131},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1131},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1131}}

@misc{https://doi.org/10.48550/arxiv.1906.04687,
	author = {Perez-Beltrachini, Laura and Liu, Yang and Lapata, Mirella},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.48550/ARXIV.1906.04687},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {Association for Computational Linguistics},
	title = {Generating Summaries with Topic Templates and Structured Convolutional Decoders},
	url = {https://arxiv.org/abs/1906.04687},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1906.04687}}

@inproceedings{lund-etal-2019-automatic,
	abstract = {Topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as classification. Even recent models, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to global metrics. We propose a task designed to elicit human judgments of token-level topic assignments. We use a variety of topic model types and parameters and discover that global metrics agree poorly with human assignments. Since human evaluation is expensive we propose a variety of automated metrics to evaluate topic models at a local level. Finally, we correlate our proposed metrics with human judgments from the task on several datasets. We show that an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. We suggest that this new metric, which we call consistency, be adopted alongside global metrics such as topic coherence when evaluating new topic models.},
	address = {Florence, Italy},
	author = {Lund, Jeffrey and Armstrong, Piper and Fearn, Wilson and Cowley, Stephen and Byun, Courtni and Boyd-Graber, Jordan and Seppi, Kevin},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1076},
	month = jul,
	pages = {788--796},
	publisher = {Association for Computational Linguistics},
	title = {Automatic Evaluation of Local Topic Quality},
	url = {https://aclanthology.org/P19-1076},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1076},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1076}}

@inproceedings{kumar-etal-2019-didnt,
	abstract = {To address the lack of comparative evaluation of Human-in-the-Loop Topic Modeling (HLTM) systems, we implement and evaluate three contrasting HLTM modeling approaches using simulation experiments. These approaches extend previously proposed frameworks, including constraints and informed prior-based methods. Users should have a sense of control in HLTM systems, so we propose a control metric to measure whether refinement operations{'} results match users{'} expectations. Informed prior-based methods provide better control than constraints, but constraints yield higher quality topics.},
	address = {Florence, Italy},
	author = {Kumar, Varun and Smith-Renner, Alison and Findlater, Leah and Seppi, Kevin and Boyd-Graber, Jordan},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1637},
	month = jul,
	pages = {6323--6330},
	publisher = {Association for Computational Linguistics},
	title = {Why Didn{'}t You Listen to Me? Comparing User Control of Human-in-the-Loop Topic Models},
	url = {https://aclanthology.org/P19-1637},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1637},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1637}}

@inproceedings{haj-yahia-etal-2019-towards,
	abstract = {Text classification aims at mapping documents into a set of predefined categories. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds. In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification.},
	address = {Florence, Italy},
	author = {Haj-Yahia, Zied and Sieg, Adrien and Deleris, L{\'e}a A.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1036},
	month = jul,
	pages = {371--379},
	publisher = {Association for Computational Linguistics},
	title = {Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings},
	url = {https://aclanthology.org/P19-1036},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1036},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1036}}

@inproceedings{xu-etal-2019-topic,
	abstract = {In the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics. In this paper, we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentence-level and topic-level representations. In particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. Moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors. Experimentation on CDTB, a Chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores.},
	address = {Florence, Italy},
	author = {Xu, Sheng and Li, Peifeng and Kong, Fang and Zhu, Qiaoming and Zhou, Guodong},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1058},
	month = jul,
	pages = {608--618},
	publisher = {Association for Computational Linguistics},
	title = {Topic Tensor Network for Implicit Discourse Relation Recognition in {C}hinese},
	url = {https://aclanthology.org/P19-1058},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1058},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1058}}

@inproceedings{nan-etal-2019-topic,
	abstract = {We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.},
	address = {Florence, Italy},
	author = {Nan, Feng and Ding, Ran and Nallapati, Ramesh and Xiang, Bing},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1640},
	month = jul,
	pages = {6345--6381},
	publisher = {Association for Computational Linguistics},
	title = {Topic Modeling with {W}asserstein Autoencoders},
	url = {https://aclanthology.org/P19-1640},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1640},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1640}}

@inproceedings{zhao-etal-2019-leveraging,
	abstract = {Short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models. To deal with the insufficiency, we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information. Our model can generate more interpretable topics as well as document clusters. We develop an effective Gibbs sampling algorithm favoured by the fully local conjugacy in the model. Extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence.},
	address = {Florence, Italy},
	author = {Zhao, He and Du, Lan and Liu, Guanfeng and Buntine, Wray},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1396},
	month = jul,
	pages = {4042--4049},
	publisher = {Association for Computational Linguistics},
	title = {Leveraging Meta Information in Short Text Aggregation},
	url = {https://aclanthology.org/P19-1396},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1396},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1396}}

@inproceedings{fujita-etal-2019-dataset,
	abstract = {Ranking comments on an online news service is a practically important task for the service provider, and thus there have been many studies on this task. However, most of them considered users{'} positive feedback, such as {``}Like{''}-button clicks, as a quality measure. In this paper, we address directly evaluating the quality of comments on the basis of {``}constructiveness,{''} separately from user feedback. To this end, we create a new dataset including 100K+ Japanese comments with constructiveness scores (C-scores). Our experiments clarify that C-scores are not always related to users{'} positive feedback, and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles.},
	address = {Florence, Italy},
	author = {Fujita, Soichiro and Kobayashi, Hayato and Okumura, Manabu},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1250},
	month = jul,
	pages = {2619--2626},
	publisher = {Association for Computational Linguistics},
	title = {Dataset Creation for Ranking Constructive News Comments},
	url = {https://aclanthology.org/P19-1250},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1250},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1250}}

@inproceedings{frermann-klementiev-2019-inducing,
	abstract = {Automatic summarization is typically treated as a 1-to-1 mapping from document to summary. Documents such as news articles, however, are structured and often cover multiple topics or aspects; and readers may be interested in only some of them. We tackle the task of aspect-based summarization, where, given a document and a target aspect, our models generate a summary centered around the aspect. We induce latent document structure jointly with an abstractive summarization objective, and train our models in a scalable synthetic setup. In addition to improvements in summarization over topic-agnostic baselines, we demonstrate the benefit of the learnt document structure: we show that our models (a) learn to accurately segment documents by aspect; (b) can leverage the structure to produce both abstractive and extractive aspect-based summaries; and (c) that structure is particularly advantageous for summarizing long documents. All results transfer from synthetic training documents to natural news articles from CNN/Daily Mail and RCV1.},
	address = {Florence, Italy},
	author = {Frermann, Lea and Klementiev, Alexandre},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1630},
	month = jul,
	pages = {6263--6273},
	publisher = {Association for Computational Linguistics},
	title = {Inducing Document Structure for Aspect-based Summarization},
	url = {https://aclanthology.org/P19-1630},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1630},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1630}}

@inproceedings{li-etal-2019-semi-supervised,
	abstract = {Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting. Unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. We compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning.},
	address = {Florence, Italy},
	author = {Li, Yitong and Baldwin, Timothy and Cohn, Trevor},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1186},
	month = jul,
	pages = {1923--1934},
	publisher = {Association for Computational Linguistics},
	title = {Semi-supervised Stochastic Multi-Domain Learning using Variational Inference},
	url = {https://aclanthology.org/P19-1186},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1186},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1186}}

@inproceedings{fujinuma-etal-2019-resource,
	abstract = {Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. An important requirement for many downstream tasks is that word similarity should be independent of language{---}i.e., word vectors within one language should not be more similar to each other than to words in another language. We measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings.},
	address = {Florence, Italy},
	author = {Fujinuma, Yoshinari and Boyd-Graber, Jordan and Paul, Michael J.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1489},
	month = jul,
	pages = {4952--4962},
	publisher = {Association for Computational Linguistics},
	title = {A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity},
	url = {https://aclanthology.org/P19-1489},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1489},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1489}}

@inproceedings{maiti-vucetic-2019-spatial,
	abstract = {Spatial aggregation refers to merging of documents created at the same spatial location. We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. Our experiments on synthetic data confirm this observation. Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents.},
	address = {Florence, Italy},
	author = {Maiti, Aniruddha and Vucetic, Slobodan},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1025},
	month = jul,
	pages = {252--262},
	publisher = {Association for Computational Linguistics},
	title = {Spatial Aggregation Facilitates Discovery of Spatial Topics},
	url = {https://aclanthology.org/P19-1025},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1025},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1025}}

@inproceedings{tay-2019-reviews,
	abstract = {Consumers read online reviews for insights which help them to make decisions. Given the large volumes of reviews, succinct review summaries are important for many applications. Existing research has focused on mining for opinions from only review texts and largely ignores the reviewers. However, reviewers have biases and may write lenient or harsh reviews; they may also have preferences towards some topics over others. Therefore, not all reviews are equal. Ignoring the biases in reviews can generate misleading summaries. We aim for summarization of reviews to include balanced opinions from reviewers of different biases and preferences. We propose to model reviewer biases from their review texts and rating distributions, and learn a bias-aware opinion representation. We further devise an approach for balanced opinion summarization of reviews using our bias-aware opinion representation.},
	address = {Florence, Italy},
	author = {Tay, Wenyi},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-2005},
	month = jul,
	pages = {34--42},
	publisher = {Association for Computational Linguistics},
	title = {Not All Reviews Are Equal: Towards Addressing Reviewer Biases for Opinion Summarization},
	url = {https://aclanthology.org/P19-2005},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-2005},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-2005}}

@inproceedings{dai-song-2019-neural,
	abstract = {Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. To alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. The mined rules are then applied to label a large amount of auxiliary data. Finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art.},
	address = {Florence, Italy},
	author = {Dai, Hongliang and Song, Yangqiu},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1520},
	month = jul,
	pages = {5268--5277},
	publisher = {Association for Computational Linguistics},
	title = {Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision},
	url = {https://aclanthology.org/P19-1520},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1520},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1520}}

@inproceedings{hu-etal-2019-open,
	abstract = {Open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence. Prior work typically formulates this task as a sequence tagging problem. However, such formulation suffers from problems such as huge search space and sentiment inconsistency. To address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations. We further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models. Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline. Moreover, we find that the pipeline model achieves the best performance compared with the other two models.},
	address = {Florence, Italy},
	author = {Hu, Minghao and Peng, Yuxing and Huang, Zhen and Li, Dongsheng and Lv, Yiwei},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1051},
	month = jul,
	pages = {537--546},
	publisher = {Association for Computational Linguistics},
	title = {Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification},
	url = {https://aclanthology.org/P19-1051},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1051},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1051}}

@inproceedings{yang-etal-2019-cross,
	abstract = {Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on textual content. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task: cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a large-scale dataset for this task and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed model can achieve better performance than competitive baselines.},
	address = {Florence, Italy},
	author = {Yang, Pengcheng and Zhang, Zhihan and Luo, Fuli and Li, Lei and Huang, Chengyang and Sun, Xu},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1257},
	month = jul,
	pages = {2680--2686},
	publisher = {Association for Computational Linguistics},
	title = {Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information},
	url = {https://aclanthology.org/P19-1257},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1257},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1257}}

@inproceedings{gururangan-etal-2019-variational,
	abstract = {We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.},
	address = {Florence, Italy},
	author = {Gururangan, Suchin and Dang, Tam and Card, Dallas and Smith, Noah A.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1590},
	month = jul,
	pages = {5880--5894},
	publisher = {Association for Computational Linguistics},
	title = {Variational Pretraining for Semi-supervised Text Classification},
	url = {https://aclanthology.org/P19-1590},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1590},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1590}}

@inproceedings{vempala-preotiuc-pietro-2019-categorizing,
	abstract = {Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality. We show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. Further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. These methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate.},
	address = {Florence, Italy},
	author = {Vempala, Alakananda and Preo{\c{t}}iuc-Pietro, Daniel},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1272},
	month = jul,
	pages = {2830--2840},
	publisher = {Association for Computational Linguistics},
	title = {Categorizing and Inferring the Relationship between the Text and Image of {T}witter Posts},
	url = {https://aclanthology.org/P19-1272},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1272},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1272}}

@inproceedings{liu-etal-2019-open,
	abstract = {We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.},
	address = {Florence, Italy},
	author = {Liu, Xiao and Huang, Heyan and Zhang, Yue},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1276},
	month = jul,
	pages = {2860--2871},
	publisher = {Association for Computational Linguistics},
	title = {Open Domain Event Extraction Using Neural Latent Variable Models},
	url = {https://aclanthology.org/P19-1276},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1276},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1276}}

@inproceedings{bhavan-etal-2019-investigating,
	abstract = {Analyzing polarities and sentiments inherent in political speeches and debates poses an important problem today. This experiment aims to address this issue by analyzing publicly-available Hansard transcripts of the debates conducted in the UK Parliament. Our proposed approach, which uses community-based graph information to augment hand-crafted features based on topic modeling and emotion detection on debate transcripts, currently surpasses the benchmark results on the same dataset. Such sentiment classification systems could prove to be of great use in today{'}s politically turbulent times, for public knowledge of politicians{'} stands on various relevant issues proves vital for good governance and citizenship. The experiments also demonstrate that continuous feature representations learned from graphs can improve performance on sentiment classification tasks significantly.},
	address = {Florence, Italy},
	author = {Bhavan, Anjali and Mishra, Rohan and Sinha, Pradyumna Prakhar and Sawhney, Ramit and Shah, Rajiv Ratn},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-2039},
	month = jul,
	pages = {281--287},
	publisher = {Association for Computational Linguistics},
	title = {Investigating Political Herd Mentality: A Community Sentiment Based Approach},
	url = {https://aclanthology.org/P19-2039},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-2039},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-2039}}

@inproceedings{panigrahi-etal-2019-word2sense,
	abstract = {We present an unsupervised method to generate Word2Sense word embeddings that are interpretable {---} each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other word embeddings generated by unsupervised methods. Across tasks such as word similarity, entailment, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as prior art.},
	address = {Florence, Italy},
	author = {Panigrahi, Abhishek and Simhadri, Harsha Vardhan and Bhattacharyya, Chiranjib},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1570},
	month = jul,
	pages = {5692--5705},
	publisher = {Association for Computational Linguistics},
	title = {{W}ord2{S}ense: Sparse Interpretable Word Embeddings},
	url = {https://aclanthology.org/P19-1570},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1570},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1570}}

@inproceedings{sap-etal-2019-risk,
	abstract = {We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.},
	address = {Florence, Italy},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/P19-1163},
	month = jul,
	pages = {1668--1678},
	publisher = {Association for Computational Linguistics},
	title = {The Risk of Racial Bias in Hate Speech Detection},
	url = {https://aclanthology.org/P19-1163},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1163},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1163}}

@misc{https://doi.org/10.48550/arxiv.2006.00998,
	author = {Pavlopoulos, John and Sorensen, Jeffrey and Dixon, Lucas and Thain, Nithum and Androutsopoulos, Ion},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.48550/ARXIV.2006.00998},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {Association for Computational Linguistics},
	title = {Toxicity Detection: Does Context Really Matter?},
	url = {https://arxiv.org/abs/2006.00998},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2006.00998},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.1906.04687}}

@inproceedings{veselova-vorontsov-2020-topic,
	abstract = {This article proposes a new approach for building topic models on unbalanced collections in topic modelling, based on the existing methods and our experiments with such methods. Real-world data collections contain topics in various proportions, and often documents of the relatively small theme become distributed all over the larger topics instead of being grouped into one topic. To address this issue, we design a new regularizer for Theta and Phi matrices in probabilistic Latent Semantic Analysis (pLSA) model. We make sure this regularizer increases the quality of topic models, trained on unbalanced collections. Besides, we conceptually support this regularizer by our experiments.},
	address = {Online},
	author = {Veselova, Eugeniia and Vorontsov, Konstantin},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-srw.9},
	month = jul,
	pages = {59--65},
	publisher = {Association for Computational Linguistics},
	title = {Topic Balancing with Additive Regularization of Topic Models},
	url = {https://aclanthology.org/2020.acl-srw.9},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-srw.9},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-srw.9}}

@inproceedings{peinelt-etal-2020-tbert,
	abstract = {Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.},
	address = {Online},
	author = {Peinelt, Nicole and Nguyen, Dong and Liakata, Maria},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.630},
	month = jul,
	pages = {7047--7055},
	publisher = {Association for Computational Linguistics},
	title = {t{BERT}: Topic Models and {BERT} Joining Forces for Semantic Similarity Detection},
	url = {https://aclanthology.org/2020.acl-main.630},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.630},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.630}}

@inproceedings{wang-etal-2020-neural-topic,
	abstract = {Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6{\%} is observed in accuracy.},
	address = {Online},
	author = {Wang, Rui and Hu, Xuemeng and Zhou, Deyu and He, Yulan and Xiong, Yuxuan and Ye, Chenchen and Xu, Haiyang},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.32},
	month = jul,
	pages = {340--350},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Modeling with Bidirectional Adversarial Training},
	url = {https://aclanthology.org/2020.acl-main.32},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.32},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.32}}

@inproceedings{viegas-etal-2020-cluhtm,
	abstract = {Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the {``}ideal{''} number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM{'}s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information {--} when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the {``}shape{''} the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500{\%} over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.},
	address = {Online},
	author = {Viegas, Felipe and Cunha, Washington and Gomes, Christian and Pereira, Ant{\^o}nio and Rocha, Leonardo and Goncalves, Marcos},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.724},
	month = jul,
	pages = {8138--8150},
	publisher = {Association for Computational Linguistics},
	title = {{C}lu{HTM} - Semantic Hierarchical Topic Modeling based on {C}lu{W}ords},
	url = {https://aclanthology.org/2020.acl-main.724},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.724},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.724}}

@inproceedings{wu-etal-2020-neural,
	abstract = {Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.},
	address = {Online},
	author = {Wu, Jiemin and Rao, Yanghui and Zhang, Zusheng and Xie, Haoran and Li, Qing and Wang, Fu Lee and Chen, Ziye},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.548},
	month = jul,
	pages = {6159--6169},
	publisher = {Association for Computational Linguistics},
	title = {Neural Mixed Counting Models for Dispersed Topic Discovery},
	url = {https://aclanthology.org/2020.acl-main.548},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.548},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.548}}

@inproceedings{vafa-etal-2020-text,
	abstract = {Ideal point models analyze lawmakers{'} votes to quantify their political positions, or ideal points. But votes are not the only way to express a political position. Lawmakers also give speeches, release press statements, and post tweets. In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets. Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points. One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors. To this end, we use it to study tweets from the 2020 Democratic presidential candidates. Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.},
	address = {Online},
	author = {Vafa, Keyon and Naidu, Suresh and Blei, David},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.475},
	month = jul,
	pages = {5345--5357},
	publisher = {Association for Computational Linguistics},
	title = {Text-Based Ideal Points},
	url = {https://aclanthology.org/2020.acl-main.475},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.475},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.475}}

@inproceedings{keith-etal-2020-text,
	abstract = {Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual{'}s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.},
	address = {Online},
	author = {Keith, Katherine and Jensen, David and O{'}Connor, Brendan},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.474},
	month = jul,
	pages = {5332--5344},
	publisher = {Association for Computational Linguistics},
	title = {Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates},
	url = {https://aclanthology.org/2020.acl-main.474},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.474},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.474}}

@inproceedings{jia-etal-2020-mitigating,
	abstract = {Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models{'} top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.},
	address = {Online},
	author = {Jia, Shengyu and Meng, Tao and Zhao, Jieyu and Chang, Kai-Wei},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.264},
	month = jul,
	pages = {2936--2942},
	publisher = {Association for Computational Linguistics},
	title = {Mitigating Gender Bias Amplification in Distribution by Posterior Regularization},
	url = {https://aclanthology.org/2020.acl-main.264},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.264},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.264}}

@inproceedings{zhu-etal-2020-batch,
	abstract = {Variational Autoencoder (VAE) is widely used as a generative model to approximate a model{'}s posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as {``}posterior collapse{''}. Previous approaches consider the Kullback{--}Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL{'}s distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior{'}s parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.},
	address = {Online},
	author = {Zhu, Qile and Bi, Wei and Liu, Xiaojiang and Ma, Xiyao and Li, Xiaolin and Wu, Dapeng},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.235},
	month = jul,
	pages = {2636--2649},
	publisher = {Association for Computational Linguistics},
	title = {A Batch Normalized Inference Network Keeps the {KL} Vanishing Away},
	url = {https://aclanthology.org/2020.acl-main.235},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.235},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.235}}

@inproceedings{zang-etal-2020-word,
	abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
	address = {Online},
	author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2020.acl-main.540},
	month = jul,
	pages = {6066--6080},
	publisher = {Association for Computational Linguistics},
	title = {Word-level Textual Adversarial Attacking as Combinatorial Optimization},
	url = {https://aclanthology.org/2020.acl-main.540},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.540},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.540}}

@misc{https://doi.org/10.48550/arxiv.2105.14189,
	author = {Wang, Lingzhi and Zeng, Xingshan and Wong, Kam-Fai},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.48550/ARXIV.2105.14189},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {Association for Computational Linguistics},
	title = {Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations},
	url = {https://arxiv.org/abs/2105.14189},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2105.14189}}

@misc{https://doi.org/10.48550/arxiv.2106.04408,
	author = {Qi, Tao and Wu, Fangzhao and Wu, Chuhan and Yang, Peiru and Yu, Yang and Xie, Xing and Huang, Yongfeng},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.48550/ARXIV.2106.04408},
	keywords = {Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {Association for Computational Linguistics},
	title = {HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation},
	url = {https://arxiv.org/abs/2106.04408},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2106.04408}}

@misc{https://doi.org/10.48550/arxiv.2106.01071,
	author = {Zhu, Lixing and Pergola, Gabriele and Gui, Lin and Zhou, Deyu and He, Yulan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.48550/ARXIV.2106.01071},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {Association for Computational Linguistics},
	title = {Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection},
	url = {https://arxiv.org/abs/2106.01071},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2106.01071}}

@inproceedings{mou-etal-2021-align,
	abstract = {Ideology of legislators is typically estimated by ideal point models from historical records of votes. It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior. However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories. In order to mitigate these two problems, we explore to incorporate both voting behavior and public statements on Twitter to jointly model legislators. In addition, we propose a novel task, namely hashtag usage prediction to model the ideology of legislators on Twitter. In practice, we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage. Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction. Further analysis further demonstrates that legislator representation we learned captures nuances in statements.},
	address = {Online},
	author = {Mou, Xinyi and Wei, Zhongyu and Chen, Lei and Ning, Shangyi and He, Yancheng and Jiang, Changjian and Huang, Xuanjing},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2021.acl-long.99},
	month = aug,
	pages = {1236--1246},
	publisher = {Association for Computational Linguistics},
	title = {Align Voting Behavior with Public Statements for Legislator Representation Learning},
	url = {https://aclanthology.org/2021.acl-long.99},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.99},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.99}}

@inproceedings{liu-etal-2021-element,
	abstract = {Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction. Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type. To address this issue, we conduct \textit{Element Intervention}, which intervene on the context and entities respectively to obtain the underlying causal effects of them. We also provide two specific implementations of the interventions based on entity ranking and context contrasting. Experimental results on unsupervised relation extraction datasets show our method to outperform previous state-of-the-art methods and is robust across different datasets.},
	address = {Online},
	author = {Liu, Fangchao and Yan, Lingyong and Lin, Hongyu and Han, Xianpei and Sun, Le},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2021.acl-long.361},
	month = aug,
	pages = {4683--4693},
	publisher = {Association for Computational Linguistics},
	title = {Element Intervention for Open Relation Extraction},
	url = {https://aclanthology.org/2021.acl-long.361},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.361},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.361}}

@inproceedings{duan-etal-2021-enslm,
	abstract = {Natural language processing (NLP) often faces the problem of data diversity such as different domains, themes, styles, and so on. Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples. To solve this problem, we firstly propose an autoencoding topic model with a mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity. Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation. Specifically, EnsLM contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters. As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. EnsLM can be trained jointly with mATM with a flexible LM backbone. We evaluate the effectiveness of both mATM and EnsLM on various tasks.},
	address = {Online},
	author = {Duan, Zhibin and Zhang, Hao and Wang, Chaojie and Wang, Zhengjue and Chen, Bo and Zhou, Mingyuan},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2021.acl-long.230},
	month = aug,
	pages = {2954--2967},
	publisher = {Association for Computational Linguistics},
	title = {{E}ns{LM}: Ensemble Language Model for Data Diversity by Semantic Clustering},
	url = {https://aclanthology.org/2021.acl-long.230},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.230},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.230}}

@inproceedings{bilal-etal-2021-evaluation,
	abstract = {Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows.},
	address = {Online},
	author = {Bilal, Iman Munire and Wang, Bo and Liakata, Maria and Procter, Rob and Tsakalidis, Adam},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2021.acl-long.530},
	month = aug,
	pages = {6800--6814},
	publisher = {Association for Computational Linguistics},
	title = {Evaluation of Thematic Coherence in Microblogs},
	url = {https://aclanthology.org/2021.acl-long.530},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.530},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.530}}

@misc{https://doi.org/10.48550/arxiv.2202.13469,
	author = {Li, Jiacheng and Shang, Jingbo and McAuley, Julian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.48550/ARXIV.2202.13469},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {Association for Computational Linguistics},
	title = {UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining},
	url = {https://arxiv.org/abs/2202.13469},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2202.13469}}

@inproceedings{2022_Broscheit,
	author = {Samuel Broscheit and Quynh Do and Judith Gaspers},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2022.acl-long.139},
	publisher = {Association for Computational Linguistics},
	title = {Distributionally Robust Finetuning {BERT} for Covariate Drift in Spoken Language Understanding},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.139},
	year = 2022,
	bdsk-url-1 = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.139},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.acl-long.139}}

@inproceedings{Jin_2022,
	author = {Mali Jin and Daniel Preotiuc-Pietro and A. Seza Do{\u{g}}ru{\"o}z and Nikolaos Aletras},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2022.acl-long.273},
	publisher = {Association for Computational Linguistics},
	title = {Automatic Identification and Classification of Bragging in Social Media},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.273},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.273}}

@inproceedings{Gabriel_2022,
	author = {Saadia Gabriel and Skyler Hallinan and Maarten Sap and Pemi Nguyen and Franziska Roesner and Eunsol Choi and Yejin Choi},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2022.acl-long.222},
	publisher = {Association for Computational Linguistics},
	title = {Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.222},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.222}}

@inproceedings{Ribeiro_2022,
	author = {Marco Tulio Ribeiro and Scott Lundberg},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2022.acl-long.230},
	publisher = {Association for Computational Linguistics},
	title = {Adaptive Testing and Debugging of {NLP} Models},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.230},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.230}}

@inproceedings{Malkin_2022,
	author = {Nikolay Malkin and Zhen Wang and Nebojsa Jojic},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-10-27 21:28:56 +0200},
	date-modified = {2022-10-27 21:28:56 +0200},
	doi = {10.18653/v1/2022.acl-long.565},
	publisher = {Association for Computational Linguistics},
	title = {Coherence boosting: When your pretrained language model is not paying enough attention},
	url = {https://doi.org/10.18653%2Fv1%2F2022.acl-long.565},
	year = 2022,
	bdsk-url-1 = {http://dx.doi.org/10.18653/v1/2022.acl-long.565}}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>ACL</string>
		<key>keys</key>
		<string>lund-etal-2017-tandem,lau-etal-2017-topically,shu-etal-2017-lifelong,zhan-etal-2017-network,he-etal-2017-unsupervised,amoualian-etal-2017-topical,malmasi-etal-2017-unsupervised,zhang-etal-2017-exploiting,ji-smith-2017-neural,cagan-etal-2017-data,johnson-etal-2017-leveraging,card-etal-2018-neural,huang-2018-phrasectm,peng-etal-2018-neural,hancock-etal-2018-training,uva-etal-2018-injecting,fabbri-etal-2018-tutorialbank,li-etal-2018-deep,xu-etal-2018-double,wu-etal-2018-question,shen-etal-2018-nash,ustalov-etal-2018-unsupervised-semantic,dong-de-melo-2018-helping,blodgett-etal-2018-twitter,https://doi.org/10.48550/arxiv.1906.04687,lund-etal-2019-automatic,kumar-etal-2019-didnt,haj-yahia-etal-2019-towards,xu-etal-2019-topic,nan-etal-2019-topic,zhao-etal-2019-leveraging,fujita-etal-2019-dataset,frermann-klementiev-2019-inducing,li-etal-2019-semi-supervised,fujinuma-etal-2019-resource,maiti-vucetic-2019-spatial,tay-2019-reviews,dai-song-2019-neural,hu-etal-2019-open,yang-etal-2019-cross,gururangan-etal-2019-variational,vempala-preotiuc-pietro-2019-categorizing,liu-etal-2019-open,bhavan-etal-2019-investigating,panigrahi-etal-2019-word2sense,sap-etal-2019-risk,https://doi.org/10.48550/arxiv.2006.00998,veselova-vorontsov-2020-topic,peinelt-etal-2020-tbert,wang-etal-2020-neural-topic,viegas-etal-2020-cluhtm,wu-etal-2020-neural,vafa-etal-2020-text,keith-etal-2020-text,jia-etal-2020-mitigating,zhu-etal-2020-batch,zang-etal-2020-word,https://doi.org/10.48550/arxiv.2105.14189,https://doi.org/10.48550/arxiv.2106.04408,https://doi.org/10.48550/arxiv.2106.01071,mou-etal-2021-align,liu-etal-2021-element,duan-etal-2021-enslm,bilal-etal-2021-evaluation,https://doi.org/10.48550/arxiv.2202.13469,2022_Broscheit,Jin_2022,Gabriel_2022,Ribeiro_2022,Malkin_2022</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>CIKM</string>
		<key>keys</key>
		<string>10.1145/3132847.3132942,10.1145/3132847.3133181,10.1145/3132847.3133162,10.1145/3132847.3133145,10.1145/3132847.3133109,10.1145/3132847.3133011,10.1145/3132847.3132864,10.1145/3132847.3132968,10.1145/3132847.3132988,10.1145/3132847.3133071,10.1145/3132847.3133063,10.1145/3132847.3133150,10.1145/3132847.3132967,10.1145/3132847.3133123,10.1145/3132847.3132906,10.1145/3132847.3132971,10.1145/3132847.3133007,10.1145/3132847.3132965,10.1145/3132847.3133023,10.1145/3132847.3132963,10.1145/3269206.3271671,10.1145/3269206.3269309,10.1145/3269206.3271737,10.1145/3269206.3272011,10.1145/3269206.3269273,10.1145/3357384.3357941,10.1145/3357384.3357828,10.1145/3357384.3358020,10.1145/3357384.3358048,10.1145/3357384.3358099,10.1145/3357384.3358017,10.1145/3340531.3412050,10.1145/3340531.3412878,10.1145/3340531.3411932,10.1145/3340531.3411933,10.1145/3340531.3411906,10.1145/3340531.3411890,10.1145/3459637.3482398,10.1145/3459637.3482253,10.1145/3459637.3482075,10.1145/3459637.3482450,10.1145/3511808.3557410,10.1145/3511808.3557561,10.1145/3511808.3557329</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>COLING</string>
		<key>keys</key>
		<string>hanselowski-etal-2018-retrospective,you-etal-2018-attribute,yang-etal-2018-aspect,barnes-etal-2018-projecting,gupta-etal-2018-semantic,sari-etal-2018-topic,an-etal-2018-model,li-etal-2018-document,jiang-etal-2018-identifying,hao-paul-2018-learning,li-yang-2018-pseudo,nouri-etal-2020-mining,srinivasa-desikan-etal-2020-comp,pham-le-2020-auto,du-etal-2020-pointing,zhao-etal-2020-improving,bai-etal-2020-pre,nevezhin-etal-2020-topic,li-etal-2020-neural,zhang-etal-2020-topic,shang-etal-2020-speaker,takatsu-etal-2020-sentiment,an-etal-2020-multimodal,keh-etal-2022-pineapple,huynh-etal-2022-vinli,yin-etal-2022-improving,amplayo-etal-2022-attribute,wang-etal-2022-imci,xie-etal-2022-gretel,liu-etal-2022-bert,antypas-etal-2022-twitter,austin-etal-2022-community</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>EACL</string>
		<key>keys</key>
		<string>sari-etal-2017-continuous,bonadiman-etal-2017-effective,muzny-etal-2017-two,brychcin-kral-2017-unsupervised,sato-etal-2017-distributed,ramrakhiyani-etal-2017-measuring,mukherjee-etal-2017-creating,sorodoc-etal-2017-multimodal,sawhney-etal-2021-phase,shen-etal-2021-source,zhou-etal-2021-challenges,saravanakumar-etal-2021-event,zehe-etal-2021-detecting,paul-etal-2021-multi,zhao-etal-2021-adversarial,popa-rebedea-2021-bart</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>ECIR</string>
		<key>keys</key>
		<string>Aletras_2017,Azarbonyad_2017,Fang_2017,Vu_2017,Mourad_2017,Spitz_2018,Bahrainian_2018,Badjatiya_2018,Zhang_2018,Sumikawa_2018,Meladianos_2018,Wang_2019,Bahrainian_2019,Potha_2019,Bi_2019,Fisher_2019,Mullick_2019,Pritsos_2019,Almasian_2019,Bhattacharya_2019,Ghadery_2019,Fard_2020,Brochier_2020,Kovalchuk_2020,Montazeralghaem_2020,Batra_2020,Chelliah_2020,Ishigaki_2020,_ahinu__2021,Jatowt_2021,Kuzi_2021,Lugo_2021,Abazari_Kia_2021,Meng_2021,Bondarenko_2021,Zosa_2022,Valero_2022,Palencia_Olivar_2022,Esuli_2022,Wang_2022</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>ECML PKDD</string>
		<key>keys</key>
		<string>Burkhardt_2017,Melvin_2017,Saha_2017,Barbieri_2017,Wenzel_2017,Lan_2019,Hu_2019,Appel_2019,Lim_2019,Chalapathy_2019,Liu_2019,10.1007/978-3-030-46133-1_33,Ferner_2020,Madrid_2020,Farruque_2020,Audebert_2020,Zhang_2021,Bai_2021,Harada_2021,Khandelwal_2021,Yang_2021</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>EMNLP</string>
		<key>keys</key>
		<string>morales-zhai-2017-identifying,sterckx-etal-2017-break,serban-etal-2017-piecewise,wang-etal-2017-learning-fine,rahimi-etal-2017-continuous,mysore-sathyendra-etal-2017-identifying,wang-zhang-2017-opinion,gui-etal-2017-question,wang-etal-2017-sentiment,yin-etal-2017-document,zhuang-etal-2017-identifying,vilares-he-2017-detecting,mekala-etal-2017-scdv,yang-etal-2017-identifying,yang-etal-2017-adapting,wang-jurgens-2018-going,chen-etal-2018-iterative,baheti-etal-2018-generating,qi-etal-2018-cross,luo-etal-2018-extra,liu-etal-2018-automatic,srivatsan-etal-2018-modeling,yang-etal-2018-interpretable,lund-etal-2018-labeled,zeng-etal-2018-topic,huang-etal-2018-siamese,desai-etal-2019-adaptive,tang-etal-2019-topic,yin-etal-2019-benchmarking,zhang-singh-2019-leveraging,chaudhary-etal-2019-little,liao-etal-2019-coupling,karamanolakis-etal-2019-leveraging,linmei-etal-2019-heterogeneous,caragea-etal-2019-myth,ruckle-etal-2019-neural,pethe-skiena-2019-trumpiest,balashankar-etal-2019-identifying,cao-etal-2019-latent,zhou-jurgens-2020-condolence,sawhney-etal-2020-time,pei-jurgens-2020-quantifying,field-tsvetkov-2020-unsupervised,wang-etal-2020-continuity,roy-goldwasser-2020-weakly,zhao-chang-2020-logan,bommasani-cardie-2020-intrinsic,hoyle-etal-2020-improving,meng-etal-2020-text,hu-etal-2020-neural,ousidhoum-etal-2020-comparative,liu-etal-2020-cross-lingual-spoken,gao-gormley-2020-training,tian-etal-2020-learning,spell-etal-2020-embedding,jin-etal-2021-neural,ye-etal-2021-beyond,zhang-etal-2021-howyoutagtweets,wang-etal-2021-phrase,kang-etal-2021-leveraging,liu-etal-2021-leveraging,situ-etal-2021-lifelong,saxon-etal-2021-modeling,manchanda-karypis-2021-evaluating,yu-etal-2021-exophoric,lin-etal-2021-csds,milbauer-etal-2021-aligning,li-etal-2021-detecting,dodge-etal-2021-documenting</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>KDD</string>
		<key>keys</key>
		<string>10.1145/3097983.3098067,10.1145/3097983.3098122,10.1145/3097983.3098074,10.1145/3097983.3098017,10.1145/3097983.3098110,10.1145/3097983.3098068,10.1145/3219819.3219929,10.1145/3219819.3220064,10.1145/3219819.3219964,10.1145/3219819.3219827,10.1145/3292500.3330706,10.1145/3292500.3330737,10.1145/3292500.3330698,10.1145/3292500.3330924,10.1145/3292500.3330721,10.1145/3394486.3403242,10.1145/3394486.3403244,10.1145/3394486.3403179,10.1145/3447548.3467410,10.1145/3447548.3467426,10.1145/3447548.3467390,10.1145/3447548.3467302,10.1145/3534678.3542675,10.1145/3534678.3539310,10.1145/3534678.3539107</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>NAACL</string>
		<key>keys</key>
		<string>naacl-2018-2018,habernal-etal-2018-name,wu-tsai-2018-cross,chambers-etal-2018-detecting,ambroselli-etal-2018-prediction,hessel-etal-2018-quantifying,benton-dredze-2018-deep,gupta-etal-2018-deep-temporal,jin-etal-2018-combining,demszky-etal-2019-analyzing,nguyen-etal-2019-capsule,wang-etal-2019-microblog,zeng-etal-2019-variational,chitkara-etal-2019-topic,hao-paul-2019-analyzing,subramanian-etal-2021-spanpredict,ji-etal-2021-discrete,schiller-etal-2021-aspect,cui-hu-2021-sliding,he-etal-2021-model,zeng-nie-2021-simple,zhan-etal-2021-augmenting,khanehzar-etal-2021-framing,iida-etal-2021-tabbie,shen-etal-2021-taxoclass,sun-etal-2021-tita,mueller-dredze-2021-fine,xie-etal-2021-inductive,gupta-etal-2021-multi,pergola-etal-2021-disentangled,doogan-buntine-2021-topic,zhu-etal-2022-disentangled,kulkarni-etal-2022-ctm,li-etal-2022-corwa,bhattacharjee-etal-2022-users,spangher-etal-2022-newsedits,zhang-etal-2022-seed,sircar-etal-2022-distantly,liu-etal-2022-hiure,voigt-etal-2022-survey,zhang-etal-2022-kcd</string>
	</dict>
	<dict>
		<key>group name</key>
		<string>SIGIR</string>
		<key>keys</key>
		<string>10.1145/3077136.3080772,10.1145/3077136.3080781,10.1145/3077136.3084138,10.1145/3077136.3080703,10.1145/3209978.3210046,10.1145/3209978.3209984,10.1145/3209978.3209985,10.1145/3209978.3210054,10.1145/3331184.3331287,10.1145/3331184.3331367,10.1145/3331184.3331228,10.1145/3331184.3338062,10.1145/3331184.3331216,10.1145/3397271.3401168,10.1145/3397271.3401179,10.1145/3397271.3401185,10.1145/3397271.3401269,10.1145/3397271.3401128,10.1145/3397271.3401150,10.1145/3397271.3401047,10.1145/3404835.3463080,10.1145/3404835.3462975,10.1145/3404835.3462938,10.1145/3404835.3463100,10.1145/3404835.3462798,10.1145/3477495.3531877,10.1145/3477495.3531812,10.1145/3477495.3531817,10.1145/3477495.3531784,10.1145/3477495.3532084</string>
	</dict>
</array>
</plist>
}}
