@article{alokaili2020a,
  title = {Acknowledgment This research has been partially supported by EEA Grants 2014-2021 and UEFISCDI, under project contract EEA-RO-NO-2018-0496},
  author = {Alokaili, Nikolaos Aletras and Stevenson, Mark},
  date = {2020},
  note = {arXiv preprint arXiv:2006.00127.},
  arxiv = {2006.00127},
  journal = {References Areej}
}
@misc{bahdanau2014a,
  title = {Figure 4: Average rating of new labels in top-k},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2014},
  note = {arXiv preprint}
}
@inproceedings{bhatia2016a,
  title = {rating between 2.0 and 2.5. Table 2 showcases a arXiv:1409.0473. few samples of original labels. The results highlight that generative},
  author = {Bhatia, B.A.R.T.-T.L.Shraey and Lau, Jey Han and Baldwin, Timothy},
  date = {2016},
  pages = {953–963,},
  note = {Osas their number increases. There is also no clear aka, Japan. The COLING 2016 Organizing Committee. winner between the supervised and unsupervised versions of the proposed models, as they have sim- David M Blei, Andrew Y Ng, and Michael I Jordan.},
  booktitle = {Proceedings of COLING 2016, methods when considering the top 1-2 labels. How- the 26th International Conference on Computational ever, the quality of the generated labels degrades Linguistics: Technical Papers}
}
@misc{trends-a,
  author = {trends},
  title = {At the same time, the novelty tends 2003. Latent Dirichlet Allocation. Journal of mato improve slightly with the number of considered chine Learning research, 3(Jan):993–1022. labels. On average, 40% of the labels were never Scott Deerwester, Susan T Dumais, George W Furprovided when fine-tuning the models},
  note = {While nov- nas, Thomas K Landauer, and Richard Harshman.}
}
@inproceedings{keskar1990a,
  title = {elty is an important feature for BART-TL, it can},
  date = {1990},
  author = {Keskar and Gourru, Antoine and Velcin, Julien and Roche, Mathieu and Bach and Blei, David M.},
  note = {arXiv preprint arXiv:1810.04805. amount of topics and are outperformed on the rest.},
  pages = {352–363},
  publisher = {Springer},
  arxiv = {1810.04805},
  booktitle = {International Conference on Applica-6 Conclusion tions of Natural Language to Information Systems}
}
@article{lafferty2010a,
  title = {erative deep learning strategy},
  date = {2010},
  editor = {J. and Taylor, R.S.Zemel and Culotta, A.},
  author = {Lafferty, D. and Williams, C.K.I. and J.},
  pages = {856–864},
  journal = {Curran Associates, Inc. have varying quality compared NETL, BART-TL Thomas Hofmann},
  type = {While current results 23,}
}
@inproceedings{jarvelin2002a,
  author = {Jarvelin, Kalervo and Kekalainen, Jaana and Sorodoc, Cu- Ionut and Lau, Jey Han and Aletras, Nikolaos and Baldwin, Timothy and Xiong, Caiming and Socher, Richard and Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Paisley and Blei, David},
  date = {2002},
  title = {Multimodal topic labelling},
  volume = {2},
  pages = {701–},
  editor = {Keskar, Shirish and McCann, Bryan and Varshney, Lav R. and Uszkoreit, Llion Jones and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  note = {arXiv preprint arXiv:1909.05858.},
  arxiv = {1909.05858},
  series = {Short Papers},
  booktitle = {ACM Transactions on Information Systems (TOIS), In Proceedings of the 15th Conference of the Euro-20(4):422–446. pean Chapter of the Association for Computational Linguistics}
}
@inproceedings{lau2011a,
  author = {Lau, Jey Han and Grieser, Karl and Newman, David and Baldwin, Timo-Online Variational Inference},
  date = {2011},
  title = {Automatic Labelling of Topic Dirichlet Process},
  volume = {15},
  pages = {1536–1545,},
  publisher = {Fort Laud-Portland},
  note = {PMLR. tional Linguistics.},
  booktitle = {Proceedings of the Fourteenth Models”. In Proceedings of the 49th Annual Meet-International Conference on Artificial Intelligence ing of the Association for Computational Linguistics: and Statistics},
  address = {Oregon, USA}
}
@inproceedings{le2014a,
  author = {Le, Quoc and Mikolov, Tomas},
  date = {2014},
  title = {Distributed repre- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q sentations of sentences and documents},
  pages = {1188–},
  note = {uating text generation with bert. arXiv preprint 1196. arXiv:1904.09675.},
  arxiv = {1904.09675},
  booktitle = {Interna- Weinberger, and Yoav Artzi. 2019. Bertscore: Evaltional conference on machine learning}
}
@misc{lewis2019a,
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019},
  note = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.},
  arxiv = {1910.13461}
}
@inproceedings{liu2019a,
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin and Mei, Qiaozhu and Shen, Xuehua and Zhai, ChengXiang and Mikolov, A.C.M.Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  date = {2019},
  title = {Roberta: A Robustly Optimized BERT Pretraining Approach},
  note = {arXiv preprint arXiv:1907.11692.},
  pages = {490–499,},
  publisher = {Sergey Brin, Rajeev Motwani, and Terry Winograd},
  arxiv = {1907.11692},
  booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’07},
  type = {Technical report,},
  address = {New York, NY, USA}
}
@misc{radford2019a,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  title = {Language models are unsupervised multitask learners}
}
@inproceedings{roder2015a,
  author = {Roder, Michael and Both, Andreas and Hinneburg, Alexander},
  date = {2015},
  title = {Exploring the space of topic coherence measures},
  pages = {399–408},
  booktitle = {Proceedings of the eighth ACM international conference on Web search and data mining}
}
