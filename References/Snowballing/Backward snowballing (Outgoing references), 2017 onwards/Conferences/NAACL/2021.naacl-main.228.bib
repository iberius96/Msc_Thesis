%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 17:52:31 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@incollection{Masada_2018,
	author = {Tomonari Masada and Atsuhiro Takasu},
	booktitle = {Advanced Data Mining and Applications},
	date-added = {2022-11-17 17:52:27 +0100},
	date-modified = {2022-11-17 17:52:27 +0100},
	doi = {10.1007/978-3-030-05090-0_25},
	pages = {292--302},
	publisher = {Springer International Publishing},
	title = {Adversarial Learning for Topic Models},
	url = {https://doi.org/10.1007%2F978-3-030-05090-0_25},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-05090-0_25},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-05090-0_25}}

@misc{https://doi.org/10.48550/arxiv.1702.08139,
	author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:52:10 +0100},
	date-modified = {2022-11-17 17:52:10 +0100},
	doi = {10.48550/ARXIV.1702.08139},
	keywords = {Neural and Evolutionary Computing (cs.NE), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Improved Variational Autoencoders for Text Modeling using Dilated Convolutions},
	url = {https://arxiv.org/abs/1702.08139},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1702.08139},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1702.08139}}

@misc{https://doi.org/10.48550/arxiv.1603.02514,
	author = {Xu, Weidi and Sun, Haoze and Deng, Chao and Tan, Ying},
	copyright = {Creative Commons Zero v1.0 Universal},
	date-added = {2022-11-17 17:51:59 +0100},
	date-modified = {2022-11-17 17:51:59 +0100},
	doi = {10.48550/ARXIV.1603.02514},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Variational Autoencoders for Semi-supervised Text Classification},
	url = {https://arxiv.org/abs/1603.02514},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1603.02514},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1603.02514}}

@article{https://doi.org/10.48550/arxiv.1811.00265,
	author = {Wang, Rui and Zhou, Deyu and He, Yulan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:51:52 +0100},
	date-modified = {2022-11-17 17:51:52 +0100},
	doi = {10.48550/ARXIV.1811.00265},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {ATM:Adversarial-neural Topic Model},
	url = {https://arxiv.org/abs/1811.00265},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1811.00265},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1811.00265}}

@inproceedings{wan-etal-2019-fine,
	abstract = {This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. First, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. Second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. Third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines.},
	address = {Florence, Italy},
	author = {Wan, Mengting and Misra, Rishabh and Nakashole, Ndapa and McAuley, Julian},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 17:51:47 +0100},
	date-modified = {2022-11-17 17:51:47 +0100},
	doi = {10.18653/v1/P19-1248},
	month = jul,
	pages = {2605--2610},
	publisher = {Association for Computational Linguistics},
	title = {Fine-Grained Spoiler Detection from Large-Scale Review Corpora},
	url = {https://aclanthology.org/P19-1248},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1248},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1248}}

@misc{https://doi.org/10.48550/arxiv.1802.09484,
	author = {Thomas, Valentin and Bengio, Emmanuel and Fedus, William and Pondard, Jules and Beaudoin, Philippe and Larochelle, Hugo and Pineau, Joelle and Precup, Doina and Bengio, Yoshua},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:51:42 +0100},
	date-modified = {2022-11-17 17:51:42 +0100},
	doi = {10.48550/ARXIV.1802.09484},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Disentangling the independently controllable factors of variation by interacting with the world},
	url = {https://arxiv.org/abs/1802.09484},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1802.09484},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1802.09484}}

@misc{https://doi.org/10.48550/arxiv.1703.01488,
	author = {Srivastava, Akash and Sutton, Charles},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:51:38 +0100},
	date-modified = {2022-11-17 17:51:38 +0100},
	doi = {10.48550/ARXIV.1703.01488},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/abs/1703.01488},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.01488},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.01488}}

@misc{https://doi.org/10.48550/arxiv.1908.10084,
	author = {Reimers, Nils and Gurevych, Iryna},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	date-added = {2022-11-17 17:51:32 +0100},
	date-modified = {2022-11-17 17:51:32 +0100},
	doi = {10.48550/ARXIV.1908.10084},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	url = {https://arxiv.org/abs/1908.10084},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1908.10084},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1908.10084}}

@misc{https://doi.org/10.48550/arxiv.1911.10180,
	author = {Pergola, Gabriele and He, Yulan and Lowe, David},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:51:28 +0100},
	date-modified = {2022-11-17 17:51:28 +0100},
	doi = {10.48550/ARXIV.1911.10180},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topical Phrase Extraction from Clinical Reports by Incorporating both Local and Global Context},
	url = {https://arxiv.org/abs/1911.10180},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1911.10180},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1911.10180}}

@article{PERGOLA2019102084,
	abstract = {We propose a topic-dependent attention model for sentiment classification and topic extraction. Our model assumes that a global topic embedding is shared across documents and employs an attention mechanism to derive local topic embedding for words and sentences. These are subsequently incorporated in a modified Gated Recurrent Unit (GRU) for sentiment classification and extraction of topics bearing different sentiment polarities. Those topics emerge from the words' local topic embeddings learned by the internal attention of the GRU cells in the context of a multi-task learning framework. In this paper, we present the hierarchical architecture, the new GRU unit and the experiments conducted on users' reviews which demonstrate classification performance on a par with the state-of-the-art methodologies for sentiment classification and topic coherence outperforming the current approaches for supervised topic extraction. In addition, our model is able to extract coherent aspect-sentiment clusters despite using no aspect-level annotations for training.},
	author = {Gabriele Pergola and Lin Gui and Yulan He},
	date-added = {2022-11-17 17:51:23 +0100},
	date-modified = {2022-11-17 17:51:23 +0100},
	doi = {https://doi.org/10.1016/j.ipm.2019.102084},
	issn = {0306-4573},
	journal = {Information Processing & Management},
	keywords = {Sentiment analysis, Neural attention, Topic modeling},
	number = {6},
	pages = {102084},
	title = {TDAM: A topic-dependent attention model for sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457319305461},
	volume = {56},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0306457319305461},
	bdsk-url-2 = {https://doi.org/10.1016/j.ipm.2019.102084}}

@article{https://doi.org/10.48550/arxiv.1904.12347,
	author = {Peng, Xingchao and Huang, Zijun and Sun, Ximeng and Saenko, Kate},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:51:17 +0100},
	date-modified = {2022-11-17 17:51:17 +0100},
	doi = {10.48550/ARXIV.1904.12347},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Domain Agnostic Learning with Disentangled Representations},
	url = {https://arxiv.org/abs/1904.12347},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.12347},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.12347}}

@inproceedings{nan-etal-2019-topic,
	abstract = {We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.},
	address = {Florence, Italy},
	author = {Nan, Feng and Ding, Ran and Nallapati, Ramesh and Xiang, Bing},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 17:51:12 +0100},
	date-modified = {2022-11-17 17:51:12 +0100},
	doi = {10.18653/v1/P19-1640},
	month = jul,
	pages = {6345--6381},
	publisher = {Association for Computational Linguistics},
	title = {Topic Modeling with {W}asserstein Autoencoders},
	url = {https://aclanthology.org/P19-1640},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1640},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1640}}

@misc{https://doi.org/10.48550/arxiv.1706.00359,
	author = {Miao, Yishu and Grefenstette, Edward and Blunsom, Phil},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:51:08 +0100},
	date-modified = {2022-11-17 17:51:08 +0100},
	doi = {10.48550/ARXIV.1706.00359},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Discovering Discrete Latent Topics with Neural Variational Inference},
	url = {https://arxiv.org/abs/1706.00359},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1706.00359},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1706.00359}}

@misc{https://doi.org/10.48550/arxiv.1711.00848,
	author = {Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:50:53 +0100},
	date-modified = {2022-11-17 17:50:53 +0100},
	doi = {10.48550/ARXIV.1711.00848},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Variational Inference of Disentangled Latent Concepts from Unlabeled Observations},
	url = {https://arxiv.org/abs/1711.00848},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1711.00848},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1711.00848}}

@inproceedings{kar-etal-2018-mpst,
	address = {Miyazaki, Japan},
	author = {Kar, Sudipta and Maharjan, Suraj and L{\'o}pez-Monroy, A. Pastor and Solorio, Thamar},
	booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
	date-added = {2022-11-17 17:50:48 +0100},
	date-modified = {2022-11-17 17:50:48 +0100},
	month = may,
	publisher = {European Language Resources Association (ELRA)},
	title = {{MPST}: A Corpus of Movie Plot Synopses with Tags},
	url = {https://aclanthology.org/L18-1274},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/L18-1274}}

@inproceedings{john-etal-2019-disentangled,
	abstract = {This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.},
	address = {Florence, Italy},
	author = {John, Vineet and Mou, Lili and Bahuleyan, Hareesh and Vechtomova, Olga},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 17:50:44 +0100},
	date-modified = {2022-11-17 17:50:44 +0100},
	doi = {10.18653/v1/P19-1041},
	month = jul,
	pages = {424--434},
	publisher = {Association for Computational Linguistics},
	title = {Disentangled Representation Learning for Non-Parallel Text Style Transfer},
	url = {https://aclanthology.org/P19-1041},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1041},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1041}}

@article{https://doi.org/10.48550/arxiv.1902.02507,
	author = {Hoang, Tai and Le, Huy and Quan, Tho},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:50:39 +0100},
	date-modified = {2022-11-17 17:50:39 +0100},
	doi = {10.48550/ARXIV.1902.02507},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Towards Autoencoding Variational Inference for Aspect-based Opinion Summary},
	url = {https://arxiv.org/abs/1902.02507},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1902.02507},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1902.02507}}

@misc{https://doi.org/10.48550/arxiv.1812.02230,
	author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:50:34 +0100},
	date-modified = {2022-11-17 17:50:34 +0100},
	doi = {10.48550/ARXIV.1812.02230},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Towards a Definition of Disentangled Representations},
	url = {https://arxiv.org/abs/1812.02230},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1812.02230},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1812.02230}}

@inproceedings{higgins2017betavae,
	author = {Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
	booktitle = {International Conference on Learning Representations},
	date-added = {2022-11-17 17:50:28 +0100},
	date-modified = {2022-11-17 17:50:28 +0100},
	title = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	year = {2017},
	bdsk-url-1 = {https://openreview.net/forum?id=Sy2fzU9gl}}

@inproceedings{gui-etal-2019-neural,
	abstract = {In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.},
	address = {Hong Kong, China},
	author = {Gui, Lin and Leng, Jia and Pergola, Gabriele and Zhou, Yu and Xu, Ruifeng and He, Yulan},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-11-17 17:49:42 +0100},
	date-modified = {2022-11-17 17:49:42 +0100},
	doi = {10.18653/v1/D19-1350},
	month = nov,
	pages = {3478--3483},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Model with Reinforcement Learning},
	url = {https://aclanthology.org/D19-1350},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1350},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1350}}

@inproceedings{pmlr-v89-gao19a,
	abstract = {Advances in unsupervised learning enable reconstruction and generation of samples from complex distributions, but this success is marred by the inscrutability of the representations learned. We propose an information-theoretic approach to characterizing disentanglement and dependence in representation learning using multivariate mutual information, also called total correlation. The principle of Total Cor-relation Ex-planation (CorEx) has motivated successful unsupervised learning applications across a variety of domains but under some restrictive assumptions. Here we relax those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly, we find this lower bound is equivalent to the one in variational autoencoders (VAE) under certain conditions. This information-theoretic view of VAE deepens our understanding of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent codes more interpretable through information maximization and enables generation of richer and more realistic samples.},
	author = {Gao, Shuyang and Brekelmans, Rob and Steeg, Greg Ver and Galstyan, Aram},
	booktitle = {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
	date-added = {2022-11-17 17:49:37 +0100},
	date-modified = {2022-11-17 17:49:37 +0100},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	month = {16--18 Apr},
	pages = {1157--1166},
	pdf = {http://proceedings.mlr.press/v89/gao19a/gao19a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Auto-Encoding Total Correlation Explanation},
	url = {https://proceedings.mlr.press/v89/gao19a.html},
	volume = {89},
	year = {2019},
	bdsk-url-1 = {https://proceedings.mlr.press/v89/gao19a.html}}

@misc{https://doi.org/10.48550/arxiv.1804.02086,
	author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and van de Meent, Jan-Willem},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:49:32 +0100},
	date-modified = {2022-11-17 17:49:32 +0100},
	doi = {10.48550/ARXIV.1804.02086},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Structured Disentangled Representations},
	url = {https://arxiv.org/abs/1804.02086},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1804.02086},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1804.02086}}

@inproceedings{ding-etal-2018-coherence,
	abstract = {Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence.},
	address = {Brussels, Belgium},
	author = {Ding, Ran and Nallapati, Ramesh and Xiang, Bing},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-11-17 17:48:42 +0100},
	date-modified = {2022-11-17 17:48:42 +0100},
	doi = {10.18653/v1/D18-1096},
	month = oct # {-} # nov,
	pages = {830--836},
	publisher = {Association for Computational Linguistics},
	title = {Coherence-Aware Neural Topic Modeling},
	url = {https://aclanthology.org/D18-1096},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1096},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1096}}

@misc{https://doi.org/10.48550/arxiv.1611.01702,
	author = {Dieng, Adji B. and Wang, Chong and Gao, Jianfeng and Paisley, John},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:48:34 +0100},
	date-modified = {2022-11-17 17:48:34 +0100},
	doi = {10.48550/ARXIV.1611.01702},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency},
	url = {https://arxiv.org/abs/1611.01702},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1611.01702},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.01702}}

@misc{https://doi.org/10.48550/arxiv.1810.04805,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:48:29 +0100},
	date-modified = {2022-11-17 17:48:29 +0100},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://arxiv.org/abs/1810.04805},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1810.04805},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1810.04805}}

@misc{https://doi.org/10.48550/arxiv.1603.09025,
	author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'e}sar and G{\"u}l{\c c}ehre, {\c C}a{\u g}lar and Courville, Aaron},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:48:23 +0100},
	date-modified = {2022-11-17 17:48:23 +0100},
	doi = {10.48550/ARXIV.1603.09025},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Recurrent Batch Normalization},
	url = {https://arxiv.org/abs/1603.09025},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1603.09025},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1603.09025}}

@article{https://doi.org/10.48550/arxiv.1705.09296,
	author = {Card, Dallas and Tan, Chenhao and Smith, Noah A.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:48:17 +0100},
	date-modified = {2022-11-17 17:48:17 +0100},
	doi = {10.48550/ARXIV.1705.09296},
	keywords = {Machine Learning (stat.ML), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Neural Models for Documents with Metadata},
	url = {https://arxiv.org/abs/1705.09296},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1705.09296},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1705.09296}}

@misc{https://doi.org/10.48550/arxiv.1703.07718,
	author = {Bengio, Emmanuel and Thomas, Valentin and Pineau, Joelle and Precup, Doina and Bengio, Yoshua},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:48:12 +0100},
	date-modified = {2022-11-17 17:48:12 +0100},
	doi = {10.48550/ARXIV.1703.07718},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Independently Controllable Features},
	url = {https://arxiv.org/abs/1703.07718},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.07718},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.07718}}

@misc{https://doi.org/10.48550/arxiv.1706.01350,
	author = {Achille, Alessandro and Soatto, Stefano},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:48:06 +0100},
	date-modified = {2022-11-17 17:48:06 +0100},
	doi = {10.48550/ARXIV.1706.01350},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Emergence of Invariance and Disentanglement in Deep Representations},
	url = {https://arxiv.org/abs/1706.01350},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1706.01350},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1706.01350}}
