%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 18:08:14 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{https://doi.org/10.48550/arxiv.1703.01488,
	author = {Srivastava, Akash and Sutton, Charles},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 18:08:09 +0100},
	date-modified = {2022-11-17 18:08:09 +0100},
	doi = {10.48550/ARXIV.1703.01488},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/abs/1703.01488},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.01488},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.01488}}

@inproceedings{peters-etal-2018-deep,
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	address = {New Orleans, Louisiana},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-11-17 18:08:04 +0100},
	date-modified = {2022-11-17 18:08:04 +0100},
	doi = {10.18653/v1/N18-1202},
	month = jun,
	pages = {2227--2237},
	publisher = {Association for Computational Linguistics},
	title = {Deep Contextualized Word Representations},
	url = {https://aclanthology.org/N18-1202},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1202},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1202}}

@misc{https://doi.org/10.48550/arxiv.1907.11692,
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 18:07:58 +0100},
	date-modified = {2022-11-17 18:07:58 +0100},
	doi = {10.48550/ARXIV.1907.11692},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	url = {https://arxiv.org/abs/1907.11692},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.11692},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.11692}}

@inproceedings{pmlr-v119-gupta20a,
	abstract = {Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task. Code: https://github.com/pgcool/Lifelong-Neural-Topic-Modeling},
	author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Schuetze, Hinrich},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	date-added = {2022-11-17 18:07:52 +0100},
	date-modified = {2022-11-17 18:07:52 +0100},
	editor = {III, Hal Daum{\'e} and Singh, Aarti},
	month = {13--18 Jul},
	pages = {3907--3917},
	pdf = {http://proceedings.mlr.press/v119/gupta20a/gupta20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Neural Topic Modeling with Continual Lifelong Learning},
	url = {https://proceedings.mlr.press/v119/gupta20a.html},
	volume = {119},
	year = {2020},
	bdsk-url-1 = {https://proceedings.mlr.press/v119/gupta20a.html}}

@inproceedings{gupta2018texttovec,
	author = {Pankaj Gupta and Yatin Chaudhary and Florian Buettner and Hinrich Schuetze},
	booktitle = {International Conference on Learning Representations},
	date-added = {2022-11-17 18:07:46 +0100},
	date-modified = {2022-11-17 18:07:46 +0100},
	title = {text{TO}vec: {DEEP} {CONTEXTUALIZED} {NEURAL} {AUTOREGRESSIVE} {TOPIC} {MODELS} {OF} {LANGUAGE} {WITH} {DISTRIBUTED} {COMPOSITIONAL} {PRIOR}},
	url = {https://openreview.net/forum?id=rkgoyn09KQ},
	year = {2019},
	bdsk-url-1 = {https://openreview.net/forum?id=rkgoyn09KQ}}

@misc{https://doi.org/10.48550/arxiv.1809.06709,
	author = {Gupta, Pankaj and Chaudhary, Yatin and Buettner, Florian and Sch{\"u}tze, Hinrich},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 18:07:33 +0100},
	date-modified = {2022-11-17 18:07:33 +0100},
	doi = {10.48550/ARXIV.1809.06709},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Document Informed Neural Autoregressive Topic Models with Distributional Prior},
	url = {https://arxiv.org/abs/1809.06709},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1809.06709},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1809.06709}}

@inproceedings{devlin-etal-2019-bert,
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	address = {Minneapolis, Minnesota},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-11-17 18:07:26 +0100},
	date-modified = {2022-11-17 18:07:26 +0100},
	doi = {10.18653/v1/N19-1423},
	month = jun,
	pages = {4171--4186},
	publisher = {Association for Computational Linguistics},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://aclanthology.org/N19-1423},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1423},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1423}}

@misc{https://doi.org/10.48550/arxiv.1607.04606,
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 18:07:21 +0100},
	date-modified = {2022-11-17 18:07:21 +0100},
	doi = {10.48550/ARXIV.1607.04606},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Enriching Word Vectors with Subword Information},
	url = {https://arxiv.org/abs/1607.04606},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1607.04606},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1607.04606}}

@inproceedings{beltagy-etal-2019-scibert,
	abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	address = {Hong Kong, China},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-11-17 18:07:11 +0100},
	date-modified = {2022-11-17 18:07:11 +0100},
	doi = {10.18653/v1/D19-1371},
	month = nov,
	pages = {3615--3620},
	publisher = {Association for Computational Linguistics},
	title = {{S}ci{BERT}: A Pretrained Language Model for Scientific Text},
	url = {https://aclanthology.org/D19-1371},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1371},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1371}}
