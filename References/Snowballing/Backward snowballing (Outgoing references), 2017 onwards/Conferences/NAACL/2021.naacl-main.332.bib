@article{blei2003a,
  title = {AGnews DocNADE miners, earthquake, explosion, stormed, quake TMN DocNADE tsunami, quake, japan, earthquake, radiation Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and -GVT strike, jackson, kill, earthquake, injures +GVT earthquake, radiation, explosion, wildfire Christian Janvin},
  date = {2003},
  note = {after (+) topic transfer (GVT) from one/more source(s},
  editor = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  volume = {3},
  pages = {993–1022},
  journal = {Journal of Machine Learning Research}
}
@article{piotr2017a,
  citation-number = {20NSshort},
  note = {target) corpus, before (-},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas and Chang, A.A.A.I.Press Jonathan and Boyd-Graber, Jordan L. and Gerrish, Sean and Chong Wang, We and Blei, David M.},
  date = {2017},
  title = {Enriching word vectors with subword information},
  volume = {5},
  director = {sources},
  pages = {288–296},
  journal = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010},
  number = {135–146},
  address = {Atlanta, 4 Conclusion Georgia, USA}
}
@inproceedings{das2015a,
  author = {Das, Rajarshi and Zaheer, Manzil and Dyer, Chris and Miao, Yishu and Yu, Lei and Blunsom, Phil and Tomas Mikolov, JMLRorg and Chen, Kai and Corrado, Greg and Devlin, Jeffrey Jacob and Chang, Ming-Wei and Lee, Kenton and Dean and MN, Tomas Mikolov and Sutskever, Ilya and Chen, Kai and USA, Gregory S.},
  date = {2015},
  title = {Neu-Gaussian lda for topic models with word embed- ral variational inference for text processing},
  volume = {48},
  pages = {1727– 795–804},
  publisher = {Association for Computational Lin},
  editor = {Long and Corrado, Short Pa- and Dean, Jeffrey and Gupta, Pankaj and Chaudhary, Yatin and Buettner, Florian and and},
  booktitle = {In dings. In Proceedings of the 53rd Annual Meet- Proceedings of the 33nd International Conference ing of the Association for Computational Linguistics on Machine Learning, ICML 2016, New York City, and the 7th International Joint Conference on Natu},
  address = {NY, USA}
}
@article{schuetze2019a,
  author = {Schütze, Hinrich},
  date = {2019},
  title = {textTOvec: Deep contex- Dat Quoc Nguyen, Richard Billingsley, Lan Du, and tualized neural autoregressive topic models of lan- Mark Johnson},
  volume = {3},
  note = {OpenReview.net. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep-},
  journal = {Conference on Learning Representa- 313. tions, ICLR 2019},
  number = {299},
  address = {New Orleans, LA, USA}
}
@article{gupta2020a,
  author = {Gupta, Pankaj and Chaudhary, Yatin and A, Thomas and Schütze, Hinrich and Gardner, P.M.L.R. and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title = {Runkler, resentation},
  date = {2020},
  volume = {ICML 2020},
  pages = {13–18},
  publisher = {Association for Computational Linguistics},
  journal = {Proceedings of the 2014 Conference},
  series = {Long Papers}
}
@inproceedings{larochelle2011a,
  author = {Larochelle, Hugo and Murray, Iain and Le, A.C.M.Quoc V. and Mikolov, Tomas},
  date = {2011},
  title = {The neural au- Michael Röder, Andreas Both, and Alexander Hinnebtoregressive distribution estimator},
  volume = {15},
  pages = {29–37},
  note = {Curran Associates, Inc.},
  booktitle = {Proceedings urg. 2015. Exploring the space of topic coherence of the Fourteenth International Conference on Artifi- measures. In Proceedings of the Eighth ACM Intercial Intelligence and Statistics, AISTATS},
  address = {Shanghai, China}
}
@inproceedings{liu2017a,
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Srivastava, Akash and Sutton, Charles and Zettlemoyer, Autoen-Luke and Stoyanov, Veselin},
  date = {2017},
  title = {coding variational inference for topic models},
  note = {tations, ICLR.},
  booktitle = {Roberta: A robustly optimized BERT pretraining ap- 5th International Conference on Learning Represenproach. CoRR}
}
