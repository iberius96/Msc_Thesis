%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 16:02:13 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{https://doi.org/10.48550/arxiv.1706.07276,
	author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:01:58 +0100},
	date-modified = {2022-11-17 16:01:58 +0100},
	doi = {10.48550/ARXIV.1706.07276},
	keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Jointly Learning Word Embeddings and Latent Topics},
	url = {https://arxiv.org/abs/1706.07276},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1706.07276},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1706.07276}}

@inproceedings{salant-berant-2018-contextualized,
	abstract = {Reading a document and extracting an answer to a question about its content has attracted substantial attention recently. While most work has focused on the interaction between the question and the document, in this work we evaluate the importance of context when the question and document are processed independently. We take a standard neural architecture for this task, and show that by providing rich contextualized word representations from a large pre-trained language model as well as allowing the model to choose between context-dependent and context-independent word representations, we can obtain dramatic improvements and reach performance comparable to state-of-the-art on the competitive SQuAD dataset.},
	address = {New Orleans, Louisiana},
	author = {Salant, Shimi and Berant, Jonathan},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	date-added = {2022-11-17 16:01:53 +0100},
	date-modified = {2022-11-17 16:01:53 +0100},
	doi = {10.18653/v1/N18-2088},
	month = jun,
	pages = {554--559},
	publisher = {Association for Computational Linguistics},
	title = {Contextualized Word Representations for Reading Comprehension},
	url = {https://aclanthology.org/N18-2088},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-2088},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-2088}}

@misc{https://doi.org/10.48550/arxiv.1802.05365,
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:01:47 +0100},
	date-modified = {2022-11-17 16:01:47 +0100},
	doi = {10.48550/ARXIV.1802.05365},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Deep contextualized word representations},
	url = {https://arxiv.org/abs/1802.05365},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1802.05365},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1802.05365}}

@article{7755816,
	author = {Zhao, Rui and Mao, Kezhi},
	date-added = {2022-11-17 16:01:41 +0100},
	date-modified = {2022-11-17 16:01:41 +0100},
	doi = {10.1109/TASLP.2016.2632521},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	number = {2},
	pages = {248-260},
	title = {Topic-Aware Deep Compositional Models for Sentence Classification},
	volume = {25},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TASLP.2016.2632521}}

@inproceedings{fadaee-etal-2017-learning,
	abstract = {Distributed word representations are widely used for modeling words in NLP tasks. Most of the existing models generate one representation per word and do not consider different meanings of a word. We present two approaches to learn multiple topic-sensitive representations per word by using Hierarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task.},
	address = {Vancouver, Canada},
	author = {Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-11-17 16:01:35 +0100},
	date-modified = {2022-11-17 16:01:35 +0100},
	doi = {10.18653/v1/P17-2070},
	month = jul,
	pages = {441--447},
	publisher = {Association for Computational Linguistics},
	title = {Learning Topic-Sensitive Word Representations},
	url = {https://aclanthology.org/P17-2070},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-2070},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-2070}}
