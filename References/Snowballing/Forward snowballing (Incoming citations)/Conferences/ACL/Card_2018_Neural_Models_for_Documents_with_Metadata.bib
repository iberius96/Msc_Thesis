%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 15:53:16 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@webpage{gururangan_2018_polyglot_text_classification_with_neural_document_models,
	author = {Gururangan, Suchin},
	year = {2018},
	date-added = {2022-11-25 15:53:05 +0100},
	date-modified = {2022-11-25 15:53:05 +0100},
	lastchecked = {Friday, 25 November 2022},
	title = {Polyglot Text Classification with Neural Document Models},
	url = {http://hdl.handle.net/1773/43081},
	bdsk-url-1 = {http://hdl.handle.net/1773/43081}}

@misc{benton_2018_learning_representations_of_social_media_users,
	author = {Benton, Adrian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 15:52:57 +0100},
	date-modified = {2022-11-25 15:52:57 +0100},
	doi = {10.48550/ARXIV.1812.00436},
	keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Learning Representations of Social Media Users},
	url = {https://arxiv.org/abs/1812.00436},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1812.00436}}

@inproceedings{balalau_2019_learning_to_rank_trees_in_a_heterogeneous_graph_with_applications_in_investigative_journalism,
	author = {Oana-Denisa B{\u a}l{\u a}l{\u a}u and Ioana Manolescu and Fabian M. Suchanek},
	date-added = {2022-11-25 15:52:37 +0100},
	date-modified = {2022-11-25 15:52:37 +0100},
	title = {Learning to Rank Trees in a Heterogeneous Graph with Applications in Investigative Journalism},
	year = {2019}}

@inproceedings{chai_2019_towards_deep_learning_interpretability_a_topic_modeling_approach,
	author = {Yidong Chai and Weifeng Li},
	booktitle = {ICIS},
	date-added = {2022-11-25 15:52:17 +0100},
	date-modified = {2022-11-25 15:52:17 +0100},
	title = {Towards Deep Learning Interpretability: A Topic Modeling Approach},
	year = {2019}}

@inproceedings{gururangan_2019_variational_pretraining_for_semi_supervised_text_classification,
	abstract = {We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.},
	address = {Florence, Italy},
	author = {Gururangan, Suchin and Dang, Tam and Card, Dallas and Smith, Noah A.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-25 15:52:10 +0100},
	date-modified = {2022-11-25 15:52:10 +0100},
	doi = {10.18653/v1/P19-1590},
	month = jul,
	pages = {5880--5894},
	publisher = {Association for Computational Linguistics},
	title = {Variational Pretraining for Semi-supervised Text Classification},
	url = {https://aclanthology.org/P19-1590},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1590},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1590}}

@unknown{zhao_2019_deep_generative_models_for_sparse_high_dimensional_and_overdispersed_discrete_data,
	author = {Zhao, He and Rai, Piyush and Du, Lan and Buntine, Wray and Zhou, Mingyuan},
	date-added = {2022-11-25 15:52:00 +0100},
	date-modified = {2022-11-25 15:52:00 +0100},
	month = {05},
	title = {Deep Generative Models for Sparse, High-dimensional, and Overdispersed Discrete Data},
	year = {2019}}

@misc{dieng_2019_the_dynamic_embedded_topic_model,
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 15:51:49 +0100},
	date-modified = {2022-11-25 15:51:49 +0100},
	doi = {10.48550/ARXIV.1907.05545},
	keywords = {Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {The Dynamic Embedded Topic Model},
	url = {https://arxiv.org/abs/1907.05545},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1907.05545}}

@inproceedings{fan_2020_topic_modeling_with_semantic_frames,
	author = {Rong Fan},
	date-added = {2022-11-25 15:51:41 +0100},
	date-modified = {2022-11-25 15:51:41 +0100},
	title = {Topic Modeling with Semantic Frames},
	year = {2020}}

@inproceedings{lu_2021_cross_structural_factor_topic_model_document_analysis_with_sophisticated_covariates,
	abstract = {Modern text data is increasingly gathered in situations where it is paired with a  high-dimensional collection of covariates:  then both the text, the covariates, and their relationships are of interest to analyze. Despite the growing amount of such data, current topic models are unable to take into account large amounts of covariates successfully:  they fail to model structure among covariates and distort findings of both text and covariates. This paper presents a solution: a novel factor-topic model that enables researchers to analyze latent structure in both text and sophisticated document-level covariates collectively. The key innovation is that besides learning the underlying topical structure,  the model also learns the underlying factorial structure from the covariates and the interactions between the two structures.  A set of tailored variational inference algorithms for efficient computation are provided.  Experiments on three different datasets show the model outperforms comparable topic models in the ability to predict held-out document content.  Two case studies focusing on Finnish parliamentary election candidates and game players on Steam demonstrate the model discovers semantically meaningful topics, factors, and their interactions.  The model both outperforms state-of-the-art models in predictive accuracy and offers new factor-topic insights beyond other topic models.},
	author = {Lu, Chien and Peltonen, Jaakko and Nummenmaa, Timo and Nummenmaa, Jyrki and J\"arvelin, Kalervo},
	booktitle = {Proceedings of The 13th Asian Conference on Machine Learning},
	date-added = {2022-11-25 15:50:57 +0100},
	date-modified = {2022-11-25 15:50:57 +0100},
	editor = {Balasubramanian, Vineeth N. and Tsang, Ivor},
	month = {17--19 Nov},
	pages = {1129--1144},
	pdf = {https://proceedings.mlr.press/v157/lu21a/lu21a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Cross-structural Factor-topic Model: Document Analysis with Sophisticated Covariates},
	url = {https://proceedings.mlr.press/v157/lu21a.html},
	volume = {157},
	year = {2021},
	bdsk-url-1 = {https://proceedings.mlr.press/v157/lu21a.html}}

@inproceedings{scelsi_2021_principled_analysis_of_energy_discourse_across_domains_with_thesaurus_based_automatic_topic_labeling,
	abstract = {With the increasing impact of Natural Language Processing tools like topic models in social science research, the experimental rigor and comparability of models and datasets has come under scrutiny. Especially when contributing to research on topics with worldwide impacts like energy policy, objective analyses and reliable datasets are necessary. We contribute toward this goal in two ways: first, we release two diachronic corpora covering 23 years of energy discussions in the U.S. Energy Information Administration. Secondly, we propose a simple and theoretically sound method for automatic topic labelling drawing on political thesauri. We empirically evaluate the quality of our labels, and apply our labelling to topics induced by diachronic topic models on our energy corpora, and present a detailed analysis.},
	address = {Online},
	author = {Scelsi, Thomas and Arranz, Alfonso Martinez and Frermann, Lea},
	booktitle = {Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association},
	date-added = {2022-11-25 15:50:48 +0100},
	date-modified = {2022-11-25 15:50:48 +0100},
	month = dec,
	pages = {107--118},
	publisher = {Australasian Language Technology Association},
	title = {Principled Analysis of Energy Discourse across Domains with Thesaurus-based Automatic Topic Labeling},
	url = {https://aclanthology.org/2021.alta-1.11},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.alta-1.11}}

@misc{zhao_2020_neural_topic_model_via_optimal_transport,
	author = {Zhao, He and Phung, Dinh and Huynh, Viet and Le, Trung and Buntine, Wray},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	date-added = {2022-11-25 15:50:42 +0100},
	date-modified = {2022-11-25 15:50:42 +0100},
	doi = {10.48550/ARXIV.2008.13537},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Neural Topic Model via Optimal Transport},
	url = {https://arxiv.org/abs/2008.13537},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2008.13537},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBALi4vLi4vLi4vLi4vLi4vLi4vLi4vLi4vLi4vLi4vLi4vRG93bmxvYWRzL3BtbHItdjEwOC16aGFvMjBjLmJpYk8RAXAAAAAAAXAAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAN+BYtVCRAAB/////xVwbWxyLXYxMDgtemhhbzIwYy5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////36aVBAAAAAAAAAAAAAsAAgAACiBjdQAAAAAAAAAAAAAAAAAJRG93bmxvYWRzAAACADMvOlVzZXJzOnNhbXVlbGVjZW9sOkRvd25sb2FkczpwbWxyLXYxMDgtemhhbzIwYy5iaWIAAA4ALAAVAHAAbQBsAHIALQB2ADEAMAA4AC0AegBoAGEAbwAyADAAYwAuAGIAaQBiAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAMVVzZXJzL3NhbXVlbGVjZW9sL0Rvd25sb2Fkcy9wbWxyLXYxMDgtemhhbzIwYy5iaWIAABMAAS8AABUAAgAS//8AAAAIAA0AGgAkAGcAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAB2w==}}

@misc{wang_2021_variational_gaussian_topic_model_with_invertible_neural_projections,
	author = {Wang, Rui and Zhou, Deyu and Xiong, Yuxuan and Huang, Haiping},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 15:50:37 +0100},
	date-modified = {2022-11-25 15:50:37 +0100},
	doi = {10.48550/ARXIV.2105.10095},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Variational Gaussian Topic Model with Invertible Neural Projections},
	url = {https://arxiv.org/abs/2105.10095},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2105.10095}}

@misc{pang_2021_latent_space_energy_based_model_of_symbol_vector_coupling_for_text_generation_and_classification,
	author = {Pang, Bo and Wu, Ying Nian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 15:50:22 +0100},
	date-modified = {2022-11-25 15:50:22 +0100},
	doi = {10.48550/ARXIV.2108.11556},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Latent Space Energy-Based Model of Symbol-Vector Coupling for Text Generation and Classification},
	url = {https://arxiv.org/abs/2108.11556},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2108.11556}}

@misc{hoyle_2021_is_automated_topic_model_evaluation_broken_the_incoherence_of_coherence,
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 15:49:48 +0100},
	date-modified = {2022-11-25 15:49:48 +0100},
	doi = {10.48550/ARXIV.2107.02173},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
	url = {https://arxiv.org/abs/2107.02173},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2107.02173}}

@misc{nguyen_2021_contrastive_learning_for_neural_topic_model,
	author = {Nguyen, Thong and Luu, Anh Tuan},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 15:49:27 +0100},
	date-modified = {2022-11-25 15:49:27 +0100},
	doi = {10.48550/ARXIV.2110.12764},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Contrastive Learning for Neural Topic Model},
	url = {https://arxiv.org/abs/2110.12764},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2110.12764},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2110.12764}}

@misc{park_2022_challenges_and_opportunities_in_information_manipulation_detection_an_examination_of_wartime_russian_media,
	author = {Park, Chan Young and Mendelsohn, Julia and Field, Anjalie and Tsvetkov, Yulia},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	date-added = {2022-11-25 15:49:23 +0100},
	date-modified = {2022-11-25 15:49:23 +0100},
	doi = {10.48550/ARXIV.2205.12382},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime Russian Media},
	url = {https://arxiv.org/abs/2205.12382},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2205.12382}}

@article{hoyle_2022_are_neural_topic_models_broken,
	abstract = {Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground. Moreover, existing evaluation paradigms are often divorced from real-world use. Motivated by content analysis as a dominant real-world use case for topic modeling, we analyze two related aspects of topic models that affect their effectiveness and trustworthiness in practice for that purpose: the stability of their estimates and the extent to which the model's discovered categories align with human-determined categories in the data. We find that neural topic models fare worse in both respects compared to an established classical method. We take a step toward addressing both issues in tandem by demonstrating that a straightforward ensembling method can reliably outperform the members of the ensemble.},
	annote = {Other
Accepted to Findings of EMNLP 2022},
	author = {Hoyle, Alexander and Goel, Pranav and Sarkar, Rupak and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2210.16162},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Human-Computer Interaction (cs.HC)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Are {Neural} {Topic} {Models} {Broken}?},
	url = {https://arxiv.org/abs/2210.16162},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.16162},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.16162}}

@article{zhu_2022_disentangled_learning_of_stance_and_aspect_topics_for_vaccine_attitude_detection_in_social_media,
	abstract = {Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering.},
	author = {Zhu, Lixing and Fang, Zheng and Pergola, Gabriele and Procter, Rob and He, Yulan},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	doi = {10.48550/ARXIV.2205.03296},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 2},
	title = {Disentangled {Learning} of {Stance} and {Aspect} {Topics} for {Vaccine} {Attitude} {Detection} in {Social} {Media}},
	url = {https://arxiv.org/abs/2205.03296},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2205.03296},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2205.03296}}

@inproceedings{nadeem_2019_automated_essay_scoring_with_discourse_aware_neural_models,
	address = {Florence, Italy},
	author = {Nadeem, Farah and Nguyen, Huy and Liu, Yang and Ostendorf, Mari},
	booktitle = {Proceedings of the {Fourteenth} {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications}},
	doi = {10.18653/v1/W19-4450},
	language = {en},
	pages = {484--493},
	publisher = {Association for Computational Linguistics},
	title = {Automated {Essay} {Scoring} with {Discourse}-{Aware} {Neural} {Models}},
	url = {https://www.aclweb.org/anthology/W19-4450},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://www.aclweb.org/anthology/W19-4450},
	bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4450}}

@inproceedings{pergola_2021_a_disentangled_adversarial_neural_topic_model_for_separating_opinions_from_plots_in_user_reviews,
	address = {Online},
	author = {Pergola, Gabriele and Gui, Lin and He, Yulan},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	doi = {10.18653/v1/2021.naacl-main.228},
	file = {Full Text:files/1344/Pergola et al. - 2021 - A Disentangled Adversarial Neural Topic Model for .pdf:application/pdf},
	language = {en},
	pages = {2870--2883},
	publisher = {Association for Computational Linguistics},
	title = {A {Disentangled} {Adversarial} {Neural} {Topic} {Model} for {Separating} {Opinions} from {Plots} in {User} {Reviews}},
	url = {https://aclanthology.org/2021.naacl-main.228},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.228},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.228}}

@techreport{wang_2021_neural_labeled_lda_a_topic_model_for_semi_supervised_document_classification,
	abstract = {Abstract
          Recently, some statistical topic modeling approaches based on LDA have been applied in the field of supervised document classification, where the model generation procedure incorporates prior knowledge to improve the classification performance. However, these customizations of topic modeling are limited by the cumbersome derivation of a specific inference algorithm for each modification. In this paper, we propose a new supervised topic modeling approach for   document classification problems, Neural Labeled LDA (NL-LDA), which builds on the VAE framework, and designs a special generative network to incorporate prior information. The proposed model can support semi-supervised learning based on the  manifold   assumption and  low-density   assumption. Meanwhile, NL-LDA has a consistent and concise inference method while semi-supervised learning and predicting. Quantitative experimental results demonstrate our model has outstanding performance on supervised document classification relative to the compared approaches, including traditional statistical and neural topic models. Specially, the proposed model can support both single-label and multi-label document classification. The proposed NL-LDA performs significantly well on semi-supervised classification, especially under a small amount of labeled data. Further comparisons with related works also indicate our model is competitive with state-of-the-art topic modeling approaches on semi-supervised classification.},
	author = {Wang, Wei and Guo, Bing and Shen, Yan and Yang, Han and Chen, Yaosen and Suo, Xinhua},
	doi = {10.21203/rs.3.rs-171202/v1},
	file = {Submitted Version:files/1326/Wang et al. - 2021 - Neural labeled LDA a topic model for semi-supervi.pdf:application/pdf},
	institution = {In Review},
	month = apr,
	shorttitle = {Neural labeled {LDA}},
	title = {Neural labeled {LDA}: a topic model for semi-supervised document classification},
	type = {preprint},
	url = {https://www.researchsquare.com/article/rs-171202/v1},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.researchsquare.com/article/rs-171202/v1},
	bdsk-url-2 = {https://doi.org/10.21203/rs.3.rs-171202/v1}}

@inproceedings{zhao_2021_adversarial_learning_of_poisson_factorisation_model_for_gauging_brand_sentiment_in_user_reviews,
	address = {Online},
	author = {Zhao, Runcong and Gui, Lin and Pergola, Gabriele and He, Yulan},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	doi = {10.18653/v1/2021.eacl-main.199},
	file = {Full Text:files/1325/Zhao et al. - 2021 - Adversarial Learning of Poisson Factorisation Mode.pdf:application/pdf},
	language = {en},
	pages = {2341--2351},
	publisher = {Association for Computational Linguistics},
	title = {Adversarial {Learning} of {Poisson} {Factorisation} {Model} for {Gauging} {Brand} {Sentiment} in {User} {Reviews}},
	url = {https://aclanthology.org/2021.eacl-main.199},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.199}}

@article{zhao_2019_structured_bayesian_latent_factor_models_with_meta_data,
	abstract = {In the era of big data, huge amounts of data are being generated from the internet, social networks, phone apps, and so on, which creates high demand for powerful and efficient data analysis techniques. In areas such as collaborative filtering, text analysis, graph analysis, and bioinformatics, a large proportion of such data can be formulated into discrete matrices. This research focuses on developing structured Bayesian latent factor models with meta-data for analysing discrete data in the above areas. Compared with state-of-the-art methods, the proposed approaches have achieved not only better modelling performance and efficiency, but also preferable interpretability for intuitively understanding those data.},
	author = {HE ZHAO},
	copyright = {In Copyright},
	doi = {10.26180/5D23C9A348A83},
	keywords = {10401 Applied Statistics, 170203 Knowledge Representation and Machine Learning, Applied Computer Science, FOS: Mathematics, FOS: Psychology},
	note = {Artwork Size: 10968025 Bytes Publisher: Monash University},
	pages = {10968025 Bytes},
	title = {Structured {Bayesian} {Latent} {Factor} {Models} with {Meta}-data},
	url = {https://bridges.monash.edu/articles/thesis/Structured_Bayesian_Latent_Factor_Models_with_Meta-data/8813471},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://bridges.monash.edu/articles/thesis/Structured_Bayesian_Latent_Factor_Models_with_Meta-data/8813471},
	bdsk-url-2 = {https://doi.org/10.26180/5D23C9A348A83}}

@inproceedings{gururangan_2022_demix_layers_disentangling_domains_for_modular_language_modeling,
	address = {Seattle, United States},
	author = {Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah and Zettlemoyer, Luke},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	doi = {10.18653/v1/2022.naacl-main.407},
	file = {Full Text:files/1365/Gururangan et al. - 2022 - DEMix Layers Disentangling Domains for Modular La.pdf:application/pdf},
	language = {en},
	pages = {5557--5576},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{DEMix} {Layers}},
	title = {{DEMix} {Layers}: {Disentangling} {Domains} for {Modular} {Language} {Modeling}},
	url = {https://aclanthology.org/2022.naacl-main.407},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.naacl-main.407},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.naacl-main.407}}

@article{yan_2022_hierarchical_interpretation_of_neural_text_classification,
	abstract = {Abstract
            Recent years have witnessed increasing interest in developing interpretable models in Natural Language Processing (NLP). Most existing models aim at identifying input features such as words or phrases important for model predictions. Neural models developed in NLP, however, often compose word semantics in a hierarchical manner. As such, interpretation by words or phrases only cannot faithfully explain model decisions in text classification. This article proposes a novel Hierarchical Interpretable Neural Text classifier, called Hint, which can automatically generate explanations of model predictions in the form of label-associated topics in a hierarchical manner. Model interpretation is no longer at the word level, but built on topics as the basic semantic unit. Experimental results on both review datasets and news datasets show that our proposed approach achieves text classification results on par with existing state-of-the-art text classifiers, and generates interpretations more faithful to model predictions and better understood by humans than other interpretable neural text classifiers.},
	author = {Yan, Hanqi and Gui, Lin and He, Yulan},
	doi = {10.1162/coli_a_00459},
	file = {Full Text:files/1356/Yan et al. - 2022 - Hierarchical Interpretation of Neural Text Classif.pdf:application/pdf},
	issn = {0891-2017, 1530-9312},
	journal = {Computational Linguistics},
	language = {en},
	month = nov,
	pages = {1--34},
	title = {Hierarchical {Interpretation} of {Neural} {Text} {Classification}},
	url = {https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00459/112768/Hierarchical-Interpretation-of-Neural-Text},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00459/112768/Hierarchical-Interpretation-of-Neural-Text},
	bdsk-url-2 = {https://doi.org/10.1162/coli_a_00459}}

@article{yu_2022_latent_diffusion_energy_based_model_for_interpretable_text_modeling,
	abstract = {Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned latent space. Experiments on several challenging tasks demonstrate the superior performance of our model on interpretable text modeling over strong counterparts.},
	annote = {Other
ICML 2022},
	author = {Yu, Peiyu and Xie, Sirui and Ma, Xiaojian and Jia, Baoxiong and Pang, Bo and Gao, Ruiqi and Zhu, Yixin and Zhu, Song-Chun and Wu, Ying Nian},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	doi = {10.48550/ARXIV.2206.05895},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 3},
	title = {Latent {Diffusion} {Energy}-{Based} {Model} for {Interpretable} {Text} {Modeling}},
	url = {https://arxiv.org/abs/2206.05895},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2206.05895},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2206.05895},
	bdsk-url-3 = {https://doi.org/10.48550/arXiv.2107.02173}}

@article{xu_2022_neural_topic_modeling_with_deep_mutual_information_estimation,
	abstract = {The emerging neural topic models make topic modeling more easily adaptable and extendable in unsupervised text mining. However, the existing neural topic models is difficult to retain representative information of the documents within the learnt topic representation. In this paper, we propose a neural topic model which incorporates deep mutual information estimation, i.e., Neural Topic Modeling with Deep Mutual Information Estimation(NTM-DMIE). NTM-DMIE is a neural network method for topic learning which maximizes the mutual information between the input documents and their latent topic representation. To learn robust topic representation, we incorporate the discriminator to discriminate negative examples and positive examples via adversarial learning. Moreover, we use both global and local mutual information to preserve the rich information of the input documents in the topic representation. We evaluate NTM-DMIE on several metrics, including accuracy of text clustering, with topic representation, topic uniqueness and topic coherence. Compared to the existing methods, the experimental results show that NTM-DMIE can outperform in all the metrics on the four datasets.},
	annote = {Other
24 page, 10 Figures and 7 Tables},
	author = {Xu, Kang and Lu, Xiaoqiu and Li, Yuan-fang and Wu, Tongtong and Qi, Guilin and Ye, Ning and Wang, Dong and Zhou, Zheng},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2203.06298},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Artificial Intelligence (cs.AI)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Neural {Topic} {Modeling} with {Deep} {Mutual} {Information} {Estimation}},
	url = {https://arxiv.org/abs/2203.06298},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2203.06298},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.06298}}

@inproceedings{ahrens_2021_bayesian_topic_regression_for_causal_inference,
	address = {Online and Punta Cana, Dominican Republic},
	author = {Ahrens, Maximilian and Ashwin, Julian and Calliess, Jan-Peter and Nguyen, Vu},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	doi = {10.18653/v1/2021.emnlp-main.644},
	file = {Full Text:files/1349/Ahrens et al. - 2021 - Bayesian Topic Regression for Causal Inference.pdf:application/pdf},
	language = {en},
	pages = {8162--8188},
	publisher = {Association for Computational Linguistics},
	title = {Bayesian {Topic} {Regression} for {Causal} {Inference}},
	url = {https://aclanthology.org/2021.emnlp-main.644},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.644},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.644}}

@article{wang_2022_representing_mixtures_of_word_embeddings_with_mixtures_of_topic_embeddings,
	abstract = {A topic model is often formulated as a generative model that explains how each word of a document is generated given a set of topics and document-specific topic proportions. It is focused on capturing the word co-occurrences in a document and hence often suffers from poor performance in analyzing short documents. In addition, its parameter estimation often relies on approximate posterior inference that is either not scalable or suffers from large approximation error. This paper introduces a new topic-modeling framework where each document is viewed as a set of word embedding vectors and each topic is modeled as an embedding vector in the same embedding space. Embedding the words and topics in the same vector space, we define a method to measure the semantic difference between the embedding vectors of the words of a document and these of the topics, and optimize the topic embeddings to minimize the expected difference over all documents. Experiments on text analysis demonstrate that the proposed method, which is amenable to mini-batch stochastic gradient descent based optimization and hence scalable to big corpora, provides competitive performance in discovering more coherent and diverse topics and extracting better document representations.},
	annote = {Other
Proceedings of ICLR, 2022},
	author = {Wang, Dongsheng and Guo, Dandan and Zhao, He and Zheng, Huangjie and Tanwisuth, Korawat and Chen, Bo and Zhou, Mingyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2203.01570},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Methodology (stat.ME)},
	note = {Publisher: arXiv Version Number: 2},
	title = {Representing {Mixtures} of {Word} {Embeddings} with {Mixtures} of {Topic} {Embeddings}},
	url = {https://arxiv.org/abs/2203.01570},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2203.01570},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.01570}}

@inproceedings{cao_2019_belink_querying_networks_of_facts_statements_and_beliefs,
	address = {Beijing China},
	author = {Cao, Tien-Duc and Duroyon, Ludivine and Goasdou{\'e}, Fran{\c c}ois and Manolescu, Ioana and Tannier, Xavier},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	doi = {10.1145/3357384.3357851},
	file = {Submitted Version:files/1377/Cao et al. - 2019 - BeLink Querying Networks of Facts, Statements and.pdf:application/pdf},
	isbn = {978-1-4503-6976-3},
	language = {en},
	month = nov,
	pages = {2941--2944},
	publisher = {ACM},
	shorttitle = {{BeLink}},
	title = {{BeLink}: {Querying} {Networks} of {Facts}, {Statements} and {Beliefs}},
	url = {https://dl.acm.org/doi/10.1145/3357384.3357851},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3357384.3357851},
	bdsk-url-2 = {https://doi.org/10.1145/3357384.3357851}}

@article{li_2022_classifying_covid_19_vaccine_narratives,
	abstract = {COVID-19 vaccine hesitancy is widespread, despite governments' information campaigns and WHO efforts. One of the reasons behind this is vaccine disinformation which widely spreads in social media. In particular, recent surveys have established that vaccine disinformation is impacting negatively citizen trust in COVID-19 vaccination. At the same time, fact-checkers are struggling with detecting and tracking of vaccine disinformation, due to the large scale of social media. To assist fact-checkers in monitoring vaccine narratives online, this paper studies a new vaccine narrative classification task, which categorises COVID-19 vaccine claims into one of seven categories. Following a data augmentation approach, we first construct a novel dataset for this new classification task, focusing on the minority classes. We also make use of fact-checker annotated data. The paper also presents a neural vaccine narrative classifier that achieves an accuracy of 84\% under cross-validation. The classifier is publicly available for researchers and journalists.},
	author = {Li, Yue and Scarton, Carolina and Song, Xingyi and Bontcheva, Kalina},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2207.08522},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Classifying {COVID}-19 vaccine narratives},
	url = {https://arxiv.org/abs/2207.08522},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2207.08522},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2207.08522}}

@incollection{tripodi_2020_topic_modelling_games,
	author = {Tripodi, Rocco},
	booktitle = {Proceedings of the {Seventh} {Italian} {Conference} on {Computational} {Linguistics} {CLiC}-it 2020},
	doi = {10.4000/books.aaccademia.8940},
	editor = {Dell'Orletta, Felice and Monti, Johanna and Tamburini, Fabio},
	file = {Full Text:files/1343/Tripodi - 2020 - Topic Modelling Games.pdf:application/pdf},
	isbn = {9791280136336},
	language = {en},
	pages = {435--442},
	publisher = {Accademia University Press},
	title = {Topic {Modelling} {Games}},
	url = {http://books.openedition.org/aaccademia/8940},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {http://books.openedition.org/aaccademia/8940},
	bdsk-url-2 = {https://doi.org/10.4000/books.aaccademia.8940}}

@inproceedings{jin_2021_neural_attention_aware_hierarchical_topic_model,
	address = {Online and Punta Cana, Dominican Republic},
	author = {Jin, Yuan and Zhao, He and Liu, Ming and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	doi = {10.18653/v1/2021.emnlp-main.80},
	file = {Full Text:files/1345/Jin et al. - 2021 - Neural Attention-Aware Hierarchical Topic Model.pdf:application/pdf},
	language = {en},
	pages = {1042--1052},
	publisher = {Association for Computational Linguistics},
	title = {Neural {Attention}-{Aware} {Hierarchical} {Topic} {Model}},
	url = {https://aclanthology.org/2021.emnlp-main.80},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.80},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.80}}

@article{sanaullah_2022_applications_of_machine_learning_for_covid_19_misinformation_a_systematic_review,
	author = {Sanaullah, A. R. and Das, Anupam and Das, Anik and Kabir, Muhammad Ashad and Shu, Kai},
	doi = {10.1007/s13278-022-00921-9},
	file = {Full Text:files/1346/Sanaullah et al. - 2022 - Applications of machine learning for COVID-19 misi.pdf:application/pdf},
	issn = {1869-5450, 1869-5469},
	journal = {Social Network Analysis and Mining},
	language = {en},
	month = dec,
	number = {1},
	pages = {94},
	shorttitle = {Applications of machine learning for {COVID}-19 misinformation},
	title = {Applications of machine learning for {COVID}-19 misinformation: a systematic review},
	url = {https://link.springer.com/10.1007/s13278-022-00921-9},
	urldate = {2022-11-25},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/s13278-022-00921-9},
	bdsk-url-2 = {https://doi.org/10.1007/s13278-022-00921-9}}

@article{song_2021_classification_aware_neural_topic_model_for_covid_19_disinformation_categorisation,
	abstract = {The explosion of disinformation accompanying the COVID-19 pandemic has overloaded fact-checkers and media worldwide, and brought a new major challenge to government responses worldwide. Not only is disinformation creating confusion about medical science amongst citizens, but it is also amplifying distrust in policy makers and governments. To help tackle this, we developed computational methods to categorise COVID-19 disinformation. The COVID-19 disinformation categories could be used for a) focusing fact-checking efforts on the most damaging kinds of COVID-19 disinformation; b) guiding policy makers who are trying to deliver effective public health messages and counter effectively COVID-19 disinformation. This paper presents: 1) a corpus containing what is currently the largest available set of manually annotated COVID-19 disinformation categories; 2) a classification-aware neural topic model (CANTM) designed for COVID-19 disinformation category classification and topic discovery; 3) an extensive analysis of COVID-19 disinformation categories with respect to time, volume, false type, media type and origin source.},
	author = {Song, Xingyi and Petrak, Johann and Jiang, Ye and Singh, Iknoor and Maynard, Diana and Bontcheva, Kalina},
	doi = {10.1371/journal.pone.0247086},
	editor = {Martin{\v c}i{\'c}-Ip{\v s}i{\'c}, Sanda},
	file = {Full Text:files/1347/Song et al. - 2021 - Classification aware neural topic model for COVID-.pdf:application/pdf},
	issn = {1932-6203},
	journal = {PLOS ONE},
	language = {en},
	month = feb,
	number = {2},
	pages = {e0247086},
	title = {Classification aware neural topic model for {COVID}-19 disinformation categorisation},
	url = {https://dx.plos.org/10.1371/journal.pone.0247086},
	urldate = {2022-11-25},
	volume = {16},
	year = {2021},
	bdsk-url-1 = {https://dx.plos.org/10.1371/journal.pone.0247086},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0247086}}

@article{adusumilli_2022_the_structure_and_dynamics_of_modern_united_states_federal_case_law,
	abstract = {The structure and dynamics of modern United States Federal Case Law are examined here. The analyses utilize large-scale network analysis tools, natural language processing techniques, and information theory to examine all the federal opinions in the Court Listener database, containing approximately 1.3 million judicial opinions and 11.4 million citations. The analyses are focused on modern United States Federal Case Law, as cases in the Court Listener database range from approximately 1926--2020 and include most Federal jurisdictions. We examine the data set from a structural perspective using the citation network, overall and by time and space (jurisdiction). In addition to citation structure, we examine the dataset from a topical and information theoretic perspective, again, overall and by time and space.},
	author = {Adusumilli, Keerthi and Brown, Bradford and Harrison, Joey and Koehler, Matthew and Kutarnia, Jason and Michel, Shaun and Olivier, Max and Pfeifer, Craig and Slater, Zoryanna and Thompson, William and Vetter, Dianna and Zacharowicz, Renee},
	doi = {10.3389/fphy.2021.695219},
	file = {Full Text:files/1367/Adusumilli et al. - 2022 - The Structure and Dynamics of Modern United States.pdf:application/pdf},
	issn = {2296-424X},
	journal = {Frontiers in Physics},
	month = jan,
	pages = {695219},
	title = {The {Structure} and {Dynamics} of {Modern} {United} {States} {Federal} {Case} {Law}},
	url = {https://www.frontiersin.org/articles/10.3389/fphy.2021.695219/full},
	urldate = {2022-11-25},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://www.frontiersin.org/articles/10.3389/fphy.2021.695219/full},
	bdsk-url-2 = {https://doi.org/10.3389/fphy.2021.695219}}

@inproceedings{gui_2019_neural_topic_model_with_reinforcement_learning,
	address = {Hong Kong, China},
	author = {Gui, Lin and Leng, Jia and Pergola, Gabriele and Zhou, Yu and Xu, Ruifeng and He, Yulan},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	doi = {10.18653/v1/D19-1350},
	file = {Full Text:files/1362/Gui et al. - 2019 - Neural Topic Model with Reinforcement Learning.pdf:application/pdf},
	language = {en},
	pages = {3476--3481},
	publisher = {Association for Computational Linguistics},
	title = {Neural {Topic} {Model} with {Reinforcement} {Learning}},
	url = {https://www.aclweb.org/anthology/D19-1350},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://www.aclweb.org/anthology/D19-1350},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1350}}

@inproceedings{zhao_2021_topic_modelling_meets_deep_neural_networks_a_survey,
	abstract = {Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with nearly a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review on this specific topic.},
	address = {Montreal, Canada},
	author = {Zhao, He and Phung, Dinh and Huynh, Viet and Jin, Yuan and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	doi = {10.24963/ijcai.2021/638},
	file = {Full Text:files/1354/Zhao et al. - 2021 - Topic Modelling Meets Deep Neural Networks A Surv.pdf:application/pdf},
	isbn = {978-0-9992411-9-6},
	language = {en},
	month = aug,
	pages = {4713--4720},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	shorttitle = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}},
	title = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}: {A} {Survey}},
	url = {https://www.ijcai.org/proceedings/2021/638},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.ijcai.org/proceedings/2021/638},
	bdsk-url-2 = {https://doi.org/10.24963/ijcai.2021/638}}

@inproceedings{zhang_2022_pre_training_and_fine_tuning_neural_topic_model_a_simple_yet_effective_approach_to_incorporating_external_knowledge,
	address = {Dublin, Ireland},
	author = {Zhang, Linhai and Hu, Xuemeng and Wang, Boyu and Zhou, Deyu and Zhang, Qian-Wen and Cao, Yunbo},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2022.acl-long.413},
	file = {Full Text:files/1348/Zhang et al. - 2022 - Pre-training and Fine-tuning Neural Topic Model A.pdf:application/pdf},
	language = {en},
	pages = {5980--5989},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Pre-training and {Fine}-tuning {Neural} {Topic} {Model}},
	title = {Pre-training and {Fine}-tuning {Neural} {Topic} {Model}: {A} {Simple} yet {Effective} {Approach} to {Incorporating} {External} {Knowledge}},
	url = {https://aclanthology.org/2022.acl-long.413},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.acl-long.413},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.acl-long.413}}

@inproceedings{doan_2021_benchmarking_neural_topic_models_an_empirical_study,
	address = {Online},
	author = {Doan, Thanh-Nam and Hoang, Tuan-Anh},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	doi = {10.18653/v1/2021.findings-acl.382},
	file = {Full Text:files/1353/Doan and Hoang - 2021 - Benchmarking Neural Topic Models An Empirical Stu.pdf:application/pdf},
	language = {en},
	pages = {4363--4368},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Benchmarking {Neural} {Topic} {Models}},
	title = {Benchmarking {Neural} {Topic} {Models}: {An} {Empirical} {Study}},
	url = {https://aclanthology.org/2021.findings-acl.382},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.findings-acl.382},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.findings-acl.382}}

@inproceedings{hu_2020_neural_topic_modeling_with_cycle_consistent_adversarial_training,
	address = {Online},
	author = {Hu, Xuemeng and Wang, Rui and Zhou, Deyu and Xiong, Yuxuan},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	doi = {10.18653/v1/2020.emnlp-main.725},
	file = {Full Text:files/1351/Hu et al. - 2020 - Neural Topic Modeling with Cycle-Consistent Advers.pdf:application/pdf},
	language = {en},
	pages = {9018--9030},
	publisher = {Association for Computational Linguistics},
	title = {Neural {Topic} {Modeling} with {Cycle}-{Consistent} {Adversarial} {Training}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.725},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2020.emnlp-main.725},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.725}}

@inproceedings{panwar_2021_tan_ntm_topic_attention_networks_for_neural_topic_modeling,
	address = {Online},
	author = {Panwar, Madhur and Shailabh, Shashank and Aggarwal, Milan and Krishnamurthy, Balaji},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2021.acl-long.299},
	file = {Full Text:files/1363/Panwar et al. - 2021 - TAN-NTM Topic Attention Networks for Neural Topic.pdf:application/pdf},
	language = {en},
	pages = {3865--3880},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{TAN}-{NTM}},
	title = {{TAN}-{NTM}: {Topic} {Attention} {Networks} for {Neural} {Topic} {Modeling}},
	url = {https://aclanthology.org/2021.acl-long.299},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.299},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.299}}

@inproceedings{tang_2019_multi_label_topic_model_conditioned_on_label_embedding,
	address = {Kunming, China},
	author = {Tang, Lin and Liu, Lin and Gan, Jianhou},
	booktitle = {2019 {IEEE} {International} {Conference} on {Computer} {Science} and {Educational} {Informatization} ({CSEI})},
	doi = {10.1109/CSEI47661.2019.8938881},
	isbn = {978-1-72812-308-0},
	month = aug,
	pages = {305--309},
	publisher = {IEEE},
	title = {Multi-{Label} {Topic} {Model} {Conditioned} on {Label} {Embedding}},
	url = {https://ieeexplore.ieee.org/document/8938881/},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8938881/},
	bdsk-url-2 = {https://doi.org/10.1109/CSEI47661.2019.8938881}}

@article{joseph_2022_local_news_online_and_covid_in_the_us_relationships_among_coverage_cases_deaths_and_audience,
	abstract = {We present analyses from a real-time information monitoring system of online local news in the U.S. We study relationships among online local news coverage of COVID, cases and deaths in an area, and properties of local news outlets and their audiences. Our analysis relies on a unique dataset of the online content of over 300 local news outlets, encompassing over 750,000 articles over a period of 10 months spanning April 2020 to February 2021. We find that the rate of COVID coverage over time by local news outlets was primarily associated with death rates at the national level, but that this effect dissipated over the course of the pandemic as news about COVID was steadily displaced by sociopolitical events, like the 2020 U.S. elections. We also find that both the volume and content of COVID coverage differed depending on local politics, and outlet audience size, as well as evidence that more vulnerable populations received less pandemic-related news.},
	author = {Joseph, Kenneth and Horne, Benjamin D. and Green, Jon and Wihbey, John P.},
	doi = {10.1609/icwsm.v16i1.19305},
	file = {Full Text:files/1368/Joseph et al. - 2022 - Local News Online and COVID in the U.S. Relations.pdf:application/pdf},
	issn = {2334-0770, 2162-3449},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	month = may,
	pages = {441--452},
	shorttitle = {Local {News} {Online} and {COVID} in the {U}.{S}.},
	title = {Local {News} {Online} and {COVID} in the {U}.{S}.: {Relationships} among {Coverage}, {Cases}, {Deaths}, and {Audience}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/19305},
	urldate = {2022-11-25},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/ICWSM/article/view/19305},
	bdsk-url-2 = {https://doi.org/10.1609/icwsm.v16i1.19305}}

@inproceedings{wu_2021_discovering_topics_in_long_tailed_corpora_with_causal_intervention,
	address = {Online},
	author = {Wu, Xiaobao and Li, Chunping and Miao, Yishu},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	doi = {10.18653/v1/2021.findings-acl.15},
	file = {Full Text:files/1361/Wu et al. - 2021 - Discovering Topics in Long-tailed Corpora with Cau.pdf:application/pdf},
	language = {en},
	pages = {175--185},
	publisher = {Association for Computational Linguistics},
	title = {Discovering {Topics} in {Long}-tailed {Corpora} with {Causal} {Intervention}},
	url = {https://aclanthology.org/2021.findings-acl.15},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.findings-acl.15},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.findings-acl.15},
	bdsk-url-3 = {https://doi.org/10.48550/arXiv.2108.11556},
	bdsk-url-4 = {https://doi.org/10.48550/arXiv.2107.02173}}

@inproceedings{qin_2021_lifelong_learning_of_topics_and_domain_specific_word_embeddings,
	address = {Online},
	author = {Qin, Xiaorui and Lu, Yuyin and Chen, Yufu and Rao, Yanghui},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	doi = {10.18653/v1/2021.findings-acl.202},
	file = {Full Text:files/1369/Qin et al. - 2021 - Lifelong Learning of Topics and Domain-Specific Wo.pdf:application/pdf},
	language = {en},
	pages = {2294--2309},
	publisher = {Association for Computational Linguistics},
	title = {Lifelong {Learning} of {Topics} and {Domain}-{Specific} {Word} {Embeddings}},
	url = {https://aclanthology.org/2021.findings-acl.202},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.findings-acl.202},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.findings-acl.202}}

@incollection{li_2020_neural_topic_models_with_survival_supervision_jointly_predicting_time_to_event_outcomes_and_learning_how_clinical_features_relate,
	address = {Cham},
	author = {Li, Linhong and Zuo, Ren and Coston, Amanda and Weiss, Jeremy C. and Chen, George H.},
	booktitle = {Artificial {Intelligence} in {Medicine}},
	doi = {10.1007/978-3-030-59137-3_33},
	editor = {Michalowski, Martin and Moskovitch, Robert},
	file = {Submitted Version:files/1370/Li et al. - 2020 - Neural Topic Models with Survival Supervision Joi.pdf:application/pdf},
	isbn = {978-3-030-59136-6 978-3-030-59137-3},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {371--381},
	publisher = {Springer International Publishing},
	shorttitle = {Neural {Topic} {Models} with {Survival} {Supervision}},
	title = {Neural {Topic} {Models} with {Survival} {Supervision}: {Jointly} {Predicting} {Time}-to-{Event} {Outcomes} and {Learning} {How} {Clinical} {Features} {Relate}},
	url = {https://link.springer.com/10.1007/978-3-030-59137-3_33},
	urldate = {2022-11-25},
	volume = {12299},
	year = {2020},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-030-59137-3_33},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-59137-3_33}}

@article{dreier_2022_patterns_of_bias_how_mainstream_media_operationalize_links_between_mass_shootings_and_terrorism,
	author = {Dreier, Sarah K. and Gade, Emily K. and Card, Dallas and Smith, Noah A.},
	doi = {10.1080/10584609.2022.2111484},
	issn = {1058-4609, 1091-7675},
	journal = {Political Communication},
	language = {en},
	month = nov,
	number = {6},
	pages = {755--778},
	shorttitle = {Patterns of {Bias}},
	title = {Patterns of {Bias}: {How} {Mainstream} {Media} {Operationalize} {Links} between {Mass} {Shootings} and {Terrorism}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10584609.2022.2111484},
	urldate = {2022-11-25},
	volume = {39},
	year = {2022},
	bdsk-url-1 = {https://www.tandfonline.com/doi/full/10.1080/10584609.2022.2111484},
	bdsk-url-2 = {https://doi.org/10.1080/10584609.2022.2111484}}

@article{gui_2021_understanding_patient_reviews_with_minimum_supervision,
	author = {Gui, Lin and He, Yulan},
	doi = {10.1016/j.artmed.2021.102160},
	file = {Accepted Version:files/1366/Gui and He - 2021 - Understanding patient reviews with minimum supervi.pdf:application/pdf},
	issn = {09333657},
	journal = {Artificial Intelligence in Medicine},
	language = {en},
	month = oct,
	pages = {102160},
	title = {Understanding patient reviews with minimum supervision},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365721001536},
	urldate = {2022-11-25},
	volume = {120},
	year = {2021},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0933365721001536},
	bdsk-url-2 = {https://doi.org/10.1016/j.artmed.2021.102160}}

@inproceedings{hoyle_2020_improving_neural_topic_models_using_knowledge_distillation,
	address = {Online},
	author = {Hoyle, Alexander Miserlis and Goel, Pranav and Resnik, Philip},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	doi = {10.18653/v1/2020.emnlp-main.137},
	file = {Full Text:files/1372/Hoyle et al. - 2020 - Improving Neural Topic Models using Knowledge Dist.pdf:application/pdf},
	language = {en},
	pages = {1752--1771},
	publisher = {Association for Computational Linguistics},
	title = {Improving {Neural} {Topic} {Models} using {Knowledge} {Distillation}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.137},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2020.emnlp-main.137},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.137}}

@article{sun_2022_pslda_a_novel_supervised_pseudo_document_based_topic_model_for_short_texts,
	author = {Sun, Mingtao and Zhao, Xiaowei and Lin, Jingjing and Jing, Jian and Wang, Deqing and Jia, Guozhu},
	doi = {10.1007/s11704-021-0606-3},
	issn = {2095-2228, 2095-2236},
	journal = {Frontiers of Computer Science},
	language = {en},
	month = dec,
	number = {6},
	pages = {166350},
	shorttitle = {{PSLDA}},
	title = {{PSLDA}: a novel supervised pseudo document-based topic model for short texts},
	url = {https://link.springer.com/10.1007/s11704-021-0606-3},
	urldate = {2022-11-25},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/s11704-021-0606-3},
	bdsk-url-2 = {https://doi.org/10.1007/s11704-021-0606-3}}

@book{martin_michalowski_2020_artificial_intelligence_in_medicine_18th_international_conference_on_artificial_intelligence_in_medicine_aime_2020_minneapolis_mn_usa_august_25__28_2020_proceedings,
	author = {Martin Michalowski, Robert Moskovitch},
	address = {Cham},
	doi = {10.1007/978-3-030-59137-3},
	editor = {Michalowski, Martin and Moskovitch, Robert},
	file = {Submitted Version:files/1371/Michalowski and Moskovitch - 2020 - Artificial Intelligence in Medicine 18th Internat.pdf:application/pdf},
	isbn = {978-3-030-59136-6 978-3-030-59137-3},
	language = {en},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Artificial {Intelligence} in {Medicine}},
	title = {Artificial {Intelligence} in {Medicine}: 18th {International} {Conference} on {Artificial} {Intelligence} in {Medicine}, {AIME} 2020, {Minneapolis}, {MN}, {USA}, {August} 25--28, 2020, {Proceedings}},
	url = {https://link.springer.com/10.1007/978-3-030-59137-3},
	urldate = {2022-11-25},
	volume = {12299},
	year = {2020},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-030-59137-3},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-59137-3}}

@article{pergola_2019_tdam_a_topic_dependent_attention_model_for_sentiment_analysis,
	author = {Pergola, Gabriele and Gui, Lin and He, Yulan},
	doi = {10.1016/j.ipm.2019.102084},
	file = {Accepted Version:files/1374/Pergola et al. - 2019 - TDAM A topic-dependent attention model for sentim.pdf:application/pdf},
	issn = {03064573},
	journal = {Information Processing \& Management},
	language = {en},
	month = nov,
	number = {6},
	pages = {102084},
	shorttitle = {{TDAM}},
	title = {{TDAM}: {A} topic-dependent attention model for sentiment analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457319305461},
	urldate = {2022-11-25},
	volume = {56},
	year = {2019},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0306457319305461},
	bdsk-url-2 = {https://doi.org/10.1016/j.ipm.2019.102084}}

@article{pittaras_2021_text_classification_with_semantically_enriched_word_embeddings,
	abstract = {Abstract
            The recent breakthroughs in deep neural architectures across multiple machine learning fields have led to the widespread use of deep neural models. These learners are often applied as black-box models that ignore or insufficiently utilize a wealth of preexisting semantic information. In this study, we focus on the text classification task, investigating methods for augmenting the input to deep neural networks (DNNs) with semantic information. We extract semantics for the words in the preprocessed text from the WordNet semantic graph, in the form of weighted concept terms that form a semantic frequency vector. Concepts are selected via a variety of semantic disambiguation techniques, including a basic, a part-of-speech-based, and a semantic embedding projection method. Additionally, we consider a weight propagation mechanism that exploits semantic relationships in the concept graph and conveys a spreading activation component. We enrich word2vec embeddings with the resulting semantic vector through concatenation or replacement and apply the semantically augmented word embeddings on the classification task via a DNN. Experimental results over established datasets demonstrate that our approach of semantic augmentation in the input space boosts classification performance significantly, with concatenation offering the best performance. We also note additional interesting findings produced by our approach regarding the behavior of term frequency - inverse document frequency normalization on semantic vectors, along with the radical dimensionality reduction potential with negligible performance loss.},
	author = {Pittaras, N. and Giannakopoulos, G. and Papadakis, G. and Karkaletsis, V.},
	doi = {10.1017/S1351324920000170},
	file = {Full Text:files/1373/Pittaras et al. - 2021 - Text classification with semantically enriched wor.pdf:application/pdf},
	issn = {1351-3249, 1469-8110},
	journal = {Natural Language Engineering},
	language = {en},
	month = jul,
	number = {4},
	pages = {391--425},
	title = {Text classification with semantically enriched word embeddings},
	url = {https://www.cambridge.org/core/product/identifier/S1351324920000170/type/journal_article},
	urldate = {2022-11-25},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://www.cambridge.org/core/product/identifier/S1351324920000170/type/journal_article},
	bdsk-url-2 = {https://doi.org/10.1017/S1351324920000170}}

@inproceedings{rodriguez_2021_evaluation_examples_are_not_equally_informative_how_should_that_change_nlp_leaderboards,
	address = {Online},
	author = {Rodriguez, Pedro and Barrow, Joe and Hoyle, Alexander Miserlis and Lalor, John P. and Jia, Robin and Boyd-Graber, Jordan},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2021.acl-long.346},
	file = {Full Text:files/1375/Rodriguez et al. - 2021 - Evaluation Examples are not Equally Informative H.pdf:application/pdf},
	language = {en},
	pages = {4486--4503},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Evaluation {Examples} are not {Equally} {Informative}},
	title = {Evaluation {Examples} are not {Equally} {Informative}: {How} should that change {NLP} {Leaderboards}?},
	url = {https://aclanthology.org/2021.acl-long.346},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.346},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.346}}

@article{ning_2020_nonparametric_topic_modeling_with_neural_inference,
	author = {Ning, Xuefei and Zheng, Yin and Jiang, Zhuxi and Wang, Yu and Yang, Huazhong and Huang, Junzhou and Zhao, Peilin},
	doi = {10.1016/j.neucom.2019.12.128},
	file = {Submitted Version:files/1376/Ning et al. - 2020 - Nonparametric Topic Modeling with Neural Inference.pdf:application/pdf},
	issn = {09252312},
	journal = {Neurocomputing},
	language = {en},
	month = jul,
	pages = {296--306},
	title = {Nonparametric {Topic} {Modeling} with {Neural} {Inference}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220303015},
	urldate = {2022-11-25},
	volume = {399},
	year = {2020},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220303015},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2019.12.128}}
