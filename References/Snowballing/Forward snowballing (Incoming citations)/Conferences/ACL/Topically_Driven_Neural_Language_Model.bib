%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 20:38:49 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{https://doi.org/10.48550/arxiv.2010.12055,
	author = {Rezaee, Mehdi and Ferraro, Francis},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:38:46 +0100},
	date-modified = {2022-11-25 20:38:46 +0100},
	doi = {10.48550/ARXIV.2010.12055},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {A Discrete Variational Recurrent Topic Model without the Reparametrization Trick},
	url = {https://arxiv.org/abs/2010.12055},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2010.12055}}

@misc{https://doi.org/10.48550/arxiv.2008.12878,
	author = {Wang, Hai},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:38:42 +0100},
	date-modified = {2022-11-25 20:38:42 +0100},
	doi = {10.48550/ARXIV.2008.12878},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Knowledge Efficient Deep Learning for Natural Language Processing},
	url = {https://arxiv.org/abs/2008.12878},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2008.12878}}

@misc{https://doi.org/10.48550/arxiv.2006.10632,
	author = {Chaudhary, Yatin and Sch{\"u}tze, Hinrich and Gupta, Pankaj},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:38:37 +0100},
	date-modified = {2022-11-25 20:38:37 +0100},
	doi = {10.48550/ARXIV.2006.10632},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Explainable and Discourse Topic-aware Neural Language Understanding},
	url = {https://arxiv.org/abs/2006.10632},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2006.10632}}

@misc{lu2021topicaware,
	author = {Ruiying Lu and Bo Chen and Dan dan Guo and Dongsheng Wang and Mingyuan Zhou},
	date-added = {2022-11-25 20:38:13 +0100},
	date-modified = {2022-11-25 20:38:13 +0100},
	title = {Topic-aware Contextualized Transformers},
	url = {https://openreview.net/forum?id=ml1LSu49FLZ},
	year = {2021},
	bdsk-url-1 = {https://openreview.net/forum?id=ml1LSu49FLZ}}

@misc{https://doi.org/10.48550/arxiv.1909.13315,
	author = {Chaudhary, Yatin and Gupta, Pankaj and Runkler, Thomas},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:38:07 +0100},
	date-modified = {2022-11-25 20:38:07 +0100},
	doi = {10.48550/ARXIV.1909.13315},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Lifelong Neural Topic Learning in Contextualized Autoregressive Topic Models of Language via Informative Transfers},
	url = {https://arxiv.org/abs/1909.13315},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1909.13315}}

@misc{https://doi.org/10.48550/arxiv.1912.10337,
	author = {Guo, Dandan and Chen, Bo and Lu, Ruiying and Zhou, Mingyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:38:02 +0100},
	date-modified = {2022-11-25 20:38:02 +0100},
	doi = {10.48550/ARXIV.1912.10337},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Methodology (stat.ME), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Recurrent Hierarchical Topic-Guided RNN for Language Generation},
	url = {https://arxiv.org/abs/1912.10337},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1912.10337}}

@misc{https://doi.org/10.48550/arxiv.1905.08622,
	author = {Zhang, Hao and Chen, Bo and Tian, Long and Wang, Zhengjue and Zhou, Mingyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:37:56 +0100},
	date-modified = {2022-11-25 20:37:56 +0100},
	doi = {10.48550/ARXIV.1905.08622},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling},
	url = {https://arxiv.org/abs/1905.08622},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1905.08622}}

@article{https://doi.org/10.48550/arxiv.2011.06237,
	author = {Aggarwal, Samarth and Garg, Rohin and Sancheti, Abhilasha and Guda, Bhanu Prakash Reddy and Burhanuddin, Iftikhar Ahamath},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:37:40 +0100},
	date-modified = {2022-11-25 20:37:40 +0100},
	doi = {10.48550/ARXIV.2011.06237},
	keywords = {Human-Computer Interaction (cs.HC), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Goal-driven Command Recommendations for Analysts},
	url = {https://arxiv.org/abs/2011.06237},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2011.06237}}

@inproceedings{zhu-qian-2018-enhanced,
	abstract = {In aspect level sentiment classification, there are two common tasks: to identify the sentiment of an aspect (category) or a term. As specific instances of aspects, terms explicitly occur in sentences. It is beneficial for models to focus on nearby context words. In contrast, as high level semantic concepts of terms, aspects usually have more generalizable representations. However, conventional methods cannot utilize the information of aspects and terms at the same time, because few datasets are annotated with both aspects and terms. In this paper, we propose a novel deep memory network with auxiliary memory to address this problem. In our model, a main memory is used to capture the important context words for sentiment classification. In addition, we build an auxiliary memory to implicitly convert aspects and terms to each other, and feed both of them to the main memory. With the interaction between two memories, the features of aspects and terms can be learnt simultaneously. We compare our model with the state-of-the-art methods on four datasets from different domains. The experimental results demonstrate the effectiveness of our model.},
	address = {Santa Fe, New Mexico, USA},
	author = {Zhu, Peisong and Qian, Tieyun},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-11-25 20:37:35 +0100},
	date-modified = {2022-11-25 20:37:35 +0100},
	month = aug,
	pages = {1077--1087},
	publisher = {Association for Computational Linguistics},
	title = {Enhanced Aspect Level Sentiment Classification with Auxiliary Memory},
	url = {https://aclanthology.org/C18-1092},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1092}}

@misc{https://doi.org/10.48550/arxiv.1712.09783,
	author = {Wang, Wenlin and Gan, Zhe and Wang, Wenqi and Shen, Dinghan and Huang, Jiaji and Ping, Wei and Satheesh, Sanjeev and Carin, Lawrence},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 20:37:30 +0100},
	date-modified = {2022-11-25 20:37:30 +0100},
	doi = {10.48550/ARXIV.1712.09783},
	keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic Compositional Neural Language Model},
	url = {https://arxiv.org/abs/1712.09783},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1712.09783}}

@misc{https://doi.org/10.48550/arxiv.1911.10180,
	author = {Pergola, Gabriele and He, Yulan and Lowe, David},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:37:26 +0100},
	date-modified = {2022-11-25 20:37:26 +0100},
	doi = {10.48550/ARXIV.1911.10180},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topical Phrase Extraction from Clinical Reports by Incorporating both Local and Global Context},
	url = {https://arxiv.org/abs/1911.10180},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1911.10180}}

@misc{https://doi.org/10.48550/arxiv.1810.03947,
	author = {Gupta, Pankaj and Chaudhary, Yatin and Buettner, Florian and Sch{\"u}tze, Hinrich},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:37:22 +0100},
	date-modified = {2022-11-25 20:37:22 +0100},
	doi = {10.48550/ARXIV.1810.03947},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {textTOvec: Deep Contextualized Neural Autoregressive Topic Models of Language with Distributed Compositional Prior},
	url = {https://arxiv.org/abs/1810.03947},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1810.03947}}

@inproceedings{Winston2018AnsweringCS,
	author = {Ezra Winston and Bhuwan Dhingra and Kathryn Mazaitis and Graham Neubig and William W. Cohen},
	date-added = {2022-11-25 20:37:17 +0100},
	date-modified = {2022-11-25 20:37:17 +0100},
	title = {Answering Cloze-style Software Questions Using Stack Overflow},
	year = {2018}}

@article{hoyle_are_2022,
	abstract = {Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground. Moreover, existing evaluation paradigms are often divorced from real-world use. Motivated by content analysis as a dominant real-world use case for topic modeling, we analyze two related aspects of topic models that affect their effectiveness and trustworthiness in practice for that purpose: the stability of their estimates and the extent to which the model's discovered categories align with human-determined categories in the data. We find that neural topic models fare worse in both respects compared to an established classical method. We take a step toward addressing both issues in tandem by demonstrating that a straightforward ensembling method can reliably outperform the members of the ensemble.},
	annote = {Other
Accepted to Findings of EMNLP 2022},
	author = {Hoyle, Alexander and Goel, Pranav and Sarkar, Rupak and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2210.16162},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Human-Computer Interaction (cs.HC)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Are {Neural} {Topic} {Models} {Broken}?},
	url = {https://arxiv.org/abs/2210.16162},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.16162},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.16162}}

@incollection{dellorletta_topic_2020,
	author = {Tripodi, Rocco},
	booktitle = {Proceedings of the {Seventh} {Italian} {Conference} on {Computational} {Linguistics} {CLiC}-it 2020},
	doi = {10.4000/books.aaccademia.8940},
	editor = {Dell'Orletta, Felice and Monti, Johanna and Tamburini, Fabio},
	file = {Full Text:files/1620/Tripodi - 2020 - Topic Modelling Games.pdf:application/pdf},
	isbn = {9791280136336},
	language = {en},
	pages = {435--442},
	publisher = {Accademia University Press},
	title = {Topic {Modelling} {Games}},
	url = {http://books.openedition.org/aaccademia/8940},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {http://books.openedition.org/aaccademia/8940},
	bdsk-url-2 = {https://doi.org/10.4000/books.aaccademia.8940}}

@inproceedings{tang_topic_2019,
	address = {Hong Kong, China},
	author = {Tang, Hongyin and Li, Miao and Jin, Beihong},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	doi = {10.18653/v1/D19-1513},
	file = {Full Text:files/1633/Tang et al. - 2019 - A Topic Augmented Text Generation Model Joint Lea.pdf:application/pdf},
	language = {en},
	pages = {5089--5098},
	publisher = {Association for Computational Linguistics},
	shorttitle = {A {Topic} {Augmented} {Text} {Generation} {Model}},
	title = {A {Topic} {Augmented} {Text} {Generation} {Model}: {Joint} {Learning} of {Semantics} and {Structural} {Features}},
	url = {https://www.aclweb.org/anthology/D19-1513},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://www.aclweb.org/anthology/D19-1513},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1513}}

@article{wei_text_2019,
	author = {Wei, Wei and Guo, Chonghui},
	doi = {10.1016/j.neucom.2019.08.047},
	issn = {09252312},
	journal = {Neurocomputing},
	language = {en},
	month = nov,
	pages = {11--24},
	title = {A text semantic topic discovery method based on the conditional co-occurrence degree},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219311853},
	urldate = {2022-11-25},
	volume = {368},
	year = {2019},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219311853},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2019.08.047}}

@article{sridhar_heterogeneous_2022,
	abstract = {Abstract
            Researchers in the social sciences are often interested in the relationship between text and an outcome of interest, where the goal is to both uncover latent patterns in the text and predict outcomes for unseen texts. To this end, this paper develops the heterogeneous supervised topic model (HSTM), a probabilistic approach to text analysis and prediction. HSTMs posit a joint model of text and outcomes to find heterogeneous patterns that help with both text analysis and prediction. The main benefit of HSTMs is that they capture heterogeneity in the relationship between text and the outcome across latent topics. To fit HSTMs, we develop a variational inference algorithm based on the auto-encoding variational Bayes framework. We study the performance of HSTMs on eight datasets and find that they consistently outperform related methods, including fine-tuned black-box models. Finally, we apply HSTMs to analyze news articles labeled with pro- or anti-tone. We find evidence of differing language used to signal a pro- and anti-tone.},
	author = {Sridhar, Dhanya and Daum{\'e}, Hal and Blei, David},
	doi = {10.1162/tacl_a_00487},
	file = {Full Text:files/1644/Sridhar et al. - 2022 - Heterogeneous Supervised Topic Models.pdf:application/pdf},
	issn = {2307-387X},
	journal = {Transactions of the Association for Computational Linguistics},
	language = {en},
	month = jun,
	pages = {732--745},
	title = {Heterogeneous {Supervised} {Topic} {Models}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00487/111727/Heterogeneous-Supervised-Topic-Models},
	urldate = {2022-11-25},
	volume = {10},
	year = {2022},
	bdsk-url-1 = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00487/111727/Heterogeneous-Supervised-Topic-Models},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00487}}

@inproceedings{bose_generalisability_2021,
	address = {Online},
	author = {Bose, Tulika and Illina, Irina and Fohr, Dominique},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {NLP} for {Internet} {Freedom}: {Censorship}, {Disinformation}, and {Propaganda}},
	doi = {10.18653/v1/2021.nlp4if-1.8},
	file = {Full Text:files/1632/Bose et al. - 2021 - Generalisability of Topic Models in Cross-corpora .pdf:application/pdf},
	language = {en},
	pages = {51--56},
	publisher = {Association for Computational Linguistics},
	title = {Generalisability of {Topic} {Models} in {Cross}-corpora {Abusive} {Language} {Detection}},
	url = {https://www.aclweb.org/anthology/2021.nlp4if-1.8},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2021.nlp4if-1.8},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.nlp4if-1.8}}

@article{joshi_deepsumm_2023,
	author = {Joshi, Akanksha and Fidalgo, Eduardo and Alegre, Enrique and Fern{\'a}ndez-Robles, Laura},
	doi = {10.1016/j.eswa.2022.118442},
	issn = {09574174},
	journal = {Expert Systems with Applications},
	language = {en},
	month = jan,
	pages = {118442},
	shorttitle = {{DeepSumm}},
	title = {{DeepSumm}: {Exploiting} topic models and sequence to sequence networks for extractive text summarization},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422015391},
	urldate = {2022-11-25},
	volume = {211},
	year = {2023},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422015391},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2022.118442}}

@article{lau_how_2020,
	abstract = {We study the influence of context on sentence acceptability. First we compare the acceptability ratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results show that context induces a cognitive load for humans, which compresses the distribution of ratings. Moreover, in relevant contexts we observe a discourse coherence effect that uniformly raises acceptability. Next, we test unidirectional and bidirectional language models in their ability to predict acceptability ratings. The bidirectional models show very promising results, with the best model achieving a new state-of-the-art for unsupervised acceptability prediction. The two sets of experiments provide insights into the cognitive aspects of sentence processing and central issues in the computational modeling of text and discourse.},
	author = {Lau, Jey Han and Armendariz, Carlos and Lappin, Shalom and Purver, Matthew and Shu, Chang},
	doi = {10.1162/tacl_a_00315},
	file = {Full Text:files/1646/Lau et al. - 2020 - How Furiously Can Colorless Green Ideas Sleep Sen.pdf:application/pdf},
	issn = {2307-387X},
	journal = {Transactions of the Association for Computational Linguistics},
	language = {en},
	month = dec,
	pages = {296--310},
	shorttitle = {How {Furiously} {Can} {Colorless} {Green} {Ideas} {Sleep}?},
	title = {How {Furiously} {Can} {Colorless} {Green} {Ideas} {Sleep}? {Sentence} {Acceptability} in {Context}},
	url = {https://direct.mit.edu/tacl/article/96455},
	urldate = {2022-11-25},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://direct.mit.edu/tacl/article/96455},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00315}}

@article{liu_semantic_2019,
	abstract = {In this work, we aim to re-rank the n-best hypotheses of an automatic speech recognition system by punishing the sentences which have words that are semantically different from the context and rewarding the sentences where all words are in semantical harmony. To achieve this, we proposed a topic similarity score that measures the difference between topic distribution of words and the corresponding sentence. We also proposed another word-discourse score that quantifies the likeliness for a word to appear in the sentence by the inner production of word vector and discourse vector. Besides, we used the latent semantic marginal and a variation of log bi-linear model to get the sentence coordination score. In addition we introduce a fallibility weight, which assists the computation of the sentence semantically coordination score by instructing the model to pay more attention to the words that appear less in the hypotheses list and we show how to use the scores and the fallibility weight in hypotheses rescoring. None of the rescoring methods need extra parameters other than the semantic models. Experiments conducted on the Wall Street Journal corpus show that, by using the proposed word-discourse score on 50-dimension word embedding, we can achieve 0.29\% and 0.51\% absolute word error rate (WER) reductions on the two testsets.},
	author = {Liu, Chang and Zhang, Pengyuan and Li, Ta and Yan, Yonghong},
	doi = {10.3390/app9235053},
	file = {Full Text:files/1647/Liu et al. - 2019 - Semantic Features Based N-Best Rescoring Methods f.pdf:application/pdf},
	issn = {2076-3417},
	journal = {Applied Sciences},
	language = {en},
	month = nov,
	number = {23},
	pages = {5053},
	title = {Semantic {Features} {Based} {N}-{Best} {Rescoring} {Methods} for {Automatic} {Speech} {Recognition}},
	url = {https://www.mdpi.com/2076-3417/9/23/5053},
	urldate = {2022-11-25},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://www.mdpi.com/2076-3417/9/23/5053},
	bdsk-url-2 = {https://doi.org/10.3390/app9235053}}

@article{li_review_2019,
	abstract = {With the massive growth of the Internet, text data has become one of the main formats of tourism big data. As an effective expression means of tourists' opinions, text mining of such data has big potential to inspire innovations for tourism practitioners. In the past decade, a variety of text mining techniques have been proposed and applied to tourism analysis to develop tourism value analysis models, build tourism recommendation systems, create tourist profiles, and make policies for supervising tourism markets. The successes of these techniques have been further boosted by the progress of natural language processing (NLP), machine learning, and deep learning. With the understanding of the complexity due to this diverse set of techniques and tourism text data sources, this work attempts to provide a detailed and up-to-date review of text mining techniques that have been, or have the potential to be, applied to modern tourism big data analysis. We summarize and discuss different text representation strategies, text-based NLP techniques for topic extraction, text classification, sentiment analysis, and text clustering in the context of tourism text mining, and their applications in tourist profiling, destination image analysis, market demand, etc. Our work also provides guidelines for constructing new tourism big data applications and outlines promising research areas in this field for incoming years.},
	author = {Li, Qin and Li, Shaobo and Zhang, Sen and Hu, Jie and Hu, Jianjun},
	doi = {10.3390/app9163300},
	file = {Full Text:files/1634/Li et al. - 2019 - A Review of Text Corpus-Based Tourism Big Data Min.pdf:application/pdf},
	issn = {2076-3417},
	journal = {Applied Sciences},
	language = {en},
	month = aug,
	number = {16},
	pages = {3300},
	title = {A {Review} of {Text} {Corpus}-{Based} {Tourism} {Big} {Data} {Mining}},
	url = {https://www.mdpi.com/2076-3417/9/16/3300},
	urldate = {2022-11-25},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://www.mdpi.com/2076-3417/9/16/3300},
	bdsk-url-2 = {https://doi.org/10.3390/app9163300}}

@inproceedings{aggarwal_goal-driven_2020,
	address = {Virtual Event Brazil},
	author = {Aggarwal, Samarth and Garg, Rohin and Sancheti, Abhilasha and Guda, Bhanu Prakash Reddy and Burhanuddin, Iftikhar Ahamath},
	booktitle = {Fourteenth {ACM} {Conference} on {Recommender} {Systems}},
	doi = {10.1145/3383313.3412255},
	file = {Submitted Version:files/1658/Aggarwal et al. - 2020 - Goal-driven Command Recommendations for Analysts.pdf:application/pdf},
	isbn = {978-1-4503-7583-2},
	language = {en},
	month = sep,
	pages = {160--169},
	publisher = {ACM},
	title = {Goal-driven {Command} {Recommendations} for {Analysts}},
	url = {https://dl.acm.org/doi/10.1145/3383313.3412255},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3383313.3412255},
	bdsk-url-2 = {https://doi.org/10.1145/3383313.3412255}}

@inproceedings{qin_supervised_2020,
	address = {Atlanta, GA, USA},
	author = {Qin, Xiao and Xiao, Cao and Ma, Tengfei and kakar, Tabassum and Wunnava, Susmitha and Kong, Xiangnan and Rundensteiner, Elke and Wang, Fei},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	doi = {10.1109/BigData50022.2020.9378347},
	isbn = {978-1-72816-251-5},
	month = dec,
	pages = {758--767},
	publisher = {IEEE},
	title = {Supervised {Topic} {Compositional} {Neural} {Language} {Model} for {Clinical} {Narrative} {Understanding}},
	url = {https://ieeexplore.ieee.org/document/9378347/},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9378347/},
	bdsk-url-2 = {https://doi.org/10.1109/BigData50022.2020.9378347}}

@inproceedings{kawamae_text_2021,
	address = {ESSENDON VIC Australia},
	author = {Kawamae, Noriaki},
	booktitle = {{IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence}},
	doi = {10.1145/3486622.3493968},
	isbn = {978-1-4503-9115-3},
	language = {en},
	month = dec,
	pages = {262--269},
	publisher = {ACM},
	title = {A {Text} {Generation} {Model} that {Maintains} the {Order} of {Words}, {Topics}, and {Parts} of {Speech} via {Their} {Embedding} {Representations} and {Neural} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3486622.3493968},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3486622.3493968},
	bdsk-url-2 = {https://doi.org/10.1145/3486622.3493968}}

@article{wang_hierarchical_2021,
	abstract = {For guiding natural language generation, many semantic-driven methods have been proposed. While clearly improving the performance of the end-to-end training task, these existing semantic-driven methods still have clear limitations: for example, (i) they only utilize shallow semantic signals (e.g., from topic models) with only a single stochastic hidden layer in their data generation process, which suffer easily from noise (especially adapted for short-text etc.) and lack of interpretation; (ii) they ignore the sentence order and document context, as they treat each document as a bag of sentences, and fail to capture the long-distance dependencies and global semantic meaning of a document. To overcome these problems, we propose a novel semantic-driven language modeling framework, which is a method to learn a Hierarchical Language Model and a Recurrent Conceptualization-enhanced Gamma Belief Network, simultaneously. For scalable inference, we develop the auto-encoding Variational Recurrent Inference, allowing efficient end-to-end training and simultaneously capturing global semantics from a text corpus. Especially, this article introduces concept information derived from high-quality lexical knowledge graph Probase, which leverages strong interpretability and anti-nose capability for the proposed model. Moreover, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence concept dependence. Experiments conducted on several NLP tasks validate the superiority of the proposed approach, which could effectively infer meaningful hierarchical concept structure of document and hierarchical multi-scale structures of sequences, even compared with latest state-of-the-art Transformer-based models.},
	author = {Wang, Yashen and Zhang, Huanhuan and Liu, Zhirun and Zhou, Qiang},
	doi = {10.1145/3451167},
	issn = {1556-4681, 1556-472X},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	language = {en},
	month = may,
	number = {6},
	pages = {1--22},
	title = {Hierarchical {Concept}-{Driven} {Language} {Model}},
	url = {https://dl.acm.org/doi/10.1145/3451167},
	urldate = {2022-11-25},
	volume = {15},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3451167},
	bdsk-url-2 = {https://doi.org/10.1145/3451167}}

@article{zandie_topical_2022,
	abstract = {Abstract
            Large-scale transformer-based language models (LMs) demonstrate impressive capabilities in open-text generation. However, controlling the generated text's properties such as the topic, style, and sentiment is challenging and often requires significant changes to the model architecture or retraining and fine-tuning the model on new supervised data. This paper presents a novel approach for topical language generation (TLG) by combining a pre-trained LM with topic modeling information. We cast the problem using Bayesian probability formulation with topic probabilities as a prior, LM probabilities as the likelihood, and TLG probability as the posterior. In learning the model, we derive the topic probability distribution from the user-provided document's natural structure. Furthermore, we extend our model by introducing new parameters and functions to influence the quantity of the topical features presented in the generated text. This feature would allow us to easily control the topical properties of the generated text. Our experimental results demonstrate that our model outperforms the state-of-the-art results on coherency, diversity, and fluency while being faster in decoding.},
	author = {Zandie, Rohola and Mahoor, Mohammad H.},
	doi = {10.1017/S1351324922000031},
	file = {Submitted Version:files/1657/Zandie and Mahoor - 2022 - Topical language generation using transformers.pdf:application/pdf},
	issn = {1351-3249, 1469-8110},
	journal = {Natural Language Engineering},
	language = {en},
	month = feb,
	pages = {1--23},
	title = {Topical language generation using transformers},
	url = {https://www.cambridge.org/core/product/identifier/S1351324922000031/type/journal_article},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://www.cambridge.org/core/product/identifier/S1351324922000031/type/journal_article},
	bdsk-url-2 = {https://doi.org/10.1017/S1351324922000031}}

@inproceedings{yang_topnet_2021,
	address = {Virtual Event Singapore},
	author = {Yang, Yazheng and Pan, Boyuan and Cai, Deng and Sun, Huan},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	doi = {10.1145/3447548.3467410},
	file = {Submitted Version:files/1655/Yang et al. - 2021 - TopNet Learning from Neural Topic Model to Genera.pdf:application/pdf},
	isbn = {978-1-4503-8332-5},
	language = {en},
	month = aug,
	pages = {1997--2005},
	publisher = {ACM},
	shorttitle = {{TopNet}},
	title = {{TopNet}: {Learning} from {Neural} {Topic} {Model} to {Generate} {Long} {Stories}},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467410},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3447548.3467410},
	bdsk-url-2 = {https://doi.org/10.1145/3447548.3467410}}

@inproceedings{duan_enslm_2021,
	address = {Online},
	author = {Duan, Zhibin and Zhang, Hao and Wang, Chaojie and Wang, Zhengjue and Chen, Bo and Zhou, Mingyuan},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2021.acl-long.230},
	file = {Full Text:files/1662/Duan et al. - 2021 - EnsLM Ensemble Language Model for Data Diversity .pdf:application/pdf},
	language = {en},
	pages = {2954--2967},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{EnsLM}},
	title = {{EnsLM}: {Ensemble} {Language} {Model} for {Data} {Diversity} by {Semantic} {Clustering}},
	url = {https://aclanthology.org/2021.acl-long.230},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.230},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.230}}

@inproceedings{wang_--fly_2020,
	address = {Online},
	author = {Wang, Hai and McAllester, David},
	booktitle = {Proceedings of the {First} {Joint} {Workshop} on {Narrative} {Understanding}, {Storylines}, and {Events}},
	doi = {10.18653/v1/2020.nuse-1.14},
	file = {Full Text:files/1643/Wang and McAllester - 2020 - On-The-Fly Information Retrieval Augmentation for .pdf:application/pdf},
	language = {en},
	pages = {114--119},
	publisher = {Association for Computational Linguistics},
	title = {On-{The}-{Fly} {Information} {Retrieval} {Augmentation} for {Language} {Models}},
	url = {https://www.aclweb.org/anthology/2020.nuse-1.14},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2020.nuse-1.14},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.nuse-1.14}}

@incollection{kamath_convolutional_2019,
	address = {Cham},
	author = {Kamath, Uday and Liu, John and Whitaker, James},
	booktitle = {Deep {Learning} for {NLP} and {Speech} {Recognition}},
	collaborator = {Kamath, Uday and Liu, John and Whitaker, James},
	doi = {10.1007/978-3-030-14596-5_6},
	isbn = {978-3-030-14595-8 978-3-030-14596-5},
	language = {en},
	pages = {263--314},
	publisher = {Springer International Publishing},
	title = {Convolutional {Neural} {Networks}},
	url = {http://link.springer.com/10.1007/978-3-030-14596-5_6},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-14596-5_6},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-14596-5_6}}

@article{naik_context_2018,
	abstract = {We describe an intelligent context-aware conversational system that incorporates screen context information to service multimodal user requests. Screen content is used for disambiguation of utterances that refer to screen objects and for enabling the user to act upon screen objects using voice commands. We propose a deep learning architecture that jointly models the user utterance and the screen and incorporates detailed screen content features. Our model is trained to optimize end to end semantic accuracy across contextual and non-contextual functionality, therefore learns the desired behavior directly from the data. We show that this approach outperforms a rule-based alternative, and can be extended in a straightforward manner to new contextual use cases. We perform detailed evaluation of contextual and non-contextual use cases and show that our system displays accurate contextual behavior without degrading the performance of non-contextual user requests.},
	author = {Naik, Vishal and Metallinou, Angeliki and Goel, Rahul},
	doi = {10.1609/aaai.v32i1.11952},
	file = {Full Text:files/1663/Naik et al. - 2018 - Context Aware Conversational Understanding for Int.pdf:application/pdf},
	issn = {2374-3468, 2159-5399},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = apr,
	number = {1},
	title = {Context {Aware} {Conversational} {Understanding} for {Intelligent} {Agents} {With} a {Screen}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11952},
	urldate = {2022-11-25},
	volume = {32},
	year = {2018},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/11952},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v32i1.11952}}

@article{song_hierarchical_2019,
	abstract = {A long user history inevitably reflects the transitions of personal interests over time. The analyses on the user history require the robust sequential model to anticipate the transitions and the decays of user interests. The user history is often modeled by various RNN structures, but the RNN structures in the recommendation system still suffer from the long-term dependency and the interest drifts. To resolve these challenges, we suggest HCRNN with three hierarchical contexts of the global, the local, and the temporary interests. This structure is designed to withhold the global long-term interest of users, to reflect the local sub-sequence interests, and to attend the temporary interests of each transition. Besides, we propose a hierarchical context-based gate structure to incorporate our interest drift assumption. As we suggest a new RNN structure, we support HCRNN with a complementary bi-channel attention structure to utilize hierarchical context. We experimented the suggested structure on the sequential recommendation tasks with CiteULike, MovieLens, and LastFM, and our model showed the best performances in the sequential recommendations.},
	author = {Song, Kyungwoo and Ji, Mingi and Park, Sungrae and Moon, Il-Chul},
	doi = {10.1609/aaai.v33i01.33014983},
	file = {Full Text:files/1665/Song et al. - 2019 - Hierarchical Context Enabled Recurrent Neural Netw.pdf:application/pdf},
	issn = {2374-3468, 2159-5399},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = jul,
	number = {01},
	pages = {4983--4991},
	title = {Hierarchical {Context} {Enabled} {Recurrent} {Neural} {Network} for {Recommendation}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4429},
	urldate = {2022-11-25},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/4429},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v33i01.33014983}}

@inproceedings{khandelwal_sharp_2018,
	address = {Melbourne, Australia},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/P18-1027},
	file = {Full Text:files/1654/Khandelwal et al. - 2018 - Sharp Nearby, Fuzzy Far Away How Neural Language .pdf:application/pdf},
	language = {en},
	pages = {284--294},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	title = {Sharp {Nearby}, {Fuzzy} {Far} {Away}: {How} {Neural} {Language} {Models} {Use} {Context}},
	url = {http://aclweb.org/anthology/P18-1027},
	urldate = {2022-11-25},
	year = {2018},
	bdsk-url-1 = {http://aclweb.org/anthology/P18-1027},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1027}}

@inproceedings{card_neural_2018,
	address = {Melbourne, Australia},
	author = {Card, Dallas and Tan, Chenhao and Smith, Noah A.},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/P18-1189},
	file = {Full Text:files/1651/Card et al. - 2018 - Neural Models for Documents with Metadata.pdf:application/pdf},
	language = {en},
	pages = {2031--2040},
	publisher = {Association for Computational Linguistics},
	title = {Neural {Models} for {Documents} with {Metadata}},
	url = {http://aclweb.org/anthology/P18-1189},
	urldate = {2022-11-25},
	year = {2018},
	bdsk-url-1 = {http://aclweb.org/anthology/P18-1189},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1189}}

@inproceedings{zhao_topic_2021,
	abstract = {Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with nearly a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review on this specific topic.},
	address = {Montreal, Canada},
	author = {Zhao, He and Phung, Dinh and Huynh, Viet and Jin, Yuan and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	doi = {10.24963/ijcai.2021/638},
	file = {Full Text:files/1656/Zhao et al. - 2021 - Topic Modelling Meets Deep Neural Networks A Surv.pdf:application/pdf},
	isbn = {978-0-9992411-9-6},
	language = {en},
	month = aug,
	pages = {4713--4720},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	shorttitle = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}},
	title = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}: {A} {Survey}},
	url = {https://www.ijcai.org/proceedings/2021/638},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.ijcai.org/proceedings/2021/638},
	bdsk-url-2 = {https://doi.org/10.24963/ijcai.2021/638}}

@incollection{lu_coherence_2019,
	address = {Cham},
	author = {Krasnashchok, Katsiaryna and Cherif, Aymen},
	booktitle = {Advances in {Neural} {Networks} -- {ISNN} 2019},
	doi = {10.1007/978-3-030-22796-8_45},
	editor = {Lu, Huchuan and Tang, Huajin and Wang, Zhanshan},
	isbn = {978-3-030-22795-1 978-3-030-22796-8},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {426--433},
	publisher = {Springer International Publishing},
	title = {Coherence {Regularization} for {Neural} {Topic} {Models}},
	url = {http://link.springer.com/10.1007/978-3-030-22796-8_45},
	urldate = {2022-11-25},
	volume = {11554},
	year = {2019},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-22796-8_45},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-22796-8_45}}

@inproceedings{wang_topic-guided_2019,
	address = {Minneapolis, Minnesota},
	author = {Wang, Wenlin and Gan, Zhe and Xu, Hongteng and Zhang, Ruiyi and Wang, Guoyin and Shen, Dinghan and Chen, Changyou and Carin, Lawrence},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	doi = {10.18653/v1/N19-1015},
	language = {en},
	pages = {166--177},
	publisher = {Association for Computational Linguistics},
	title = {Topic-{Guided} {Variational} {Auto}-{Encoder} for {Text} {Generation}},
	url = {http://aclweb.org/anthology/N19-1015},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {http://aclweb.org/anthology/N19-1015},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1015}}

@article{jin_topic_2022,
	abstract = {Short text clustering is a challenging and important task in many practical applications. However, many Bag-of-Word--based methods for short text clustering are often limited by the sparsity of text representation, while many sentence embedding--based methods fail to capture the document structure dependencies within a text corpus. In considerations of the shortcomings of many existing studies, a topic attention encoder (TAE) is proposed in this study. Given topics derived from corpus by the techniques of topic modelling, the cross-document information is introduced. This encoder assumes the document-topic vector to be the learning target and the concatenating vectors of the word embedding and corresponding topic-word vector to be the input. Also, a self-attention mechanism is employed in the encoder, which aims to extract weights of hidden states adaptively and encode the semantics of each short text document. With captured global dependencies and local semantics, TAE integrates the superiority of Bag-of-Word methods and sentence embedding methods. Finally, categories of benchmarking experiments were conducted by analysing three public data sets. It demonstrates that the proposed TAE outperforms many document representation benchmark methods for short text clustering.},
	author = {Jin, Jian and Zhao, Haiyuan and Ji, Ping},
	doi = {10.1177/0165551520977453},
	issn = {0165-5515, 1741-6485},
	journal = {Journal of Information Science},
	language = {en},
	month = oct,
	number = {5},
	pages = {701--717},
	shorttitle = {Topic attention encoder},
	title = {Topic attention encoder: {A} self-supervised approach for short text clustering},
	url = {http://journals.sagepub.com/doi/10.1177/0165551520977453},
	urldate = {2022-11-25},
	volume = {48},
	year = {2022},
	bdsk-url-1 = {http://journals.sagepub.com/doi/10.1177/0165551520977453},
	bdsk-url-2 = {https://doi.org/10.1177/0165551520977453}}

@article{hourrane_topic-transformer_2022,
	author = {Hourrane, Oumaima and Benlahmar, El Habib},
	doi = {10.3844/jcssp.2022.18.25},
	file = {Full Text:files/1659/Hourrane and Benlahmar - 2022 - Topic-Transformer for Document-Level Language Unde.pdf:application/pdf},
	issn = {1549-3636},
	journal = {Journal of Computer Science},
	month = jan,
	number = {1},
	pages = {18--25},
	title = {Topic-{Transformer} for {Document}-{Level} {Language} {Understanding}},
	url = {https://thescipub.com/abstract/10.3844/jcssp.2022.18.25},
	urldate = {2022-11-25},
	volume = {18},
	year = {2022},
	bdsk-url-1 = {https://thescipub.com/abstract/10.3844/jcssp.2022.18.25},
	bdsk-url-2 = {https://doi.org/10.3844/jcssp.2022.18.25}}

@article{gao_topic-driven_2019,
	author = {Gao, Ce and Ren, Jiangtao},
	doi = {10.1016/j.neucom.2019.01.002},
	issn = {09252312},
	journal = {Neurocomputing},
	language = {en},
	month = mar,
	pages = {374--380},
	title = {A topic-driven language model for learning to generate diverse sentences},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219300128},
	urldate = {2022-11-25},
	volume = {333},
	year = {2019},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219300128},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2019.01.002}}

@inproceedings{hosseiny_marani_one_2022,
	address = {Atlanta GA USA},
	author = {Hosseiny Marani, Amin and Levine, Joshua and Baumer, Eric P.S.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	doi = {10.1145/3511808.3557410},
	file = {Full Text:files/1660/Hosseiny Marani et al. - 2022 - One Rating to Rule Them All Evidence of Multidim.pdf:application/pdf},
	isbn = {978-1-4503-9236-5},
	language = {en},
	month = oct,
	pages = {768--779},
	publisher = {ACM},
	shorttitle = {One {Rating} to {Rule} {Them} {All}?},
	title = {One {Rating} to {Rule} {Them} {All}?: {Evidence} of {Multidimensionality} in {Human} {Assessment} of {Topic} {Labeling} {Quality}},
	url = {https://dl.acm.org/doi/10.1145/3511808.3557410},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3511808.3557410},
	bdsk-url-2 = {https://doi.org/10.1145/3511808.3557410}}

@incollection{hacid_topic-net_2018,
	address = {Cham},
	author = {Peng, Min and Chen, Dian and Xie, Qianqian and Zhang, Yanchun and Wang, Hua and Hu, Gang and Gao, Wang and Zhang, Yihan},
	booktitle = {Web {Information} {Systems} {Engineering} -- {WISE} 2018},
	doi = {10.1007/978-3-030-02922-7_33},
	editor = {Hacid, Hakim and Cellary, Wojciech and Wang, Hua and Paik, Hye-Young and Zhou, Rui},
	isbn = {978-3-030-02921-0 978-3-030-02922-7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {483--496},
	publisher = {Springer International Publishing},
	title = {Topic-{Net} {Conversation} {Model}},
	url = {http://link.springer.com/10.1007/978-3-030-02922-7_33},
	urldate = {2022-11-25},
	volume = {11233},
	year = {2018},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-02922-7_33},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-02922-7_33}}

@inproceedings{kawamae_topic_2019,
	address = {San Francisco, CA, USA},
	author = {Kawamae, Noriaki},
	booktitle = {The {World} {Wide} {Web} {Conference} on - {WWW} '19},
	doi = {10.1145/3308558.3313757},
	isbn = {978-1-4503-6674-8},
	language = {en},
	pages = {2900--2906},
	publisher = {ACM Press},
	shorttitle = {Topic {Structure}-{Aware} {Neural} {Language} {Model}},
	title = {Topic {Structure}-{Aware} {Neural} {Language} {Model}: {Unified} language model that maintains word and topic ordering by their embedded representations},
	url = {http://dl.acm.org/citation.cfm?doid=3308558.3313757},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?doid=3308558.3313757},
	bdsk-url-2 = {https://doi.org/10.1145/3308558.3313757}}

@inproceedings{nambhi_stuck_2019,
	address = {Larnaca Cyprus},
	author = {Nambhi, Aadhavan M. and Reddy, Bhanu Prakash and Agarwal, Aarsh Prakash and Verma, Gaurav and Singh, Harvineet and Burhanuddin, Iftikhar Ahamath},
	booktitle = {Proceedings of the 27th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	doi = {10.1145/3320435.3320477},
	isbn = {978-1-4503-6021-0},
	language = {en},
	month = jun,
	pages = {271--275},
	publisher = {ACM},
	shorttitle = {Stuck?},
	title = {Stuck? {No} worries!: {Task}-aware {Command} {Recommendation} and {Proactive} {Help} for {Analysts}},
	url = {https://dl.acm.org/doi/10.1145/3320435.3320477},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3320435.3320477},
	bdsk-url-2 = {https://doi.org/10.1145/3320435.3320477}}

@inproceedings{zhou_early_2019,
	address = {Minneapolis, Minnesota},
	author = {Zhou, Kaimin and Shu, Chang and Li, Binyang and Lau, Jey Han},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	doi = {10.18653/v1/N19-1163},
	language = {en},
	pages = {1614--1623},
	publisher = {Association for Computational Linguistics},
	title = {Early {Rumour} {Detection}},
	url = {http://aclweb.org/anthology/N19-1163},
	urldate = {2022-11-25},
	year = {2019},
	bdsk-url-1 = {http://aclweb.org/anthology/N19-1163},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1163}}

@inproceedings{bernardy_influence_2018,
	address = {Melbourne, Australia},
	author = {Bernardy, Jean-Philippe and Lappin, Shalom and Lau, Jey Han},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	doi = {10.18653/v1/P18-2073},
	file = {Full Text:files/1661/Bernardy et al. - 2018 - The Influence of Context on Sentence Acceptability.pdf:application/pdf},
	language = {en},
	pages = {456--461},
	publisher = {Association for Computational Linguistics},
	title = {The {Influence} of {Context} on {Sentence} {Acceptability} {Judgements}},
	url = {http://aclweb.org/anthology/P18-2073},
	urldate = {2022-11-25},
	year = {2018},
	bdsk-url-1 = {http://aclweb.org/anthology/P18-2073},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2073}}

@inproceedings{tran_context-dependent_2018,
	address = {New Orleans, Louisiana},
	author = {Tran, Quan Hung and Lai, Tuan and Haffari, Gholamreza and Zukerman, Ingrid and Bui, Trung and Bui, Hung},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	doi = {10.18653/v1/N18-1115},
	file = {Full Text:files/1664/Tran et al. - 2018 - The Context-Dependent Additive Recurrent Neural Ne.pdf:application/pdf},
	language = {en},
	pages = {1274--1283},
	publisher = {Association for Computational Linguistics},
	title = {The {Context}-{Dependent} {Additive} {Recurrent} {Neural} {Net}},
	url = {http://aclweb.org/anthology/N18-1115},
	urldate = {2022-11-25},
	year = {2018},
	bdsk-url-1 = {http://aclweb.org/anthology/N18-1115},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1115}}

@article{guo_recurrent_2022,
	abstract = {The abundant sequential documents such as online archival, social media, and news feeds are streamingly updated, where each chunk of documents is incorporated with smoothly evolving yet dependent topics. Such digital texts have attracted extensive research on dynamic topic modeling to infer hidden evolving topics and their temporal dependencies. However, most of the existing approaches focus on single-topic-thread evolution and ignore the fact that a current topic may be coupled with multiple relevant prior topics. In addition, these approaches also incur the intractable inference problem when inferring latent parameters, resulting in a high computational cost and performance degradation. In this work, we assume that a current topic evolves from all prior topics with corresponding coupling weights, forming the
              multi-topic-thread evolution
              . Our method models the dependencies between evolving topics and thoroughly encodes their complex multi-couplings across time steps. To conquer the intractable inference challenge, a new solution with a set of novel data augmentation techniques is proposed, which successfully discomposes the multi-couplings between evolving topics. A fully conjugate model is thus obtained to guarantee the effectiveness and efficiency of the inference technique. A novel Gibbs sampler with a backward--forward filter algorithm efficiently learns latent time-evolving parameters in a closed-form. In addition, the latent Indian Buffet Process compound distribution is exploited to automatically infer the overall topic number and customize the sparse topic proportions for each sequential document without bias. The proposed method is evaluated on both synthetic and real-world datasets against the competitive baselines, demonstrating its superiority over the baselines in terms of the low per-word perplexity, high coherent topics, and better document time prediction.},
	author = {Guo, Jinjin and Cao, Longbing and Gong, Zhiguo},
	doi = {10.1145/3451530},
	file = {Submitted Version:files/1667/Guo et al. - 2022 - Recurrent Coupled Topic Modeling over Sequential D.pdf:application/pdf},
	issn = {1556-4681, 1556-472X},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	language = {en},
	month = feb,
	number = {1},
	pages = {1--32},
	title = {Recurrent {Coupled} {Topic} {Modeling} over {Sequential} {Documents}},
	url = {https://dl.acm.org/doi/10.1145/3451530},
	urldate = {2022-11-25},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3451530},
	bdsk-url-2 = {https://doi.org/10.1145/3451530}}

@article{yasunaga_topiceq_2019,
	abstract = {Scientific documents rely on both mathematics and text to communicate ideas. Inspired by the topical correspondence between mathematical equations and word contexts observed in scientific texts, we propose a novel topic model that jointly generates mathematical equations and their surrounding text (TopicEq). Using an extension of the correlated topic model, the context is generated from a mixture of latent topics, and the equation is generated by an RNN that depends on the latent topic activations. To experiment with this model, we create a corpus of 400K equation-context pairs extracted from a range of scientific articles from arXiv, and fit the model using a variational autoencoder approach. Experimental results show that this joint model significantly outperforms existing topic models and equation models for scientific texts. Moreover, we qualitatively show that the model effectively captures the relationship between topics and mathematics, enabling novel applications such as topic-aware equation generation, equation topic inference, and topic-aware alignment of mathematical symbols and words.},
	author = {Yasunaga, Michihiro and Lafferty, John D.},
	doi = {10.1609/aaai.v33i01.33017394},
	file = {Full Text:files/1666/Yasunaga and Lafferty - 2019 - TopicEq A Joint Topic and Mathematical Equation M.pdf:application/pdf},
	issn = {2374-3468, 2159-5399},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = jul,
	number = {01},
	pages = {7394--7401},
	shorttitle = {{TopicEq}},
	title = {{TopicEq}: {A} {Joint} {Topic} and {Mathematical} {Equation} {Model} for {Scientific} {Texts}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4728},
	urldate = {2022-11-25},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/4728},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v33i01.33017394}}
