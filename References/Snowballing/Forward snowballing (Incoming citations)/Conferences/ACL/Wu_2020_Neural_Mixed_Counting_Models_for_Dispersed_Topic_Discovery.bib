%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 15:45:50 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{hoyle_2021_is_automated_topic_model_evaluation_broken_the_incoherence_of_coherence,
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 15:45:45 +0100},
	date-modified = {2022-11-25 15:45:45 +0100},
	doi = {10.48550/ARXIV.2107.02173},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
	url = {https://arxiv.org/abs/2107.02173},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2107.02173}}

@inproceedings{zhang_2022_nonparametric_forest_structured_neural_topic_modeling,
	abstract = {Neural topic models have been widely used in discovering the latent semantics from a corpus. Recently, there are several researches on hierarchical neural topic models since the relationships among topics are valuable for data analysis and exploration. However, the existing hierarchical neural topic models are limited to generate a single topic tree. In this study, we present a nonparametric forest-structured neural topic model by firstly applying the self-attention mechanism to capture parent-child topic relationships, and then build a sparse directed acyclic graph to form a topic forest. Experiments indicate that our model can automatically learn a forest-structured topic hierarchy with indefinite numbers of trees and leaves, and significantly outperforms the baseline models on topic hierarchical rationality and affinity.},
	address = {Gyeongju, Republic of Korea},
	author = {Zhang, Zhihong and Zhang, Xuewen and Rao, Yanghui},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-11-25 15:45:42 +0100},
	date-modified = {2022-11-25 15:45:42 +0100},
	month = oct,
	pages = {2585--2597},
	publisher = {International Committee on Computational Linguistics},
	title = {Nonparametric Forest-Structured Neural Topic Modeling},
	url = {https://aclanthology.org/2022.coling-1.228},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.228}}

@article{jiang_2022_parallel_dynamic_topic_modeling_via_evolving_topic_adjustment_and_term_weighting_scheme,
	author = {Jiang, Hongyu and Lei, Zhiqi and Rao, Yanghui and Xie, Haoran and Wang, Fu Lee},
	doi = {10.1016/j.ins.2021.11.060},
	issn = {00200255},
	journal = {Information Sciences},
	language = {en},
	month = mar,
	pages = {176--193},
	title = {Parallel dynamic topic modeling via evolving topic adjustment and term weighting scheme},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025521011828},
	urldate = {2022-11-25},
	volume = {585},
	year = {2022},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0020025521011828},
	bdsk-url-2 = {https://doi.org/10.1016/j.ins.2021.11.060}}

@article{li_2021_topic_level_knowledge_sub_graphs_for_multi_turn_dialogue_generation,
	author = {Li, Jing and Huang, Qingbao and Cai, Yi and Liu, Yongkang and Fu, Mingyi and Li, Qing},
	doi = {10.1016/j.knosys.2021.107499},
	issn = {09507051},
	journal = {Knowledge-Based Systems},
	language = {en},
	month = dec,
	pages = {107499},
	title = {Topic-level knowledge sub-graphs for multi-turn dialogue generation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121007619},
	urldate = {2022-11-25},
	volume = {234},
	year = {2021},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121007619},
	bdsk-url-2 = {https://doi.org/10.1016/j.knosys.2021.107499}}

@article{chen_2021_hierarchical_neural_topic_modeling_with_manifold_regularization,
	author = {Chen, Ziye and Ding, Cheng and Rao, Yanghui and Xie, Haoran and Tao, Xiaohui and Cheng, Gary and Wang, Fu Lee},
	doi = {10.1007/s11280-021-00963-7},
	file = {Accepted Version:files/1302/Chen et al. - 2021 - Hierarchical neural topic modeling with manifold r.pdf:application/pdf},
	issn = {1386-145X, 1573-1413},
	journal = {World Wide Web},
	language = {en},
	month = nov,
	number = {6},
	pages = {2139--2160},
	title = {Hierarchical neural topic modeling with manifold regularization},
	url = {https://link.springer.com/10.1007/s11280-021-00963-7},
	urldate = {2022-11-25},
	volume = {24},
	year = {2021},
	bdsk-url-1 = {https://link.springer.com/10.1007/s11280-021-00963-7},
	bdsk-url-2 = {https://doi.org/10.1007/s11280-021-00963-7}}

@inproceedings{chen_2021_tree_structured_topic_modeling_with_nonparametric_neural_variational_inference,
	address = {Online},
	author = {Chen, Ziye and Ding, Cheng and Zhang, Zusheng and Rao, Yanghui and Xie, Haoran},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2021.acl-long.182},
	file = {Full Text:files/1301/Chen et al. - 2021 - Tree-Structured Topic Modeling with Nonparametric .pdf:application/pdf},
	language = {en},
	pages = {2343--2353},
	publisher = {Association for Computational Linguistics},
	title = {Tree-{Structured} {Topic} {Modeling} with {Nonparametric} {Neural} {Variational} {Inference}},
	url = {https://aclanthology.org/2021.acl-long.182},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.182},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.182}}

@inproceedings{liu_2022_innovation_hierarchy_based_patent_representation_model,
	address = {Padua, Italy},
	author = {Liu, Weidong and Zhang, Haijie},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	doi = {10.1109/IJCNN55064.2022.9892513},
	isbn = {978-1-72818-671-9},
	month = jul,
	pages = {1--8},
	publisher = {IEEE},
	title = {Innovation {Hierarchy} {Based} {Patent} {Representation} {Model}},
	url = {https://ieeexplore.ieee.org/document/9892513/},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9892513/},
	bdsk-url-2 = {https://doi.org/10.1109/IJCNN55064.2022.9892513}}

@inproceedings{panwar_2021_tan_ntm_topic_attention_networks_for_neural_topic_modeling,
	address = {Online},
	author = {Panwar, Madhur and Shailabh, Shashank and Aggarwal, Milan and Krishnamurthy, Balaji},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2021.acl-long.299},
	file = {Full Text:files/1300/Panwar et al. - 2021 - TAN-NTM Topic Attention Networks for Neural Topic.pdf:application/pdf},
	language = {en},
	pages = {3865--3880},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{TAN}-{NTM}},
	title = {{TAN}-{NTM}: {Topic} {Attention} {Networks} for {Neural} {Topic} {Modeling}},
	url = {https://aclanthology.org/2021.acl-long.299},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.299},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.299}}

@inproceedings{zhao_2021_topic_modelling_meets_deep_neural_networks_a_survey,
	abstract = {Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with nearly a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review on this specific topic.},
	address = {Montreal, Canada},
	author = {Zhao, He and Phung, Dinh and Huynh, Viet and Jin, Yuan and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	doi = {10.24963/ijcai.2021/638},
	file = {Full Text:files/1299/Zhao et al. - 2021 - Topic Modelling Meets Deep Neural Networks A Surv.pdf:application/pdf},
	isbn = {978-0-9992411-9-6},
	language = {en},
	month = aug,
	pages = {4713--4720},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	shorttitle = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}},
	title = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}: {A} {Survey}},
	url = {https://www.ijcai.org/proceedings/2021/638},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.ijcai.org/proceedings/2021/638},
	bdsk-url-2 = {https://doi.org/10.24963/ijcai.2021/638}}
