%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 16:13:17 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{nguyen_2021_contrastive_learning_for_neural_topic_model,
	author = {Nguyen, Thong and Luu, Anh Tuan},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 16:13:16 +0100},
	date-modified = {2022-11-25 16:13:16 +0100},
	doi = {10.48550/ARXIV.2110.12764},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Contrastive Learning for Neural Topic Model},
	url = {https://arxiv.org/abs/2110.12764},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2110.12764}}

@misc{hoyle_2021_is_automated_topic_model_evaluation_broken_the_incoherence_of_coherence,
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 16:13:11 +0100},
	date-modified = {2022-11-25 16:13:11 +0100},
	doi = {10.48550/ARXIV.2107.02173},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
	url = {https://arxiv.org/abs/2107.02173},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2107.02173}}

@article{kashyap_2022_so_different_yet_so_alike_constrained_unsupervised_text_style_transfer,
	abstract = {Automatic transfer of text between domains has become popular in recent times. One of its aims is to preserve the semantic content of text being translated from source to target domain. However, it does not explicitly maintain other attributes between the source and translated text, for e.g., text length and descriptiveness. Maintaining constraints in transfer has several downstream applications, including data augmentation and de-biasing. We introduce a method for such constrained unsupervised text style transfer by introducing two complementary losses to the generative adversarial network (GAN) family of models. Unlike the competing losses used in GANs, we introduce cooperative losses where the discriminator and the generator cooperate and reduce the same loss. The first is a contrastive loss and the second is a classification loss, aiming to regularize the latent space further and bring similar sentences across domains closer together. We demonstrate that such training retains lexical, syntactic, and domain-specific constraints between domains for multiple benchmark datasets, including ones where more than one attribute change. We show that the complementary cooperative losses improve text quality, according to both automated and human evaluation measures.},
	annote = {Other
Accepted to ACL 2022},
	author = {Kashyap, Abhinav Ramesh and Hazarika, Devamanyu and Kan, Min-Yen and Zimmermann, Roger and Poria, Soujanya},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2205.04093},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {So {Different} {Yet} {So} {Alike}! {Constrained} {Unsupervised} {Text} {Style} {Transfer}},
	url = {https://arxiv.org/abs/2205.04093},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2205.04093},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2205.04093}}

@inproceedings{zhao_2021_adversarial_learning_of_poisson_factorisation_model_for_gauging_brand_sentiment_in_user_reviews,
	address = {Online},
	author = {Zhao, Runcong and Gui, Lin and Pergola, Gabriele and He, Yulan},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	doi = {10.18653/v1/2021.eacl-main.199},
	file = {Full Text:files/1389/Zhao et al. - 2021 - Adversarial Learning of Poisson Factorisation Mode.pdf:application/pdf},
	language = {en},
	pages = {2341--2351},
	publisher = {Association for Computational Linguistics},
	title = {Adversarial {Learning} of {Poisson} {Factorisation} {Model} for {Gauging} {Brand} {Sentiment} in {User} {Reviews}},
	url = {https://aclanthology.org/2021.eacl-main.199},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.199},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.199}}

@inproceedings{panwar_2021_tan_ntm_topic_attention_networks_for_neural_topic_modeling,
	address = {Online},
	author = {Panwar, Madhur and Shailabh, Shashank and Aggarwal, Milan and Krishnamurthy, Balaji},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2021.acl-long.299},
	file = {Full Text:files/1391/Panwar et al. - 2021 - TAN-NTM Topic Attention Networks for Neural Topic.pdf:application/pdf},
	language = {en},
	pages = {3865--3880},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{TAN}-{NTM}},
	title = {{TAN}-{NTM}: {Topic} {Attention} {Networks} for {Neural} {Topic} {Modeling}},
	url = {https://aclanthology.org/2021.acl-long.299},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.acl-long.299},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.299}}

@inproceedings{shao_2022_towards_better_understanding_with_uniformity_and_explicit_regularization_of_embeddings_in_embedding_based_neural_topic_models,
	address = {Padua, Italy},
	author = {Shao, Wei and Huang, Lei and Liu, Shuqi and Ma, Shihua and Song, Linqi},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	doi = {10.1109/IJCNN55064.2022.9892128},
	file = {Submitted Version:files/1394/Shao et al. - 2022 - Towards Better Understanding with Uniformity and E.pdf:application/pdf},
	isbn = {978-1-72818-671-9},
	month = jul,
	pages = {1--9},
	publisher = {IEEE},
	title = {Towards {Better} {Understanding} with {Uniformity} and {Explicit} {Regularization} of {Embeddings} in {Embedding}-based {Neural} {Topic} {Models}},
	url = {https://ieeexplore.ieee.org/document/9892128/},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9892128/},
	bdsk-url-2 = {https://doi.org/10.1109/IJCNN55064.2022.9892128}}

@inproceedings{sia_2021_adaptive_mixed_component_lda_for_low_resource_topic_modeling,
	address = {Online},
	author = {Sia, Suzanna and Duh, Kevin},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	doi = {10.18653/v1/2021.eacl-main.209},
	file = {Full Text:files/1390/Sia and Duh - 2021 - Adaptive Mixed Component LDA for Low Resource Topi.pdf:application/pdf},
	language = {en},
	pages = {2451--2469},
	publisher = {Association for Computational Linguistics},
	title = {Adaptive {Mixed} {Component} {LDA} for {Low} {Resource} {Topic} {Modeling}},
	url = {https://aclanthology.org/2021.eacl-main.209},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.209},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.209}}

@inproceedings{lenzi_2022_collaborative_is_better_than_adversarial_generative_cooperative_networks_for_topic_clustering,
	address = {Virtual Event},
	author = {Lenzi, Andrea and Velardi, Paola},
	booktitle = {Proceedings of the 37th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	doi = {10.1145/3477314.3506997},
	isbn = {978-1-4503-8713-2},
	language = {en},
	month = apr,
	pages = {688--695},
	publisher = {ACM},
	shorttitle = {Collaborative is better than adversarial},
	title = {Collaborative is better than adversarial: generative cooperative networks for topic clustering},
	url = {https://dl.acm.org/doi/10.1145/3477314.3506997},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3477314.3506997},
	bdsk-url-2 = {https://doi.org/10.1145/3477314.3506997}}

@inproceedings{zhao_2021_topic_modelling_meets_deep_neural_networks_a_survey,
	abstract = {Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with nearly a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review on this specific topic.},
	address = {Montreal, Canada},
	author = {Zhao, He and Phung, Dinh and Huynh, Viet and Jin, Yuan and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	doi = {10.24963/ijcai.2021/638},
	file = {Full Text:files/1395/Zhao et al. - 2021 - Topic Modelling Meets Deep Neural Networks A Surv.pdf:application/pdf},
	isbn = {978-0-9992411-9-6},
	language = {en},
	month = aug,
	pages = {4713--4720},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	shorttitle = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}},
	title = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}: {A} {Survey}},
	url = {https://www.ijcai.org/proceedings/2021/638},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.ijcai.org/proceedings/2021/638},
	bdsk-url-2 = {https://doi.org/10.24963/ijcai.2021/638}}

@inproceedings{zhang_2022_pre_training_and_fine_tuning_neural_topic_model_a_simple_yet_effective_approach_to_incorporating_external_knowledge,
	address = {Dublin, Ireland},
	author = {Zhang, Linhai and Hu, Xuemeng and Wang, Boyu and Zhou, Deyu and Zhang, Qian-Wen and Cao, Yunbo},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2022.acl-long.413},
	file = {Full Text:files/1392/Zhang et al. - 2022 - Pre-training and Fine-tuning Neural Topic Model A.pdf:application/pdf},
	language = {en},
	pages = {5980--5989},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Pre-training and {Fine}-tuning {Neural} {Topic} {Model}},
	title = {Pre-training and {Fine}-tuning {Neural} {Topic} {Model}: {A} {Simple} yet {Effective} {Approach} to {Incorporating} {External} {Knowledge}},
	url = {https://aclanthology.org/2022.acl-long.413},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.acl-long.413},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.acl-long.413}}

@article{chang_2021_ta_bilstm_an_interpretable_topic_aware_model_for_misleading_information_detection_in_mobile_social_networks,
	author = {Chang, Shuyu and Wang, Rui and Huang, Haiping and Luo, Jian},
	doi = {10.1007/s11036-021-01847-w},
	file = {Full Text:files/1393/Chang et al. - 2021 - TA-BiLSTM An Interpretable Topic-Aware Model for .pdf:application/pdf},
	issn = {1383-469X, 1572-8153},
	journal = {Mobile Networks and Applications},
	language = {en},
	month = dec,
	number = {6},
	pages = {2298--2314},
	shorttitle = {{TA}-{BiLSTM}},
	title = {{TA}-{BiLSTM}: {An} {Interpretable} {Topic}-{Aware} {Model} for {Misleading} {Information} {Detection} in {Mobile} {Social} {Networks}},
	url = {https://link.springer.com/10.1007/s11036-021-01847-w},
	urldate = {2022-11-25},
	volume = {26},
	year = {2021},
	bdsk-url-1 = {https://link.springer.com/10.1007/s11036-021-01847-w},
	bdsk-url-2 = {https://doi.org/10.1007/s11036-021-01847-w}}

@article{zhang_2022_lifelong_topic_modeling_with_knowledge_enhanced_adversarial_network,
	author = {Zhang, Xuewen and Rao, Yanghui and Li, Qing},
	doi = {10.1007/s11280-021-00984-2},
	issn = {1386-145X, 1573-1413},
	journal = {World Wide Web},
	language = {en},
	month = jan,
	number = {1},
	pages = {219--238},
	title = {Lifelong topic modeling with knowledge-enhanced adversarial network},
	url = {https://link.springer.com/10.1007/s11280-021-00984-2},
	urldate = {2022-11-25},
	volume = {25},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/s11280-021-00984-2},
	bdsk-url-2 = {https://doi.org/10.1007/s11280-021-00984-2}}
