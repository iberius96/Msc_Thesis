%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 20:33:37 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{https://doi.org/10.48550/arxiv.2107.02173,
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-25 20:33:36 +0100},
	date-modified = {2022-11-25 20:33:36 +0100},
	doi = {10.48550/ARXIV.2107.02173},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
	url = {https://arxiv.org/abs/2107.02173},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2107.02173}}

@inproceedings{terragni-etal-2021-octis,
	abstract = {In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: https://github.com/MIND-Lab/OCTIS.},
	address = {Online},
	author = {Terragni, Silvia and Fersini, Elisabetta and Galuzzi, Bruno Giovanni and Tropeano, Pietro and Candelieri, Antonio},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
	date-added = {2022-11-25 20:33:25 +0100},
	date-modified = {2022-11-25 20:33:25 +0100},
	doi = {10.18653/v1/2021.eacl-demos.31},
	month = apr,
	pages = {263--270},
	publisher = {Association for Computational Linguistics},
	title = {{OCTIS}: Comparing and Optimizing Topic models is Simple!},
	url = {https://aclanthology.org/2021.eacl-demos.31},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-demos.31},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-demos.31}}

@inproceedings{wood-etal-2022-bayesian,
	abstract = {One desiderata of topic modeling is to produce interpretable topics. Given a cluster of document-tokens comprising a topic, we can order the topic by counting each word. It is natural to think that each topic could easily be labeled by looking at the words with the highest word count. However, this is not always the case. A human evaluator can often have difficulty identifying a single label that accurately describes the topic as many top words seem unrelated. This paper aims to improve interpretability in topic modeling by providing a novel, outperforming interpretable topic model Our approach combines two previously established subdomains in topic modeling: nonparametric and weakly-supervised topic models. Given a nonparametric topic model, we can include weakly-supervised input using novel modifications to the nonparametric generative model. These modifications lay the groundwork for a compelling setting{---}one in which most corpora, without any previous supervised or weakly-supervised input, can discover interpretable topics. This setting also presents various challenging sub-problems of which we provide resolutions. Combining nonparametric topic models with weakly-supervised topic models leads to an exciting discovery{---}a complete, self-contained and outperforming topic model for interpretability.},
	address = {Marseille, France},
	author = {Wood, Justin and Arnold, Corey and Wang, Wei},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	date-added = {2022-11-25 20:33:20 +0100},
	date-modified = {2022-11-25 20:33:20 +0100},
	month = jun,
	pages = {6271--6279},
	publisher = {European Language Resources Association},
	title = {A {B}ayesian Topic Model for Human-Evaluated Interpretability},
	url = {https://aclanthology.org/2022.lrec-1.674},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.lrec-1.674}}

@inproceedings{austin-etal-2022-community,
	abstract = {We present our novel, hyperparameter-free topic modelling algorithm, Community Topic. Our algorithm is based on mining communities from term co-occurrence networks. We empirically evaluate and compare Community Topic with Latent Dirichlet Allocation and the recently developed top2vec algorithm. We find that Community Topic runs faster than the competitors and produces topics that achieve higher coherence scores. Community Topic can discover coherent topics at various scales. The network representation used by Community Topic results in a natural relationship between topics and a topic hierarchy. This allows sub- and super-topics to be found on demand. These features make Community Topic the ideal tool for downstream applications such as applied research and conversational agents.},
	address = {Gyeongju, Republic of Korea},
	author = {Austin, Eric and Za{\"\i}ane, Osmar R. and Largeron, Christine},
	booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
	date-added = {2022-11-25 20:33:16 +0100},
	date-modified = {2022-11-25 20:33:16 +0100},
	month = oct,
	pages = {971--983},
	publisher = {International Committee on Computational Linguistics},
	title = {Community Topic: Topic Model Inference by Consecutive Word Community Discovery},
	url = {https://aclanthology.org/2022.coling-1.81},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.coling-1.81}}

@inproceedings{bittermann-rieger-2022-finding,
	abstract = {The ever growing amount of research publications demands computational assistance for everyone trying to keep track with scientific processes. Topic modeling has become a popular approach for finding scientific topics in static collections of research papers. However, the reality of continuously growing corpora of scholarly documents poses a major challenge for traditional approaches. We introduce RollingLDA for an ongoing monitoring of research topics, which offers the possibility of sequential modeling of dynamically growing corpora with time consistency of time series resulting from the modeled texts. We evaluate its capability to detect research topics and present a Shiny App as an easy-to-use interface. In addition, we illustrate usage scenarios for different user groups such as researchers, students, journalists, or policy-makers.},
	address = {Gyeongju, Republic of Korea},
	author = {Bittermann, Andr{\'e} and Rieger, Jonas},
	booktitle = {Proceedings of the Third Workshop on Scholarly Document Processing},
	date-added = {2022-11-25 20:33:12 +0100},
	date-modified = {2022-11-25 20:33:12 +0100},
	month = oct,
	pages = {7--18},
	publisher = {Association for Computational Linguistics},
	title = {Finding Scientific Topics in Continuously Growing Text Corpora},
	url = {https://aclanthology.org/2022.sdp-1.2},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.sdp-1.2}}

@article{hoyle_are_2022,
	abstract = {Recently, the relationship between automated and human evaluation of topic models has been called into question. Method developers have staked the efficacy of new topic model variants on automated measures, and their failure to approximate human preferences places these models on uncertain ground. Moreover, existing evaluation paradigms are often divorced from real-world use. Motivated by content analysis as a dominant real-world use case for topic modeling, we analyze two related aspects of topic models that affect their effectiveness and trustworthiness in practice for that purpose: the stability of their estimates and the extent to which the model's discovered categories align with human-determined categories in the data. We find that neural topic models fare worse in both respects compared to an established classical method. We take a step toward addressing both issues in tandem by demonstrating that a straightforward ensembling method can reliably outperform the members of the ensemble.},
	annote = {Other
Accepted to Findings of EMNLP 2022},
	author = {Hoyle, Alexander and Goel, Pranav and Sarkar, Rupak and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2210.16162},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Human-Computer Interaction (cs.HC)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Are {Neural} {Topic} {Models} {Broken}?},
	url = {https://arxiv.org/abs/2210.16162},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.16162},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.16162}}

@article{rosati_moving_2022,
	abstract = {Topic models represent groups of documents as a list of words (the topic labels). This work asks whether an alternative approach to topic labeling can be developed that is closer to a natural language description of a topic than a word list. To this end, we present an approach to generating human-like topic labels using abstractive multi-document summarization (MDS). We investigate our approach with an exploratory case study. We model topics in citation sentences in order to understand what further research needs to be done to fully operationalize MDS for topic labeling. Our case study shows that in addition to more human-like topics there are additional advantages to evaluation by using clustering and summarization measures instead of topic model measures. However, we find that there are several developments needed before we can design a well-powered study to evaluate MDS for topic modeling fully. Namely, improving cluster cohesion, improving the factuality and faithfulness of MDS, and increasing the number of documents that might be supported by MDS. We present a number of ideas on how these can be tackled and conclude with some thoughts on how topic modeling can also be used to improve MDS in general.},
	annote = {Other
Accepted to WIESP @ AACL-IJCNLP},
	author = {Rosati, Domenic},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2211.05599},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {Moving beyond word lists},
	title = {Moving beyond word lists: towards abstractive topic labels for human-like topics of scientific documents},
	url = {https://arxiv.org/abs/2211.05599},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2211.05599},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2211.05599}}

@article{silva_no_2022,
	abstract = {Extracting knowledge from unlabeled texts using machine learning algorithms can be complex. Document categorization and information retrieval are two applications that may benefit from unsupervised learning (e.g., text clustering and topic modeling), including exploratory data analysis. However, the unsupervised learning paradigm poses reproducibility issues. The initialization can lead to variability depending on the machine learning algorithm. Furthermore, the distortions can be misleading when regarding cluster geometry. Amongst the causes, the presence of outliers and anomalies can be a determining factor. Despite the relevance of initialization and outlier issues for text clustering and topic modeling, the authors did not find an in-depth analysis of them. This survey provides a systematic literature review (2011-2022) of these subareas and proposes a common terminology since similar procedures have different terms. The authors describe research opportunities, trends, and open issues. The appendices summarize the theoretical background of the text vectorization, the factorization, and the clustering algorithms that are directly or indirectly related to the reviewed works.},
	author = {Silva, Mar{\'\i}lia Costa Rosendo and Siqueira, Felipe Alves and Tarrega, Jo{\~a}o Pedro Mantovani and Beinotti, Jo{\~a}o Vitor Pataca and Nunes, Augusto Sousa and Gardini, Miguel de Mattos and da Silva, Vin{\'\i}cius Adolfo Pereira and da Silva, N{\'a}dia F{\'e}lix Felipe and de Carvalho, Andr{\'e} Carlos Ponce de Leon Ferreira},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	doi = {10.48550/ARXIV.2208.01712},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (stat.ML), I.2; I.2.7; I.5.3},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {No {Pattern}, {No} {Recognition}},
	title = {No {Pattern}, {No} {Recognition}: a {Survey} about {Reproducibility} and {Distortion} {Issues} of {Text} {Clustering} and {Topic} {Modeling}},
	url = {https://arxiv.org/abs/2208.01712},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2208.01712},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2208.01712}}

@article{wood_knowledge_2022,
	abstract = {Recent work suggests knowledge sources can be added into the topic modeling process to label topics and improve topic discovery. The knowledge sources typically consist of a collection of human-constructed articles, each describing a topic (article-topic) for an entire domain. However, these semisupervised topic models assume a corpus to contain topics on only a subset of a domain. Therefore, during inference, the model must consider which article-topics were theoretically used to generate the corpus. Since the knowledge sources tend to be quite large, the many article-topics considered slow down the inference process. The increase in execution time is significant, with knowledge source input greater than 103 becoming unfeasible for use in topic modeling. To increase the applicability of semisupervised topic models, approaches are needed to speed up the overall execution time. This paper presents a way of ranking knowledge source topics to satisfy the above goal. Our approach utilizes a knowledge source ranking, based on the PageRank algorithm, to determine the importance of an article-topic. By applying our ranking technique we can eliminate low scoring article-topics before inference, speeding up the overall process. Remarkably, this ranking technique can also improve perplexity and interpretability. Results show our approach to outperform baseline methods and significantly aid semisupervised topic models. In our evaluation, knowledge source rankings yield a 44\% increase in topic retrieval f-score, a 42.6\% increase in inter-inference topic elimination, a 64\% increase in perplexity, a 30\% increase in token assignment accuracy, a 20\% increase in topic composition interpretability, and a 5\% increase in document assignment interpretability over baseline methods.},
	author = {Wood, Justin and Arnold, Corey and Wang, Wei},
	doi = {10.3390/info13020057},
	file = {Full Text:files/1576/Wood et al. - 2022 - Knowledge Source Rankings for Semi-Supervised Topi.pdf:application/pdf},
	issn = {2078-2489},
	journal = {Information},
	language = {en},
	month = jan,
	number = {2},
	pages = {57},
	title = {Knowledge {Source} {Rankings} for {Semi}-{Supervised} {Topic} {Modeling}},
	url = {https://www.mdpi.com/2078-2489/13/2/57},
	urldate = {2022-11-25},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://www.mdpi.com/2078-2489/13/2/57},
	bdsk-url-2 = {https://doi.org/10.3390/info13020057}}

@inproceedings{harrando_discovering_2021,
	address = {Virtual Event USA},
	author = {Harrando, Ismail and Troncy, Rapha{\"e}l},
	booktitle = {Proceedings of the 11th on {Knowledge} {Capture} {Conference}},
	doi = {10.1145/3460210.3493586},
	isbn = {978-1-4503-8457-5},
	language = {en},
	month = dec,
	pages = {265--268},
	publisher = {ACM},
	title = {Discovering {Interpretable} {Topics} by {Leveraging} {Common} {Sense} {Knowledge}},
	url = {https://dl.acm.org/doi/10.1145/3460210.3493586},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3460210.3493586},
	bdsk-url-2 = {https://doi.org/10.1145/3460210.3493586}}

@article{pita_probabilistic_2022,
	author = {Pita, Marcelo and Nunes, Matheus and Pappa, Gisele L.},
	doi = {10.1007/s10489-022-03388-5},
	issn = {0924-669X, 1573-7497},
	journal = {Applied Intelligence},
	language = {en},
	month = dec,
	number = {15},
	pages = {17829--17844},
	title = {Probabilistic topic modeling for short text based on word embedding networks},
	url = {https://link.springer.com/10.1007/s10489-022-03388-5},
	urldate = {2022-11-25},
	volume = {52},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/s10489-022-03388-5},
	bdsk-url-2 = {https://doi.org/10.1007/s10489-022-03388-5}}

@inproceedings{zhao_topic_2021,
	abstract = {Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with nearly a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review on this specific topic.},
	address = {Montreal, Canada},
	author = {Zhao, He and Phung, Dinh and Huynh, Viet and Jin, Yuan and Du, Lan and Buntine, Wray},
	booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	doi = {10.24963/ijcai.2021/638},
	file = {Full Text:files/1577/Zhao et al. - 2021 - Topic Modelling Meets Deep Neural Networks A Surv.pdf:application/pdf},
	isbn = {978-0-9992411-9-6},
	language = {en},
	month = aug,
	pages = {4713--4720},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	shorttitle = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}},
	title = {Topic {Modelling} {Meets} {Deep} {Neural} {Networks}: {A} {Survey}},
	url = {https://www.ijcai.org/proceedings/2021/638},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://www.ijcai.org/proceedings/2021/638},
	bdsk-url-2 = {https://doi.org/10.24963/ijcai.2021/638}}

@article{chiu_joint_2022,
	abstract = {Topic models are some of the most popular ways to represent textual data in an interpret-able manner. Recently, advances in deep generative models, specifically auto-encoding variational Bayes (AEVB), have led to the introduction of unsupervised neural topic models, which leverage deep generative models as opposed to traditional statistics-based topic models. We extend upon these neural topic models by introducing the Label-Indexed Neural Topic Model (LI-NTM), which is, to the extent of our knowledge, the first effective upstream semi-supervised neural topic model. We find that LI-NTM outperforms existing neural topic models in document reconstruction benchmarks, with the most notable results in low labeled data regimes and for data-sets with informative labels; furthermore, our jointly learned classifier outperforms baseline classifiers in ablation studies.},
	annote = {Other
To appear in the 6th ACL Workshop on Structured Prediction for NLP (SPNLP)},
	author = {Chiu, Jeffrey and Mittal, Rajat and Tumma, Neehal and Sharma, Abhishek and Doshi-Velez, Finale},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2204.03208},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (stat.ML)},
	note = {Publisher: arXiv Version Number: 1},
	title = {A {Joint} {Learning} {Approach} for {Semi}-supervised {Neural} {Topic} {Modeling}},
	url = {https://arxiv.org/abs/2204.03208},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2204.03208},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2204.03208}}

@article{abdelrazek_topic_2023,
	author = {Abdelrazek, Aly and Eid, Yomna and Gawish, Eman and Medhat, Walaa and Hassan, Ahmed},
	doi = {10.1016/j.is.2022.102131},
	issn = {03064379},
	journal = {Information Systems},
	language = {en},
	month = feb,
	pages = {102131},
	shorttitle = {Topic modeling algorithms and applications},
	title = {Topic modeling algorithms and applications: {A} survey},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306437922001090},
	urldate = {2022-11-25},
	volume = {112},
	year = {2023},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0306437922001090},
	bdsk-url-2 = {https://doi.org/10.1016/j.is.2022.102131}}

@article{korencic_topic_2021,
	author = {Korencic, Damir and Ristov, Strahil and Repar, Jelena and Snajder, Jan},
	doi = {10.1109/ACCESS.2021.3109425},
	file = {Full Text:files/1580/Korencic et al. - 2021 - A Topic Coverage Approach to Evaluation of Topic M.pdf:application/pdf},
	issn = {2169-3536},
	journal = {IEEE Access},
	pages = {123280--123312},
	title = {A {Topic} {Coverage} {Approach} to {Evaluation} of {Topic} {Models}},
	url = {https://ieeexplore.ieee.org/document/9526605/},
	urldate = {2022-11-25},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9526605/},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2021.3109425}}

@inproceedings{eurecom_sophia_antipolis_france_apples_2021,
	author = {{EURECOM, Sophia Antipolis, France} and Harrando, Ismail and Lisena, Pasquale and {EURECOM, Sophia Antipolis, France} and Troncy, Rapha{\"e}l and {EURECOM, Sophia Antipolis, France}},
	booktitle = {Proceedings of the {Conference} {Recent} {Advances} in {Natural} {Language} {Processing} - {Deep} {Learning} for {Natural} {Language} {Processing} {Methods} and {Applications}},
	doi = {10.26615/978-954-452-072-4_055},
	file = {Full Text:files/1578/EURECOM, Sophia Antipolis, France et al. - 2021 - Apples to Apples A Systematic Evaluation of Topic.pdf:application/pdf},
	isbn = {978-954-452-072-4},
	pages = {483--493},
	publisher = {INCOMA Ltd. Shoumen, BULGARIA},
	shorttitle = {Apples to {Apples}},
	title = {Apples to {Apples}: {A} {Systematic} {Evaluation} of {Topic} {Models}},
	url = {https://acl-bg.org/proceedings/2021/RANLP%202021/pdf/2021.ranlp-1.55.pdf},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://acl-bg.org/proceedings/2021/RANLP%202021/pdf/2021.ranlp-1.55.pdf},
	bdsk-url-2 = {https://doi.org/10.26615/978-954-452-072-4_055}}

@article{meaney_non-negative_2022,
	author = {Meaney, Christopher and Escobar, Michael and Moineddin, Rahim and Stukel, Therese A. and Kalia, Sumeet and Aliarzadeh, Babak and Chen, Tao and O'Neill, Braden and Greiver, Michelle},
	doi = {10.1016/j.jbi.2022.104034},
	file = {Full Text:files/1579/Meaney et al. - 2022 - Non-negative matrix factorization temporal topic m.pdf:application/pdf},
	issn = {15320464},
	journal = {Journal of Biomedical Informatics},
	language = {en},
	month = apr,
	pages = {104034},
	title = {Non-negative matrix factorization temporal topic models and clinical text data identify {COVID}-19 pandemic effects on primary healthcare and community health in {Toronto}, {Canada}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046422000508},
	urldate = {2022-11-25},
	volume = {128},
	year = {2022},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S1532046422000508},
	bdsk-url-2 = {https://doi.org/10.1016/j.jbi.2022.104034}}
