%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 14:55:24 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{https://doi.org/10.48550/arxiv.2110.14286,
	author = {Duan, Zhibin and Xu, Yishi and Chen, Bo and Wang, Dongsheng and Wang, Chaojie and Zhou, Mingyuan},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 14:55:23 +0100},
	date-modified = {2022-11-25 14:55:23 +0100},
	doi = {10.48550/ARXIV.2110.14286},
	keywords = {Machine Learning (cs.LG), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {TopicNet: Semantic Graph-Guided Topic Discovery},
	url = {https://arxiv.org/abs/2110.14286},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2110.14286}}

@misc{https://doi.org/10.48550/arxiv.2110.08845,
	author = {Ge, Suyu and Huang, Jiaxin and Meng, Yu and Wang, Sharon and Han, Jiawei},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 14:55:20 +0100},
	date-modified = {2022-11-25 14:55:20 +0100},
	doi = {10.48550/ARXIV.2110.08845},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Fine-Grained Opinion Summarization with Minimal Supervision},
	url = {https://arxiv.org/abs/2110.08845},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2110.08845}}

@article{lee_topic_2022,
	abstract = {Topic taxonomies display hierarchical topic structures of a text corpus and provide topical knowledge to enhance various NLP applications. To dynamically incorporate new topic information, several recent studies have tried to expand (or complete) a topic taxonomy by inserting emerging topics identified in a set of new documents. However, existing methods focus only on frequent terms in documents and the local topic-subtopic relations in a taxonomy, which leads to limited topic term coverage and fails to model the global topic hierarchy. In this work, we propose a novel framework for topic taxonomy expansion, named TopicExpan, which directly generates topic-related terms belonging to new topics. Specifically, TopicExpan leverages the hierarchical relation structure surrounding a new topic and the textual content of an input document for topic term generation. This approach encourages newly-inserted topics to further cover important but less frequent terms as well as to keep their relation consistency within the taxonomy. Experimental results on two real-world text corpora show that TopicExpan significantly outperforms other baseline methods in terms of the quality of output taxonomies.},
	annote = {Other
14 pages, 7 figures, Findings of EMNLP 2022 (Long paper)},
	author = {Lee, Dongha and Shen, Jiaming and Lee, Seonghyeon and Yoon, Susik and Yu, Hwanjo and Han, Jiawei},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2211.01981},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Artificial Intelligence (cs.AI)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Topic {Taxonomy} {Expansion} via {Hierarchy}-{Aware} {Topic} {Phrase} {Generation}},
	url = {https://arxiv.org/abs/2211.01981},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2211.01981},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2211.01981}}

@article{wang_knowledge-aware_2022,
	abstract = {We propose a Bayesian generative model for incorporating prior domain knowledge into hierarchical topic modeling. Although embedded topic models (ETMs) and its variants have gained promising performance in text analysis, they mainly focus on mining word co-occurrence patterns, ignoring potentially easy-to-obtain prior topic hierarchies that could help enhance topic coherence. While several knowledge-based topic models have recently been proposed, they are either only applicable to shallow hierarchies or sensitive to the quality of the provided prior knowledge. To this end, we develop a novel deep ETM that jointly models the documents and the given prior knowledge by embedding the words and topics into the same space. Guided by the provided knowledge, the proposed model tends to discover topic hierarchies that are organized into interpretable taxonomies. Besides, with a technique for adapting a given graph, our extended version allows the provided prior topic structure to be finetuned to match the target corpus. Extensive experiments show that our proposed model efficiently integrates the prior knowledge and improves both hierarchical topic discovery and document representation.},
	annote = {Other
Proceedings of NeurIPS2022},
	author = {Wang, Dongsheng and Xu, Yishi and Li, Miaoge and Duan, Zhibin and Wang, Chaojie and Chen, Bo and Zhou, Mingyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2209.14228},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Knowledge-{Aware} {Bayesian} {Deep} {Topic} {Model}},
	url = {https://arxiv.org/abs/2209.14228},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2209.14228},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2209.14228}}

@inproceedings{li_taxotrans_2022,
	address = {Washington DC USA},
	author = {Li, Zhuliu and Wang, Yiming and Yan, Xiao and Meng, Weizhi and Li, Yanen and Yang, Jaewon},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	doi = {10.1145/3534678.3539188},
	isbn = {978-1-4503-9385-0},
	language = {en},
	month = aug,
	pages = {3279--3287},
	publisher = {ACM},
	shorttitle = {{TaxoTrans}},
	title = {{TaxoTrans}: {Taxonomy}-{Guided} {Entity} {Translation}},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539188},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3534678.3539188},
	bdsk-url-2 = {https://doi.org/10.1145/3534678.3539188}}

@inproceedings{lee_taxocom_2022,
	address = {Virtual Event, Lyon France},
	author = {Lee, Dongha and Shen, Jiaming and Kang, Seongku and Yoon, Susik and Han, Jiawei and Yu, Hwanjo},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	doi = {10.1145/3485447.3512002},
	file = {Submitted Version:files/680/Lee et al. - 2022 - TaxoCom Topic Taxonomy Completion with Hierarchic.pdf:application/pdf},
	isbn = {978-1-4503-9096-5},
	language = {en},
	month = apr,
	pages = {2819--2829},
	publisher = {ACM},
	shorttitle = {{TaxoCom}},
	title = {{TaxoCom}: {Topic} {Taxonomy} {Completion} with {Hierarchical} {Discovery} of {Novel} {Topic} {Clusters}},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512002},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3485447.3512002},
	bdsk-url-2 = {https://doi.org/10.1145/3485447.3512002}}

@inproceedings{meng_adapting_2022,
	address = {Washington DC USA},
	author = {Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	doi = {10.1145/3534678.3542607},
	isbn = {978-1-4503-9385-0},
	language = {en},
	month = aug,
	pages = {4806--4807},
	publisher = {ACM},
	title = {Adapting {Pretrained} {Representations} for {Text} {Mining}},
	url = {https://dl.acm.org/doi/10.1145/3534678.3542607},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3534678.3542607},
	bdsk-url-2 = {https://doi.org/10.1145/3534678.3542607}}

@article{saha_seeded_2022,
	abstract = {Practitioners from many disciplines (e.g., political science) use expert-crafted taxonomies to make sense of large, unlabeled corpora. In this work, we study Seeded Hierarchical Clustering (SHC): the task of automatically fitting unlabeled data to such taxonomies using only a small set of labeled examples. We propose HierSeed, a novel weakly supervised algorithm for this task that uses only a small set of labeled seed examples. It is both data and computationally efficient. HierSeed assigns documents to topics by weighing document density against topic hierarchical structure. It outperforms both unsupervised and supervised baselines for the SHC task on three real-world datasets.},
	author = {Saha, Anish and Ananthram, Amith and Allaway, Emily and Ji, Heng and McKeown, Kathleen},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2205.11602},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Seeded {Hierarchical} {Clustering} for {Expert}-{Crafted} {Taxonomies}},
	url = {https://arxiv.org/abs/2205.11602},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2205.11602},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2205.11602}}

@inproceedings{lee_out--category_2021,
	address = {Auckland, New Zealand},
	author = {Lee, Dongha and Hyun, Dongmin and Han, Jiawei and Yu, Hwanjo},
	booktitle = {2021 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	doi = {10.1109/ICDM51629.2021.00041},
	file = {Submitted Version:files/683/Lee et al. - 2021 - Out-of-Category Document Identification Using Targ.pdf:application/pdf},
	isbn = {978-1-66542-398-4},
	month = dec,
	pages = {1186--1191},
	publisher = {IEEE},
	title = {Out-of-{Category} {Document} {Identification} {Using} {Target}-{Category} {Names} as {Weak} {Supervision}},
	url = {https://ieeexplore.ieee.org/document/9679127/},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9679127/},
	bdsk-url-2 = {https://doi.org/10.1109/ICDM51629.2021.00041}}

@article{guo_dhcm_2021,
	abstract = {The online event discovery in social media based documents is useful, such as for disaster recognition and intervention. However, the diverse events incrementally identified from social media streams remain accumulated, ad hoc, and unstructured. They cannot assist users in digesting the tremendous amount of information and finding their interested events. Further, most of the existing work is challenged by jointly identifying incremental events and dynamically organizing them in an adaptive hierarchy. To address these problems, this article proposes
              d
              ynamic and
              h
              ierarchical
              C
              ategorization
              M
              odeling (dhCM) for social media stream. Instead of manually dividing the timeframe, a multimodal event miner exploits a density estimation technique to continuously capture the temporal influence between documents and incrementally identify online events in textual, temporal, and spatial spaces. At the same time, an adaptive categorization hierarchy is formed to automatically organize the documents into proper categories at multiple levels of granularities. In a nonparametric manner, dhCM accommodates the increasing complexity of data streams with automatically growing the categorization hierarchy over adaptive growth. A sequential Monte Carlo algorithm is used for the online inference of the dhCM parameters. Extensive experiments show that dhCM outperforms the state-of-the-art models in terms of term coherence, category abstraction and specialization, hierarchical affinity, and event categorization and discovery accuracy.},
	author = {Guo, Jinjin and Gong, Zhiguo and Cao, Longbing},
	doi = {10.1145/3470888},
	issn = {2157-6904, 2157-6912},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	language = {en},
	month = oct,
	number = {5},
	pages = {1--25},
	shorttitle = {{dhCM}},
	title = {{dhCM}: {Dynamic} and {Hierarchical} {Event} {Categorization} and {Discovery} for {Social} {Media} {Stream}},
	url = {https://dl.acm.org/doi/10.1145/3470888},
	urldate = {2022-11-25},
	volume = {12},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3470888},
	bdsk-url-2 = {https://doi.org/10.1145/3470888}}

@article{xu_hyperminer_2022,
	abstract = {Embedded topic models are able to learn interpretable topics even with large and heavy-tailed vocabularies. However, they generally hold the Euclidean embedding space assumption, leading to a basic limitation in capturing hierarchical relations. To this end, we present a novel framework that introduces hyperbolic embeddings to represent words and topics. With the tree-likeness property of hyperbolic space, the underlying semantic hierarchy among words and topics can be better exploited to mine more interpretable topics. Furthermore, due to the superiority of hyperbolic geometry in representing hierarchical data, tree-structure knowledge can also be naturally injected to guide the learning of a topic hierarchy. Therefore, we further develop a regularization term based on the idea of contrastive learning to inject prior structural knowledge efficiently. Experiments on both topic taxonomy discovery and document representation demonstrate that the proposed framework achieves improved performance against existing embedded topic models.},
	author = {Xu, Yishi and Wang, Dongsheng and Chen, Bo and Lu, Ruiying and Duan, Zhibin and Zhou, Mingyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2210.10625},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Information Retrieval (cs.IR)},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {{HyperMiner}},
	title = {{HyperMiner}: {Topic} {Taxonomy} {Mining} with {Hyperbolic} {Embedding}},
	url = {https://arxiv.org/abs/2210.10625},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.10625},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.10625}}

@article{ganguli_nonparametric_2021,
	author = {Ganguli, Isha and Sil, Jaya and Sengupta, Nandita},
	doi = {10.1007/s00521-020-05662-4},
	issn = {0941-0643, 1433-3058},
	journal = {Neural Computing and Applications},
	language = {en},
	month = jan,
	title = {Nonparametric method of topic identification using granularity concept and graph-based modeling},
	url = {http://link.springer.com/10.1007/s00521-020-05662-4},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {http://link.springer.com/10.1007/s00521-020-05662-4},
	bdsk-url-2 = {https://doi.org/10.1007/s00521-020-05662-4}}

@inproceedings{huang_few-shot_2022,
	address = {Washington DC USA},
	author = {Huang, Jiaxin and Meng, Yu and Han, Jiawei},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	doi = {10.1145/3534678.3539443},
	file = {Submitted Version:files/682/Huang et al. - 2022 - Few-Shot Fine-Grained Entity Typing with Automatic.pdf:application/pdf},
	isbn = {978-1-4503-9385-0},
	language = {en},
	month = aug,
	pages = {605--614},
	publisher = {ACM},
	title = {Few-{Shot} {Fine}-{Grained} {Entity} {Typing} with {Automatic} {Label} {Interpretation} and {Instance} {Generation}},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539443},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3534678.3539443},
	bdsk-url-2 = {https://doi.org/10.1145/3534678.3539443}}

@inproceedings{li_hiercdf_2022,
	address = {Washington DC USA},
	author = {Li, Jiatong and Wang, Fei and Liu, Qi and Zhu, Mengxiao and Huang, Wei and Huang, Zhenya and Chen, Enhong and Su, Yu and Wang, Shijin},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	doi = {10.1145/3534678.3539486},
	isbn = {978-1-4503-9385-0},
	language = {en},
	month = aug,
	pages = {904--913},
	publisher = {ACM},
	shorttitle = {{HierCDF}},
	title = {{HierCDF}: {A} {Bayesian} {Network}-based {Hierarchical} {Cognitive} {Diagnosis} {Framework}},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539486},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3534678.3539486},
	bdsk-url-2 = {https://doi.org/10.1145/3534678.3539486}}

@article{zhou_comprehensive_2022,
	abstract = {Clustering is a fundamental machine learning task which has been widely studied in the literature. Classic clustering methods follow the assumption that data are represented as features in a vectorized form through various representation learning techniques. As the data become increasingly complicated and complex, the shallow (traditional) clustering methods can no longer handle the high-dimensional data type. With the huge success of deep learning, especially the deep unsupervised learning, many representation learning techniques with deep architectures have been proposed in the past decade. Recently, the concept of Deep Clustering, i.e., jointly optimizing the representation learning and clustering, has been proposed and hence attracted growing attention in the community. Motivated by the tremendous success of deep learning in clustering, one of the most fundamental machine learning tasks, and the large number of recent advances in this direction, in this paper we conduct a comprehensive survey on deep clustering by proposing a new taxonomy of different state-of-the-art approaches. We summarize the essential components of deep clustering and categorize existing methods by the ways they design interactions between deep representation learning and clustering. Moreover, this survey also provides the popular benchmark datasets, evaluation metrics and open-source implementations to clearly illustrate various experimental settings. Last but not least, we discuss the practical applications of deep clustering and suggest challenging topics deserving further investigations as future directions.},
	annote = {Other
Github Repo: https://github.com/zhoushengisnoob/DeepClustering},
	author = {Zhou, Sheng and Xu, Hongjia and Zheng, Zhuonan and Chen, Jiawei and li, Zhao and Bu, Jiajun and Wu, Jia and Wang, Xin and Zhu, Wenwu and Ester, Martin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2206.07579},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI)},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {A {Comprehensive} {Survey} on {Deep} {Clustering}},
	title = {A {Comprehensive} {Survey} on {Deep} {Clustering}: {Taxonomy}, {Challenges}, and {Future} {Directions}},
	url = {https://arxiv.org/abs/2206.07579},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2206.07579},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2206.07579}}

@incollection{fang_disentangled_2021,
	address = {Cham},
	author = {Chen, Xiang and Xie, Xin and Bi, Zhen and Ye, Hongbin and Deng, Shumin and Zhang, Ningyu and Chen, Huajun},
	booktitle = {Artificial {Intelligence}},
	doi = {10.1007/978-3-030-93049-3_18},
	editor = {Fang, Lu and Chen, Yiran and Zhai, Guangtao and Wang, Jane and Wang, Ruiping and Dong, Weisheng},
	file = {Submitted Version:files/679/Chen et al. - 2021 - Disentangled Contrastive Learning for Learning Rob.pdf:application/pdf},
	isbn = {978-3-030-93048-6 978-3-030-93049-3},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {215--226},
	publisher = {Springer International Publishing},
	title = {Disentangled {Contrastive} {Learning} for {Learning} {Robust} {Textual} {Representations}},
	url = {https://link.springer.com/10.1007/978-3-030-93049-3_18},
	urldate = {2022-11-25},
	volume = {13070},
	year = {2021},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-030-93049-3_18},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-93049-3_18}}

@inproceedings{meng_topic_2022,
	address = {Virtual Event, Lyon France},
	author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	doi = {10.1145/3485447.3512034},
	file = {Submitted Version:files/684/Meng et al. - 2022 - Topic Discovery via Latent Space Clustering of Pre.pdf:application/pdf},
	isbn = {978-1-4503-9096-5},
	language = {en},
	month = apr,
	pages = {3143--3152},
	publisher = {ACM},
	title = {Topic {Discovery} via {Latent} {Space} {Clustering} of {Pretrained} {Language} {Model} {Representations}},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512034},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3485447.3512034},
	bdsk-url-2 = {https://doi.org/10.1145/3485447.3512034}}

@inproceedings{qian_bert-based_2022,
	address = {Padua, Italy},
	author = {Qian, Haoda and Yuan, Minjie and Li, Qiudan and Zeng, Daniel},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	doi = {10.1109/IJCNN55064.2022.9892965},
	isbn = {978-1-72818-671-9},
	month = jul,
	pages = {1--8},
	publisher = {IEEE},
	title = {A {BERT}-based {Heterogeneous} {Graph} {Convolution} {Approach} for {Mining} {Organization}-{Related} {Topics}},
	url = {https://ieeexplore.ieee.org/document/9892965/},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9892965/},
	bdsk-url-2 = {https://doi.org/10.1109/IJCNN55064.2022.9892965}}

@article{khan_hierarchical_2022,
	abstract = {Topic models extract latent concepts from texts in the form of topics. Lifelong topic models extend topic models by learning topics continuously based on accumulated knowledge from the past which is updated continuously as new information becomes available. Hierarchical topic modeling extends topic modeling by extracting topics and organizing them into a hierarchical structure. In this study, we combine the two and introduce hierarchical lifelong topic models. Hierarchical lifelong topic models not only allow to examine the topics at different levels of granularity but also allows to continuously adjust the granularity of the topics as more information becomes available. A fundamental issue in hierarchical lifelong topic modeling is the extraction of rules that are used to preserve the hierarchical structural information among the rules and will continuously update based on new information. To address this issue, we introduce a network communities based rule mining approach for hierarchical lifelong topic models (NHLTM). The proposed approach extracts hierarchical structural information among the rules by representing textual documents as graphs and analyzing the underlying communities in the graph. Experimental results indicate improvement of the hierarchical topic structures in terms of topic coherence that increases from general to specific topics.},
	author = {Khan, Muhammad Taimoor and Azam, Nouman and Khalid, Shehzad and Aziz, Furqan},
	doi = {10.1371/journal.pone.0264481},
	editor = {Lin, Jerry Chun-Wei},
	file = {Full Text:files/674/Khan et al. - 2022 - Hierarchical lifelong topic modeling using rules e.pdf:application/pdf},
	issn = {1932-6203},
	journal = {PLOS ONE},
	language = {en},
	month = mar,
	number = {3},
	pages = {e0264481},
	title = {Hierarchical lifelong topic modeling using rules extracted from network communities},
	url = {https://dx.plos.org/10.1371/journal.pone.0264481},
	urldate = {2022-11-25},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://dx.plos.org/10.1371/journal.pone.0264481},
	bdsk-url-2 = {https://doi.org/10.1371/journal.pone.0264481}}

@inproceedings{yang_pathway2text_2022,
	address = {Seattle, United States},
	author = {Yang, Junwei and Liu, Zequn and Zhang, Ming and Wang, Sheng},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	doi = {10.18653/v1/2022.findings-naacl.108},
	file = {Full Text:files/681/Yang et al. - 2022 - Pathway2Text Dataset and Method for Biomedical Pa.pdf:application/pdf},
	language = {en},
	pages = {1441--1454},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{Pathway2Text}},
	title = {{Pathway2Text}: {Dataset} and {Method} for {Biomedical} {Pathway} {Description} {Generation}},
	url = {https://aclanthology.org/2022.findings-naacl.108},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.findings-naacl.108},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.findings-naacl.108}}

@inproceedings{li_hypon_2021,
	address = {Gainesville Florida},
	author = {Li, Zhuoyan and Wang, Sheng},
	booktitle = {Proceedings of the 12th {ACM} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	doi = {10.1145/3459930.3469515},
	isbn = {978-1-4503-8450-6},
	language = {en},
	month = aug,
	pages = {1--7},
	publisher = {ACM},
	shorttitle = {{HYPON}},
	title = {{HYPON}: embedding biomedical ontology with entity sets},
	url = {https://dl.acm.org/doi/10.1145/3459930.3469515},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3459930.3469515},
	bdsk-url-2 = {https://doi.org/10.1145/3459930.3469515}}

@inproceedings{shen_corpus-based_2021,
	address = {Online and Punta Cana, Dominican Republic},
	author = {Shen, Jiaming and Zhang, Yunyi and Ji, Heng and Han, Jiawei},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	doi = {10.18653/v1/2021.emnlp-main.441},
	file = {Full Text:files/677/Shen et al. - 2021 - Corpus-based Open-Domain Event Type Induction.pdf:application/pdf},
	language = {en},
	pages = {5427--5440},
	publisher = {Association for Computational Linguistics},
	title = {Corpus-based {Open}-{Domain} {Event} {Type} {Induction}},
	url = {https://aclanthology.org/2021.emnlp-main.441},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.441},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.441}}

@inproceedings{zhang_hierarchical_2021,
	address = {Virtual Event Israel},
	author = {Zhang, Yu and Chen, Xiusi and Meng, Yu and Han, Jiawei},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	doi = {10.1145/3437963.3441730},
	file = {Submitted Version:files/685/Zhang et al. - 2021 - Hierarchical Metadata-Aware Document Categorizatio.pdf:application/pdf},
	isbn = {978-1-4503-8297-7},
	language = {en},
	month = mar,
	pages = {770--778},
	publisher = {ACM},
	title = {Hierarchical {Metadata}-{Aware} {Document} {Categorization} under {Weak} {Supervision}},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441730},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3437963.3441730},
	bdsk-url-2 = {https://doi.org/10.1145/3437963.3441730}}

@inproceedings{huang_corel_2020,
	address = {Virtual Event CA USA},
	author = {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Zhang, Yunyi and Han, Jiawei},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	doi = {10.1145/3394486.3403244},
	file = {Full Text:files/675/Huang et al. - 2020 - CoRel Seed-Guided Topical Taxonomy Construction b.pdf:application/pdf},
	isbn = {978-1-4503-7998-4},
	language = {en},
	month = aug,
	pages = {1928--1936},
	publisher = {ACM},
	shorttitle = {{CoRel}},
	title = {{CoRel}: {Seed}-{Guided} {Topical} {Taxonomy} {Construction} by {Concept} {Learning} and {Relation} {Transferring}},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403244},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3394486.3403244},
	bdsk-url-2 = {https://doi.org/10.1145/3394486.3403244}}

@inproceedings{meng_embedding-driven_2020,
	address = {Virtual Event CA USA},
	author = {Meng, Yu and Huang, Jiaxin and Han, Jiawei},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	doi = {10.1145/3394486.3406483},
	file = {Full Text:files/676/Meng et al. - 2020 - Embedding-Driven Multi-Dimensional Topic Mining an.pdf:application/pdf},
	isbn = {978-1-4503-7998-4},
	language = {en},
	month = aug,
	pages = {3573--3574},
	publisher = {ACM},
	title = {Embedding-{Driven} {Multi}-{Dimensional} {Topic} {Mining} and {Text} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3394486.3406483},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3394486.3406483},
	bdsk-url-2 = {https://doi.org/10.1145/3394486.3406483}}

@inproceedings{meng_text_2020,
	address = {Online},
	author = {Meng, Yu and Zhang, Yunyi and Huang, Jiaxin and Xiong, Chenyan and Ji, Heng and Zhang, Chao and Han, Jiawei},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	doi = {10.18653/v1/2020.emnlp-main.724},
	file = {Full Text:files/678/Meng et al. - 2020 - Text Classification Using Label Names Only A Lang.pdf:application/pdf},
	language = {en},
	pages = {9006--9017},
	publisher = {Association for Computational Linguistics},
	shorttitle = {Text {Classification} {Using} {Label} {Names} {Only}},
	title = {Text {Classification} {Using} {Label} {Names} {Only}: {A} {Language} {Model} {Self}-{Training} {Approach}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.724},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2020.emnlp-main.724},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.724}}

@inproceedings{wang_textual_2020,
	address = {Atlanta, GA, USA},
	author = {Wang, Xuan and Zhang, Yu and Chauhan, Aabhas and Li, Qi and Han, Jiawei},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	doi = {10.1109/BigData50022.2020.9377958},
	isbn = {978-1-72816-251-5},
	month = dec,
	pages = {828--837},
	publisher = {IEEE},
	title = {Textual {Evidence} {Mining} via {Spherical} {Heterogeneous} {Information} {Network} {Embedding}},
	url = {https://ieeexplore.ieee.org/document/9377958/},
	urldate = {2022-11-25},
	year = {2020},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9377958/},
	bdsk-url-2 = {https://doi.org/10.1109/BigData50022.2020.9377958}}

@inproceedings{meng_power_2021,
	address = {Virtual Event Singapore},
	author = {Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	doi = {10.1145/3447548.3470810},
	isbn = {978-1-4503-8332-5},
	language = {en},
	month = aug,
	pages = {4052--4053},
	publisher = {ACM},
	shorttitle = {On the {Power} of {Pre}-{Trained} {Text} {Representations}},
	title = {On the {Power} of {Pre}-{Trained} {Text} {Representations}: {Models} and {Applications} in {Text} {Mining}},
	url = {https://dl.acm.org/doi/10.1145/3447548.3470810},
	urldate = {2022-11-25},
	year = {2021},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3447548.3470810},
	bdsk-url-2 = {https://doi.org/10.1145/3447548.3470810}}
