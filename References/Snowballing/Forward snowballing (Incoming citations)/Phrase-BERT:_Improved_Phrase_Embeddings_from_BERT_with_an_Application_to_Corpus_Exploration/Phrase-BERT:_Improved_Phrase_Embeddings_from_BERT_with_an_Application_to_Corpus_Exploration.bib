%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 16:14:25 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{https://doi.org/10.48550/arxiv.2201.07311,
	author = {Biderman, Stella and Bicheno, Kieran and Gao, Leo},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 16:14:24 +0100},
	date-modified = {2022-11-25 16:14:24 +0100},
	doi = {10.48550/ARXIV.2201.07311},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Datasheet for the Pile},
	url = {https://arxiv.org/abs/2201.07311},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2201.07311}}

@misc{https://doi.org/10.48550/arxiv.2202.09509,
	author = {Tang, Kenan},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-25 16:14:21 +0100},
	date-modified = {2022-11-25 16:14:21 +0100},
	doi = {10.48550/ARXIV.2202.09509},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {PETCI: A Parallel English Translation Dataset of Chinese Idioms},
	url = {https://arxiv.org/abs/2202.09509},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2202.09509}}

@article{gou_pretrained_2022,
	abstract = {Aspect Sentiment Triplet Extraction (ASTE) aims to extract the spans of aspect, opinion, and their sentiment relations as sentiment triplets. Existing works usually formulate the span detection as a 1D token tagging problem, and model the sentiment recognition with a 2D tagging matrix of token pairs. Moreover, by leveraging the token representation of Pretrained Language Encoders (PLEs) like BERT, they can achieve better performance. However, they simply leverage PLEs as feature extractors to build their modules but never have a deep look at what specific knowledge does PLEs contain. In this paper, we argue that instead of further designing modules to capture the inductive bias of ASTE, PLEs themselves contain "enough" features for 1D and 2D tagging: (1) The token representation contains the contextualized meaning of token itself, so this level feature carries necessary information for 1D tagging. (2) The attention matrix of different PLE layers can further capture multi-level linguistic knowledge existing in token pairs, which benefits 2D tagging. (3) Furthermore, with simple transformations, these two features can also be easily converted to the 2D tagging matrix and 1D tagging sequence, respectively. That will further boost the tagging results. By doing so, PLEs can be natural tagging frameworks and achieve a new state of the art, which is verified by extensive experiments and deep analyses.},
	author = {Gou, Yanjie and Lei, Yinjie and Liu, Lingqiao and Dai, Yong and Shen, Chunxu and Tong, Yongqi},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2208.09617},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Pretrained {Language} {Encoders} are {Natural} {Tagging} {Frameworks} for {Aspect} {Sentiment} {Triplet} {Extraction}},
	url = {https://arxiv.org/abs/2208.09617},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2208.09617},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2208.09617}}

@article{li_improving_2022,
	abstract = {Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages. In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI task. At Stage C1, we propose to refine standard cross-lingual linear maps between static word embeddings (WEs) via a contrastive learning objective; we also show how to integrate it into the self-learning procedure for even more refined cross-lingual maps. In Stage C2, we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word translation capability. We also show that static WEs induced from the `C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments on standard BLI datasets for diverse languages and different experimental setups demonstrate substantial gains achieved by our framework. While the BLI method from Stage C1 already yields substantial gains over all state-of-the-art BLI methods in our comparison, even stronger improvements are met with the full two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28 language pairs.},
	annote = {Other
ACL 2022 Main},
	author = {Li, Yaoyiran and Liu, Fangyu and Collier, Nigel and Korhonen, Anna and Vuli{\'c}, Ivan},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2203.08307},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computation and Language (cs.CL), Information Retrieval (cs.IR), Artificial Intelligence (cs.AI)},
	note = {Publisher: arXiv Version Number: 3},
	title = {Improving {Word} {Translation} via {Two}-{Stage} {Contrastive} {Learning}},
	url = {https://arxiv.org/abs/2203.08307},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2203.08307},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.08307}}

@inproceedings{li_uctopic_2022,
	address = {Dublin, Ireland},
	author = {Li, Jiacheng and Shang, Jingbo and McAuley, Julian},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	doi = {10.18653/v1/2022.acl-long.426},
	file = {Full Text:files/1415/Li et al. - 2022 - UCTopic Unsupervised Contrastive Learning for Phr.pdf:application/pdf},
	language = {en},
	pages = {6159--6169},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{UCTopic}},
	title = {{UCTopic}: {Unsupervised} {Contrastive} {Learning} for {Phrase} {Representations} and {Topic} {Mining}},
	url = {https://aclanthology.org/2022.acl-long.426},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.acl-long.426},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.acl-long.426}}

@article{gan_semglove_2022,
	author = {Gan, Leilei and Teng, Zhiyang and Zhang, Yue and Zhu, Linchao and Wu, Fei and Yang, Yi},
	doi = {10.1109/TASLP.2022.3197316},
	file = {Submitted Version:files/1422/Gan et al. - 2022 - SemGloVe Semantic Co-Occurrences for GloVe From B.pdf:application/pdf},
	issn = {2329-9290, 2329-9304},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	pages = {2696--2704},
	shorttitle = {{SemGloVe}},
	title = {{SemGloVe}: {Semantic} {Co}-{Occurrences} for {GloVe} {From} {BERT}},
	url = {https://ieeexplore.ieee.org/document/9852311/},
	urldate = {2022-11-25},
	volume = {30},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9852311/},
	bdsk-url-2 = {https://doi.org/10.1109/TASLP.2022.3197316}}

@article{yang_rethink_2022,
	abstract = {Word-level Quality Estimation (QE) of Machine Translation (MT) aims to find out potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are designed to predict the translation quality in terms of the post-editing effort, where the word labels ("OK" and "BAD") are automatically generated by comparing words between MT sentences and the post-edited sentences through a Translation Error Rate (TER) toolkit. While the post-editing effort can be used to measure the translation quality to some extent, we find it usually conflicts with the human judgement on whether the word is well or poorly translated. To overcome the limitation, we first create a golden benchmark dataset, namely {\textbackslash}emph\{HJQE\} (Human Judgement on Quality Estimation), where the expert translators directly annotate the poorly translated words on their judgements. Additionally, to further make use of the parallel corpus, we propose the self-supervised pre-training with two tag correcting strategies, namely tag refinement strategy and tree-based annotation strategy, to make the TER-based artificial QE corpus closer to {\textbackslash}emph\{HJQE\}. We conduct substantial experiments based on the publicly available WMT En-De and En-Zh corpora. The results not only show our proposed dataset is more consistent with human judgment but also confirm the effectiveness of the proposed tag correcting strategies.{\textbackslash}footnote\{The data can be found at {\textbackslash}url\{https://github.com/ZhenYangIACAS/HJQE\}.\}},
	annote = {Other
8 pages, 6 figures},
	author = {Yang, Zhen and Meng, Fandong and Yan, Yuanmeng and Zhou, Jie},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	doi = {10.48550/ARXIV.2209.05695},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {Rethink about the {Word}-level {Quality} {Estimation} for {Machine} {Translation} from {Human} {Judgement}},
	url = {https://arxiv.org/abs/2209.05695},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2209.05695},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2209.05695}}

@inproceedings{zhang_judging_2022,
	address = {Oxford United Kingdom},
	author = {Zhang, Kang and Shinden, Hiroaki and Mutsuro, Tatsuki and Suzuki, Einoshin},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	doi = {10.1145/3514094.3534171},
	file = {Full Text:files/1420/Zhang et al. - 2022 - Judging Instinct Exploitation in Statistical Data .pdf:application/pdf},
	isbn = {978-1-4503-9247-1},
	language = {en},
	month = jul,
	pages = {867--879},
	publisher = {ACM},
	title = {Judging {Instinct} {Exploitation} in {Statistical} {Data} {Explanations} {Based} on {Word} {Embedding}},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534171},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3514094.3534171},
	bdsk-url-2 = {https://doi.org/10.1145/3514094.3534171}}

@article{pham_pic_2022,
	abstract = {Since BERT (Devlin et al., 2018), learning contextualized word embeddings has been a de-facto standard in NLP. However, the progress of learning contextualized phrase embeddings is hindered by the lack of a human-annotated, phrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of {\textasciitilde}28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks of increasing difficulty for evaluating the quality of phrase embeddings. We find that training on our dataset improves ranking models' accuracy and remarkably pushes Question Answering (QA) models to near-human accuracy which is 95\% Exact Match (EM) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the QA models learn to better capture the common meaning of a phrase regardless of its actual context. That is, on our Phrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially (60\% EM), failing to differentiate between two different senses of the same phrase under two different contexts. Further results on our 3-task PiC benchmark reveal that learning contextualized phrase embeddings remains an interesting, open challenge.},
	annote = {Other
25 pages, 12 figures},
	author = {Pham, Thang M. and Yoon, Seunghyun and Bui, Trung and Nguyen, Anh},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2207.09068},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Artificial Intelligence (cs.AI)},
	note = {Publisher: arXiv Version Number: 3},
	shorttitle = {{PiC}},
	title = {{PiC}: {A} {Phrase}-in-{Context} {Dataset} for {Phrase} {Understanding} and {Semantic} {Search}},
	url = {https://arxiv.org/abs/2207.09068},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2207.09068},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2207.09068}}

@article{huang_can_2022,
	abstract = {A good speaker not only needs to be correct, but also has the ability to be specific when desired, and so are language models. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given ``J. K. Rowling was born in [MASK].'', we want to test whether a more specific answer will be better filled in by PLMs, e.g., Yate instead of England. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We believe this work can provide new insights for language modeling and encourage the research community to further explore this important but understudied problem.},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan and Xiong, Jinjun and Hwu, Wen-mei},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2210.05159},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL), Artificial Intelligence (cs.AI)},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {Can {Language} {Models} {Be} {Specific}?},
	title = {Can {Language} {Models} {Be} {Specific}? {How}?},
	url = {https://arxiv.org/abs/2210.05159},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.05159},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.05159}}

@article{mohamed_self-supervised_2022,
	author = {Mohamed, Abdelrahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob D. and Edin, Joakim and Igel, Christian and Kirchhoff, Katrin and Li, Shang-Wen and Livescu, Karen and Maaloe, Lars and Sainath, Tara N. and Watanabe, Shinji},
	doi = {10.1109/JSTSP.2022.3207050},
	file = {Submitted Version:files/1424/Mohamed et al. - 2022 - Self-Supervised Speech Representation Learning A .pdf:application/pdf},
	issn = {1932-4553, 1941-0484},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	month = oct,
	number = {6},
	pages = {1179--1210},
	shorttitle = {Self-{Supervised} {Speech} {Representation} {Learning}},
	title = {Self-{Supervised} {Speech} {Representation} {Learning}: {A} {Review}},
	url = {https://ieeexplore.ieee.org/document/9893562/},
	urldate = {2022-11-25},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9893562/},
	bdsk-url-2 = {https://doi.org/10.1109/JSTSP.2022.3207050}}

@incollection{avidan_improving_2022,
	address = {Cham},
	author = {Pham, Khoi and Kafle, Kushal and Lin, Zhe and Ding, Zhihong and Cohen, Scott and Tran, Quan and Shrivastava, Abhinav},
	booktitle = {Computer {Vision} -- {ECCV} 2022},
	doi = {10.1007/978-3-031-19806-9_12},
	editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	isbn = {978-3-031-19805-2 978-3-031-19806-9},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {201--219},
	publisher = {Springer Nature Switzerland},
	title = {Improving {Closed} and {Open}-{Vocabulary} {Attribute} {Prediction} {Using} {Transformers}},
	url = {https://link.springer.com/10.1007/978-3-031-19806-9_12},
	urldate = {2022-11-25},
	volume = {13685},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-031-19806-9_12},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-031-19806-9_12}}

@inproceedings{romana_enabling_2022,
	author = {Romana, Amrit and Niu, Minxue and Perez, Matthew and Roberts, Angela and Provost, Emily Mower},
	booktitle = {Interspeech 2022},
	doi = {10.21437/Interspeech.2022-10971},
	language = {en},
	month = sep,
	pages = {1916--1920},
	publisher = {ISCA},
	title = {Enabling {Off}-the-{Shelf} {Disfluency} {Detection} and {Categorization} for {Pathological} {Speech}},
	url = {https://www.isca-speech.org/archive/interspeech_2022/romana22_interspeech.html},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://www.isca-speech.org/archive/interspeech_2022/romana22_interspeech.html},
	bdsk-url-2 = {https://doi.org/10.21437/Interspeech.2022-10971}}

@article{yang_knowledge_2022,
	abstract = {Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5\% in marco F1 (from 10.3 to 11.8, P\&lt;0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.},
	annote = {Other
Accepted by Findings of EMNLP 2022, code is available at https://github.com/whaleloops/KEPT},
	author = {Yang, Zhichao and Wang, Shufan and Rawat, Bhanu Pratap Singh and Mitra, Avijit and Yu, Hong},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	doi = {10.48550/ARXIV.2210.03304},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 2},
	title = {Knowledge {Injected} {Prompt} {Based} {Fine}-tuning for {Multi}-label {Few}-shot {ICD} {Coding}},
	url = {https://arxiv.org/abs/2210.03304},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.03304},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.03304}}

@article{meng_general--specific_2022,
	abstract = {Training keyphrase generation (KPG) models requires a large amount of annotated data, which can be prohibitively expensive and often limited to specific domains. In this study, we first demonstrate that large distribution shifts among different domains severely hinder the transferability of KPG models. We then propose a three-stage pipeline, which gradually guides KPG models' learning focus from general syntactical features to domain-related semantics, in a data-efficient manner. With Domain-general Phrase pre-training, we pre-train Sequence-to-Sequence models with generic phrase annotations that are widely available on the web, which enables the models to generate phrases in a wide range of domains. The resulting model is then applied in the Transfer Labeling stage to produce domain-specific pseudo keyphrases, which help adapt models to a new domain. Finally, we fine-tune the model with limited data with true labels to fully adapt it to the target domain. Our experiment results show that the proposed process can produce good quality keyphrases in new domains and achieve consistent improvements after adaptation with limited in-domain annotated data.},
	author = {Meng, Rui and Wang, Tong and Yuan, Xingdi and Zhou, Yingbo and He, Daqing},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	doi = {10.48550/ARXIV.2208.09606},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	title = {General-to-{Specific} {Transfer} {Labeling} for {Domain} {Adaptable} {Keyphrase} {Generation}},
	url = {https://arxiv.org/abs/2208.09606},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2208.09606},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2208.09606}}

@article{yan_clip_2022,
	abstract = {Contrastive Language-Image Pretraining (CLIP) efficiently learns visual concepts by pre-training with natural language supervision. CLIP and its visual encoder have been explored on various vision and language tasks and achieve strong zero-shot or transfer learning performance. However, the application of its text encoder solely for text understanding has been less explored. In this paper, we find that the text encoder of CLIP actually demonstrates strong ability for phrase understanding, and can even significantly outperform popular language models such as BERT with a properly designed prompt. Extensive experiments validate the effectiveness of our method across different datasets and domains on entity clustering and entity set expansion tasks.},
	annote = {Other
Work in progress},
	author = {Yan, An and Li, Jiacheng and Zhu, Wanrong and Lu, Yujie and Wang, William Yang and McAuley, Julian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2210.05836},
	keywords = {FOS: Computer and information sciences, Computation and Language (cs.CL)},
	note = {Publisher: arXiv Version Number: 1},
	shorttitle = {{CLIP} also {Understands} {Text}},
	title = {{CLIP} also {Understands} {Text}: {Prompting} {CLIP} for {Phrase} {Understanding}},
	url = {https://arxiv.org/abs/2210.05836},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://arxiv.org/abs/2210.05836},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2210.05836}}

@article{korner_erkennung_2022,
	author = {K{\"o}rner, Erik},
	doi = {10.14220/mdge.2022.69.2.171},
	issn = {0418-9426, 2196-8756},
	journal = {Mitteilungen des Deutschen Germanistenverbandes},
	language = {de},
	month = jun,
	number = {2},
	pages = {171--176},
	title = {Die {Erkennung} und {Benennung} von {Themen}-{Clustern} mit dem {Dornseiff}},
	url = {https://www.vr-elibrary.de/doi/10.14220/mdge.2022.69.2.171},
	urldate = {2022-11-25},
	volume = {69},
	year = {2022},
	bdsk-url-1 = {https://www.vr-elibrary.de/doi/10.14220/mdge.2022.69.2.171},
	bdsk-url-2 = {https://doi.org/10.14220/mdge.2022.69.2.171}}

@inproceedings{su_tacl_2022,
	address = {Seattle, United States},
	author = {Su, Yixuan and Liu, Fangyu and Meng, Zaiqiao and Lan, Tian and Shu, Lei and Shareghi, Ehsan and Collier, Nigel},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	doi = {10.18653/v1/2022.findings-naacl.191},
	file = {Full Text:files/1421/Su et al. - 2022 - TaCL Improving BERT Pre-training with Token-aware.pdf:application/pdf},
	language = {en},
	pages = {2497--2507},
	publisher = {Association for Computational Linguistics},
	shorttitle = {{TaCL}},
	title = {{TaCL}: {Improving} {BERT} {Pre}-training with {Token}-aware {Contrastive} {Learning}},
	url = {https://aclanthology.org/2022.findings-naacl.191},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.findings-naacl.191},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2022.findings-naacl.191}}

@article{nguyen_refined_2022,
	author = {Nguyen, Tuan-Phong and Razniewski, Simon and Romero, Julien and Weikum, Gerhard},
	doi = {10.1109/TKDE.2022.3206505},
	file = {Submitted Version:files/1423/Nguyen et al. - 2022 - Refined Commonsense Knowledge from Large-Scale Web.pdf:application/pdf},
	issn = {1041-4347, 1558-2191, 2326-3865},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	pages = {1--16},
	title = {Refined {Commonsense} {Knowledge} from {Large}-{Scale} {Web} {Contents}},
	url = {https://ieeexplore.ieee.org/document/9891823/},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9891823/},
	bdsk-url-2 = {https://doi.org/10.1109/TKDE.2022.3206505}}

@incollection{avidan_webly_2022,
	address = {Cham},
	author = {Kamath, Amita and Clark, Christopher and Gupta, Tanmay and Kolve, Eric and Hoiem, Derek and Kembhavi, Aniruddha},
	booktitle = {Computer {Vision} -- {ECCV} 2022},
	doi = {10.1007/978-3-031-20059-5_38},
	editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	file = {Submitted Version:files/1425/Kamath et al. - 2022 - Webly Supervised Concept Expansion for General Pur.pdf:application/pdf},
	isbn = {978-3-031-20058-8 978-3-031-20059-5},
	language = {en},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {662--681},
	publisher = {Springer Nature Switzerland},
	title = {Webly {Supervised} {Concept} {Expansion} for {General} {Purpose} {Vision} {Models}},
	url = {https://link.springer.com/10.1007/978-3-031-20059-5_38},
	urldate = {2022-11-25},
	volume = {13696},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/978-3-031-20059-5_38},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-031-20059-5_38}}
