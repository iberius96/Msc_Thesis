%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-25 12:39:17 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{wood-etal-2022-bayesian,
	abstract = {One desiderata of topic modeling is to produce interpretable topics. Given a cluster of document-tokens comprising a topic, we can order the topic by counting each word. It is natural to think that each topic could easily be labeled by looking at the words with the highest word count. However, this is not always the case. A human evaluator can often have difficulty identifying a single label that accurately describes the topic as many top words seem unrelated. This paper aims to improve interpretability in topic modeling by providing a novel, outperforming interpretable topic model Our approach combines two previously established subdomains in topic modeling: nonparametric and weakly-supervised topic models. Given a nonparametric topic model, we can include weakly-supervised input using novel modifications to the nonparametric generative model. These modifications lay the groundwork for a compelling setting{---}one in which most corpora, without any previous supervised or weakly-supervised input, can discover interpretable topics. This setting also presents various challenging sub-problems of which we provide resolutions. Combining nonparametric topic models with weakly-supervised topic models leads to an exciting discovery{---}a complete, self-contained and outperforming topic model for interpretability.},
	address = {Marseille, France},
	author = {Wood, Justin and Arnold, Corey and Wang, Wei},
	booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	date-added = {2022-11-25 12:39:16 +0100},
	date-modified = {2022-11-25 12:39:16 +0100},
	month = jun,
	pages = {6271--6279},
	publisher = {European Language Resources Association},
	title = {A {B}ayesian Topic Model for Human-Evaluated Interpretability},
	url = {https://aclanthology.org/2022.lrec-1.674},
	year = {2022},
	bdsk-url-1 = {https://aclanthology.org/2022.lrec-1.674}}

@article{shi_survey_2023,
	author = {Shi, Lei and Luo, Jia and Zhu, Chuangying and Kou, Feifei and Cheng, Gang and Liu, Xia},
	doi = {10.1016/j.inffus.2022.11.017},
	issn = {15662535},
	journal = {Information Fusion},
	language = {en},
	month = mar,
	pages = {566--581},
	title = {A survey on cross-media search based on user intention understanding in social networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253522002275},
	urldate = {2022-11-25},
	volume = {91},
	year = {2023},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S1566253522002275},
	bdsk-url-2 = {https://doi.org/10.1016/j.inffus.2022.11.017}}

@article{jin_user-based_2022,
	abstract = {With the development of the Internet and mobile networks, social networks have gradually become an essential tool and widespread application. Therefore, the research on short text semantic modelling of social networks has attracted widespread attention. However, modelling short texts encounter the semantics sparsity and multiple meanings of a word in social networks. To solve the above problems, we propose a user-based topic model with topical word embeddings semantic modelling method, namely SM-UTM. Firstly, we construct the user topic model to aggregate short text. Secondly, we build word pair in the user topic model to alleviate semantics sparsity in social networks. In addition, we introduce the time information of social networks into the topic model to jointly constrain the generation process of topics, to improve the quality of semantic representation of social network short texts. Finally, we use the topic word embedding learning based on deep learning to train and optimize the word vector according to the learning results of the user topic model, to alleviate the problem of polysemy in social networks. We build multiple groups of quantitative and qualitative experiments based on the crawled real Sina Weibo data. The experimental results show that our SM-UTM is significantly better than the comparison method in the evaluation indicators of topic consistency, purity and entropy.},
	author = {Jin, Xin},
	doi = {10.3233/JIFS-212614},
	issn = {10641246, 18758967},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	month = jun,
	number = {1},
	pages = {1467--1480},
	title = {A user-based topic model with topical word embeddings for semantic modelling in social network},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/JIFS-212614},
	urldate = {2022-11-25},
	volume = {43},
	year = {2022},
	bdsk-url-1 = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/JIFS-212614},
	bdsk-url-2 = {https://doi.org/10.3233/JIFS-212614}}

@article{murshed_short_2022,
	author = {Murshed, Belal Abdullah Hezam and Mallappa, Suresha and Abawajy, Jemal and Saif, Mufeed Ahmed Naji and Al-ariki, Hasib Daowd Esmail and Abdulwahab, Hudhaifa Mohammed},
	doi = {10.1007/s10462-022-10254-w},
	file = {Full Text:files/167/Murshed et al. - 2022 - Short text topic modelling approaches in the conte.pdf:application/pdf},
	issn = {0269-2821, 1573-7462},
	journal = {Artificial Intelligence Review},
	language = {en},
	month = oct,
	shorttitle = {Short text topic modelling approaches in the context of big data},
	title = {Short text topic modelling approaches in the context of big data: taxonomy, survey, and analysis},
	url = {https://link.springer.com/10.1007/s10462-022-10254-w},
	urldate = {2022-11-25},
	year = {2022},
	bdsk-url-1 = {https://link.springer.com/10.1007/s10462-022-10254-w},
	bdsk-url-2 = {https://doi.org/10.1007/s10462-022-10254-w}}

@article{niu_nested_2021,
	abstract = {In recent years, short texts have become a kind of prevalent text on the internet. Due to the short length of each text, conventional topic models for short texts suffer from the sparsity of word co-occurrence information. Researchers have proposed different kinds of customized topic models for short texts by providing additional word co-occurrence information. However, these models cannot incorporate sufficient semantic word co-occurrence information and may bring additional noisy information. To address these issues, we propose a self-aggregated topic model incorporating document embeddings. Aggregating short texts into long documents according to document embeddings can provide sufficient word co-occurrence information and avoid incorporating non-semantic word co-occurrence information. However, document embeddings of short texts contain a lot of noisy information resulting from the sparsity of word co-occurrence information. So we discard noisy information by changing the document embeddings into global and local semantic information. The global semantic information is the similarity probability distribution on the entire dataset and the local semantic information is the distances of similar short texts. Then we adopt a nested Chinese restaurant process to incorporate these two kinds of information. Finally, we compare our model to several state-of-the-art models on four real-world short texts corpus. The experiment results show that our model achieves better performances in terms of topic coherence and classification accuracy.},
	author = {Niu, Yue and Zhang, Hongjie and Li, Jing},
	doi = {10.3390/app11188708},
	file = {Full Text:files/169/Niu et al. - 2021 - A Nested Chinese Restaurant Topic Model for Short .pdf:application/pdf},
	issn = {2076-3417},
	journal = {Applied Sciences},
	language = {en},
	month = sep,
	number = {18},
	pages = {8708},
	title = {A {Nested} {Chinese} {Restaurant} {Topic} {Model} for {Short} {Texts} with {Document} {Embeddings}},
	url = {https://www.mdpi.com/2076-3417/11/18/8708},
	urldate = {2022-11-25},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://www.mdpi.com/2076-3417/11/18/8708},
	bdsk-url-2 = {https://doi.org/10.3390/app11188708}}
