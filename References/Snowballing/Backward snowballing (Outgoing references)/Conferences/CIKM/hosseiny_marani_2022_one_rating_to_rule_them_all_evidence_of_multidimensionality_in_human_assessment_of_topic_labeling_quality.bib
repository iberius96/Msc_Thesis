%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 14:52:48 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@book{underwood_2019_distant_horizons_digital_evidence_and_literary_change,
	author = {Underwood, T.},
	date-added = {2022-11-17 14:52:41 +0100},
	date-modified = {2022-11-17 14:52:41 +0100},
	isbn = {9780226612836},
	lccn = {2018036446},
	publisher = {University of Chicago Press},
	title = {Distant Horizons: Digital Evidence and Literary Change},
	url = {https://books.google.it/books?id=fQo5uwEACAAJ},
	year = {2019},
	bdsk-url-1 = {https://books.google.it/books?id=fQo5uwEACAAJ}}

@inproceedings{vaccaro_2019_contestability_in_algorithmic_systems,
	abstract = {As algorithmic (and particularly machine learning) decision making systems become both more widespread and make more important decisions, there are growing concerns about their embedded values and ability to establish legitimacy among decision subjects. We argue that designing for contestability in these systems can assist in surfacing values, aligning system design and use with context, and building legitimacy. However, designing for contestability can be challenging, particularly in systems that are designed to be opaque: systems need to accurately surface embedded values, expose decision making processes in ways that are meaningful for users, support engagement with and allow influence over system performance, and so on. In addition to these technical aspects, designing for contestability may by challenged by the need to protect intellectual property and prevent gaming of the system. In this workshop, we will address goals, audiences, and designs for contestability in algorithmic systems. We hope to develop a taxonomy of contestable systems and understand the value provided by contestability, while bringing together a community to work on this multidisciplinary problem.},
	address = {New York, NY, USA},
	author = {Vaccaro, Kristen and Karahalios, Karrie and Mulligan, Deirdre K. and Kluttz, Daniel and Hirsch, Tad},
	booktitle = {Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing},
	date-added = {2022-11-17 14:51:30 +0100},
	date-modified = {2022-11-17 14:51:30 +0100},
	doi = {vaccaro_2019_contestability_in_algorithmic_systems},
	isbn = {9781450366922},
	keywords = {contestability, algorithmic experience},
	location = {Austin, TX, USA},
	numpages = {5},
	pages = {523--527},
	publisher = {Association for Computing Machinery},
	series = {CSCW '19},
	title = {Contestability in Algorithmic Systems},
	url = {https://doi.org/10.1145/3311957.3359435},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3311957.3359435}}

@inproceedings{wang_2019_balanced_datasets_are_not_enough_estimating_and_mitigating_gender_bias_in_deep_image_representations,
	author = {Tianlu Wang and Jieyu Zhao and Mark Yatskar and Kai-Wei Chang and Vicente Ordonez},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	date-added = {2022-11-17 14:51:24 +0100},
	date-modified = {2022-11-17 14:51:24 +0100},
	doi = {10.1109/iccv.2019.00541},
	month = {oct},
	publisher = {{IEEE}},
	title = {Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations},
	url = {https://doi.org/10.1109%2Ficcv.2019.00541},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1109%2Ficcv.2019.00541},
	bdsk-url-2 = {https://doi.org/10.1109/iccv.2019.00541}}

@article{tapal_2017_the_sense_of_agency_scale_a_measure_of_consciously_perceived_control_over_onetextquotesingles_mind_body_and_the_immediate_environment,
	author = {Adam Tapal and Ela Oren and Reuven Dar and Baruch Eitam},
	date-added = {2022-11-17 14:51:09 +0100},
	date-modified = {2022-11-17 14:51:09 +0100},
	doi = {10.3389/fpsyg.2017.01552},
	journal = {Frontiers in Psychology},
	month = {sep},
	publisher = {Frontiers Media {SA}},
	title = {The Sense of Agency Scale: A Measure of Consciously Perceived Control over One{\textquotesingle}s Mind, Body, and the Immediate Environment},
	url = {https://doi.org/10.3389%2Ffpsyg.2017.01552},
	volume = {8},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.3389%2Ffpsyg.2017.01552},
	bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2017.01552}}

@inproceedings{wang_2019_implicit_gender_biases_in_professional_software_development_an_empirical_study,
	author = {Wang, Yi and Redmiles, David},
	booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS)},
	date-added = {2022-11-17 14:50:53 +0100},
	date-modified = {2022-11-17 14:50:53 +0100},
	doi = {10.1109/ICSE-SEIS.2019.00009},
	pages = {1-10},
	title = {Implicit Gender Biases in Professional Software Development: An Empirical Study},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/ICSE-SEIS.2019.00009}}

@inproceedings{woodruff_2018_a_qualitative_exploration_of_perceptions_of_algorithmic_fairness,
	abstract = {Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussions about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.},
	address = {New York, NY, USA},
	author = {Woodruff, Allison and Fox, Sarah E. and Rousso-Schindler, Steven and Warshaw, Jeffrey},
	booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	date-added = {2022-11-17 14:50:36 +0100},
	date-modified = {2022-11-17 14:50:36 +0100},
	doi = {woodruff_2018_a_qualitative_exploration_of_perceptions_of_algorithmic_fairness},
	isbn = {9781450356206},
	keywords = {algorithmic fairness, algorithmic discrimination},
	location = {Montreal QC, Canada},
	numpages = {14},
	pages = {1--14},
	publisher = {Association for Computing Machinery},
	series = {CHI '18},
	title = {A Qualitative Exploration of Perceptions of Algorithmic Fairness},
	url = {https://doi.org/10.1145/3173574.3174230},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3173574.3174230}}

@inproceedings{yang_2018_investigating_how_experienced_ux_designers_effectively_work_with_machine_learning,
	abstract = {Machine learning (ML) plays an increasingly important role in improving a user's experience. However, most UX practitioners face challenges in understanding ML's capabilities or envisioning what it might be. We interviewed 13 designers who had many years of experience designing the UX of ML-enhanced products and services. We probed them to characterize their practices. They shared they do not view themselves as ML experts, nor do they think learning more about ML would make them better designers. Instead, our participants appeared to be the most successful when they engaged in ongoing collaboration with data scientists to help envision what to make and when they embraced a data-centric culture. We discuss the implications of these findings in terms of UX education and as opportunities for additional design research in support of UX designers working with ML.},
	address = {New York, NY, USA},
	author = {Yang, Qian and Scuito, Alex and Zimmerman, John and Forlizzi, Jodi and Steinfeld, Aaron},
	booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
	date-added = {2022-11-17 14:50:29 +0100},
	date-modified = {2022-11-17 14:50:29 +0100},
	doi = {yang_2018_investigating_how_experienced_ux_designers_effectively_work_with_machine_learning},
	isbn = {9781450351980},
	keywords = {user experience design, interaction design, design material, ux practice, machine learning},
	location = {Hong Kong, China},
	numpages = {12},
	pages = {585--596},
	publisher = {Association for Computing Machinery},
	series = {DIS '18},
	title = {Investigating How Experienced UX Designers Effectively Work with Machine Learning},
	url = {https://doi.org/10.1145/3196709.3196730},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3196709.3196730}}

@article{zafar_2016_fairness_beyond_disparate_treatment_amp;_disparate_impact_learning_classification_without_disparate_mistreatment,
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:50:23 +0100},
	date-modified = {2022-11-17 14:50:23 +0100},
	doi = {10.48550/ARXIV.1610.08452},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification without Disparate Mistreatment},
	url = {https://arxiv.org/abs/1610.08452},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1610.08452},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1610.08452}}

@article{smith_2017_evaluating_visual_representations_for_topic_understanding_and_their_effects_on_manually_generated_topic_labels,
	abstract = {Probabilistic topic models are important tools for indexing, summarizing, and analyzing large document collections by their themes. However, promoting end-user understanding of topics remains an open research problem. We compare labels generated by users given four topic visualization techniques{---}word lists, word lists with bars, word clouds, and network graphs{---}against each other and against automatically generated labels. Our basis of comparison is participant ratings of how well labels describe documents from the topic. Our study has two phases: a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics{'} documents. Although all visualizations produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure. Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms.},
	address = {Cambridge, MA},
	author = {Smith, Alison and Lee, Tak Yeon and Poursabzi-Sangdeh, Forough and Boyd-Graber, Jordan and Elmqvist, Niklas and Findlater, Leah},
	date-added = {2022-11-17 14:49:09 +0100},
	date-modified = {2022-11-17 14:49:09 +0100},
	doi = {10.1162/tacl_a_00042},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {1--16},
	publisher = {MIT Press},
	title = {Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels},
	url = {https://aclanthology.org/Q17-1001},
	volume = {5},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/Q17-1001},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00042}}

@article{schwemmer_2020_diagnosing_gender_bias_in_image_recognition_systems,
	author = {Carsten Schwemmer and Carly Knight and Emily D. Bello-Pardo and Stan Oklobdzija and Martijn Schoonvelde and Jeffrey W. Lockhart},
	date-added = {2022-11-17 14:49:01 +0100},
	date-modified = {2022-11-17 14:49:01 +0100},
	doi = {10.1177/2378023120967171},
	journal = {Socius: Sociological Research for a Dynamic World},
	month = {jan},
	pages = {237802312096717},
	publisher = {{SAGE} Publications},
	title = {Diagnosing Gender Bias in Image Recognition Systems},
	url = {https://doi.org/10.1177%2F2378023120967171},
	volume = {6},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1177%2F2378023120967171},
	bdsk-url-2 = {https://doi.org/10.1177/2378023120967171}}

@article{scheuerman_2019_how_computers_see_gender_an_evaluation_of_gender_classification_in_commercial_facial_analysis_services,
	abstract = {Investigations of facial analysis (FA) technologies-such as facial detection and facial recognition-have been central to discussions about Artificial Intelligence's (AI) impact on human beings. Research on automatic gender recognition, the classification of gender by FA technologies, has raised potential concerns around issues of racial and gender bias. In this study, we augment past work with empirical data by conducting a systematic analysis of how gender classification and gender labeling in computer vision services operate when faced with gender diversity. We sought to understand how gender is concretely conceptualized and encoded into commercial facial analysis and image labeling technologies available today. We then conducted a two-phase study: (1) a system analysis of ten commercial FA and image labeling services and (2) an evaluation of five services using a custom dataset of diverse genders using self-labeled Instagram images. Our analysis highlights how gender is codified into both classifiers and data standards. We found that FA services performed consistently worse on transgender individuals and were universally unable to classify non-binary genders. In contrast, image labeling often presented multiple gendered concepts. We also found that user perceptions about gender performance and identity contradict the way gender performance is encoded into the computer vision infrastructure. We discuss our findings from three perspectives of gender identity (self-identity, gender performativity, and demographic identity) and how these perspectives interact across three layers: the classification infrastructure, the third-party applications that make use of that infrastructure, and the individuals who interact with that software. We employ Bowker and Star's concepts of "torque" and "residuality" to further discuss the social implications of gender classification. We conclude by outlining opportunities for creating more inclusive classification infrastructures and datasets, as well as with implications for policy.},
	address = {New York, NY, USA},
	articleno = {144},
	author = {Scheuerman, Morgan Klaus and Paul, Jacob M. and Brubaker, Jed R.},
	date-added = {2022-11-17 14:48:50 +0100},
	date-modified = {2022-11-17 14:48:50 +0100},
	doi = {scheuerman_2019_how_computers_see_gender_an_evaluation_of_gender_classification_in_commercial_facial_analysis_services},
	issue_date = {November 2019},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	keywords = {gender, Instagram, facial recognition, identity, image labeling, facial detection, facial analysis, computer vision, classification},
	month = {nov},
	number = {CSCW},
	numpages = {33},
	publisher = {Association for Computing Machinery},
	title = {How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services},
	url = {https://doi.org/10.1145/3359246},
	volume = {3},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3359246}}

@inproceedings{robertson_2018_auditing_the_personalization_and_composition_of_politically_related_search_engine_results_pages,
	abstract = {Search engines are a primary means through which people obtain information in today»s connected world. Yet, apart from the search engine companies themselves, little is known about how their algorithms filter, rank, and present the web to users. This question is especially pertinent with respect to political queries, given growing concerns about filter bubbles, and the recent finding that bias or favoritism in search rankings can influence voting behavior. In this study, we conduct a targeted algorithm audit of Google Search using a dynamic set of political queries. We designed a Chrome extension to survey participants and collect the Search Engine Results Pages (SERPs) and autocomplete suggestions that they would have been exposed to while searching our set of political queries during the month after Donald Trump»s Presidential inauguration. Using this data, we found significant differences in the composition and personalization of politically-related SERPs by query type, subjects» characteristics, and date.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Robertson, Ronald E. and Lazer, David and Wilson, Christo},
	booktitle = {Proceedings of the 2018 World Wide Web Conference},
	date-added = {2022-11-17 14:48:40 +0100},
	date-modified = {2022-11-17 14:48:40 +0100},
	doi = {robertson_2018_auditing_the_personalization_and_composition_of_politically_related_search_engine_results_pages},
	isbn = {9781450356398},
	keywords = {search engine manipulation, filter bubble, search engine results, search ranking bias, political personalization, autocomplete search suggestions},
	location = {Lyon, France},
	numpages = {11},
	pages = {955--965},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '18},
	title = {Auditing the Personalization and Composition of Politically-Related Search Engine Results Pages},
	url = {https://doi.org/10.1145/3178876.3186143},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3178876.3186143}}

@article{robertson_2018_auditing_partisan_audience_bias_within_google_search,
	abstract = {There is a growing consensus that online platforms have a systematic influence on the democratic process. However, research beyond social media is limited. In this paper, we report the results of a mixed-methods algorithm audit of partisan audience bias and personalization within Google Search. Following Donald Trump's inauguration, we recruited 187 participants to complete a survey and install a browser extension that enabled us to collect Search Engine Results Pages (SERPs) from their computers. To quantify partisan audience bias, we developed a domain-level score by leveraging the sharing propensities of registered voters on a large Twitter panel. We found little evidence for the "filter bubble'' hypothesis. Instead, we found that results positioned toward the bottom of Google SERPs were more left-leaning than results positioned toward the top, and that the direction and magnitude of overall lean varied by search query, component type (e.g. "answer boxes"), and other factors. Utilizing rank-weighted metrics that we adapted from prior work, we also found that Google's rankings shifted the average lean of SERPs to the right of their unweighted average.},
	address = {New York, NY, USA},
	articleno = {148},
	author = {Robertson, Ronald E. and Jiang, Shan and Joseph, Kenneth and Friedland, Lisa and Lazer, David and Wilson, Christo},
	date-added = {2022-11-17 14:48:28 +0100},
	date-modified = {2022-11-17 14:48:28 +0100},
	doi = {robertson_2018_auditing_partisan_audience_bias_within_google_search},
	issue_date = {November 2018},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	keywords = {filter bubble, political personalization, search engine rankings, algorithm auditing, quantifying partisan bias},
	month = {nov},
	number = {CSCW},
	numpages = {22},
	publisher = {Association for Computing Machinery},
	title = {Auditing Partisan Audience Bias within Google Search},
	url = {https://doi.org/10.1145/3274417},
	volume = {2},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3274417}}

@article{nelson_2017_computational_grounded_theory_a_methodological_framework,
	author = {Laura K. Nelson},
	date-added = {2022-11-17 14:48:05 +0100},
	date-modified = {2022-11-17 14:48:05 +0100},
	doi = {10.1177/0049124117729703},
	journal = {Sociological Methods {\&}amp$\mathsemicolon$ Research},
	month = {nov},
	number = {1},
	pages = {3--42},
	publisher = {{SAGE} Publications},
	title = {Computational Grounded Theory: A Methodological Framework},
	url = {https://doi.org/10.1177%2F0049124117729703},
	volume = {49},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1177%2F0049124117729703},
	bdsk-url-2 = {https://doi.org/10.1177/0049124117729703}}

@misc{moreira_2018_image_provenance_analysis_at_scale,
	author = {Moreira, Daniel and Bharati, Aparna and Brogan, Joel and Pinto, Allan and Parowski, Michael and Bowyer, Kevin W. and Flynn, Patrick J. and Rocha, Anderson and Scheirer, Walter J.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:47:58 +0100},
	date-modified = {2022-11-17 14:47:58 +0100},
	doi = {10.48550/ARXIV.1801.06510},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Image Provenance Analysis at Scale},
	url = {https://arxiv.org/abs/1801.06510},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1801.06510},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1801.06510}}

@article{mantyla_2018_measuring_lda_topic_stability_from_clusters_of_replicated_runs,
	author = {M{\"a}ntyl{\"a}, Mika and Claes, Ma{\"e}lick and Farooq, Umar},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:47:11 +0100},
	date-modified = {2022-11-17 14:47:11 +0100},
	doi = {10.48550/ARXIV.1808.08098},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Measuring LDA Topic Stability from Clusters of Replicated Runs},
	url = {https://arxiv.org/abs/1808.08098},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1808.08098},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.08098}}

@inproceedings{lau_2017_topically_driven_neural_language_model,
	abstract = {Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.},
	address = {Vancouver, Canada},
	author = {Lau, Jey Han and Baldwin, Timothy and Cohn, Trevor},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-17 14:47:00 +0100},
	date-modified = {2022-11-17 14:47:00 +0100},
	doi = {10.18653/v1/P17-1033},
	month = jul,
	pages = {355--365},
	publisher = {Association for Computational Linguistics},
	title = {Topically Driven Neural Language Model},
	url = {https://aclanthology.org/P17-1033},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1033},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1033}}

@article{kulshrestha_2017_quantifying_search_bias_investigating_sources_of_bias_for_political_searches_in_social_media,
	author = {Kulshrestha, Juhi and Eslami, Motahhare and Messias, Johnnatan and Zafar, Muhammad Bilal and Ghosh, Saptarshi and Gummadi, Krishna P. and Karahalios, Karrie},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:46:41 +0100},
	date-modified = {2022-11-17 14:46:41 +0100},
	doi = {10.48550/ARXIV.1704.01347},
	keywords = {Social and Information Networks (cs.SI), Computers and Society (cs.CY), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Quantifying Search Bias: Investigating Sources of Bias for Political Searches in Social Media},
	url = {https://arxiv.org/abs/1704.01347},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1704.01347},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1704.01347}}

@techreport{kleinberg_2017_human_decisions_and_machine_predictions,
	author = {Jon Kleinberg and Himabindu Lakkaraju and Jure Leskovec and Jens Ludwig and Sendhil Mullainathan},
	date-added = {2022-11-17 14:46:36 +0100},
	date-modified = {2022-11-17 14:46:36 +0100},
	doi = {10.3386/w23180},
	month = {feb},
	publisher = {National Bureau of Economic Research},
	title = {Human Decisions and Machine Predictions},
	url = {https://doi.org/10.3386%2Fw23180},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.3386%2Fw23180},
	bdsk-url-2 = {https://doi.org/10.3386/w23180}}

@article{john_2019_the_case_of_computer_science_education_employment_gender_and_raceethnicity_in_silicon_valley_1980textendash2015,
	author = {June Park John and Martin Carnoy},
	date-added = {2022-11-17 14:46:14 +0100},
	date-modified = {2022-11-17 14:46:14 +0100},
	doi = {10.1080/13639080.2019.1679728},
	journal = {Journal of Education and Work},
	month = {jul},
	number = {5},
	pages = {421--435},
	publisher = {Informa {UK} Limited},
	title = {The case of computer science education, employment, gender, and race/ethnicity in Silicon Valley, 1980{\textendash}2015},
	url = {https://doi.org/10.1080%2F13639080.2019.1679728},
	volume = {32},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1080%2F13639080.2019.1679728},
	bdsk-url-2 = {https://doi.org/10.1080/13639080.2019.1679728}}

@article{jiang_2021_supporting_serendipity_opportunities_and_challenges_for_human_ai_collaboration_in_qualitative_analysis,
	author = {Jiang, Jialun Aaron and Wade, Kandrea and Fiesler, Casey and Brubaker, Jed R.},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	date-added = {2022-11-17 14:46:03 +0100},
	date-modified = {2022-11-17 14:46:03 +0100},
	doi = {10.48550/ARXIV.2102.03702},
	keywords = {Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Supporting Serendipity: Opportunities and Challenges for Human-AI Collaboration in Qualitative Analysis},
	url = {https://arxiv.org/abs/2102.03702},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2102.03702},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2102.03702}}

@misc{hoyle_2021_is_automated_topic_model_evaluation_broken_the_incoherence_of_coherence,
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:45:52 +0100},
	date-modified = {2022-11-17 14:45:52 +0100},
	doi = {10.48550/ARXIV.2107.02173},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
	url = {https://arxiv.org/abs/2107.02173},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2107.02173},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2107.02173}}

@article{hoffman_2017_evaluating_a_computational_approach_to_labeling_politeness_challenges_for_the_application_of_machine_classification_to_social_computing_data,
	abstract = {Social computing researchers are beginning to apply machine learning tools to classify and analyze social media data. Our interest in understanding politeness in an online community focused our attention on tools that would help automate politeness analysis. This paper highlights one popular classification tool designed to score the politeness of text. Our application of this tool to Wikipedia data yielded some unexpected results. Those unexpected results led us to question how the tool worked and its effectiveness relative to human judgment and classification. We designed a user study to revalidate the tool with crowdworkers labeling samples of content from Wikipedia talk pages, imitating the original efforts to validate the tool. This revalidation points to challenges for automatic labeling. Based on our results, this paper reconsiders politeness in online communities as well as broader trends in the use of machine classifiers in social computing research.},
	address = {New York, NY, USA},
	articleno = {52},
	author = {Hoffman, Erin R. and McDonald, David W. and Zachry, Mark},
	date-added = {2022-11-17 14:45:46 +0100},
	date-modified = {2022-11-17 14:45:46 +0100},
	doi = {hoffman_2017_evaluating_a_computational_approach_to_labeling_politeness_challenges_for_the_application_of_machine_classification_to_social_computing_data},
	issue_date = {November 2017},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	keywords = {social computing, politeness, machine classification},
	month = {dec},
	number = {CSCW},
	numpages = {14},
	publisher = {Association for Computing Machinery},
	title = {Evaluating a Computational Approach to Labeling Politeness: Challenges for the Application of Machine Classification to Social Computing Data},
	url = {https://doi.org/10.1145/3134687},
	volume = {1},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3134687}}

@inproceedings{hirsch_2017_designing_contestability,
	author = {Tad Hirsch and Kritzia Merced and Shrikanth Narayanan and Zac E. Imel and David C. Atkins},
	booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
	date-added = {2022-11-17 14:45:07 +0100},
	date-modified = {2022-11-17 14:45:07 +0100},
	doi = {10.1145/3064663.3064703},
	month = {jun},
	publisher = {{ACM}},
	title = {Designing Contestability},
	url = {https://doi.org/10.1145%2F3064663.3064703},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1145%2F3064663.3064703},
	bdsk-url-2 = {https://doi.org/10.1145/3064663.3064703}}

@article{fiebrink_2018_introduction_to_the_special_issue_on_human_centered_machine_learning,
	abstract = {Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Fiebrink, Rebecca and Gillies, Marco},
	date-added = {2022-11-17 14:43:59 +0100},
	date-modified = {2022-11-17 14:43:59 +0100},
	doi = {fiebrink_2018_introduction_to_the_special_issue_on_human_centered_machine_learning},
	issn = {2160-6455},
	issue_date = {June 2018},
	journal = {ACM Trans. Interact. Intell. Syst.},
	keywords = {interactive machine learning, Human-centered machine learning},
	month = {jun},
	number = {2},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	title = {Introduction to the Special Issue on Human-Centered Machine Learning},
	url = {https://doi.org/10.1145/3205942},
	volume = {8},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3205942}}

@inproceedings{dove_2017_ux_design_innovation_challenges_for_working_with_machine_learning_as_a_design_material,
	abstract = {Machine learning (ML) is now a fairly established technology, and user experience (UX) designers appear regularly to integrate ML services in new apps, devices, and systems. Interestingly, this technology has not experienced a wealth of design innovation that other technologies have, and this might be because it is a new and difficult design material. To better understand why we have witnessed little design innovation, we conducted a survey of current UX practitioners with regards to how new ML services are envisioned and developed in UX practice. Our survey probed on how ML may or may not have been a part of their UX design education, on how they work to create new things with developers, and on the challenges they have faced working with this material. We use the findings from this survey and our review of related literature to present a series of challenges for UX and interaction design research and education. Finally, we discuss areas where new research and new curriculum might help our community unlock the power of design thinking to re-imagine what ML might be and might do.},
	address = {New York, NY, USA},
	author = {Dove, Graham and Halskov, Kim and Forlizzi, Jodi and Zimmerman, John},
	booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
	date-added = {2022-11-17 14:43:53 +0100},
	date-modified = {2022-11-17 14:43:53 +0100},
	doi = {dove_2017_ux_design_innovation_challenges_for_working_with_machine_learning_as_a_design_material},
	isbn = {9781450346559},
	keywords = {ux practice, design material, machine learning, interaction design},
	location = {Denver, Colorado, USA},
	numpages = {11},
	pages = {278--288},
	publisher = {Association for Computing Machinery},
	series = {CHI '17},
	title = {UX Design Innovation: Challenges for Working with Machine Learning as a Design Material},
	url = {https://doi.org/10.1145/3025453.3025739},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3025453.3025739}}

@misc{cui_2018_learning_to_evaluate_image_captioning,
	author = {Cui, Yin and Yang, Guandao and Veit, Andreas and Huang, Xun and Belongie, Serge},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:43:44 +0100},
	date-modified = {2022-11-17 14:43:44 +0100},
	doi = {10.48550/ARXIV.1806.06422},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Learning to Evaluate Image Captioning},
	url = {https://arxiv.org/abs/1806.06422},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1806.06422},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1806.06422}}

@misc{sam_2018_the_measure_and_mismeasure_of_fairness_a_critical_review_of_fair_machine_learning,
	author = {Corbett-Davies, Sam and Goel, Sharad},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 14:43:36 +0100},
	date-modified = {2022-11-17 14:43:36 +0100},
	doi = {10.48550/ARXIV.1808.00023},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
	url = {https://arxiv.org/abs/1808.00023},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1808.00023},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.00023}}

@article{clementson_2017_truth_bias_and_partisan_bias_in_political_deception_detection,
	author = {David E. Clementson},
	date-added = {2022-11-17 14:43:26 +0100},
	date-modified = {2022-11-17 14:43:26 +0100},
	doi = {10.1177/0261927x17744004},
	journal = {Journal of Language and Social Psychology},
	month = {nov},
	number = {4},
	pages = {407--430},
	publisher = {{SAGE} Publications},
	title = {Truth Bias and Partisan Bias in Political Deception Detection},
	url = {https://doi.org/10.1177%2F0261927x17744004},
	volume = {37},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1177%2F0261927x17744004},
	bdsk-url-2 = {https://doi.org/10.1177/0261927x17744004}}

@inproceedings{buolamwini_2018_gender_shades_intersectional_accuracy_disparities_in_commercial_gender_classification,
	author = {Joy Buolamwini and Timnit Gebru},
	booktitle = {FAT},
	date-added = {2022-11-17 14:43:16 +0100},
	date-modified = {2022-11-17 14:43:16 +0100},
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	year = {2018}}

@article{baumer_2020_topicalizer_reframing_core_concepts_in_machine_learning_visualization_by_co_designing_for_interpretivist_scholarship,
	author = {Eric P. S. Baumer and Drew Siedel and Lena McDonnell and Jiayun Zhong and Patricia Sittikul and Micki McGee},
	date-added = {2022-11-17 14:43:03 +0100},
	date-modified = {2022-11-17 14:43:03 +0100},
	doi = {10.1080/07370024.2020.1734460},
	journal = {Human{\textendash}Computer Interaction},
	month = {apr},
	number = {5-6},
	pages = {452--480},
	publisher = {Informa {UK} Limited},
	title = {Topicalizer: reframing core concepts in machine learning visualization by co-designing for interpretivist scholarship},
	url = {https://doi.org/10.1080%2F07370024.2020.1734460},
	volume = {35},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1080%2F07370024.2020.1734460},
	bdsk-url-2 = {https://doi.org/10.1080/07370024.2020.1734460}}
