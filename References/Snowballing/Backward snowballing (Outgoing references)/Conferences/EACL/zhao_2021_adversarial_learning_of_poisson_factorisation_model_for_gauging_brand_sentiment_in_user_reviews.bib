%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 16:33:22 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@incollection{masada_2018_adversarial_learning_for_topic_models,
	author = {Tomonari Masada and Atsuhiro Takasu},
	booktitle = {Advanced Data Mining and Applications},
	date-added = {2022-11-17 16:33:16 +0100},
	date-modified = {2022-11-17 16:33:16 +0100},
	doi = {10.1007/978-3-030-05090-0_25},
	pages = {292--302},
	publisher = {Springer International Publishing},
	title = {Adversarial Learning for Topic Models},
	url = {https://doi.org/10.1007%2F978-3-030-05090-0_25},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-030-05090-0_25},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-05090-0_25}}

@incollection{silva_2017_content_based_social_recommendation_with_poisson_matrix_factorization,
	author = {Eliezer de Souza da Silva and Helge Langseth and Heri Ramampiaro},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2022-11-17 16:33:01 +0100},
	date-modified = {2022-11-17 16:33:01 +0100},
	doi = {10.1007/978-3-319-71249-9_32},
	pages = {530--546},
	publisher = {Springer International Publishing},
	title = {Content-Based Social Recommendation with Poisson Matrix Factorization},
	url = {https://doi.org/10.1007%2F978-3-319-71249-9_32},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1007%2F978-3-319-71249-9_32},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-319-71249-9_32}}

@article{wang_2018_atmadversarial_neural_topic_model,
	author = {Wang, Rui and Zhou, Deyu and He, Yulan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:32:44 +0100},
	date-modified = {2022-11-17 16:32:44 +0100},
	doi = {10.48550/ARXIV.1811.00265},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {ATM:Adversarial-neural Topic Model},
	url = {https://arxiv.org/abs/1811.00265},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1811.00265},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1811.00265}}

@inproceedings{wang_2020_neural_topic_modeling_with_bidirectional_adversarial_training,
	abstract = {Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6{\%} is observed in accuracy.},
	address = {Online},
	author = {Wang, Rui and Hu, Xuemeng and Zhou, Deyu and He, Yulan and Xiong, Yuxuan and Ye, Chenchen and Xu, Haiyang},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 16:32:40 +0100},
	date-modified = {2022-11-17 16:32:40 +0100},
	doi = {10.18653/v1/2020.acl-main.32},
	month = jul,
	pages = {340--350},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Modeling with Bidirectional Adversarial Training},
	url = {https://aclanthology.org/2020.acl-main.32},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.32},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.32}}

@misc{vafa_2020_text_based_ideal_points,
	author = {Vafa, Keyon and Naidu, Suresh and Blei, David M.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:32:36 +0100},
	date-modified = {2022-11-17 16:32:36 +0100},
	doi = {10.48550/ARXIV.2005.04232},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Text-Based Ideal Points},
	url = {https://arxiv.org/abs/2005.04232},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2005.04232},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2005.04232}}

@inproceedings{su_2019_personalized_point_of_interest_recommendation_on_ranking_with_poisson_factorization,
	author = {Su, Yijun and Li, Xiang and Tang, Wei and Zha, Daren and Xiang, Ji and Gao, Neng},
	booktitle = {2019 International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2022-11-17 16:32:31 +0100},
	date-modified = {2022-11-17 16:32:31 +0100},
	doi = {10.1109/IJCNN.2019.8852462},
	pages = {1-8},
	title = {Personalized Point-of-Interest Recommendation on Ranking with Poisson Factorization},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/IJCNN.2019.8852462}}

@misc{srivastava_2017_autoencoding_variational_inference_for_topic_models,
	author = {Srivastava, Akash and Sutton, Charles},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:32:25 +0100},
	date-modified = {2022-11-17 16:32:25 +0100},
	doi = {10.48550/ARXIV.1703.01488},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/abs/1703.01488},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.01488},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.01488}}

@inproceedings{nan_2019_topic_modeling_with_wasserstein_autoencoders,
	abstract = {We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.},
	address = {Florence, Italy},
	author = {Nan, Feng and Ding, Ran and Nallapati, Ramesh and Xiang, Bing},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 16:32:18 +0100},
	date-modified = {2022-11-17 16:32:18 +0100},
	doi = {10.18653/v1/P19-1640},
	month = jul,
	pages = {6345--6381},
	publisher = {Association for Computational Linguistics},
	title = {Topic Modeling with {W}asserstein Autoencoders},
	url = {https://aclanthology.org/P19-1640},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1640},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1640}}

@article{kuo_2021_the_framework_of_personalized_ranking_on_poisson_factorization,
	author = {Kuo, Li-Yen and Chou, Chung-Kuang and Chen, Ming-Syan},
	date-added = {2022-11-17 16:32:02 +0100},
	date-modified = {2022-11-17 16:32:02 +0100},
	doi = {10.1109/TKDE.2019.2924894},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	number = {1},
	pages = {287-301},
	title = {The Framework of Personalized Ranking on Poisson Factorization},
	volume = {33},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2019.2924894}}

@article{kang_2019_decoupling_representation_and_classifier_for_long_tailed_recognition,
	author = {Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:31:56 +0100},
	date-modified = {2022-11-17 16:31:56 +0100},
	doi = {10.48550/ARXIV.1910.09217},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Decoupling Representation and Classifier for Long-Tailed Recognition},
	url = {https://arxiv.org/abs/1910.09217},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1910.09217},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1910.09217}}

@misc{joo_2020_generalized_gumbel_softmax_gradient_estimator_for_various_discrete_random_variables,
	author = {Joo, Weonyoung and Kim, Dongjun and Shin, Seungjae and Moon, Il-Chul},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:31:52 +0100},
	date-modified = {2022-11-17 16:31:52 +0100},
	doi = {10.48550/ARXIV.2003.01847},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Generalized Gumbel-Softmax Gradient Estimator for Various Discrete Random Variables},
	url = {https://arxiv.org/abs/2003.01847},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2003.01847},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2003.01847}}

@misc{john_2018_disentangled_representation_learning_for_non_parallel_text_style_transfer,
	author = {John, Vineet and Mou, Lili and Bahuleyan, Hareesh and Vechtomova, Olga},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:31:46 +0100},
	date-modified = {2022-11-17 16:31:46 +0100},
	doi = {10.48550/ARXIV.1808.04339},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7, 68T50},
	publisher = {arXiv},
	title = {Disentangled Representation Learning for Non-Parallel Text Style Transfer},
	url = {https://arxiv.org/abs/1808.04339},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1808.04339},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.04339}}

@inproceedings{jiang_2017_a_topic_model_based_on_poisson_decomposition,
	abstract = {Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA.},
	address = {New York, NY, USA},
	author = {Jiang, Haixin and Zhou, Rui and Zhang, Limeng and Wang, Hua and Zhang, Yanchun},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	date-added = {2022-11-17 16:31:40 +0100},
	date-modified = {2022-11-17 16:31:40 +0100},
	doi = {jiang_2017_a_topic_model_based_on_poisson_decomposition},
	isbn = {9781450349185},
	keywords = {topic model, topic coherence, statistical testing, poisson decomposition, text classification},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1489--1498},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {A Topic Model Based on Poisson Decomposition},
	url = {https://doi.org/10.1145/3132847.3132942},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3132847.3132942}}

@misc{jang_2016_categorical_reparameterization_with_gumbel_softmax,
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:31:34 +0100},
	date-modified = {2022-11-17 16:31:34 +0100},
	doi = {10.48550/ARXIV.1611.01144},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Categorical Reparameterization with Gumbel-Softmax},
	url = {https://arxiv.org/abs/1611.01144},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1611.01144},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.01144}}

@inproceedings{huang_2018_siamese_network_based_supervised_topic_modeling,
	abstract = {Label-specific topics can be widely used for supporting personality psychology, aspect-level sentiment analysis, and cross-domain sentiment classification. To generate label-specific topics, several supervised topic models which adopt likelihood-driven objective functions have been proposed. However, it is hard for them to get a precise estimation on both topic discovery and supervised learning. In this study, we propose a supervised topic model based on the Siamese network, which can trade off label-specific word distributions with document-specific label distributions in a uniform framework. Experiments on real-world datasets validate that our model performs competitive in topic discovery quantitatively and qualitatively. Furthermore, the proposed model can effectively predict categorical or real-valued labels for new documents by generating word embeddings from a label-specific topical space.},
	address = {Brussels, Belgium},
	author = {Huang, Minghui and Rao, Yanghui and Liu, Yuwei and Xie, Haoran and Wang, Fu Lee},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-11-17 16:31:29 +0100},
	date-modified = {2022-11-17 16:31:29 +0100},
	doi = {10.18653/v1/D18-1494},
	month = oct # {-} # nov,
	pages = {4652--4662},
	publisher = {Association for Computational Linguistics},
	title = {{S}iamese Network-Based Supervised Topic Modeling},
	url = {https://aclanthology.org/D18-1494},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1494},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1494}}

@misc{hu_2020_neural_topic_modeling_with_cycle_consistent_adversarial_training,
	author = {Hu, Xuemeng and Wang, Rui and Zhou, Deyu and Xiong, Yuxuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:31:25 +0100},
	date-modified = {2022-11-17 16:31:25 +0100},
	doi = {10.48550/ARXIV.2009.13971},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Neural Topic Modeling with Cycle-Consistent Adversarial Training},
	url = {https://arxiv.org/abs/2009.13971},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2009.13971},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2009.13971}}

@inproceedings{he_2018_effective_attention_modeling_for_aspect_level_sentiment_classification,
	abstract = {Aspect-level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target. A sentence could contain multiple sentiment-target pairs; thus the main challenge of this task is to separate different opinion contexts for different targets. To this end, attention mechanism has played an important role in previous state-of-the-art neural models. The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations. We build upon this line of research and propose two novel approaches for improving the effectiveness of attention. First, we propose a method for target representation that better captures the semantic meaning of the opinion target. Second, we introduce an attention model that incorporates syntactic information into the attention mechanism. We experiment on attention-based LSTM (Long Short-Term Memory) models using the datasets from SemEval 2014, 2015, and 2016. The experimental results show that the conventional attention-based LSTM can be substantially improved by incorporating the two approaches.},
	address = {Santa Fe, New Mexico, USA},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	date-added = {2022-11-17 16:31:19 +0100},
	date-modified = {2022-11-17 16:31:19 +0100},
	month = aug,
	pages = {1121--1131},
	publisher = {Association for Computational Linguistics},
	title = {Effective Attention Modeling for Aspect-Level Sentiment Classification},
	url = {https://aclanthology.org/C18-1096},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/C18-1096}}

@inproceedings{he_2017_an_unsupervised_neural_attention_model_for_aspect_extraction,
	abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
	address = {Vancouver, Canada},
	author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-17 16:31:14 +0100},
	date-modified = {2022-11-17 16:31:14 +0100},
	doi = {10.18653/v1/P17-1036},
	month = jul,
	pages = {388--397},
	publisher = {Association for Computational Linguistics},
	title = {An Unsupervised Neural Attention Model for Aspect Extraction},
	url = {https://aclanthology.org/P17-1036},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1036},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1036}}

@inproceedings{guo_2017_understanding_users_budgets_for_recommendation_with_hierarchical_poisson_factorization,
	abstract = {People consume and rate products in online shopping websites. The historical purchases of customers reflect their personal consumption habits and indicate their future shopping behaviors. Traditional preference-based recommender systems try to provide recommendations by analyzing users' feedback such as ratings and clicks. But unfortunately, most of the existing recommendation algorithms ignore the budget of the users. So they cannot avoid recommending users with products that will exceed their budgets. And they also cannot understand how the users will assign their budgets to different products. In this paper, we develop a generative model named collaborative budget-aware Poisson factorization (CBPF) to connect users' ratings and budgets. The CBPF model is intuitive and highly interpretable. We compare the proposed model with several state-of-the-art budget-unaware recommendation methods on several realworld datasets. The results show the advantage of uncovering users' budgets for recommendation.},
	author = {Guo, Yunhui and Xu, Congfu and Song, Hanzhang and Wang, Xin},
	booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
	date-added = {2022-11-17 16:31:09 +0100},
	date-modified = {2022-11-17 16:31:09 +0100},
	isbn = {9780999241103},
	location = {Melbourne, Australia},
	numpages = {7},
	pages = {1781--1787},
	publisher = {AAAI Press},
	series = {IJCAI'17},
	title = {Understanding Users' Budgets for Recommendation with Hierarchical Poisson Factorization},
	year = {2017}}

@article{gui_2022_multi_task_mutual_learning_for_joint_sentiment_classification_and_topic_detection,
	author = {Gui, Lin and Leng, Jia and Zhou, Jiyun and Xu, Ruifeng and He, Yulan},
	date-added = {2022-11-17 16:31:02 +0100},
	date-modified = {2022-11-17 16:31:02 +0100},
	doi = {10.1109/TKDE.2020.2999489},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	number = {4},
	pages = {1915-1927},
	title = {Multi Task Mutual Learning for Joint Sentiment Classification and Topic Detection},
	volume = {34},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2020.2999489}}

@article{gao_2017_collaborative_dynamic_sparse_topic_regression_with_user_profile_evolution_for_item_recommendation,
	author = {Li Gao and Jia Wu and Chuan Zhou and Yue Hu},
	date-added = {2022-11-17 16:30:55 +0100},
	date-modified = {2022-11-17 16:30:55 +0100},
	doi = {10.1609/aaai.v31i1.10726},
	journal = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	month = {feb},
	number = {1},
	publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
	title = {Collaborative Dynamic Sparse Topic Regression with User Profile Evolution for Item Recommendation},
	url = {https://doi.org/10.1609%2Faaai.v31i1.10726},
	volume = {31},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1609%2Faaai.v31i1.10726},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v31i1.10726}}

@misc{chaidaroon_2017_variational_deep_semantic_hashing_for_text_documents,
	author = {Chaidaroon, Suthee and Fang, Yi},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:30:46 +0100},
	date-modified = {2022-11-17 16:30:46 +0100},
	doi = {10.48550/ARXIV.1708.03436},
	keywords = {Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Variational Deep Semantic Hashing for Text Documents},
	url = {https://arxiv.org/abs/1708.03436},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1708.03436},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1708.03436}}

@article{card_2017_neural_models_for_documents_with_metadata,
	author = {Card, Dallas and Tan, Chenhao and Smith, Noah A.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:30:40 +0100},
	date-modified = {2022-11-17 16:30:40 +0100},
	doi = {10.48550/ARXIV.1705.09296},
	keywords = {Machine Learning (stat.ML), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Neural Models for Documents with Metadata},
	url = {https://arxiv.org/abs/1705.09296},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1705.09296},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1705.09296}}

@misc{bouchacourt_2017_multi_level_variational_autoencoder_learning_disentangled_representations_from_grouped_observations,
	author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:30:32 +0100},
	date-modified = {2022-11-17 16:30:32 +0100},
	doi = {10.48550/ARXIV.1705.08841},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations},
	url = {https://arxiv.org/abs/1705.08841},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1705.08841},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1705.08841}}

@article{blei_2017_variational_inference_a_review_for_statisticians,
	author = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
	date-added = {2022-11-17 16:30:24 +0100},
	date-modified = {2022-11-17 16:30:24 +0100},
	doi = {10.1080/01621459.2017.1285773},
	journal = {Journal of the American Statistical Association},
	month = {apr},
	number = {518},
	pages = {859--877},
	publisher = {Informa {UK} Limited},
	title = {Variational Inference: A Review for Statisticians},
	url = {https://doi.org/10.1080%2F01621459.2017.1285773},
	volume = {112},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1080%2F01621459.2017.1285773},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.2017.1285773}}

@article{barry_2018_alcohol_advertising_on_twittertextemdasha_topic_model,
	author = {Adam E. Barry and Danny Valdez and Alisa A. Padon and Alex M. Russell},
	date-added = {2022-11-17 16:30:08 +0100},
	date-modified = {2022-11-17 16:30:08 +0100},
	doi = {10.1080/19325037.2018.1473180},
	journal = {American Journal of Health Education},
	month = {jun},
	number = {4},
	pages = {256--263},
	publisher = {Informa {UK} Limited},
	title = {Alcohol Advertising on Twitter{\textemdash}A Topic Model},
	url = {https://doi.org/10.1080%2F19325037.2018.1473180},
	volume = {49},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1080%2F19325037.2018.1473180},
	bdsk-url-2 = {https://doi.org/10.1080/19325037.2018.1473180}}
