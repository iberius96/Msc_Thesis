%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 15:36:44 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{ma_2019_learning_representations_for_time_series_clustering,
	author = {Ma, Qianli and Zheng, Jiawei and Li, Sen and Cottrell, Gary W},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2022-11-17 15:36:38 +0100},
	date-modified = {2022-11-17 15:36:38 +0100},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning Representations for Time Series Clustering},
	url = {https://proceedings.neurips.cc/paper/2019/file/1359aa933b48b754a2f54adb688bfa77-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/file/1359aa933b48b754a2f54adb688bfa77-Paper.pdf}}

@inproceedings{zhao_2021_a_relation_oriented_clustering_method_for_open_relation_extraction,
	abstract = {The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the model to learn to cluster relational data, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our method reduces the error rate by 29.2{\%} and 15.7{\%}, on two datasets respectively, compared with current SOTA methods.},
	address = {Online and Punta Cana, Dominican Republic},
	author = {Zhao, Jun and Gui, Tao and Zhang, Qi and Zhou, Yaqian},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-11-17 15:35:56 +0100},
	date-modified = {2022-11-17 15:35:56 +0100},
	doi = {10.18653/v1/2021.emnlp-main.765},
	month = nov,
	pages = {9707--9718},
	publisher = {Association for Computational Linguistics},
	title = {A Relation-Oriented Clustering Method for Open Relation Extraction},
	url = {https://aclanthology.org/2021.emnlp-main.765},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.emnlp-main.765},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.765}}

@misc{zhang_2018_whai_weibull_hybrid_autoencoding_inference_for_deep_topic_modeling,
	author = {Zhang, Hao and Chen, Bo and Guo, Dandan and Zhou, Mingyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:35:51 +0100},
	date-modified = {2022-11-17 15:35:51 +0100},
	doi = {10.48550/ARXIV.1803.01328},
	keywords = {Machine Learning (stat.ML), Applications (stat.AP), Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling},
	url = {https://arxiv.org/abs/1803.01328},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1803.01328},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1803.01328}}

@misc{zhang_2021_supporting_clustering_with_contrastive_learning,
	author = {Zhang, Dejiao and Nan, Feng and Wei, Xiaokai and Li, Shangwen and Zhu, Henghui and McKeown, Kathleen and Nallapati, Ramesh and Arnold, Andrew and Xiang, Bing},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	date-added = {2022-11-17 15:35:45 +0100},
	date-modified = {2022-11-17 15:35:45 +0100},
	doi = {10.48550/ARXIV.2103.12953},
	keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Supporting Clustering with Contrastive Learning},
	url = {https://arxiv.org/abs/2103.12953},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2103.12953},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2103.12953}}

@misc{zeng_2018_topic_memory_networks_for_short_text_classification,
	author = {Zeng, Jichuan and Li, Jing and Song, Yan and Gao, Cuiyun and Lyu, Michael R. and King, Irwin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:35:35 +0100},
	date-modified = {2022-11-17 15:35:35 +0100},
	doi = {10.48550/ARXIV.1809.03664},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic Memory Networks for Short Text Classification},
	url = {https://arxiv.org/abs/1809.03664},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1809.03664},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1809.03664}}

@inproceedings{yang_2020_adversarial_learning_for_robust_deep_clustering,
	abstract = {Deep clustering integrates embedding and clustering together to obtain the optimal nonlinear embedding space, which is more effective in real-world scenarios compared with conventional clustering methods. However, the robustness of the clustering network is prone to being attenuated especially when it encounters an adversarial attack. A small perturbation in the embedding space will lead to diverse clustering results since the labels are absent. In this paper, we propose a robust deep clustering method based on adversarial learning. Specifically, we first attempt to define adversarial samples in the embedding space for the clustering network. Meanwhile, we devise an adversarial attack strategy to explore samples that easily fool the clustering layers but do not impact the performance of the deep embedding. We then provide a simple yet efficient defense algorithm to improve the robustness of the clustering network. Experimental results on two popular datasets show that the proposed adversarial learning method can significantly enhance the robustness and further improve the overall clustering performance. Particularly, the proposed method is generally applicable to multiple existing clustering frameworks to boost their robustness.},
	address = {Red Hook, NY, USA},
	articleno = {763},
	author = {Yang, Xu and Deng, Cheng and Wei, Kun and Yan, Junchi and Liu, Wei},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	date-added = {2022-11-17 15:35:27 +0100},
	date-modified = {2022-11-17 15:35:27 +0100},
	isbn = {9781713829546},
	location = {Vancouver, BC, Canada},
	numpages = {11},
	publisher = {Curran Associates Inc.},
	series = {NIPS'20},
	title = {Adversarial Learning for Robust Deep Clustering},
	year = {2020}}

@inproceedings{yang_2019_deep_clustering_by_gaussian_mixture_variational_autoencoders_with_graph_embedding,
	author = {Yang, Linxiao and Cheung, Ngai-Man and Li, Jiaying and Fang, Jun},
	booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	date-added = {2022-11-17 15:35:15 +0100},
	date-modified = {2022-11-17 15:35:15 +0100},
	doi = {10.1109/ICCV.2019.00654},
	pages = {6439-6448},
	title = {Deep Clustering by Gaussian Mixture Variational Autoencoders With Graph Embedding},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/ICCV.2019.00654}}

@article{xu_2017_self_taught_convolutional_neural_networks_for_short_text_clustering,
	author = {Xu, Jiaming and Xu, Bo and Wang, Peng and Zheng, Suncong and Tian, Guanhua and Zhao, Jun and Xu, Bo},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:35:09 +0100},
	date-modified = {2022-11-17 15:35:09 +0100},
	doi = {10.48550/ARXIV.1701.00185},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Self-Taught Convolutional Neural Networks for Short Text Clustering},
	url = {https://arxiv.org/abs/1701.00185},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1701.00185},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1701.00185}}

@inproceedings{wu_2018_tracing_fake_news_footprints_characterizing_social_media_messages_by_how_they_propagate,
	abstract = {When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classification of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Specifically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classification accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.},
	address = {New York, NY, USA},
	author = {Wu, Liang and Liu, Huan},
	booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
	date-added = {2022-11-17 15:35:01 +0100},
	date-modified = {2022-11-17 15:35:01 +0100},
	doi = {wu_2018_tracing_fake_news_footprints_characterizing_social_media_messages_by_how_they_propagate},
	isbn = {9781450355810},
	keywords = {social network analysis, social media mining, classification, fake news detection, graph mining, misinformation},
	location = {Marina Del Rey, CA, USA},
	numpages = {9},
	pages = {637--645},
	publisher = {Association for Computing Machinery},
	series = {WSDM '18},
	title = {Tracing Fake-News Footprints: Characterizing Social Media Messages by How They Propagate},
	url = {https://doi.org/10.1145/3159652.3159677},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3159652.3159677}}

@misc{wu_2019_deep_comprehensive_correlation_mining_for_image_clustering,
	author = {Wu, Jianlong and Long, Keyu and Wang, Fei and Qian, Chen and Li, Cheng and Lin, Zhouchen and Zha, Hongbin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:34:52 +0100},
	date-modified = {2022-11-17 15:34:52 +0100},
	doi = {10.48550/ARXIV.1904.06925},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Deep Comprehensive Correlation Mining for Image Clustering},
	url = {https://arxiv.org/abs/1904.06925},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.06925},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.06925}}

@inproceedings{wang_2020_friendly_topic_assistant_for_transformer_based_abstractive_summarization,
	abstract = {Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.},
	address = {Online},
	author = {Wang, Zhengjue and Duan, Zhibin and Zhang, Hao and Wang, Chaojie and Tian, Long and Chen, Bo and Zhou, Mingyuan},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	date-added = {2022-11-17 15:34:42 +0100},
	date-modified = {2022-11-17 15:34:42 +0100},
	doi = {10.18653/v1/2020.emnlp-main.35},
	month = nov,
	pages = {485--497},
	publisher = {Association for Computational Linguistics},
	title = {Friendly Topic Assistant for Transformer Based Abstractive Summarization},
	url = {https://aclanthology.org/2020.emnlp-main.35},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.emnlp-main.35},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.35}}

@inproceedings{wang_2020_neural_topic_model_with_attention_for_supervised_learning,
	abstract = {Topic modeling utilizing neural variational inference has shown promising results recently. Unlike traditional Bayesian topic models, neural topic models use deep neural network to approximate the intractable marginal distribution and thus gain strong generalisation ability. However, neural topic models are unsupervised model. Directly using the document-specific topic proportions in downstream prediction tasks could lead to sub-optimal performance. This paper presents Topic Attention Model (TAM), a supervised neural topic model that integrates an attention recurrent neural network (RNN) model. We design a novel way to utilize document-specific topic proportions and global topic vectors learned from neural topic model in the attention mechanism. We also develop backpropagation inference method that allows for joint model optimisation.  Experimental results on three public datasets show that TAM  not only significantly improves supervised learning tasks, including classification and regression, but also achieves lower perplexity for the document modeling.},
	author = {Wang, Xinyi and YANG, YI},
	booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
	date-added = {2022-11-17 15:34:19 +0100},
	date-modified = {2022-11-17 15:34:19 +0100},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	month = {26--28 Aug},
	pages = {1147--1156},
	pdf = {http://proceedings.mlr.press/v108/wang20c/wang20c.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Neural Topic Model with Attention for Supervised Learning},
	url = {https://proceedings.mlr.press/v108/wang20c.html},
	volume = {108},
	year = {2020},
	bdsk-url-1 = {https://proceedings.mlr.press/v108/wang20c.html}}

@misc{wang_2019_topic_guided_variational_autoencoders_for_text_generation,
	author = {Wang, Wenlin and Gan, Zhe and Xu, Hongteng and Zhang, Ruiyi and Wang, Guoyin and Shen, Dinghan and Chen, Changyou and Carin, Lawrence},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:34:13 +0100},
	date-modified = {2022-11-17 15:34:13 +0100},
	doi = {10.48550/ARXIV.1903.07137},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic-Guided Variational Autoencoders for Text Generation},
	url = {https://arxiv.org/abs/1903.07137},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1903.07137},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1903.07137}}

@misc{van_gansbeke_2020_scan_learning_to_classify_images_without_labels,
	author = {Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Proesmans, Marc and Van Gool, Luc},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:34:04 +0100},
	date-modified = {2022-11-17 15:34:04 +0100},
	doi = {10.48550/ARXIV.2005.12320},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {SCAN: Learning to Classify Images without Labels},
	url = {https://arxiv.org/abs/2005.12320},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2005.12320},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2005.12320}}

@misc{tsai_2021_mice_mixture_of_contrastive_experts_for_unsupervised_image_clustering,
	author = {Tsai, Tsung Wei and Li, Chongxuan and Zhu, Jun},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-17 15:33:57 +0100},
	date-modified = {2022-11-17 15:33:57 +0100},
	doi = {10.48550/ARXIV.2105.01899},
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering},
	url = {https://arxiv.org/abs/2105.01899},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2105.01899},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2105.01899}}

@inproceedings{tang_2019_a_topic_augmented_text_generation_model_joint_learning_of_semantics_and_structural_features,
	abstract = {Text generation is among the most fundamental tasks in natural language processing. In this paper, we propose a text generation model that learns semantics and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on Gaussian distribution to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the model further adapts the encoder of the topic modeling component for a discriminator. The results of experiments over several datasets demonstrate that our model outperforms several states of the art models in terms of text perplexity and topic coherence. Moreover, the latent representations learned by our model is superior to others in a text classification task. Finally, given the input texts, our model can generate meaningful texts which hold similar structures but under different topics.},
	address = {Hong Kong, China},
	author = {Tang, Hongyin and Li, Miao and Jin, Beihong},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-11-17 15:33:49 +0100},
	date-modified = {2022-11-17 15:33:49 +0100},
	doi = {10.18653/v1/D19-1513},
	month = nov,
	pages = {5090--5099},
	publisher = {Association for Computational Linguistics},
	title = {A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features},
	url = {https://aclanthology.org/D19-1513},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1513},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1513}}

@misc{srivastava_2017_autoencoding_variational_inference_for_topic_models,
	author = {Srivastava, Akash and Sutton, Charles},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:33:43 +0100},
	date-modified = {2022-11-17 15:33:43 +0100},
	doi = {10.48550/ARXIV.1703.01488},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/abs/1703.01488},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.01488},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.01488}}

@misc{see_2017_get_to_the_point_summarization_with_pointer_generator_networks,
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:33:36 +0100},
	date-modified = {2022-11-17 15:33:36 +0100},
	doi = {10.48550/ARXIV.1704.04368},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Get To The Point: Summarization with Pointer-Generator Networks},
	url = {https://arxiv.org/abs/1704.04368},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1704.04368},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1704.04368}}

@misc{reimers_2019_sentence_bert_sentence_embeddings_using_siamese_bert_networks,
	author = {Reimers, Nils and Gurevych, Iryna},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	date-added = {2022-11-17 15:33:30 +0100},
	date-modified = {2022-11-17 15:33:30 +0100},
	doi = {10.48550/ARXIV.1908.10084},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	url = {https://arxiv.org/abs/1908.10084},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1908.10084},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1908.10084}}

@misc{rakib_2020_enhancement_of_short_text_clustering_by_iterative_classification,
	author = {Rakib, Md Rashadul Hasan and Zeh, Norbert and Jankowska, Magdalena and Milios, Evangelos},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:33:24 +0100},
	date-modified = {2022-11-17 15:33:24 +0100},
	doi = {10.48550/ARXIV.2001.11631},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Enhancement of Short Text Clustering by Iterative Classification},
	url = {https://arxiv.org/abs/2001.11631},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2001.11631},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2001.11631}}

@misc{madiraju_2018_deep_temporal_clustering__fully_unsupervised_learning_of_time_domain_features,
	author = {Madiraju, Naveen Sai and Sadat, Seid M. and Fisher, Dimitry and Karimabadi, Homa},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:33:14 +0100},
	date-modified = {2022-11-17 15:33:14 +0100},
	doi = {10.48550/ARXIV.1802.01059},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Deep Temporal Clustering : Fully Unsupervised Learning of Time-Domain Features},
	url = {https://arxiv.org/abs/1802.01059},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1802.01059},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1802.01059}}

@misc{liu_2019_roberta_a_robustly_optimized_bert_pretraining_approach,
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:32:43 +0100},
	date-modified = {2022-11-17 15:32:43 +0100},
	doi = {10.48550/ARXIV.1907.11692},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	url = {https://arxiv.org/abs/1907.11692},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.11692},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.11692}}

@misc{li_2020_contrastive_clustering,
	author = {Li, Yunfan and Hu, Peng and Liu, Zitao and Peng, Dezhong and Zhou, Joey Tianyi and Peng, Xi},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:32:34 +0100},
	date-modified = {2022-11-17 15:32:34 +0100},
	doi = {10.48550/ARXIV.2009.09687},
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Contrastive Clustering},
	url = {https://arxiv.org/abs/2009.09687},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2009.09687},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2009.09687}}

@inproceedings{kobayashi_2018_contextual_augmentation_data_augmentation_by_words_with_paradigmatic_relations,
	abstract = {We propose a novel data augmentation for labeled sentences called contextual augmentation. We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions. Words predicted according to a context are numerous but appropriate for the augmentation of the original words. Furthermore, we retrofit a language model with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves classifiers based on the convolutional or recurrent neural networks.},
	address = {New Orleans, Louisiana},
	author = {Kobayashi, Sosuke},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	date-added = {2022-11-17 15:32:27 +0100},
	date-modified = {2022-11-17 15:32:27 +0100},
	doi = {10.18653/v1/N18-2072},
	month = jun,
	pages = {452--457},
	publisher = {Association for Computational Linguistics},
	title = {Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations},
	url = {https://aclanthology.org/N18-2072},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-2072},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-2072}}

@inproceedings{guo_2017_improved_deep_embedded_clustering_with_local_structure_preservation,
	abstract = {Deep clustering learns deep feature representations that favor clustering task using neural networks. Some pioneering work proposes to simultaneously learn embedded features and perform clustering by explicitly defining a clustering oriented loss. Though promising performance has been demonstrated in various applications, we observe that a vital ingredient has been overlooked by these work that the defined clustering loss may corrupt feature space, which leads to non-representative meaningless features and this in turn hurts clustering performance. To address this issue, in this paper, we propose the Improved Deep Embedded Clustering (IDEC) algorithm to take care of data structure preservation. Specifically, we manipulate feature space to scatter data points using a clustering loss as guidance. To constrain the manipulation and maintain the local structure of data generating distribution, an under-complete autoencoder is applied. By integrating the clustering loss and autoencoder's reconstruction loss, IDEC can jointly optimize cluster labels assignment and learn features that are suitable for clustering with local structure preservation. The resultant optimization problem can be effectively solved by mini-batch stochastic gradient descent and backpropagation. Experiments on image and text datasets empirically validate the importance of local structure preservation and the effectiveness of our algorithm.},
	author = {Guo, Xifeng and Gao, Long and Liu, Xinwang and Yin, Jianping},
	booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
	date-added = {2022-11-17 15:32:21 +0100},
	date-modified = {2022-11-17 15:32:21 +0100},
	isbn = {9780999241103},
	location = {Melbourne, Australia},
	numpages = {7},
	pages = {1753--1759},
	publisher = {AAAI Press},
	series = {IJCAI'17},
	title = {Improved Deep Embedded Clustering with Local Structure Preservation},
	year = {2017}}

@inproceedings{ghasedi_2019_balanced_self_paced_learning_for_generative_adversarial_clustering_network,
	author = {Ghasedi, Kamran and Wang, Xiaoqian and Deng, Cheng and Huang, Heng},
	booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2022-11-17 15:32:15 +0100},
	date-modified = {2022-11-17 15:32:15 +0100},
	doi = {10.1109/CVPR.2019.00452},
	pages = {4386-4395},
	title = {Balanced Self-Paced Learning for Generative Adversarial Clustering Network},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/CVPR.2019.00452}}

@misc{dieng_2019_topic_modeling_in_embedding_spaces,
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:32:08 +0100},
	date-modified = {2022-11-17 15:32:08 +0100},
	doi = {10.48550/ARXIV.1907.04907},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic Modeling in Embedding Spaces},
	url = {https://arxiv.org/abs/1907.04907},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.04907},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.04907}}

@misc{devlin_2018_bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:53 +0100},
	date-modified = {2022-11-17 15:31:53 +0100},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://arxiv.org/abs/1810.04805},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1810.04805},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1810.04805}}

@inproceedings{cong_2017_deep_latent_dirichlet_allocation_with_topic_layer_adaptive_stochastic_gradient_riemannian_mcmc,
	abstract = {It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.},
	author = {Yulai Cong and Bo Chen and Hongwei Liu and Mingyuan Zhou},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2022-11-17 15:31:48 +0100},
	date-modified = {2022-11-17 15:31:48 +0100},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = {06--11 Aug},
	pages = {864--873},
	pdf = {http://proceedings.mlr.press/v70/cong17a/cong17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Deep Latent {D}irichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient {R}iemannian {MCMC}},
	url = {https://proceedings.mlr.press/v70/cong17a.html},
	volume = {70},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v70/cong17a.html}}

@misc{chen_2020_a_simple_framework_for_contrastive_learning_of_visual_representations,
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:41 +0100},
	date-modified = {2022-11-17 15:31:41 +0100},
	doi = {10.48550/ARXIV.2002.05709},
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {https://arxiv.org/abs/2002.05709},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2002.05709},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2002.05709}}

@misc{caron_2018_deep_clustering_for_unsupervised_learning_of_visual_features,
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:37 +0100},
	date-modified = {2022-11-17 15:31:37 +0100},
	doi = {10.48550/ARXIV.1807.05520},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Deep Clustering for Unsupervised Learning of Visual Features},
	url = {https://arxiv.org/abs/1807.05520},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1807.05520},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1807.05520}}

@misc{brahma_2018_unsupervised_learning_of_sentence_representations_using_sequence_consistency,
	author = {Brahma, Siddhartha},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:33 +0100},
	date-modified = {2022-11-17 15:31:33 +0100},
	doi = {10.48550/ARXIV.1808.04217},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Unsupervised Learning of Sentence Representations Using Sequence Consistency},
	url = {https://arxiv.org/abs/1808.04217},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1808.04217},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.04217}}

@misc{asano_2019_self_labelling_via_simultaneous_clustering_and_representation_learning,
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:28 +0100},
	date-modified = {2022-11-17 15:31:28 +0100},
	doi = {10.48550/ARXIV.1911.05371},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Self-labelling via simultaneous clustering and representation learning},
	url = {https://arxiv.org/abs/1911.05371},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1911.05371},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1911.05371}}

@misc{aljalbout_2018_clustering_with_deep_learning_taxonomy_and_new_methods,
	author = {Aljalbout, Elie and Golkov, Vladimir and Siddiqui, Yawar and Strobel, Maximilian and Cremers, Daniel},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:24 +0100},
	date-modified = {2022-11-17 15:31:24 +0100},
	doi = {10.48550/ARXIV.1801.07648},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, H.3.3; I.2.6; I.5; I.5.3; I.5.4, 62H30, 62M45, 91C20},
	publisher = {arXiv},
	title = {Clustering with Deep Learning: Taxonomy and New Methods},
	url = {https://arxiv.org/abs/1801.07648},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1801.07648},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1801.07648}}

@misc{ailem_2019_topic_augmented_generator_for_abstractive_summarization,
	author = {Ailem, Melissa and Zhang, Bowen and Sha, Fei},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:31:17 +0100},
	date-modified = {2022-11-17 15:31:17 +0100},
	doi = {10.48550/ARXIV.1908.07026},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic Augmented Generator for Abstractive Summarization},
	url = {https://arxiv.org/abs/1908.07026},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1908.07026},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1908.07026}}
