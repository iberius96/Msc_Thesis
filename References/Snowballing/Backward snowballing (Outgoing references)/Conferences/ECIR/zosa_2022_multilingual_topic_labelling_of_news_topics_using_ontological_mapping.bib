%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 16:43:32 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{zhang_2019_bertscore_evaluating_text_generation_with_bert,
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:43:27 +0100},
	date-modified = {2022-11-17 16:43:27 +0100},
	doi = {10.48550/ARXIV.1904.09675},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BERTScore: Evaluating Text Generation with BERT},
	url = {https://arxiv.org/abs/1904.09675},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.09675},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.09675}}

@misc{vaswani_2017_attention_is_all_you_need,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:43:22 +0100},
	date-modified = {2022-11-17 16:43:22 +0100},
	doi = {10.48550/ARXIV.1706.03762},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Attention Is All You Need},
	url = {https://arxiv.org/abs/1706.03762},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1706.03762},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1706.03762}}

@misc{reimers_2019_sentence_bert_sentence_embeddings_using_siamese_bert_networks,
	author = {Reimers, Nils and Gurevych, Iryna},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	date-added = {2022-11-17 16:43:17 +0100},
	date-modified = {2022-11-17 16:43:17 +0100},
	doi = {10.48550/ARXIV.1908.10084},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	url = {https://arxiv.org/abs/1908.10084},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1908.10084},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1908.10084}}

@inproceedings{popa_2021_bart_tl_weakly_supervised_topic_label_generation,
	abstract = {We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pre-trained BART models on a large number of potential labels generated by state of the art non-neural models for topic labeling, enriched with different techniques. The proposed BART-TL model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.},
	address = {Online},
	author = {Popa, Cristian and Rebedea, Traian},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	date-added = {2022-11-17 16:43:12 +0100},
	date-modified = {2022-11-17 16:43:12 +0100},
	doi = {10.18653/v1/2021.eacl-main.121},
	month = apr,
	pages = {1418--1425},
	publisher = {Association for Computational Linguistics},
	title = {{BART}-{TL}: Weakly-Supervised Topic Label Generation},
	url = {https://aclanthology.org/2021.eacl-main.121},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.eacl-main.121},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.eacl-main.121}}

@article{mueller_2018_reading_between_the_lines_prediction_of_political_violence_using_newspaper_text,
	author = {MUELLER, HANNES and RAUH, CHRISTOPHER},
	date-added = {2022-11-17 16:43:06 +0100},
	date-modified = {2022-11-17 16:43:06 +0100},
	doi = {10.1017/S0003055417000570},
	journal = {American Political Science Review},
	number = {2},
	pages = {358--375},
	publisher = {Cambridge University Press},
	title = {Reading Between the Lines: Prediction of Political Violence Using Newspaper Text},
	volume = {112},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1017/S0003055417000570}}

@misc{maurya_2021_zmbart_an_unsupervised_cross_lingual_transfer_framework_for_language_generation,
	author = {Maurya, Kaushal Kumar and Desarkar, Maunendra Sankar and Kano, Yoshinobu and Deepshikha, Kumari},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-17 16:42:53 +0100},
	date-modified = {2022-11-17 16:42:53 +0100},
	doi = {10.48550/ARXIV.2106.01597},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation},
	url = {https://arxiv.org/abs/2106.01597},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2106.01597},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.01597}}

@misc{marjanen_2020_topic_modelling_discourse_dynamics_in_historical_newspapers,
	author = {Marjanen, Jani and Zosa, Elaine and Hengchen, Simon and Pivovarova, Lidia and Tolonen, Mikko},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	date-added = {2022-11-17 16:42:47 +0100},
	date-modified = {2022-11-17 16:42:47 +0100},
	doi = {10.48550/ARXIV.2011.10428},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic modelling discourse dynamics in historical newspapers},
	url = {https://arxiv.org/abs/2011.10428},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2011.10428},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2011.10428}}

@misc{liu_2020_multilingual_denoising_pre_training_for_neural_machine_translation,
	author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:42:42 +0100},
	date-modified = {2022-11-17 16:42:42 +0100},
	doi = {10.48550/ARXIV.2001.08210},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Multilingual Denoising Pre-training for Neural Machine Translation},
	url = {https://arxiv.org/abs/2001.08210},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2001.08210},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2001.08210}}

@inproceedings{li_2020_global_surveillance_of_covid_19_by_mining_news_media_using_a_multi_source_dynamic_embedded_topic_model,
	abstract = {As the COVID-19 pandemic continues to unfold, understanding the global impact of non-pharmacological interventions (NPI) is important for formulating effective intervention strategies, particularly as many countries prepare for future waves. We used a machine learning approach to distill latent topics related to NPI from large-scale international news media. We hypothesize that these topics are informative about the timing and nature of implemented NPI, dependent on the source of the information (e.g., local news versus official government announcements) and the target countries. Given a set of latent topics associated with NPI (e.g., self-quarantine, social distancing, online education, etc), we assume that countries and media sources have different prior distributions over these topics, which are sampled to generate the news articles. To model the source-specific topic priors, we developed a semi-supervised, multi-source, dynamic, embedded topic model. Our model is able to simultaneously infer latent topics and learn a linear classifier to predict NPI labels using the topic mixtures as input for each news article. To learn these models, we developed an efficient end-to-end amortized variational inference algorithm. We applied our models to news data collected and labelled by the World Health Organization (WHO) and the Global Public Health Intelligence Network (GPHIN). Through comprehensive experiments, we observed superior topic quality and intervention prediction accuracy, compared to the baseline embedded topic models, which ignore information on media source and intervention labels. The inferred latent topics reveal distinct policies and media framing in different countries and media sources, and also characterize reaction to COVID-19 and NPI in a semantically meaningful manner. Our PyTorch code is available on Github (htps://github.com/li-lab-mcgill/covid19_media).},
	address = {New York, NY, USA},
	articleno = {34},
	author = {Li, Yue and Nair, Pratheeksha and Wen, Zhi and Chafi, Imane and Okhmatovskaia, Anya and Powell, Guido and Shen, Yannan and Buckeridge, David},
	booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
	date-added = {2022-11-17 16:42:37 +0100},
	date-modified = {2022-11-17 16:42:37 +0100},
	doi = {li_2020_global_surveillance_of_covid_19_by_mining_news_media_using_a_multi_source_dynamic_embedded_topic_model},
	isbn = {9781450379649},
	keywords = {text mining, coronavirus, Topic models, media news, Bayesian inference},
	location = {Virtual Event, USA},
	numpages = {14},
	publisher = {Association for Computing Machinery},
	series = {BCB '20},
	title = {Global Surveillance of COVID-19 by Mining News Media Using a Multi-Source Dynamic Embedded Topic Model},
	url = {https://doi.org/10.1145/3388440.3412418},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3388440.3412418}}

@misc{lewis_2019_bart_denoising_sequence_to_sequence_pre_training_for_natural_language_generation_translation_and_comprehension,
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:42:31 +0100},
	date-modified = {2022-11-17 16:42:31 +0100},
	doi = {10.48550/ARXIV.1910.13461},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {https://arxiv.org/abs/1910.13461},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1910.13461},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1910.13461}}

@article{kim_2019_an_ontology_based_labeling_of_influential_topics_using_topic_network_analysis,
	author = {Hyon Hee Kim and Hey Young Rhee},
	date-added = {2022-11-17 16:42:22 +0100},
	date-modified = {2022-11-17 16:42:22 +0100},
	journal = {J. Inf. Process. Syst.},
	pages = {1096-1107},
	title = {An Ontology-Based Labeling of Influential Topics Using Topic Network Analysis},
	volume = {15},
	year = {2019}}

@inproceedings{kanerva_2018_turku_neural_parser_pipeline_an_end_to_end_system_for_the_conll_2018_shared_task,
	abstract = {In this paper we describe the TurkuNLP entry at the CoNLL 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies. Compared to the last year, this year the shared task includes two new main metrics to measure the morphological tagging and lemmatization accuracies in addition to syntactic trees. Basing our motivation into these new metrics, we developed an end-to-end parsing pipeline especially focusing on developing a novel and state-of-the-art component for lemmatization. Our system reached the highest aggregate ranking on three main metrics out of 26 teams by achieving 1st place on metric involving lemmatization, and 2nd on both morphological tagging and parsing.},
	address = {Brussels, Belgium},
	author = {Kanerva, Jenna and Ginter, Filip and Miekka, Niko and Leino, Akseli and Salakoski, Tapio},
	booktitle = {Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
	date-added = {2022-11-17 16:40:57 +0100},
	date-modified = {2022-11-17 16:40:57 +0100},
	doi = {10.18653/v1/K18-2013},
	month = oct,
	pages = {133--142},
	publisher = {Association for Computational Linguistics},
	title = {{T}urku Neural Parser Pipeline: An End-to-End System for the {C}o{NLL} 2018 Shared Task},
	url = {https://aclanthology.org/K18-2013},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/K18-2013},
	bdsk-url-2 = {https://doi.org/10.18653/v1/K18-2013}}

@article{alokaili_2020_automatic_generation_of_topic_labels,
	author = {Alokaili, Areej and Aletras, Nikolaos and Stevenson, Mark},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 16:40:53 +0100},
	date-modified = {2022-11-17 16:40:53 +0100},
	doi = {10.48550/ARXIV.2006.00127},
	keywords = {Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Automatic Generation of Topic Labels},
	url = {https://arxiv.org/abs/2006.00127},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2006.00127},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2006.00127}}
