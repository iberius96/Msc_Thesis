%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-18 15:48:13 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{yang_2018_a_topic_model_for_co_occurring_normal_documents_and_short_texts,
	author = {Yang, Yang and Wang, Feifei and Zhang, Junni and Xu, Jin and Yu, Philip S},
	date-added = {2022-11-18 15:48:11 +0100},
	date-modified = {2022-11-18 15:48:11 +0100},
	journal = {World Wide Web},
	number = {2},
	pages = {487--513},
	publisher = {Springer},
	title = {A topic model for co-occurring normal documents and short texts},
	volume = {21},
	year = {2018}}

@inproceedings{yin_2018_model_based_clustering_of_short_text_streams,
	author = {Jianhua Yin and Daren Chao and Zhongkun Liu and Wei Zhang and Xiaohui Yu and Jianyong Wang},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&}amp$\mathsemicolon$ Data Mining},
	date-added = {2022-11-18 15:48:03 +0100},
	date-modified = {2022-11-18 15:48:03 +0100},
	doi = {10.1145/3219819.3220094},
	month = {jul},
	publisher = {{ACM}},
	title = {Model-based Clustering of Short Text Streams},
	url = {https://doi.org/10.1145%2F3219819.3220094},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1145/3219819.3220094}}

@article{chen_2019_a_nonparametric_model_for_online_topic_discovery_with_word_embeddings,
	author = {Junyang Chen and Zhiguo Gong and Weiwen Liu},
	date-added = {2022-11-18 15:47:53 +0100},
	date-modified = {2022-11-18 15:47:53 +0100},
	doi = {10.1016/j.ins.2019.07.048},
	journal = {Information Sciences},
	month = {dec},
	pages = {32--47},
	publisher = {Elsevier {BV}},
	title = {A nonparametric model for online topic discovery with word embeddings},
	url = {https://doi.org/10.1016%2Fj.ins.2019.07.048},
	volume = {504},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1016/j.ins.2019.07.048}}

@article{chen_2020_a_dirichlet_process_biterm_based_mixture_model_for_short_text_stream_clustering,
	abstract = {Short text stream clustering has become an important problem for mining textual data in diverse social media platforms (e.g., Twitter). However, most of the existing clustering methods (e.g., LDA and PLSA) are developed based on the assumption of a static corpus of long texts, while little attention has been given to short text streams. Different from the long texts, the clustering of short texts is more challenging since their word co-occurrence pattern easily suffers from a sparsity problem. In this paper, we propose a Dirichlet process biterm-based mixture model (DP-BMM), which can deal with the topic drift problem and the sparsity problem in short text stream clustering. The major advantages of DP-BMM include (1) DP-BMM explicitly exploits the word-pairs constructed from each document to enhance the word co-occurrence pattern in short texts; (2) DP-BMM can deal with the topic drift problem of short text streams naturally. Moreover, we further propose an improved algorithm of DP-BMM with forgetting property called DP-BMM-FP, which can efficiently delete biterms of outdated documents by deleting clusters of outdated batches. To perform inference, we adopt an online Gibbs sampling method for parameter estimation. Our extensive experimental results on real-world datasets show that DP-BMM and DP-BMM-FP can achieve a better performance than the state-of-the-art methods in terms of NMI metrics.},
	author = {Chen, Junyang and Gong, Zhiguo and Liu, Weiwen},
	date = {2020/05/01},
	date-added = {2022-11-18 15:47:49 +0100},
	date-modified = {2022-11-18 15:47:49 +0100},
	doi = {10.1007/s10489-019-01606-1},
	id = {Chen2020},
	isbn = {1573-7497},
	journal = {Applied Intelligence},
	number = {5},
	pages = {1609--1619},
	title = {A Dirichlet process biterm-based mixture model for short text stream clustering},
	url = {https://doi.org/10.1007/s10489-019-01606-1},
	volume = {50},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s10489-019-01606-1}}

@article{he_2020_targeted_aspects_oriented_topic_modeling_for_short_texts,
	abstract = {Topic modeling has demonstrated its value in short text topic discovery. For this task, a common way adopted by many topic models is to perform a full analysis to find all the possible topics. However, these topic models overlook the importance of deeper topics, leading to confusing topics discovered. In practice, people always tend to find more focused topics on some special aspects (or events), rather than a set of coarse topics. Therefore, in this paper, we propose a novel method, Targeted Aspects Oriented Topic Modeling (TATM), to discover more focused topics on specific aspects in short texts. Specifically, each short text is assigned to only one targeted aspect derived from an enhanced Dirichlet Multinomial Mixture process (E-DMM). This process helps group similar words as many as possible, which achieves topic homogeneity. In addition, TATM discovers the topics for each targeted aspect from as many angles as possible by performing target-level modeling, which achieves topic completeness. Thus, TATM can make a balance between the two conflicting properties without employing any additional information or pre-trained knowledge. The extensive experiments conducted on five real-world datasets demonstrate that our proposed model can effectively discover more focused and complete topics, and it outperforms the state-of-the-art baselines.},
	author = {He, Jin and Li, Lei and Wang, Yan and Wu, Xindong},
	date = {2020/08/01},
	date-added = {2022-11-18 15:47:39 +0100},
	date-modified = {2022-11-18 15:47:39 +0100},
	doi = {10.1007/s10489-020-01672-w},
	id = {He2020},
	isbn = {1573-7497},
	journal = {Applied Intelligence},
	number = {8},
	pages = {2384--2399},
	title = {Targeted aspects oriented topic modeling for short texts},
	url = {https://doi.org/10.1007/s10489-020-01672-w},
	volume = {50},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s10489-020-01672-w}}

@article{kou_2019_a_multi_feature_probabilistic_graphical_model_for_social_network_semantic_search,
	author = {Feifei Kou and Junping Du and Congxian Yang and Yansong Shi and Meiyu Liang and Zhe Xue and Haisheng Li},
	date-added = {2022-11-18 15:47:33 +0100},
	date-modified = {2022-11-18 15:47:33 +0100},
	doi = {10.1016/j.neucom.2018.03.086},
	journal = {Neurocomputing},
	month = {apr},
	pages = {67--78},
	publisher = {Elsevier {BV}},
	title = {A multi-feature probabilistic graphical model for social network semantic search},
	url = {https://doi.org/10.1016%2Fj.neucom.2018.03.086},
	volume = {336},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1016/j.neucom.2018.03.086}}

@article{bicalho_2017_a_general_framework_to_expand_short_text_for_topic_modeling,
	author = {Paulo Bicalho and Marcelo Pita and Gabriel Pedrosa and Anisio Lacerda and Gisele L. Pappa},
	date-added = {2022-11-18 15:47:30 +0100},
	date-modified = {2022-11-18 15:47:30 +0100},
	doi = {10.1016/j.ins.2017.02.007},
	journal = {Information Sciences},
	month = {jul},
	pages = {66--81},
	publisher = {Elsevier {BV}},
	title = {A general framework to expand short text for topic modeling},
	url = {https://doi.org/10.1016%2Fj.ins.2017.02.007},
	volume = {393},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.1016/j.ins.2017.02.007}}

@article{li_2017_enhancing_topic_modeling_for_short_texts_with_auxiliary_word_embeddings,
	author = {Li, Chenliang and Duan, Yu and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
	date-added = {2022-11-18 15:47:25 +0100},
	date-modified = {2022-11-18 15:47:25 +0100},
	journal = {ACM Transactions on Information Systems (TOIS)},
	number = {2},
	pages = {1--30},
	publisher = {ACM New York, NY, USA},
	title = {Enhancing topic modeling for short texts with auxiliary word embeddings},
	volume = {36},
	year = {2017}}

@misc{qiang_2016_topic_modeling_over_short_texts_by_incorporating_word_embeddings,
	author = {Qiang, Jipeng and Chen, Ping and Wang, Tong and Wu, Xindong},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-18 15:47:12 +0100},
	date-modified = {2022-11-18 15:47:12 +0100},
	doi = {10.48550/ARXIV.1609.08496},
	keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic Modeling over Short Texts by Incorporating Word Embeddings},
	url = {https://arxiv.org/abs/1609.08496},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1609.08496}}

@misc{qiang_2016_a_local_global_lda_model_for_discovering_geographical_topics_from_social_media,
	author = {Qiang, Siwei and Wang, Yongkun and Jin, Yaohui},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-18 15:47:09 +0100},
	date-modified = {2022-11-18 15:47:09 +0100},
	doi = {10.48550/ARXIV.1607.05806},
	keywords = {Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {A Local-Global LDA Model for Discovering Geographical Topics from Social Media},
	url = {https://arxiv.org/abs/1607.05806},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1607.05806}}

@inproceedings{guo_2017_a_density_based_nonparametric_model_for_online_event_discovery_from_the_social_media_data,
	author = {Jinjin Guo and Zhiguo Gong},
	booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
	date-added = {2022-11-18 15:47:04 +0100},
	date-modified = {2022-11-18 15:47:04 +0100},
	doi = {10.24963/ijcai.2017/240},
	month = {aug},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {A Density-based Nonparametric Model for Online Event Discovery from the Social Media Data},
	url = {https://doi.org/10.24963%2Fijcai.2017%2F240},
	year = 2017,
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2017/240}}

@article{wang_2021_bayesian_text_classification_and_summarization_via_a_class_specified_topic_model,
	author = {Feifei Wang and Junni L. Zhang and Yichao Li and Ke Deng and Jun S. Liu},
	date-added = {2022-11-18 15:47:00 +0100},
	date-modified = {2022-11-18 15:47:00 +0100},
	journal = {Journal of Machine Learning Research},
	number = {89},
	pages = {1--48},
	title = {Bayesian Text Classification and Summarization via A Class-Specified Topic Model},
	url = {http://jmlr.org/papers/v22/18-332.html},
	volume = {22},
	year = {2021},
	bdsk-url-1 = {http://jmlr.org/papers/v22/18-332.html}}

@article{yang_2020_a_named_entity_topic_model_for_news_popularity_prediction,
	author = {Yang Yang and Yang Liu and Xiaoling Lu and Jin Xu and Feifei Wang},
	date-added = {2022-11-18 15:46:54 +0100},
	date-modified = {2022-11-18 15:46:54 +0100},
	doi = {10.1016/j.knosys.2020.106430},
	journal = {Knowledge-Based Systems},
	month = {nov},
	pages = {106430},
	publisher = {Elsevier {BV}},
	title = {A named entity topic model for news popularity prediction},
	url = {https://doi.org/10.1016%2Fj.knosys.2020.106430},
	volume = {208},
	year = 2020,
	bdsk-url-1 = {https://doi.org/10.1016/j.knosys.2020.106430}}

@article{blair_2020_aggregated_topic_models_for_increasing_social_media_topic_coherence,
	abstract = {This research presents a novel aggregating method for constructing an aggregated topic model that is composed of the topics with greater coherence than individual models. When generating a topic model, a number of parameters have to be specified. The resulting topics can be very general or very specific, which depend on the chosen parameters. In this study we investigate the process of aggregating multiple topic models generated using different parameters with a focus on whether combining the general and specific topics is able to increase topic coherence. We employ cosine similarity and Jensen-Shannon divergence to compute the similarity among topics and combine them into an aggregated model when their similarity scores exceed a predefined threshold. The model is evaluated against the standard topics models generated by the latent Dirichlet allocation and Non-negative Matrix Factorisation. Specifically we use the coherence of topics to compare the individual models that create aggregated models against those of the aggregated model and models generated by Non-negative Matrix Factorisation, respectively. The results demonstrate that the aggregated model outperforms those topic models at a statistically significant level in terms of topic coherence over an external corpus. We also make use of the aggregated topic model on social media data to validate the method in a realistic scenario and find that again it outperforms individual topic models.},
	author = {Blair, Stuart J. and Bi, Yaxin and Mulvenna, Maurice D.},
	date = {2020/01/01},
	date-added = {2022-11-18 15:46:50 +0100},
	date-modified = {2022-11-18 15:46:50 +0100},
	doi = {10.1007/s10489-019-01438-z},
	id = {Blair2020},
	isbn = {1573-7497},
	journal = {Applied Intelligence},
	number = {1},
	pages = {138--156},
	title = {Aggregated topic models for increasing social media topic coherence},
	url = {https://doi.org/10.1007/s10489-019-01438-z},
	volume = {50},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s10489-019-01438-z}}

@article{qiang_2022_short_text_topic_modeling_techniques_applications_and_performance_a_survey,
	author = {Qiang, Jipeng and Qian, Zhenyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
	date-added = {2022-11-18 15:46:41 +0100},
	date-modified = {2022-11-18 15:46:41 +0100},
	doi = {10.1109/TKDE.2020.2992485},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	number = {3},
	pages = {1427-1445},
	title = {Short Text Topic Modeling Techniques, Applications, and Performance: A Survey},
	volume = {34},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2020.2992485}}

@inproceedings{song_2018_directional_skip_gram_explicitly_distinguishing_left_and_right_context_for_word_embeddings,
	abstract = {In this paper, we present directional skip-gram (DSG), a simple but effective enhancement of the skip-gram model by explicitly distinguishing left and right context in word prediction. In doing so, a direction vector is introduced for each word, whose embedding is thus learned by not only word co-occurrence patterns in its context, but also the directions of its contextual words. Theoretical and empirical studies on complexity illustrate that our model can be trained as efficient as the original skip-gram model, when compared to other extensions of the skip-gram model. Experimental results show that our model outperforms others on different datasets in semantic (word similarity measurement) and syntactic (part-of-speech tagging) evaluations, respectively.},
	address = {New Orleans, Louisiana},
	author = {Song, Yan and Shi, Shuming and Li, Jing and Zhang, Haisong},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	date-added = {2022-11-18 15:46:35 +0100},
	date-modified = {2022-11-18 15:46:35 +0100},
	doi = {10.18653/v1/N18-2028},
	month = jun,
	pages = {175--180},
	publisher = {Association for Computational Linguistics},
	title = {Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings},
	url = {https://aclanthology.org/N18-2028},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-2028},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-2028}}

@incollection{li_2019_unsupervised_user_behavior_representation_for_fraud_review_detection_with_cold_start_problem,
	author = {Qian Li and Qiang Wu and Chengzhang Zhu and Jian Zhang and Wentao Zhao},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	date-added = {2022-11-18 15:46:30 +0100},
	date-modified = {2022-11-18 15:46:30 +0100},
	doi = {10.1007/978-3-030-16148-4_18},
	pages = {222--236},
	publisher = {Springer International Publishing},
	title = {Unsupervised User Behavior Representation for Fraud Review Detection with Cold-Start Problem},
	url = {https://doi.org/10.1007%2F978-3-030-16148-4_18},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-16148-4_18}}

@inproceedings{kumar_2018_identifying_singleton_spammers_via_spammer_group_detection,
	abstract = {Opinion spam is a well-recognized threat to the credibility of online reviews. Existing approaches to detecting spam reviews or spammers examine review content, reviewer behavior and reviewer-product network, and often operate on the assumption that spammers write at least several if not many fake reviews. On the other hand, spammers setup multiple sockpuppet IDs and write one-time, singleton spam reviews to avoid detection. It is reported that for most review sites, a large portion, sometimes over 90{\%}, of reviewers are singletons (identified by the reviewer ID). Singleton spammers are difficult to catch due to the scarcity of behavioral clues. In this paper, we argue that the key to detect singleton spammers (and their fake reviews) is to detect group spam attacks by inferring the hidden collusiveness among them. To address the challenge of lack of explicit behavioral signals for singleton reviewers, we propose to infer the hidden reviewer-product associations by completing the review-product matrix by leveraging the product and review metadata and text. Experiments on three real-life Yelp datasets established that our approach can effectively detect singleton spammers via group detection, which are often missed by existing approaches.},
	address = {Cham},
	author = {Kumar, Dheeraj and Shaalan, Yassien and Zhang, Xiuzhen and Chan, Jeffrey},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	date-added = {2022-11-18 15:46:12 +0100},
	date-modified = {2022-11-18 15:46:12 +0100},
	editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
	isbn = {978-3-319-93034-3},
	pages = {656--667},
	publisher = {Springer International Publishing},
	title = {Identifying Singleton Spammers via Spammer Group Detection},
	year = {2018}}

@inproceedings{wang_2018_learning_sequential_correlation_for_user_generated_textual_content_popularity_prediction,
	author = {Wen Wang and Wei Zhang and Jun Wang and Junchi Yan and Hongyuan Zha},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
	date-added = {2022-11-18 15:46:04 +0100},
	date-modified = {2022-11-18 15:46:04 +0100},
	doi = {10.24963/ijcai.2018/225},
	month = {7},
	pages = {1625--1631},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {Learning Sequential Correlation for User Generated Textual Content Popularity Prediction},
	url = {https://doi.org/10.24963/ijcai.2018/225},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2018/225}}
