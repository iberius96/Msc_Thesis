%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 17:12:27 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{montani_2022_explosionspacy_v343_extended_typer_support_and_bug_fixes,
	author = {Montani, Ines and Honnibal, Matthew and Honnibal, Matthew and Van Landeghem, Sofie and Boyd, Adriane and Peters, Henning and McCann, Paul O'Leary and Geovedi, Jim and O'Regan, Jim and Samsonov, Maxim and Altinok, Duygu and Orosz, Gy{\"o}rgy and De Kok, Dani{\"e}l and Kristiansen, S{\o}ren Lind and Bournhonesque, Rapha{\"e}l and {Madeesh Kannan} and {Lj Miranda} and Baumgartner, Peter and {, Edward} and {Explosion Bot} and Hudson, Richard and {, Roman} and Fiedler, Leander and Mitsch, Raphael and {Ryn Daniels} and Howard, Gr{\'e}gory and {Wannaphong Phatthiyaphaibun} and Tamura, Yohei and Bozek, Sam},
	copyright = {Open Access},
	date-added = {2022-11-17 17:12:24 +0100},
	date-modified = {2022-11-17 17:12:24 +0100},
	doi = {10.5281/ZENODO.7310816},
	publisher = {Zenodo},
	title = {explosion/spaCy: v3.4.3: Extended Typer support and bug fixes},
	url = {https://zenodo.org/record/7310816},
	year = {2022},
	bdsk-url-1 = {https://zenodo.org/record/7310816},
	bdsk-url-2 = {https://doi.org/10.5281/ZENODO.7310816}}

@inproceedings{zhou_2017_learning_phrase_embeddings_from_paraphrases_with_grus,
	abstract = {Learning phrase representations has been widely explored in many Natural Language Processing tasks (e.g., Sentiment Analysis, Machine Translation) and has shown promising improvements. Previous studies either learn non-compositional phrase representations with general word embedding learning techniques or learn compositional phrase representations based on syntactic structures, which either require huge amounts of human annotations or cannot be easily generalized to all phrases. In this work, we propose to take advantage of large-scaled paraphrase database and present a pairwise-GRU framework to generate compositional phrase representations. Our framework can be re-used to generate representations for any phrases. Experimental results show that our framework achieves state-of-the-art results on several phrase similarity tasks.},
	address = {Taipei, Taiwan},
	author = {Zhou, Zhihao and Huang, Lifu and Ji, Heng},
	booktitle = {Proceedings of the First Workshop on Curation and Applications of Parallel and Comparable Corpora},
	date-added = {2022-11-17 17:11:49 +0100},
	date-modified = {2022-11-17 17:11:49 +0100},
	month = nov,
	pages = {16--23},
	publisher = {Asian Federation of Natural Language Processing},
	title = {Learning Phrase Embeddings from Paraphrases with {GRU}s},
	url = {https://aclanthology.org/W17-5603},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/W17-5603}}

@misc{zhang_2019_paws_paraphrase_adversaries_from_word_scrambling,
	author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:11:45 +0100},
	date-modified = {2022-11-17 17:11:45 +0100},
	doi = {10.48550/ARXIV.1904.01130},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {PAWS: Paraphrase Adversaries from Word Scrambling},
	url = {https://arxiv.org/abs/1904.01130},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.01130},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.01130}}

@misc{yu_2020_assessing_phrasal_representation_and_composition_in_transformers,
	author = {Yu, Lang and Ettinger, Allyson},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:11:40 +0100},
	date-modified = {2022-11-17 17:11:40 +0100},
	doi = {10.48550/ARXIV.2010.03763},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Assessing Phrasal Representation and Composition in Transformers},
	url = {https://arxiv.org/abs/2010.03763},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2010.03763},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2010.03763}}

@misc{wolf_2019_transfertransfo_a_transfer_learning_approach_for_neural_network_based_conversational_agents,
	author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:11:35 +0100},
	date-modified = {2022-11-17 17:11:35 +0100},
	doi = {10.48550/ARXIV.1901.08149},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents},
	url = {https://arxiv.org/abs/1901.08149},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1901.08149},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1901.08149}}

@inproceedings{wieting_2018_paranmt_50m_pushing_the_limits_of_paraphrastic_sentence_embeddings_with_millions_of_machine_translations,
	abstract = {We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.},
	address = {Melbourne, Australia},
	author = {Wieting, John and Gimpel, Kevin},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-17 17:11:30 +0100},
	date-modified = {2022-11-17 17:11:30 +0100},
	doi = {10.18653/v1/P18-1042},
	month = jul,
	pages = {451--462},
	publisher = {Association for Computational Linguistics},
	title = {{P}ara{NMT}-50{M}: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations},
	url = {https://aclanthology.org/P18-1042},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1042},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1042}}

@article{vijayakumar_2018_diverse_beam_search_for_improved_description_of_complex_scenes,
	author = {Ashwin Vijayakumar and Michael Cogswell and Ramprasaath Selvaraju and Qing Sun and Stefan Lee and David Crandall and Dhruv Batra},
	date-added = {2022-11-17 17:11:26 +0100},
	date-modified = {2022-11-17 17:11:26 +0100},
	doi = {10.1609/aaai.v32i1.12340},
	journal = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	month = {apr},
	number = {1},
	publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
	title = {Diverse Beam Search for Improved Description of Complex Scenes},
	url = {https://doi.org/10.1609%2Faaai.v32i1.12340},
	volume = {32},
	year = 2018,
	bdsk-url-1 = {https://doi.org/10.1609%2Faaai.v32i1.12340},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v32i1.12340}}

@inproceedings{toshniwal_2020_a_cross_task_analysis_of_text_span_representations,
	abstract = {Many natural language processing (NLP) tasks involve reasoning with textual spans, including question answering, entity recognition, and coreference resolution. While extensive research has focused on functional architectures for representing words and sentences, there is less work on representing arbitrary spans of text within sentences. In this paper, we conduct a comprehensive empirical evaluation of six span representation methods using eight pretrained language representation models across six tasks, including two tasks that we introduce. We find that, although some simple span representations are fairly reliable across tasks, in general the optimal span representation varies by task, and can also vary within different facets of individual tasks. We also find that the choice of span representation has a bigger impact with a fixed pretrained encoder than with a fine-tuned encoder.},
	address = {Online},
	author = {Toshniwal, Shubham and Shi, Haoyue and Shi, Bowen and Gao, Lingyu and Livescu, Karen and Gimpel, Kevin},
	booktitle = {Proceedings of the 5th Workshop on Representation Learning for NLP},
	date-added = {2022-11-17 17:11:20 +0100},
	date-modified = {2022-11-17 17:11:20 +0100},
	doi = {10.18653/v1/2020.repl4nlp-1.20},
	month = jul,
	pages = {166--176},
	publisher = {Association for Computational Linguistics},
	title = {A Cross-Task Analysis of Text Span Representations},
	url = {https://aclanthology.org/2020.repl4nlp-1.20},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.repl4nlp-1.20},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.repl4nlp-1.20}}

@inproceedings{seo_2018_phrase_indexed_question_answering_a_new_challenge_for_scalable_document_comprehension,
	abstract = {We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by building a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap. The leaderboard is at: \url{nlp.cs.washington.edu/piqa}},
	address = {Brussels, Belgium},
	author = {Seo, Minjoon and Kwiatkowski, Tom and Parikh, Ankur and Farhadi, Ali and Hajishirzi, Hannaneh},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	date-added = {2022-11-17 17:11:16 +0100},
	date-modified = {2022-11-17 17:11:16 +0100},
	doi = {10.18653/v1/D18-1052},
	month = oct # {-} # nov,
	pages = {559--564},
	publisher = {Association for Computational Linguistics},
	title = {Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension},
	url = {https://aclanthology.org/D18-1052},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/D18-1052},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1052}}

@misc{reimers_2019_sentence_bert_sentence_embeddings_using_siamese_bert_networks,
	author = {Reimers, Nils and Gurevych, Iryna},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	date-added = {2022-11-17 17:11:12 +0100},
	date-modified = {2022-11-17 17:11:12 +0100},
	doi = {10.48550/ARXIV.1908.10084},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	url = {https://arxiv.org/abs/1908.10084},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1908.10084},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1908.10084}}

@inproceedings{radford_2019_language_models_are_unsupervised_multitask_learners,
	author = {Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
	date-added = {2022-11-17 17:11:05 +0100},
	date-modified = {2022-11-17 17:11:05 +0100},
	title = {Language Models are Unsupervised Multitask Learners},
	year = {2019}}

@inproceedings{merullo_2019_investigating_sports_commentator_bias_within_a_large_corpus_of_american_football_broadcasts,
	abstract = {Sports broadcasters inject drama into play-by-play commentary by building team and player narratives through subjective analyses and anecdotes. Prior studies based on small datasets and manual coding show that such theatrics evince commentator bias in sports broadcasts. To examine this phenomenon, we assemble FOOTBALL, which contains 1,455 broadcast transcripts from American football games across six decades that are automatically annotated with 250K player mentions and linked with racial metadata. We identify major confounding factors for researchers examining racial bias in FOOTBALL, and perform a computational analysis that supports conclusions from prior social science studies.},
	address = {Hong Kong, China},
	author = {Merullo, Jack and Yeh, Luke and Handler, Abram and Grissom II, Alvin and O{'}Connor, Brendan and Iyyer, Mohit},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-11-17 17:10:58 +0100},
	date-modified = {2022-11-17 17:10:58 +0100},
	doi = {10.18653/v1/D19-1666},
	month = nov,
	pages = {6355--6361},
	publisher = {Association for Computational Linguistics},
	title = {Investigating Sports Commentator Bias within a Large Corpus of {A}merican Football Broadcasts},
	url = {https://aclanthology.org/D19-1666},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1666},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1666}}

@misc{merity_2016_pointer_sentinel_mixture_models,
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:10:53 +0100},
	date-modified = {2022-11-17 17:10:53 +0100},
	doi = {10.48550/ARXIV.1609.07843},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Pointer Sentinel Mixture Models},
	url = {https://arxiv.org/abs/1609.07843},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1609.07843},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1609.07843}}

@misc{liu_2019_roberta_a_robustly_optimized_bert_pretraining_approach,
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:10:49 +0100},
	date-modified = {2022-11-17 17:10:49 +0100},
	doi = {10.48550/ARXIV.1907.11692},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	url = {https://arxiv.org/abs/1907.11692},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.11692},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.11692}}

@misc{li_2020_on_the_sentence_embeddings_from_pre_trained_language_models,
	author = {Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:10:44 +0100},
	date-modified = {2022-11-17 17:10:44 +0100},
	doi = {10.48550/ARXIV.2011.05864},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {On the Sentence Embeddings from Pre-trained Language Models},
	url = {https://arxiv.org/abs/2011.05864},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2011.05864},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2011.05864}}

@misc{lee_2020_learning_dense_representations_of_phrases_at_scale,
	author = {Lee, Jinhyuk and Sung, Mujeen and Kang, Jaewoo and Chen, Danqi},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:10:38 +0100},
	date-modified = {2022-11-17 17:10:38 +0100},
	doi = {10.48550/ARXIV.2012.12624},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Learning Dense Representations of Phrases at Scale},
	url = {https://arxiv.org/abs/2012.12624},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2012.12624},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2012.12624}}

@misc{krishna_2020_reformulating_unsupervised_style_transfer_as_paraphrase_generation,
	author = {Krishna, Kalpesh and Wieting, John and Iyyer, Mohit},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:10:30 +0100},
	date-modified = {2022-11-17 17:10:30 +0100},
	doi = {10.48550/ARXIV.2010.05700},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Reformulating Unsupervised Style Transfer as Paraphrase Generation},
	url = {https://arxiv.org/abs/2010.05700},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2010.05700},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2010.05700}}

@misc{joshi_2019_spanbert_improving_pre_training_by_representing_and_predicting_spans,
	author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:10:24 +0100},
	date-modified = {2022-11-17 17:10:24 +0100},
	doi = {10.48550/ARXIV.1907.10529},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {SpanBERT: Improving Pre-training by Representing and Predicting Spans},
	url = {https://arxiv.org/abs/1907.10529},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.10529},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.10529}}

@misc{holtzman_2019_the_curious_case_of_neural_text_degeneration,
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:09:57 +0100},
	date-modified = {2022-11-17 17:09:57 +0100},
	doi = {10.48550/ARXIV.1904.09751},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {The Curious Case of Neural Text Degeneration},
	url = {https://arxiv.org/abs/1904.09751},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.09751},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.09751}}

@misc{gao_2021_the_pile_an_800gb_dataset_of_diverse_text_for_language_modeling,
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-17 17:09:52 +0100},
	date-modified = {2022-11-17 17:09:52 +0100},
	doi = {10.48550/ARXIV.2101.00027},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
	url = {https://arxiv.org/abs/2101.00027},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2101.00027},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2101.00027}}

@misc{gala_2020_analyzing_gender_bias_within_narrative_tropes,
	author = {Gala, Dhruvil and Khursheed, Mohammad Omar and Lerner, Hannah and O'Connor, Brendan and Iyyer, Mohit},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:09:48 +0100},
	date-modified = {2022-11-17 17:09:48 +0100},
	doi = {10.48550/ARXIV.2011.00092},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Analyzing Gender Bias within Narrative Tropes},
	url = {https://arxiv.org/abs/2011.00092},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2011.00092},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2011.00092}}

@misc{dieng_2019_topic_modeling_in_embedding_spaces,
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:09:43 +0100},
	date-modified = {2022-11-17 17:09:43 +0100},
	doi = {10.48550/ARXIV.1907.04907},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Topic Modeling in Embedding Spaces},
	url = {https://arxiv.org/abs/1907.04907},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1907.04907},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1907.04907}}

@misc{devlin_2018_bert_pre_training_of_deep_bidirectional_transformers_for_language_understanding,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:09:37 +0100},
	date-modified = {2022-11-17 17:09:37 +0100},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://arxiv.org/abs/1810.04805},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1810.04805},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1810.04805}}

@inproceedings{conneau_2018_what_you_can_cram_into_a_single__vector_probing_sentence_embeddings_for_linguistic_properties,
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. {``}Downstream{''} tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	address = {Melbourne, Australia},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"\i}c and Baroni, Marco},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-17 17:09:32 +0100},
	date-modified = {2022-11-17 17:09:32 +0100},
	doi = {10.18653/v1/P18-1198},
	month = jul,
	pages = {2126--2136},
	publisher = {Association for Computational Linguistics},
	title = {What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties},
	url = {https://aclanthology.org/P18-1198},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1198},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1198}}

@inproceedings{bommasani_2020_interpreting_pretrained_contextualized_representations_via_reductions_to_static_embeddings,
	abstract = {Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings {---} while more diverse and mature than those available for their dynamic counterparts {---} are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.},
	address = {Online},
	author = {Bommasani, Rishi and Davis, Kelly and Cardie, Claire},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 17:09:28 +0100},
	date-modified = {2022-11-17 17:09:28 +0100},
	doi = {10.18653/v1/2020.acl-main.431},
	month = jul,
	pages = {4758--4781},
	publisher = {Association for Computational Linguistics},
	title = {{I}nterpreting {P}retrained {C}ontextualized {R}epresentations via {R}eductions to {S}tatic {E}mbeddings},
	url = {https://aclanthology.org/2020.acl-main.431},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.431},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.431}}

@inproceedings{asaadi_2019_big_bird_a_large_fine_grained_bigram_relatedness_dataset_for_examining_semantic_composition,
	abstract = {Bigrams (two-word sequences) hold a special place in semantic composition research since they are the smallest unit formed by composing words. A semantic relatedness dataset that includes bigrams will thus be useful in the development of automatic methods of semantic composition. However, existing relatedness datasets only include pairs of unigrams (single words). Further, existing datasets were created using rating scales and thus suffer from limitations such as in consistent annotations and scale region bias. In this paper, we describe how we created a large, fine-grained, bigram relatedness dataset (BiRD), using a comparative annotation technique called Best{--}Worst Scaling. Each of BiRD{'}s 3,345 English term pairs involves at least one bigram. We show that the relatedness scores obtained are highly reliable (split-half reliability r= 0.937). We analyze the data to obtain insights into bigram semantic relatedness. Finally, we present benchmark experiments on using the relatedness dataset as a testbed to evaluate simple unsupervised measures of semantic composition. BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed.},
	address = {Minneapolis, Minnesota},
	author = {Asaadi, Shima and Mohammad, Saif and Kiritchenko, Svetlana},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	date-added = {2022-11-17 17:09:24 +0100},
	date-modified = {2022-11-17 17:09:24 +0100},
	doi = {10.18653/v1/N19-1050},
	month = jun,
	pages = {505--516},
	publisher = {Association for Computational Linguistics},
	title = {Big {B}i{RD}: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition},
	url = {https://aclanthology.org/N19-1050},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/N19-1050},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1050}}

@misc{akoury_2020_storium_a_dataset_and_evaluation_platform_for_machine_in_the_loop_story_generation,
	author = {Akoury, Nader and Wang, Shufan and Whiting, Josh and Hood, Stephen and Peng, Nanyun and Iyyer, Mohit},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:09:20 +0100},
	date-modified = {2022-11-17 17:09:20 +0100},
	doi = {10.48550/ARXIV.2010.01717},
	keywords = {Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation},
	url = {https://arxiv.org/abs/2010.01717},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2010.01717},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2010.01717}}
