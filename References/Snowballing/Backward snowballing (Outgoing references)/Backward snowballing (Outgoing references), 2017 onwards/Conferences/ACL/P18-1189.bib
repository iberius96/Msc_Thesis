%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-16 18:46:31 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{article,
	author = {Boyd-Graber, Jordan and Hu, Yuening and Mimno, David},
	date-added = {2022-11-16 18:46:28 +0100},
	date-modified = {2022-11-16 18:46:28 +0100},
	doi = {10.1561/1500000030},
	journal = {Foundations and Trends{\textregistered} in Information Retrieval},
	month = {01},
	pages = {143-296},
	title = {Applications of Topic Models},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1561/1500000030}}

@article{Srivastava:2017aa,
	abstract = {Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.},
	author = {Akash Srivastava and Charles Sutton},
	date-added = {2022-11-16 18:45:19 +0100},
	date-modified = {2022-11-16 18:45:19 +0100},
	eprint = {1703.01488},
	month = {03},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/pdf/1703.01488.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1703.01488.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1703.01488}}

@inproceedings{lau-etal-2017-topically,
	abstract = {Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.},
	address = {Vancouver, Canada},
	author = {Lau, Jey Han and Baldwin, Timothy and Cohn, Trevor},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-16 18:45:04 +0100},
	date-modified = {2022-11-16 18:45:04 +0100},
	doi = {10.18653/v1/P17-1033},
	month = jul,
	pages = {355--365},
	publisher = {Association for Computational Linguistics},
	title = {Topically Driven Neural Language Model},
	url = {https://aclanthology.org/P17-1033},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/P17-1033},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1033}}

@article{He:2017aa,
	abstract = {Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval.},
	author = {Junxian He and Zhiting Hu and Taylor Berg-Kirkpatrick and Ying Huang and Eric P. Xing},
	date-added = {2022-11-16 18:44:46 +0100},
	date-modified = {2022-11-16 18:44:46 +0100},
	eprint = {1707.00206},
	month = {07},
	title = {Efficient Correlated Topic Modeling with Topic Embedding},
	url = {https://arxiv.org/pdf/1707.00206.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1707.00206.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1707.00206}}

@inproceedings{benton-dredze-2018-deep,
	abstract = {Dirichlet Multinomial Regression (DMR) and other supervised topic models can incorporate arbitrary document-level features to inform topic priors. However, their ability to model corpora are limited by the representation and selection of these features {--} a choice the topic modeler must make. Instead, we seek models that can learn the feature representations upon which to condition topic selection. We present deep Dirichlet Multinomial Regression (dDMR), a generative topic model that simultaneously learns document feature representations and topics. We evaluate dDMR on three datasets: New York Times articles with fine-grained tags, Amazon product reviews with product images, and Reddit posts with subreddit identity. dDMR learns representations that outperform DMR and LDA according to heldout perplexity and are more effective at downstream predictive tasks as the number of topics grows. Additionally, human subjects judge dDMR topics as being more representative of associated document features. Finally, we find that supervision leads to faster convergence as compared to an LDA baseline and that dDMR{'}s model fit is less sensitive to training parameters than DMR.},
	address = {New Orleans, Louisiana},
	author = {Benton, Adrian and Dredze, Mark},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	date-added = {2022-11-16 18:44:34 +0100},
	date-modified = {2022-11-16 18:44:34 +0100},
	doi = {10.18653/v1/N18-1034},
	month = jun,
	pages = {365--374},
	publisher = {Association for Computational Linguistics},
	title = {Deep {D}irichlet Multinomial Regression},
	url = {https://aclanthology.org/N18-1034},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/N18-1034},
	bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1034}}
