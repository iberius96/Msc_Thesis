%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-18 11:56:42 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{galke_2017_word_embeddings_for_practical_information_retrieval,
	author = {Galke, Lukas AND Saleh, Ahmed AND Scherp, Ansgar},
	booktitle = {INFORMATIK 2017},
	date-added = {2022-11-18 11:56:41 +0100},
	date-modified = {2022-11-18 11:56:41 +0100},
	doi = {10.18420/in2017_215},
	editor = {Eibl, Maximilian AND Gaedke, Martin},
	pages = {2155-2167},
	publisher = {Gesellschaft f{\"u}r Informatik, Bonn},
	title = {Word Embeddings for Practical Information Retrieval},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.18420/in2017_215}}

@inproceedings{zhang_2018_aggregating_neural_word_embeddings_for_document_representation,
	abstract = {Recent advances in natural language processing (NLP) have shown that semantically meaningful representations of words can be efficiently acquired by distributed models. In such a case, a text document can be viewed as a bag-of-word-embeddings (BoWE), and the remaining question is how to obtain a fixed-length vector representation of the document for efficient document process. Beyond those heuristic aggregation methods, recent work has shown that one can leverage the Fisher kernel (FK) framework to generate document representations based on BoWE in a principled way. In this work, words are embedded into a Euclidean space by latent semantic indexing (LSI), and a Gaussian Mixture Model (GMM) is employed as the generative model for nonlinear FK-based aggregation. In this work, we propose an alternate FK-based aggregation method for document representation based on neural word embeddings. As we know, neural embedding models have been proven significantly better performance in word representations than LSI, where semantic relations between neural word embeddings are typically measured by cosine similarity rather than Euclidean distance. Therefore, we introduce a mixture of Von Mises-Fisher distributions (moVMF) as the generative model of neural word embeddings, and derive a new FK-based aggregation method for document representation based on BoWE. We report document classification, clustering and retrieval experiments and demonstrate that our model can produce state-of-the-art performance as compared with existing baseline methods.},
	address = {Cham},
	author = {Zhang, Ruqing and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Cheng, Xueqi},
	booktitle = {Advances in Information Retrieval},
	date-added = {2022-11-18 11:56:03 +0100},
	date-modified = {2022-11-18 11:56:03 +0100},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	isbn = {978-3-319-76941-7},
	pages = {303--315},
	publisher = {Springer International Publishing},
	title = {Aggregating Neural Word Embeddings for Document Representation},
	year = {2018}}

@inproceedings{ramrakhiyani_2017_measuring_topic_coherence_through_optimal_word_buckets,
	abstract = {Measuring topic quality is essential for scoring the learned topics and their subsequent use in Information Retrieval and Text classification. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single coherent theme, in turn indicating high topic coherence. TBuckets uses word embeddings of topic words and employs singular value decomposition (SVD) and Integer Linear Programming based optimization to create coherent word buckets. TBuckets outperforms the state-of-the-art techniques when evaluated using 3 publicly available datasets and on another one proposed in this paper.},
	address = {Valencia, Spain},
	author = {Ramrakhiyani, Nitin and Pawar, Sachin and Hingmire, Swapnil and Palshikar, Girish},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
	date-added = {2022-11-18 11:55:39 +0100},
	date-modified = {2022-11-18 11:55:39 +0100},
	month = apr,
	pages = {437--442},
	publisher = {Association for Computational Linguistics},
	title = {Measuring Topic Coherence through Optimal Word Buckets},
	url = {https://aclanthology.org/E17-2070},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-2070}}

@article{belford_2018_stability_of_topic_modeling_via_matrix_factorization,
	abstract = {Topic models can provide us with an insight into the underlying latent structure of a large corpus of documents. A range of methods have been proposed in the literature, including probabilistic topic models and techniques based on matrix factorization. However, in both cases, standard implementations rely on stochastic elements in their initialization phase, which can potentially lead to different results being generated on the same corpus when using the same parameter values. This corresponds to the concept of ``instability'' which has previously been studied in the context of k-means clustering. In many applications of topic modeling, this problem of instability is not considered and topic models are treated as being definitive, even though the results may change considerably if the initialization process is altered. In this paper we demonstrate the inherent instability of popular topic modeling approaches, using a number of new measures to assess stability. To address this issue in the context of matrix factorization for topic modeling, we propose the use of ensemble learning strategies. Based on experiments performed on annotated text corpora, we show that a K-Fold ensemble strategy, combining both ensembles and structured initialization, can significantly reduce instability, while simultaneously yielding more accurate topic models.},
	author = {Mark Belford and Brian {Mac Namee} and Derek Greene},
	date-added = {2022-11-18 11:54:55 +0100},
	date-modified = {2022-11-18 11:54:55 +0100},
	doi = {https://doi.org/10.1016/j.eswa.2017.08.047},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Topic modeling, Topic stability, LDA, NMF},
	pages = {159-169},
	title = {Stability of topic modeling via matrix factorization},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417417305948},
	volume = {91},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0957417417305948},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2017.08.047}}
