%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 14:31:13 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{yamada_2017_learning_distributed_representations_of_texts_and_entities_from_knowledge_base,
	abstract = {We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.},
	author = {Ikuya Yamada and Hiroyuki Shindo and Hideaki Takeda and Yoshiyasu Takefuji},
	date-added = {2022-11-17 14:31:11 +0100},
	date-modified = {2022-11-17 14:31:11 +0100},
	eprint = {1705.02494},
	journal = {Transactions of the Association for Computational},
	pages = {5},
	title = {Learning Distributed Representations of Texts and Entities from Knowledge Base},
	url = {https://arxiv.org/pdf/1705.02494.pdf},
	volume = {Linguistics},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1705.02494.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1705.02494}}

@inproceedings{wang_2017_combining_knowledge_with_deep_convolutional_neural_networks_for_short_text_classification,
	abstract = {Text classification is a fundamental task in NLP applications. Most existing work relied on either explicit or implicit text representation to address this problem. While these techniques work well for sentences, they can not easily be applied to short text because of its shortness and sparsity. In this paper, we propose a framework based on convolutional neural networks that combines explicit and implicit representations of short text for classification. We first conceptualize a short text as a set of relevant concepts using a large taxonomy knowledge base. We then obtain the embedding of short text by coalescing the words and relevant concepts on top of pre-trained word vectors. We further incorporate character level features into our model to capture fine-grained subword information. Experimental results on five commonly used datasets show that our proposed method significantly outperforms state-of-the-art methods.},
	author = {Wang, Jin and Wang, Zhongyuan and Zhang, Dawei and Yan, Jun},
	booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
	date-added = {2022-11-17 14:31:00 +0100},
	date-modified = {2022-11-17 14:31:00 +0100},
	isbn = {9780999241103},
	location = {Melbourne, Australia},
	numpages = {7},
	pages = {2915--2921},
	publisher = {AAAI Press},
	series = {IJCAI'17},
	title = {Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification},
	year = {2017}}

@article{venkatesh_2018_on_evaluating_and_comparing_open_domain_dialog_systems,
	abstract = {Conversational agents are exploding in popularity. However, much work remains in the area of non goal-oriented conversations, despite significant growth in research interest over recent years. To advance the state of the art in conversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar university competition where sixteen selected university teams built conversational agents to deliver the best social conversational experience. Alexa Prize provided the academic community with the unique opportunity to perform research with a live system used by millions of users. The subjectivity associated with evaluating conversations is key element underlying the challenge of building non-goal oriented dialogue systems. In this paper, we propose a comprehensive evaluation strategy with multiple metrics designed to reduce subjectivity by selecting metrics which correlate well with human judgement. The proposed metrics provide granular analysis of the conversational agents, which is not captured in human ratings. We show that these metrics can be used as a reasonable proxy for human judgment. We provide a mechanism to unify the metrics for selecting the top performing agents, which has also been applied throughout the Alexa Prize competition. To our knowledge, to date it is the largest setting for evaluating agents with millions of conversations and hundreds of thousands of ratings from users. We believe that this work is a step towards an automatic evaluation process for conversational AIs.},
	author = {Anu Venkatesh and Chandra Khatri and Ashwin Ram and Fenfei Guo and Raefer Gabriel and Ashish Nagar and Rohit Prasad and Ming Cheng and Behnam Hedayatnia and Angeliki Metallinou and Rahul Goel and Shaohua Yang and Anirudh Raju},
	date-added = {2022-11-17 14:30:29 +0100},
	date-modified = {2022-11-17 14:30:29 +0100},
	eprint = {1801.03625},
	journal = {NIPS.Workshop.ConversationalAI 2017-12-08 http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html accessed 2018-01-01},
	month = {01},
	title = {On Evaluating and Comparing Open Domain Dialog Systems},
	url = {https://arxiv.org/pdf/1801.03625.pdf},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/pdf/1801.03625.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1801.03625}}

@article{ram_2018_conversational_ai_the_science_behind_the_alexa_prize,
	abstract = {Conversational agents are exploding in popularity. However, much work remains in the area of social conversation as well as free-form conversation over a broad range of domains and topics. To advance the state of the art in conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar university competition where sixteen selected university teams were challenged to build conversational agents, known as socialbots, to converse coherently and engagingly with humans on popular topics such as Sports, Politics, Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers the academic community a unique opportunity to perform research with a live system used by millions of users. The competition provided university teams with real user conversational data at scale, along with the user-provided ratings and feedback augmented with annotations by the Alexa team. This enabled teams to effectively iterate and make improvements throughout the competition while being evaluated in real-time through live user interactions. To build their socialbots, university teams combined state-of-the-art techniques with novel strategies in the areas of Natural Language Understanding, Context Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To support the efforts of participating teams, the Alexa Prize team made significant scientific and engineering investments to build and improve Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice User Experience, and tools for traffic management and scalability. This paper outlines the advances created by the university teams as well as the Alexa Prize team to achieve the common goal of solving the problem of Conversational AI.},
	author = {Ashwin Ram and Rohit Prasad and Chandra Khatri and Anu Venkatesh and Raefer Gabriel and Qing Liu and Jeff Nunn and Behnam Hedayatnia and Ming Cheng and Ashish Nagar and Eric King and Kate Bland and Amanda Wartick and Yi Pan and Han Song and Sk Jayadevan and Gene Hwang and Art Pettigrue},
	date-added = {2022-11-17 14:30:18 +0100},
	date-modified = {2022-11-17 14:30:18 +0100},
	eprint = {1801.03604},
	journal = {Alexa.Prize.Proceedings https://developer.amazon.com/alexaprize/proceedings accessed (2018)-01-01},
	month = {01},
	title = {Conversational AI: The Science Behind the Alexa Prize},
	url = {https://arxiv.org/pdf/1801.03604.pdf},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/pdf/1801.03604.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1801.03604}}

@article{krause_2017_edina_building_an_open_domain_socialbot_with_self_dialogues,
	abstract = {We present Edina, the University of Edinburgh's social bot for the Amazon Alexa Prize competition. Edina is a conversational agent whose responses utilize data harvested from Amazon Mechanical Turk (AMT) through an innovative new technique we call self-dialogues. These are conversations in which a single AMT Worker plays both participants in a dialogue. Such dialogues are surprisingly natural, efficient to collect and reflective of relevant and/or trending topics. These self-dialogues provide training data for a generative neural network as well as a basis for soft rules used by a matching score component. Each match of a soft rule against a user utterance is associated with a confidence score which we show is strongly indicative of reply quality, allowing this component to self-censor and be effectively integrated with other components. Edina's full architecture features a rule-based system backing off to a matching score, backing off to a generative neural network. Our hybrid data-driven methodology thus addresses both coverage limitations of a strictly rule-based approach and the lack of guarantees of a strictly machine-learning approach.},
	author = {Ben Krause and Marco Damonte and Mihai Dobre and Daniel Duma and Joachim Fainberg and Federico Fancellu and Emmanuel Kahembwe and Jianpeng Cheng and Bonnie Webber},
	date-added = {2022-11-17 14:30:10 +0100},
	date-modified = {2022-11-17 14:30:10 +0100},
	eprint = {1709.09816},
	month = {09},
	title = {Edina: Building an Open Domain Socialbot with Self-dialogues},
	url = {https://arxiv.org/pdf/1709.09816.pdf},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/pdf/1709.09816.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1709.09816}}

@article{khatri_2018_contextual_topic_modeling_for_dialog_systems,
	abstract = {Accurate prediction of conversation topics can be a valuable signal for creating coherent and engaging dialog systems. In this work, we focus on context-aware topic classification methods for identifying topics in free-form human-chatbot dialogs. We extend previous work on neural topic classification and unsupervised topic keyword detection by incorporating conversational context and dialog act features. On annotated data, we show that incorporating context and dialog acts leads to relative gains in topic classification accuracy by 35% and on unsupervised keyword detection recall by 11% for conversational interactions where topics frequently span multiple utterances. We show that topical metrics such as topical depth is highly correlated with dialog evaluation metrics such as coherence and engagement implying that conversational topic models can predict user satisfaction. Our work for detecting conversation topics and keywords can be used to guide chatbots towards coherent dialog.},
	author = {Chandra Khatri and Rahul Goel and Behnam Hedayatnia and Angeliki Metanillou and Anushree Venkatesh and Raefer Gabriel and Arindam Mandal},
	date-added = {2022-11-17 14:30:02 +0100},
	date-modified = {2022-11-17 14:30:02 +0100},
	eprint = {1810.08135},
	month = {10},
	title = {Contextual Topic Modeling For Dialog Systems},
	url = {https://arxiv.org/pdf/1810.08135.pdf},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/pdf/1810.08135.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1810.08135}}

@inproceedings{joty_2018_coherence_modeling_of_asynchronous_conversations_a_neural_entity_grid_approach,
	abstract = {We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.},
	address = {Melbourne, Australia},
	author = {Joty, Shafiq and Mohiuddin, Muhammad Tasnim and Tien Nguyen, Dat},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-17 14:29:26 +0100},
	date-modified = {2022-11-17 14:29:26 +0100},
	doi = {10.18653/v1/P18-1052},
	month = jul,
	pages = {558--568},
	publisher = {Association for Computational Linguistics},
	title = {Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach},
	url = {https://aclanthology.org/P18-1052},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1052},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1052}}

@article{guo_2018_topic_based_evaluation_for_conversational_bots,
	abstract = {Dialog evaluation is a challenging problem, especially for non task-oriented dialogs where conversational success is not well-defined. We propose to evaluate dialog quality using topic-based metrics that describe the ability of a conversational bot to sustain coherent and engaging conversations on a topic, and the diversity of topics that a bot can handle. To detect conversation topics per utterance, we adopt Deep Average Networks (DAN) and train a topic classifier on a variety of question and query data categorized into multiple topics. We propose a novel extension to DAN by adding a topic-word attention table that allows the system to jointly capture topic keywords in an utterance and perform topic classification. We compare our proposed topic based metrics with the ratings provided by users and show that our metrics both correlate with and complement human judgment. Our analysis is performed on tens of thousands of real human-bot dialogs from the Alexa Prize competition and highlights user expectations for conversational bots.},
	author = {Fenfei Guo and Angeliki Metallinou and Chandra Khatri and Anirudh Raju and Anu Venkatesh and Ashwin Ram},
	date-added = {2022-11-17 14:29:18 +0100},
	date-modified = {2022-11-17 14:29:18 +0100},
	eprint = {1801.03622},
	journal = {Nips.Workshop.ConversationalAI 2017-12-08},
	month = {01},
	title = {Topic-based Evaluation for Conversational Bots},
	url = {https://arxiv.org/pdf/1801.03622.pdf},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/pdf/1801.03622.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1801.03622}}

@article{conneau_2016_very_deep_convolutional_networks_for_text_classification,
	abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.},
	author = {Alexis Conneau and Holger Schwenk and Lo{\"\i}c Barrault and Yann Lecun},
	date-added = {2022-11-17 14:29:10 +0100},
	date-modified = {2022-11-17 14:29:10 +0100},
	eprint = {1606.01781},
	month = {06},
	title = {Very Deep Convolutional Networks for Text Classification},
	url = {https://arxiv.org/pdf/1606.01781.pdf},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/pdf/1606.01781.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1606.01781}}

@article{cer_2018_universal_sentence_encoder,
	abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
	author = {Daniel Cer and Yinfei Yang and Sheng-yi Kong and Nan Hua and Nicole Limtiaco and Rhomni St. John and Noah Constant and Mario Guajardo-Cespedes and Steve Yuan and Chris Tar and Yun-Hsuan Sung and Brian Strope and Ray Kurzweil},
	date-added = {2022-11-17 14:29:02 +0100},
	date-modified = {2022-11-17 14:29:02 +0100},
	eprint = {1803.11175},
	month = {03},
	title = {Universal Sentence Encoder},
	url = {https://arxiv.org/pdf/1803.11175.pdf},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/pdf/1803.11175.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1803.11175}}

@article{bojanowski_2016_enriching_word_vectors_with_subword_information,
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	author = {Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
	date-added = {2022-11-17 14:28:53 +0100},
	date-modified = {2022-11-17 14:28:53 +0100},
	eprint = {1607.04606},
	month = {07},
	title = {Enriching Word Vectors with Subword Information},
	url = {https://arxiv.org/pdf/1607.04606.pdf},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/pdf/1607.04606.pdf},
	bdsk-url-2 = {https://arxiv.org/abs/1607.04606}}
