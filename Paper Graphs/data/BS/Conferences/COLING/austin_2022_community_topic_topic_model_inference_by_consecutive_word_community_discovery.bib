%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 15:20:30 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{burkhardt_2019_decoupling_sparsity_and_smoothness_in_the_dirichlet_variational_autoencoder_topic_model,
	author = {Sophie Burkhardt and Stefan Kramer},
	date-added = {2022-11-17 15:20:26 +0100},
	date-modified = {2022-11-17 15:20:26 +0100},
	journal = {J. Mach. Learn. Res.},
	pages = {131:1-131:27},
	title = {Decoupling Sparsity and Smoothness in the Dirichlet Variational Autoencoder Topic Model},
	volume = {20},
	year = {2019}}

@misc{zhao_2017_metalda_a_topic_model_that_efficiently_incorporates_meta_information,
	author = {Zhao, He and Du, Lan and Buntine, Wray and Liu, Gang},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:19:51 +0100},
	date-modified = {2022-11-17 15:19:51 +0100},
	doi = {10.48550/ARXIV.1709.06365},
	keywords = {Computation and Language (cs.CL), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {MetaLDA: a Topic Model that Efficiently Incorporates Meta information},
	url = {https://arxiv.org/abs/1709.06365},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1709.06365},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1709.06365}}

@article{traag_2018_from_louvain_to_leiden_guaranteeing_well_connected_communities,
	author = {Traag, Vincent and Waltman, Ludo and van Eck, Nees Jan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:19:45 +0100},
	date-modified = {2022-11-17 15:19:45 +0100},
	doi = {10.48550/ARXIV.1810.08473},
	keywords = {Social and Information Networks (cs.SI), Physics and Society (physics.soc-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
	publisher = {arXiv},
	title = {From Louvain to Leiden: guaranteeing well-connected communities},
	url = {https://arxiv.org/abs/1810.08473},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1810.08473},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1810.08473}}

@misc{srivastava_2017_autoencoding_variational_inference_for_topic_models,
	author = {Srivastava, Akash and Sutton, Charles},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:19:38 +0100},
	date-modified = {2022-11-17 15:19:38 +0100},
	doi = {10.48550/ARXIV.1703.01488},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/abs/1703.01488},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.01488},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.01488}}

@article{sakr_2020_the_future_is_big_graphs_a_community_view_on_graph_processing_systems,
	author = {Sakr, Sherif and Bonifati, Angela and Voigt, Hannes and Iosup, Alexandru and Ammar, Khaled and Angles, Renzo and Aref, Walid and Arenas, Marcelo and Besta, Maciej and Boncz, Peter A. and Daudjee, Khuzaima and Della Valle, Emanuele and Dumbrava, Stefania and Hartig, Olaf and Haslhofer, Bernhard and Hegeman, Tim and Hidders, Jan and Hose, Katja and Iamnitchi, Adriana and Kalavri, Vasiliki and Kapp, Hugo and Martens, Wim and {\"O}zsu, M. Tamer and Peukert, Eric and Plantikow, Stefan and Ragab, Mohamed and Ripeanu, Matei R. and Salihoglu, Semih and Schulz, Christian and Selmer, Petra and Sequeda, Juan F. and Shinavier, Joshua and Sz{\'a}rnyas, G{\'a}bor and Tommasini, Riccardo and Tumeo, Antonino and Uta, Alexandru and Varbanescu, Ana Lucia and Wu, Hsiang-Yun and Yakovets, Nikolay and Yan, Da and Yoneki, Eiko},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	date-added = {2022-11-17 15:19:28 +0100},
	date-modified = {2022-11-17 15:19:28 +0100},
	doi = {10.48550/ARXIV.2012.06171},
	keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Databases (cs.DB), FOS: Computer and information sciences, FOS: Computer and information sciences, C.3; E.0; H.2; J.0},
	publisher = {arXiv},
	title = {The Future is Big Graphs! A Community View on Graph Processing Systems},
	url = {https://arxiv.org/abs/2012.06171},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2012.06171},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2012.06171}}

@misc{rao_2021_vec2gc____a_graph_based_clustering_method_for_text_representations,
	author = {Rao, Rajesh N and Chakraborty, Manojit},
	copyright = {Creative Commons Attribution 4.0 International},
	date-added = {2022-11-17 15:19:21 +0100},
	date-modified = {2022-11-17 15:19:21 +0100},
	doi = {10.48550/ARXIV.2104.09439},
	keywords = {Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Vec2GC -- A Graph Based Clustering Method for Text Representations},
	url = {https://arxiv.org/abs/2104.09439},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2104.09439},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2104.09439}}

@book{newman_2018_networks,
	author = {Newman, M.},
	date-added = {2022-11-17 15:19:15 +0100},
	date-modified = {2022-11-17 15:19:15 +0100},
	isbn = {9780192527493},
	publisher = {OUP Oxford},
	title = {Networks},
	url = {https://books.google.it/books?id=YdZjDwAAQBAJ},
	year = {2018},
	bdsk-url-1 = {https://books.google.it/books?id=YdZjDwAAQBAJ}}

@inproceedings{nan_2019_topic_modeling_with_wasserstein_autoencoders,
	abstract = {We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.},
	address = {Florence, Italy},
	author = {Nan, Feng and Ding, Ran and Nallapati, Ramesh and Xiang, Bing},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 15:18:53 +0100},
	date-modified = {2022-11-17 15:18:53 +0100},
	doi = {10.18653/v1/P19-1640},
	month = jul,
	pages = {6345--6381},
	publisher = {Association for Computational Linguistics},
	title = {Topic Modeling with {W}asserstein Autoencoders},
	url = {https://aclanthology.org/P19-1640},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/P19-1640},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1640}}

@misc{nalisnick_2016_stick_breaking_variational_autoencoders,
	author = {Nalisnick, Eric and Smyth, Padhraic},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:18:49 +0100},
	date-modified = {2022-11-17 15:18:49 +0100},
	doi = {10.48550/ARXIV.1605.06197},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Stick-Breaking Variational Autoencoders},
	url = {https://arxiv.org/abs/1605.06197},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1605.06197},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1605.06197}}

@misc{mcinnes_2018_umap_uniform_manifold_approximation_and_projection_for_dimension_reduction,
	author = {McInnes, Leland and Healy, John and Melville, James},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:18:44 +0100},
	date-modified = {2022-11-17 15:18:44 +0100},
	doi = {10.48550/ARXIV.1802.03426},
	keywords = {Machine Learning (stat.ML), Computational Geometry (cs.CG), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
	url = {https://arxiv.org/abs/1802.03426},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1802.03426},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1802.03426}}

@article{mantyla_2018_measuring_lda_topic_stability_from_clusters_of_replicated_runs,
	author = {M{\"a}ntyl{\"a}, Mika and Claes, Ma{\"e}lick and Farooq, Umar},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:18:38 +0100},
	date-modified = {2022-11-17 15:18:38 +0100},
	doi = {10.48550/ARXIV.1808.08098},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Measuring LDA Topic Stability from Clusters of Replicated Runs},
	url = {https://arxiv.org/abs/1808.08098},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1808.08098},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.08098}}

@inproceedings{krasnashchok_2018_improving_topic_quality_by_promoting_named_entities_in_topic_modeling,
	abstract = {News related content has been extensively studied in both topic modeling research and named entity recognition. However, expressive power of named entities and their potential for improving the quality of discovered topics has not received much attention. In this paper we use named entities as domain-specific terms for news-centric content and present a new weighting model for Latent Dirichlet Allocation. Our experimental results indicate that involving more named entities in topic descriptors positively influences the overall quality of topics, improving their interpretability, specificity and diversity.},
	address = {Melbourne, Australia},
	author = {Krasnashchok, Katsiaryna and Jouili, Salim},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	date-added = {2022-11-17 15:18:32 +0100},
	date-modified = {2022-11-17 15:18:32 +0100},
	doi = {10.18653/v1/P18-2040},
	month = jul,
	pages = {247--253},
	publisher = {Association for Computational Linguistics},
	title = {Improving Topic Quality by Promoting Named Entities in Topic Modeling},
	url = {https://aclanthology.org/P18-2040},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-2040},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2040}}

@article{kingma_2019_an_introduction_to_variational_autoencoders,
	author = {Kingma, Diederik P. and Welling, Max},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:18:28 +0100},
	date-modified = {2022-11-17 15:18:28 +0100},
	doi = {10.48550/ARXIV.1906.02691},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {An Introduction to Variational Autoencoders},
	url = {https://arxiv.org/abs/1906.02691},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1906.02691},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1906.02691}}

@article{isoaho_2019_topic_modeling_and_text_analysis_for_qualitative_policy_research,
	author = {Karoliina Isoaho and Daria Gritsenko and Eetu M{\"a}kel{\"a}},
	date-added = {2022-11-17 15:18:14 +0100},
	date-modified = {2022-11-17 15:18:14 +0100},
	doi = {10.1111/psj.12343},
	journal = {Policy Studies Journal},
	month = {jun},
	number = {1},
	pages = {300--324},
	publisher = {Wiley},
	title = {Topic Modeling and Text Analysis for Qualitative Policy Research},
	url = {https://doi.org/10.1111%2Fpsj.12343},
	volume = {49},
	year = 2019,
	bdsk-url-1 = {https://doi.org/10.1111%2Fpsj.12343},
	bdsk-url-2 = {https://doi.org/10.1111/psj.12343}}

@misc{hoyle_2021_is_automated_topic_model_evaluation_broken_the_incoherence_of_coherence,
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:18:09 +0100},
	date-modified = {2022-11-17 15:18:09 +0100},
	doi = {10.48550/ARXIV.2107.02173},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
	url = {https://arxiv.org/abs/2107.02173},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2107.02173},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2107.02173}}

@misc{dziri_2018_augmenting_neural_response_generation_with_context_aware_topical_attention,
	author = {Dziri, Nouha and Kamalloo, Ehsan and Mathewson, Kory W. and Zaiane, Osmar},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:18:04 +0100},
	date-modified = {2022-11-17 15:18:04 +0100},
	doi = {10.48550/ARXIV.1811.01063},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Augmenting Neural Response Generation with Context-Aware Topical Attention},
	url = {https://arxiv.org/abs/1811.01063},
	year = {2018},
	bdsk-url-1 = {https://arxiv.org/abs/1811.01063},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1811.01063}}

@inproceedings{doogan_2021_topic_model_or_topic_twaddle_re_evaluating_semantic_interpretability_measures,
	abstract = {When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models{'} generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.},
	address = {Online},
	author = {Doogan, Caitlin and Buntine, Wray},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	date-added = {2022-11-17 15:17:59 +0100},
	date-modified = {2022-11-17 15:17:59 +0100},
	doi = {10.18653/v1/2021.naacl-main.300},
	month = jun,
	pages = {3824--3848},
	publisher = {Association for Computational Linguistics},
	title = {Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures},
	url = {https://aclanthology.org/2021.naacl-main.300},
	year = {2021},
	bdsk-url-1 = {https://aclanthology.org/2021.naacl-main.300},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2021.naacl-main.300}}

@article{dieng_2020_topic_modeling_in_embedding_spaces,
	abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word{'}s embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.},
	address = {Cambridge, MA},
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	date-added = {2022-11-17 15:17:53 +0100},
	date-modified = {2022-11-17 15:17:53 +0100},
	doi = {10.1162/tacl_a_00325},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {439--453},
	publisher = {MIT Press},
	title = {Topic Modeling in Embedding Spaces},
	url = {https://aclanthology.org/2020.tacl-1.29},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.tacl-1.29},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00325}}

@misc{angelov_2020_top2vec_distributed_representations_of_topics,
	author = {Angelov, Dimo},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 15:17:47 +0100},
	date-modified = {2022-11-17 15:17:47 +0100},
	doi = {10.48550/ARXIV.2008.09470},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Top2Vec: Distributed Representations of Topics},
	url = {https://arxiv.org/abs/2008.09470},
	year = {2020},
	bdsk-url-1 = {https://arxiv.org/abs/2008.09470},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2008.09470}}
