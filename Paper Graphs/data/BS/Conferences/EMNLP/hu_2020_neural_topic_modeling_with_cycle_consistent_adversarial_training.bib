%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Samuele Ceol at 2022-11-17 17:04:57 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{arjovsky_2017_wasserstein_generative_adversarial_networks,
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	author = {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2022-11-17 17:04:49 +0100},
	date-modified = {2022-11-17 17:04:49 +0100},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = {06--11 Aug},
	pages = {214--223},
	pdf = {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{W}asserstein Generative Adversarial Networks},
	url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
	volume = {70},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v70/arjovsky17a.html}}

@inproceedings{card_2018_neural_models_for_documents_with_metadata,
	abstract = {Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.},
	address = {Melbourne, Australia},
	author = {Card, Dallas and Tan, Chenhao and Smith, Noah A.},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	date-added = {2022-11-17 17:04:45 +0100},
	date-modified = {2022-11-17 17:04:45 +0100},
	doi = {10.18653/v1/P18-1189},
	month = jul,
	pages = {2031--2040},
	publisher = {Association for Computational Linguistics},
	title = {Neural Models for Documents with Metadata},
	url = {https://aclanthology.org/P18-1189},
	year = {2018},
	bdsk-url-1 = {https://aclanthology.org/P18-1189},
	bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1189}}

@misc{gulrajani_2017_improved_training_of_wasserstein_gans,
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:04:42 +0100},
	date-modified = {2022-11-17 17:04:42 +0100},
	doi = {10.48550/ARXIV.1704.00028},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Improved Training of Wasserstein GANs},
	url = {https://arxiv.org/abs/1704.00028},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1704.00028},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1704.00028}}

@inproceedings{kim_2017_learning_to_discover_cross_domain_relations_with_generative_adversarial_networks,
	abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
	author = {Taeksoo Kim and Moonsu Cha and Hyunsoo Kim and Jung Kwon Lee and Jiwon Kim},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2022-11-17 17:04:30 +0100},
	date-modified = {2022-11-17 17:04:30 +0100},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = {06--11 Aug},
	pages = {1857--1865},
	pdf = {http://proceedings.mlr.press/v70/kim17a/kim17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Learning to Discover Cross-Domain Relations with Generative Adversarial Networks},
	url = {https://proceedings.mlr.press/v70/kim17a.html},
	volume = {70},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v70/kim17a.html}}

@inproceedings{lee_2018_scalable_sentiment_for_sequence_to_sequence_chatbot_response_with_performance_analysis,
	author = {Lee, Chih-Wei and Wang, Yau-Shian and Hsu, Tsung-Yuan and Chen, Kuan-Yu and Lee, Hung-Yi and Lee, Lin-Shan},
	booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	date-added = {2022-11-17 17:04:26 +0100},
	date-modified = {2022-11-17 17:04:26 +0100},
	doi = {10.1109/ICASSP.2018.8461377},
	pages = {6164-6168},
	title = {Scalable Sentiment for Sequence-to-Sequence Chatbot Response with Performance Analysis},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/ICASSP.2018.8461377}}

@inproceedings{miao_2017_discovering_discrete_latent_topics_with_neural_variational_inference,
	abstract = {Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.},
	author = {Yishu Miao and Edward Grefenstette and Phil Blunsom},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2022-11-17 17:04:20 +0100},
	date-modified = {2022-11-17 17:04:20 +0100},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = {06--11 Aug},
	pages = {2410--2419},
	pdf = {http://proceedings.mlr.press/v70/miao17a/miao17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Discovering Discrete Latent Topics with Neural Variational Inference},
	url = {https://proceedings.mlr.press/v70/miao17a.html},
	volume = {70},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v70/miao17a.html}}

@misc{srivastava_2017_autoencoding_variational_inference_for_topic_models,
	author = {Srivastava, Akash and Sutton, Charles},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-added = {2022-11-17 17:04:14 +0100},
	date-modified = {2022-11-17 17:04:14 +0100},
	doi = {10.48550/ARXIV.1703.01488},
	keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Autoencoding Variational Inference For Topic Models},
	url = {https://arxiv.org/abs/1703.01488},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1703.01488},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1703.01488}}

@inproceedings{wang_2020_neural_topic_modeling_with_bidirectional_adversarial_training,
	abstract = {Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6{\%} is observed in accuracy.},
	address = {Online},
	author = {Wang, Rui and Hu, Xuemeng and Zhou, Deyu and He, Yulan and Xiong, Yuxuan and Ye, Chenchen and Xu, Haiyang},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	date-added = {2022-11-17 17:04:09 +0100},
	date-modified = {2022-11-17 17:04:09 +0100},
	doi = {10.18653/v1/2020.acl-main.32},
	month = jul,
	pages = {340--350},
	publisher = {Association for Computational Linguistics},
	title = {Neural Topic Modeling with Bidirectional Adversarial Training},
	url = {https://aclanthology.org/2020.acl-main.32},
	year = {2020},
	bdsk-url-1 = {https://aclanthology.org/2020.acl-main.32},
	bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.32}}

@article{wang_2019_atm_adversarial_neural_topic_model,
	abstract = {Topic models are widely used for thematic structure discovery in text. But traditional topic models often require dedicated inference procedures for specific tasks at hand. Also, they are not designed to generate word-level semantic representations. To address the limitations, we propose a neural topic modeling approach based on the Generative Adversarial Nets (GANs), called Adversarial-neural Topic Model (ATM) in this paper. To our best knowledge, this work is the first attempt to use adversarial training for topic modeling. The proposed ATM models topics with dirichlet prior and employs a generator network to capture the semantic patterns among latent topics. Meanwhile, the generator could also produce word-level semantic representations. Besides, to illustrate the feasibility of porting ATM to tasks other than topic modeling, we apply ATM for open domain event extraction. To validate the effectiveness of the proposed ATM, two topic modeling benchmark corpora and an event dataset are employed in the experiments. Our experimental results on benchmark corpora show that ATM generates more coherence topics (considering five topic coherence measures), outperforming a number of competitive baselines. Moreover, the experiments on event dataset also validate that the proposed approach is able to extract meaningful events from news articles.},
	author = {Rui Wang and Deyu Zhou and Yulan He},
	date-added = {2022-11-17 17:04:06 +0100},
	date-modified = {2022-11-17 17:04:06 +0100},
	doi = {https://doi.org/10.1016/j.ipm.2019.102098},
	issn = {0306-4573},
	journal = {Information Processing & Management},
	keywords = {Generative adversarial net, Neural-based topic model, Open domain event extraction, Topic modeling},
	number = {6},
	pages = {102098},
	title = {ATM: Adversarial-neural Topic Model},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457319300500},
	volume = {56},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0306457319300500},
	bdsk-url-2 = {https://doi.org/10.1016/j.ipm.2019.102098}}

@inproceedings{wang_2019_open_event_extraction_from_online_text_using_a_generative_adversarial_network,
	abstract = {To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15{\%} is observed in F-measure.},
	address = {Hong Kong, China},
	author = {Wang, Rui and Zhou, Deyu and He, Yulan},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	date-added = {2022-11-17 17:04:00 +0100},
	date-modified = {2022-11-17 17:04:00 +0100},
	doi = {10.18653/v1/D19-1027},
	month = nov,
	pages = {282--291},
	publisher = {Association for Computational Linguistics},
	title = {Open Event Extraction from Online Text using a Generative Adversarial Network},
	url = {https://aclanthology.org/D19-1027},
	year = {2019},
	bdsk-url-1 = {https://aclanthology.org/D19-1027},
	bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1027}}

@inproceedings{zhou_2017_event_extraction_from_twitter_using_non_parametric_bayesian_mixture_model_with_word_embeddings,
	abstract = {To extract structured representations of newsworthy events from Twitter, unsupervised models typically assume that tweets involving the same named entities and expressed using similar words are likely to belong to the same event. Hence, they group tweets into clusters based on the co-occurrence patterns of named entities and topical keywords. However, there are two main limitations. First, they require the number of events to be known beforehand, which is not realistic in practical applications. Second, they don{'}t recognise that the same named entity might be referred to by multiple mentions and tweets using different mentions would be wrongly assigned to different events. To overcome these limitations, we propose a non-parametric Bayesian mixture model with word embeddings for event extraction, in which the number of events can be inferred automatically and the issue of lexical variations for the same named entity can be dealt with properly. Our model has been evaluated on three datasets with sizes ranging between 2,499 and over 60 million tweets. Experimental results show that our model outperforms the baseline approach on all datasets by 5-8{\%} in F-measure.},
	address = {Valencia, Spain},
	author = {Zhou, Deyu and Zhang, Xuan and He, Yulan},
	booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
	date-added = {2022-11-17 17:03:56 +0100},
	date-modified = {2022-11-17 17:03:56 +0100},
	month = apr,
	pages = {808--817},
	publisher = {Association for Computational Linguistics},
	title = {Event extraction from {T}witter using Non-Parametric {B}ayesian Mixture Model with Word Embeddings},
	url = {https://aclanthology.org/E17-1076},
	year = {2017},
	bdsk-url-1 = {https://aclanthology.org/E17-1076}}

@inproceedings{zhu_2017_unpaired_image_to_image_translation_using_cycle_consistent_adversarial_networks,
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
	date-added = {2022-11-17 17:03:52 +0100},
	date-modified = {2022-11-17 17:03:52 +0100},
	doi = {10.1109/ICCV.2017.244},
	pages = {2242-2251},
	title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/ICCV.2017.244}}
