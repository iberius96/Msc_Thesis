{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import biblib.bib\n",
    "import re\n",
    "import graphviz\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout, to_agraph\n",
    "import pygraphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from semanticscholar import SemanticScholar\n",
    "from unidecode import unidecode\n",
    "from alphabet_detector import AlphabetDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cite_key(authors, year, title):\n",
    "    invalid = '<>:\"/\\|?*{}()\\''\n",
    "\n",
    "    authors = re.sub(r'\\\\. ', '', authors)\n",
    "    authors = re.sub(r'\\\\.', '', authors)\n",
    "    authors = authors.replace('.', '')\n",
    "    authors = unidecode(authors)\n",
    "\n",
    "    authors = authors.replace('-', ' ')\n",
    "    authors = authors.replace('–', ' ')\n",
    "    authors = authors.replace('‐', ' ')\n",
    "    authors = authors.replace('’', ' ')\n",
    "    authors = authors.replace(' *', '')\n",
    "    for char in invalid: authors = authors.replace(char, '')\n",
    "\n",
    "    title_invalid = '!.,$&#`+;’‐–‘^'\n",
    "    title_proc = title.lower().replace(' ', '_')\n",
    "    title_proc = title_proc.replace('~','_')\n",
    "    title_proc = title_proc.replace('ı', 'i')\n",
    "    title_proc = title_proc.replace('ó', 'o')\n",
    "    title_proc = title_proc.replace('é', 'e')\n",
    "    for char in invalid: title_proc = title_proc.replace(char, '')\n",
    "    for char in title_invalid: title_proc = title_proc.replace(char, '')\n",
    "    title_proc = title_proc.replace('&amp;', '')\n",
    "    title_proc = title_proc.replace('amp;', '')\n",
    "\n",
    "    def_authors_regex = re.compile(r'^[a-z]+( [a-z]+)*,')\n",
    "    def_authors_match = re.match(def_authors_regex, authors.lower())\n",
    "\n",
    "    if(('and' not in authors) and (',' not in authors)): # SINGLE AUTHOR\n",
    "        author_proc = authors.split().pop(-1).lower()\n",
    "        cite_key = f\"{author_proc}_{year}_{title_proc}\"\n",
    "    elif(bool(def_authors_match)): # SURNAME(S), NAME(S)\n",
    "        author_proc = def_authors_match[0].replace(',','').replace(' ', '_')\n",
    "        cite_key = f\"{author_proc}_{year}_{title_proc}\"\n",
    "    else: # NAME(S) SURNAME(S) AND ...\n",
    "        ns_authors_regex = re.compile(r'[a-z]+ \\band\\b')\n",
    "        author_proc = ns_authors_regex.search(authors.lower()).group().replace(' and', '')\n",
    "        cite_key = f\"{author_proc}_{year}_{title_proc}\"\n",
    "    \n",
    "    cite_key = cite_key.replace('-', '_')\n",
    "    for char in invalid: cite_key = cite_key.replace(char, '')\n",
    "    \n",
    "    return cite_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = {\n",
    "    'Conferences': ['ACL','CIKM','COLING','EACL','ECIR','ECML PKDD','EMNLP','KDD','NAACL','SIGIR'],\n",
    "    'Journals': ['Decision Support Systems', 'Expert Systems with Applications', 'Information Sciences', 'Journal of Infometrics', 'Knowledge-Based Systems', 'Pattern Recognition']\n",
    "}\n",
    "\n",
    "init_selection = [\n",
    "    'card_2018_neural_models_for_documents_with_metadata',\n",
    "    'lau_2017_topically_driven_neural_language_model',\n",
    "    'wu_2020_neural_mixed_counting_models_for_dispersed_topic_discovery',\n",
    "    'maiti_2019_spatial_aggregation_facilitates_discovery_of_spatial_topics',\n",
    "    'huang_2018_phrasectm_correlated_topic_modeling_on_phrases_within_markov_random_fields',\n",
    "    'ahmadvand_2019_concet_entity_aware_topic_classification_for_open_domain_conversational_agents',\n",
    "    'hosseiny_marani_2022_one_rating_to_rule_them_all_evidence_of_multidimensionality_in_human_assessment_of_topic_labeling_quality',\n",
    "    'nouri_2020_mining_crowdsourcing_problems_from_discussion_forums_of_workers',\n",
    "    'yin_2022_improving_deep_embedded_clustering_via_learning_cluster_level_representations',\n",
    "    'an_2018_model_free_context_aware_word_composition',\n",
    "    'austin_2022_community_topic_topic_model_inference_by_consecutive_word_community_discovery',\n",
    "    'sorodoc_2017_multimodal_topic_labelling',\n",
    "    'zhao_2021_adversarial_learning_of_poisson_factorisation_model_for_gauging_brand_sentiment_in_user_reviews',\n",
    "    'popa_2021_bart_tl_weakly_supervised_topic_label_generation',\n",
    "    'aletras_2017_labeling_topics_with_images_using_a_neural_network',\n",
    "    'zosa_2022_multilingual_topic_labelling_of_news_topics_using_ontological_mapping',\n",
    "    'ferner_2020_a_semi_discriminative_approach_for_sub_sentence_level_topic_classification_on_a_small_dataset',\n",
    "    'barbieri_2017_survival_factorization_on_diffusion_networks',\n",
    "    'hu_2020_neural_topic_modeling_with_cycle_consistent_adversarial_training',\n",
    "    'yang_2017_adapting_topic_models_using_lexical_associations_with_tree_priors',\n",
    "    'wang_2021_phrase_bert_improved_phrase_embeddings_from_bert_with_an_application_to_corpus_exploration',\n",
    "    'zhou_2020_condolence_and_empathy_in_online_communities',\n",
    "    'zhang_2018_taxogen_unsupervised_topic_taxonomy_construction_by_adaptive_term_embedding_and_clustering',\n",
    "    'huang_2020_corel_seed_guided_topical_taxonomy_construction_by_concept_learning_and_relation_transferring',\n",
    "    'meng_2020_hierarchical_topic_mining_via_joint_spherical_tree_and_text_embedding',\n",
    "    'song_2022_automatic_phenotyping_by_a_seed_guided_topic_model',\n",
    "    'pergola_2021_a_disentangled_adversarial_neural_topic_model_for_separating_opinions_from_plots_in_user_reviews',\n",
    "    'doogan_2021_topic_model_or_topic_twaddle_re_evaluating_semantic_interpretability_measures',\n",
    "    'mukherjee_2020_read_what_you_need_controllable_aspect_based_opinion_summarization_of_tourist_reviews',\n",
    "    'chin_2017_totem_personal_tweets_summarization_on_mobile_devices',\n",
    "    'alokaili_2020_automatic_generation_of_topic_labels',\n",
    "    'xu_2020_how_do_consumers_in_the_sharing_economy_value_sharing_evidence_from_online_reviews',\n",
    "    'ibrahim_2019_a_text_analytics_approach_for_online_retailing_service_improvement_evidence_from_twitter',\n",
    "    'goldberg_2022_sourcing_product_innovation_intelligence_from_online_reviews',\n",
    "    'roeder_2022_data_driven_decision_making_in_credit_risk_management_the_information_value_of_analyst_reports',\n",
    "    'altinel_2022_social_media_analysis_by_innovative_hybrid_algorithms_with_label_propagation',\n",
    "    'sakshi_2023_recent_trends_in_mathematical_expressions_recognition_an_lda_based_analysis',\n",
    "    'gregoriades_2021_supporting_digital_content_marketing_and_messaging_through_topic_modelling_and_decision_trees',\n",
    "    'pablos_2018_w2vlda_almost_unsupervised_system_for_aspect_based_sentiment_analysis',\n",
    "    'korfiatis_2019_measuring_service_quality_from_unstructured_data_a_topic_modeling_application_on_airline_passengers_online_reviews',\n",
    "    'gomez_2022_large_scale_analysis_of_open_mooc_reviews_to_support_learners_course_selection',\n",
    "    'fang_2021_criteria_determination_of_analytic_hierarchy_process_using_a_topic_model',\n",
    "    'lebena_2022_preliminary_exploration_of_topic_modelling_representations_for_electronic_health_records_coding_according_to_the_international_classification_of_diseases_in_spanish',\n",
    "    'effrosynidis_2022_the_climate_change_twitter_dataset',\n",
    "    'korencic_2018_document_based_topic_coherence_measures_for_news_media_text',\n",
    "    'bastani_2019_latent_dirichlet_allocation_lda_for_topic_modeling_of_the_cfpb_consumer_complaints',\n",
    "    'wahid_2022_topic2labels_a_framework_to_annotate_and_classify_the_social_media_data_through_lda_topics_and_deep_learning_models_for_crisis_response',\n",
    "    'kim_2020_word2vec_based_latent_semantic_analysis_w2v_lsa_for_topic_modeling_a_study_on_blockchain_technology_trend_analysis',\n",
    "    'campos_2022_providing_recommendations_for_communities_of_learners_in_moocs_ecosystems',\n",
    "    'zhou_2022_a_weakly_supervised_graph_based_joint_sentiment_topic_model_for_multi_topic_sentiment_analysis',\n",
    "    'yang_2021_author_topic_model_for_co_occurring_normal_documents_and_short_texts_to_explore_individual_user_preferences',\n",
    "    'liu_2018_identifying_impact_of_intrinsic_factors_on_topic_preferences_in_online_social_media_a_nonparametric_hierarchical_bayesian_approach',\n",
    "    'zhang_2018_does_deep_learning_help_topic_extraction_a_kernel_k_means_clustering_method_with_word_embedding',\n",
    "    'kim_2022_developing_a_topic_driven_method_for_interdisciplinarity_analysis',\n",
    "    'ebadi_2020_application_of_machine_learning_techniques_to_assess_the_trends_and_alignment_of_the_funded_research_output',\n",
    "    'kim_2022_exploring_scientific_trajectories_of_a_large_scale_dataset_using_topic_integrated_path_extraction',\n",
    "    'cassi_2017_improving_fitness_mapping_research_priorities_against_societal_needs_on_obesity',\n",
    "    'xu_2020_topic_linked_innovation_paths_in_science_and_technology',\n",
    "    'amon_2022_is_it_all_bafflegab____linguistic_and_meta_characteristics_of_research_articles_in_prestigious_economics_journals',\n",
    "    'chen_2020_a_topic_sensitive_trust_evaluation_approach_for_users_in_online_communities',\n",
    "    'alp_2019_influence_factorization_for_identifying_authorities_in_twitter',\n",
    "    'alp_2018_identifying_topical_influencers_on_twitter_based_on_user_behavior_and_network_topology',\n",
    "    'zhang_2017_detecting_and_predicting_the_topic_change_of_knowledge_based_systems_a_topic_based_bibliometric_analysis_from_1991_to_2016',\n",
    "    'chen_2019_experimental_explorations_on_short_text_topic_mining_between_lda_and_nmf_based_schemes',\n",
    "    'zhang_2019_learning_document_representation_via_topic_enhanced_lstm_model'\n",
    "]\n",
    "\n",
    "bs_selection = [\n",
    "    'xiang_2017_a_comparative_analysis_of_major_online_review_platforms_implications_for_social_media_analytics_in_hospitality_and_tourism',\n",
    "    'liu_2017_an_investigation_of_brand_related_user_generated_content_on_twitter',\n",
    "    'kim_2019_an_ontology_based_labeling_of_influential_topics_using_topic_network_analysis',\n",
    "    'huang_2018_analyst_information_discovery_and_interpretation_roles_a_topic_modeling_approach',\n",
    "    'maier_2018_applying_lda_topic_modeling_in_communication_research_toward_a_valid_and_reliable_methodology',\n",
    "    'karami_2018_characterizing_diabetes_diet_exercise_and_obesity_comments_on_twitter',\n",
    "    'aletras_2017_evaluating_topic_representations_for_exploring_document_collections',\n",
    "    'smith_2017_evaluating_visual_representations_for_topic_understanding_and_their_effects_on_manually_generated_topic_labels',\n",
    "    'savin_2021_free_associations_of_citizens_and_scientists_with_economic_and_green_growth_a_computational_linguistics_analysis',\n",
    "    'syed_2017_full_text_or_abstract_examining_topic_coherence_scores_using_latent_dirichlet_allocation',\n",
    "    'li_2020_global_surveillance_of_covid_19_by_mining_news_media_using_a_multi_source_dynamic_embedded_topic_model',\n",
    "    'alp_2018_identifying_topical_influencers_on_twitter_based_on_user_behavior_and_network_topology',\n",
    "    'morstatter_2017_in_search_of_coherence_and_consensus_measuring_the_interpretability_of_statistical_topics',\n",
    "    'light_2017_managing_the_boundaries_of_taste_culture_valuation_and_computational_social_science',\n",
    "    'clare_2019_modelling_research_topic_trends_in_community_forestry',\n",
    "    'nerghes_2019_narratives_of_the_refugee_crisis_a_comparative_study_of_mainstream_media_and_twitter',\n",
    "    'campos_2020_recommendation_system_for_knowledge_acquisition_in_moocs_ecosystems',\n",
    "    'zhang_2017_scientific_evolutionary_pathways_identifying_and_visualizing_relationships_for_scientific_topics',\n",
    "    'bagozzi_2018_the_politics_of_scrutiny_in_human_rights_monitoring_evidence_from_structural_topic_models_of_us_state_department_human_rights_reports',\n",
    "    'dahal_2019_topic_modeling_and_sentiment_analysis_of_global_climate_change_tweets',\n",
    "    'grajzl_2019_toward_understanding_17th_century_english_culture_a_structural_topic_model_of_francis_bacontextquotesingles_ideas',\n",
    "    'hoang_2019_towards_autoencoding_variational_inference_for_aspect_based_opinion_summary',\n",
    "    'herzog_2018_transfer_topic_labeling_with_domain_specific_knowledge_base_an_analysis_of_uk_house_of_commons_speeches_1935_2014',\n",
    "    'karami_2019_twitter_speaks_a_case_of_national_disaster_situational_awareness',\n",
    "    'gourru_2018_united_we_stand_using_multiple_strategies_for_topic_labeling',\n",
    "    'kuhn_2018_using_structural_topic_modeling_to_identify_latent_topics_and_trends_in_aviation_incident_reports',\n",
    "    'chen_2020_what_are_moocs_learners_concerns_text_analysis_of_reviews_for_computer_science_courses'\n",
    "]\n",
    "\n",
    "fs_selection = [\n",
    "    'stamolampros_2019_job_satisfaction_and_employee_turnover_determinants_in_high_contact_services_insights_from_employeesonline_reviews',\n",
    "    'chen_2020_detecting_latent_topics_and_trends_in_educational_technologies_over_four_decades_using_structural_topic_modeling_a_retrospective_of_all_volumes_of_computers__education',\n",
    "    'stamolampros_2020_harnessing_the_wisdom_of_employees_from_online_reviews',\n",
    "    'amat_lefort_2022_quality_40_big_data_analytics_to_explore_service_quality_attributes_and_their_relation_to_user_sentiment_in_airbnb_reviews',\n",
    "    'ding_2020_employing_structural_topic_modelling_to_explore_perceived_service_quality_attributes_in_airbnb_accommodation',\n",
    "    'yang_2021_revealing_industry_challenge_and_business_response_to_covid_19_a_text_mining_approach',\n",
    "    'hagen_2019_open_data_visualizations_and_analytics_as_tools_for_policy_making',\n",
    "    'monselise_2021_topics_and_sentiments_of_public_concerns_regarding_covid_19_vaccines_social_media_trend_analysis',\n",
    "    'aman_2021_listen_to_e_scooter_riders_mining_rider_satisfaction_factors_from_app_store_reviews',\n",
    "    'luo_2020_topic_modelling_for_theme_park_online_reviews_analysis_of_disneyland',\n",
    "    'meena_2022_online_food_delivery_companies_performance_and_consumers_expectations_during_covid_19_an_investigation_using_machine_learning_approach',\n",
    "    'singh_2022_modeling_the_public_attitude_towards_organic_foods_a_big_data_and_text_mining_approach',\n",
    "    'wang_2020_a_topic_based_patent_analytics_approach_for_exploring_technological_trends_in_smart_manufacturing',\n",
    "    'he_2021_automatic_topic_labeling_using_graph_based_pre_trained_neural_embedding',\n",
    "    'chen_2020_a_structural_topic_modeling_based_bibliometric_study_of_sentiment_analysis_literature',\n",
    "    'chen_2022_a_decade_of_sentic_computing_topic_modeling_and_bibliometric_analysis',\n",
    "    'yoo_2023_exploring_the_nexus_between_food_and_vegn_lifestyle_via_text_mining_based_online_community_analytics',\n",
    "    'chung_2022_understanding_music_streaming_services_via_text_mining_of_online_customer_reviews',\n",
    "    'jebari_2021_the_use_of_citation_context_to_detect_the_evolution_of_research_topics_a_large_scale_analysis',\n",
    "    'huang_2022_identification_of_topic_evolution_network_analytics_with_piecewise_linear_representation_and_word_embedding',\n",
    "    'zhang_2021_topic_evolution_disruption_and_resilience_in_early_covid_19_research',\n",
    "    'ebadi_2021_understanding_the_temporal_evolution_of_covid_19_research_through_machine_learning_and_natural_language_processing',\n",
    "    'barrera_barrera_2022_selecting_the_appropriate_leading_journal_in_hospitality_and_tourism_research_a_guide_based_on_the_topic_journal_fit_and_the_jcr_impact_factor',\n",
    "    'truica_2021_tlatr_automatic_topic_labeling_using_automatic_domain_specific_term_recognition',\n",
    "    'he_2019_automatic_labeling_of_topic_models_using_graph_based_ranking',\n",
    "    'rosati_2022_moving_beyond_word_lists_towards_abstractive_topic_labels_for_human_like_topics_of_scientific_documents',\n",
    "    'he_2021_automatic_topic_labeling_model_with_paired_attention_based_on_pre_trained_deep_neural_network',\n",
    "    'ojo_2021_what_matters_most_to_patients_on_the_core_determinants_of_patient_experience_from_free_text_feedback',\n",
    "    'scelsi_2021_principled_analysis_of_energy_discourse_across_domains_with_thesaurus_based_automatic_topic_labeling',\n",
    "    'symitsi_2020_the_informational_value_of_employee_online_reviews',\n",
    "    'chin_2019_ondemand_recent_personal_tweets_summarization_on_mobile_devices'\n",
    "]\n",
    "\n",
    "cite_key_trim_regex = re.compile('(^([a-z]+_)+[0-9]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glossary file contains:\n",
    "    # All paper from initial selection (65)\n",
    "    # All the FS and BS papers obtained from the query with proximity constraint\n",
    "def get_glossary():\n",
    "    with open('Glossary.bib', 'r') as glossary:\n",
    "        db = biblib.bib.Parser().parse(glossary, log_fp=sys.stderr).get_entries()\n",
    "        return db\n",
    "glossary = get_glossary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphviz_syntax(G):\n",
    "    A = nx.nx_agraph.to_agraph(G)\n",
    "    A.layout()\n",
    "\n",
    "    for node in A.nodes(): #Remove fixed position values\n",
    "        node.attr['pos'] = ''\n",
    "\n",
    "    A.write('Images/template.txt')\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial selection + all BS papers after query\n",
    "# Papers from initial selection are colored skyblue\n",
    "# Selected papers from BS are colored mistyrose\n",
    "# Node size increases based on nr of incoming edges\n",
    "def build_bs_graph(path, venues, snowb_selection, glossary, print_rank = False, min_incoming_edges = 1, selection_only = False):\n",
    "    incoming_edges = {}\n",
    "    year_groups = {}\n",
    "\n",
    "    # Count incoming edges for referenced papers\n",
    "    for key in venues:\n",
    "        for venue in venues[key]:\n",
    "            with os.scandir(f'{path}/{key}/{venue}') as it:\n",
    "                for entry in it:\n",
    "                    if entry.name.endswith(\".bib\") and entry.is_file():\n",
    "                        source = entry.name.replace('.bib','')\n",
    "                        if(source not in incoming_edges): \n",
    "                            incoming_edges[source] = 0\n",
    "                            year = cite_key_trim_regex.search(source).group()[-4:]\n",
    "                            if(year not in year_groups): year_groups[year] = [source]\n",
    "                            else: year_groups[year].append(source)\n",
    "                        with open(entry, 'r') as fp:\n",
    "                            db = biblib.bib.Parser().parse(fp, log_fp=sys.stderr).get_entries()\n",
    "                        \n",
    "                        for ent in db.values():\n",
    "                            cite_key = build_cite_key(ent['author'], ent['year'], ent['title'])\n",
    "\n",
    "                            if(selection_only == False or cite_key in snowb_selection):\n",
    "                                if(cite_key in incoming_edges): incoming_edges[cite_key] += 1\n",
    "                                else: incoming_edges[cite_key] = 1\n",
    "\n",
    "                                year = cite_key_trim_regex.search(cite_key).group()[-4:]\n",
    "                                if(year not in year_groups): year_groups[year] = [cite_key]\n",
    "                                else: year_groups[year].append(cite_key)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for key in venues:\n",
    "        for venue in venues[key]:\n",
    "            with os.scandir(f'{path}/{key}/{venue}') as it:\n",
    "                for entry in it:\n",
    "                    if entry.name.endswith(\".bib\") and entry.is_file():\n",
    "                        source = entry.name.replace('.bib','')\n",
    "                        with open(entry, 'r') as fp:\n",
    "                            db = biblib.bib.Parser().parse(fp, log_fp=sys.stderr).get_entries()\n",
    "\n",
    "                        out_count = 0 # Avoid generating initial selection nodes with no outgoing edges\n",
    "                        for ent in db.values():\n",
    "                            cite_key = build_cite_key(ent['author'], ent['year'], ent['title'])\n",
    "\n",
    "                            if(selection_only == False or cite_key in snowb_selection):\n",
    "                                if(incoming_edges[cite_key] >= min_incoming_edges):\n",
    "                                    if(out_count == 0):\n",
    "                                        G.add_node(str(source), id=source, color='skyblue', style='filled', label=cite_key_trim_regex.search(source).group(), shape='circle', width=2, height=2)\n",
    "                                    out_count += 1\n",
    "\n",
    "                                    G.add_node(cite_key, id=cite_key, label=cite_key_trim_regex.search(cite_key).group(), shape='circle', width=2, height=2)\n",
    "                                    if cite_key in snowb_selection: \n",
    "                                        G.nodes[cite_key]['color'] = 'mistyrose'\n",
    "                                        G.nodes[cite_key]['style'] = 'filled'\n",
    "                                    G.add_edge(source, cite_key)\n",
    "                                    \n",
    "\n",
    "    # Iterate through all nodes\n",
    "    return resize_nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial selection + all FS papers after query\n",
    "# Papers from initial selection are colored skyblue\n",
    "# Selected papers from FS are colored lightgoldenrodyellow\n",
    "# Node size increases based on nr of incoming edges\n",
    "def build_fs_graph(path, venues, snowb_selection, glossary, print_rank = False, min_outgoing_edges = 1, selection_only = False):\n",
    "    incoming_edges = {}\n",
    "    outgoing_edges = {}\n",
    "    year_groups = {}\n",
    "    ad = AlphabetDetector()\n",
    "\n",
    "    # Count incoming edges for initial selection\n",
    "    for key in venues:\n",
    "        for venue in venues[key]:\n",
    "            with os.scandir(f'{path}/{key}/{venue}') as it:\n",
    "                for entry in it:\n",
    "                    if entry.name.endswith(\".bib\") and entry.is_file():\n",
    "                        source = entry.name.replace('.bib','')\n",
    "                        if(source not in incoming_edges): \n",
    "                            incoming_edges[source] = 0\n",
    "                            year = cite_key_trim_regex.search(source).group()[-4:]\n",
    "                            if(year not in year_groups): year_groups[year] = [source]\n",
    "                            else: year_groups[year].append(source)\n",
    "                        with open(entry, 'r') as fp:\n",
    "                            db = biblib.bib.Parser().parse(fp, log_fp=sys.stderr).get_entries()\n",
    "                        \n",
    "                        for ent in db.values():\n",
    "                            cite_key = build_cite_key(ent['author'], ent['year'], ent['title'])\n",
    "\n",
    "                            if(selection_only == False or cite_key in snowb_selection):\n",
    "                                incoming_edges[source] += 1\n",
    "                                if(cite_key in outgoing_edges): outgoing_edges[cite_key] += 1\n",
    "                                else: outgoing_edges[cite_key] = 1\n",
    "\n",
    "                                year = cite_key_trim_regex.search(cite_key).group()[-4:]\n",
    "                                if(year not in year_groups): year_groups[year] = [cite_key]\n",
    "                                else: year_groups[year].append(cite_key)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for key in venues:\n",
    "        for venue in venues[key]:\n",
    "            with os.scandir(f'{path}/{key}/{venue}') as it:\n",
    "                for entry in it:\n",
    "                    if entry.name.endswith(\".bib\") and entry.is_file():\n",
    "                        source = entry.name.replace('.bib','')\n",
    "\n",
    "                        if(incoming_edges[source] != 0): # Avoid generating initial selection nodes with no incoming edges\n",
    "                            size_increase = math.log(incoming_edges[source]) + 1\n",
    "                            G.add_node(str(source), id=source, color='skyblue', style='filled', label=cite_key_trim_regex.search(source).group(), shape='circle', width=2, height=2)\n",
    "                            with open(entry, 'r') as fp:\n",
    "                                db = biblib.bib.Parser().parse(fp, log_fp=sys.stderr).get_entries()\n",
    "\n",
    "                            for ent in db.values():\n",
    "                                cite_key = build_cite_key(ent['author'], ent['year'], ent['title'])\n",
    "                                if(selection_only == False or cite_key in snowb_selection):\n",
    "                                    if(outgoing_edges[cite_key] >= min_outgoing_edges):\n",
    "                                        G.add_node(cite_key, id=cite_key, label=cite_key_trim_regex.search(cite_key).group(), shape='circle', width=2, height=2)\n",
    "                                        if cite_key in snowb_selection: \n",
    "                                            G.nodes[cite_key]['color'] = 'lightgoldenrodyellow'\n",
    "                                            G.nodes[cite_key]['style'] = 'filled'\n",
    "                                        G.add_edge(cite_key, source)\n",
    "\n",
    "    # Iterate through all nodes\n",
    "    return resize_nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_snowballing_edges(G, selection, glossary, selection_only=True, start_progress_count=0):\n",
    "    sch = SemanticScholar()\n",
    "    ad = AlphabetDetector()\n",
    "    progress_count = start_progress_count\n",
    "\n",
    "    for key in selection:\n",
    "        cur_paper_key = glossary[key]\n",
    "\n",
    "        perform_title_search = False\n",
    "        if('doi' in cur_paper_key): # DOI search\n",
    "            doi = cur_paper_key['doi'].lower().replace('https://doi.org/', '')\n",
    "            if('arxiv' in doi): doi = doi.partition('/')[2].replace('.',':',1)\n",
    "            try:\n",
    "                paper = sch.get_paper(doi)\n",
    "            except:\n",
    "                print(doi)\n",
    "                perform_title_search = True\n",
    "        else:\n",
    "            perform_title_search = True\n",
    "\n",
    "        if(perform_title_search): # Title search\n",
    "            res_list = sch.search_paper(cur_paper_key['title'].replace('{','').replace('}',''))\n",
    "            if(len(res_list) == 0): \n",
    "                print(cur_paper_key)\n",
    "                continue\n",
    "            else: paper = res_list[0]\n",
    "\n",
    "        if(paper.references):\n",
    "            for reference in paper.references:\n",
    "                if(len(reference.authors) == 0): \n",
    "                    continue\n",
    "                # Title messing up my key generation function\n",
    "                if( reference.title == 'Global Forest Resources Assessment' or \n",
    "                    reference.title == '大規模要約資源としてのNew York Times Annotated Corpus' or\n",
    "                    reference.title == 'Banana Ovate family protein MaOFP1 and MADS-box protein MuMADS1 antagonistically regulated banana fruit ripening' or\n",
    "                    reference.title == '5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding' or\n",
    "                    reference.title == 'ExtractingTarget-Opinion Pairs Based on Semantic Analysis' or\n",
    "                    reference.title == 'Memory: A Contribution to Experimental Psychology'):\n",
    "                    continue\n",
    "                if(ad.only_alphabet_chars(reference.authors[0].name, \"LATIN\") == False):\n",
    "                    #print(f'Non latin skip: {reference.authors[0]} --- {reference.title}')\n",
    "                    continue\n",
    "                #print(reference.title)\n",
    "                #print(reference.authors[0].name)\n",
    "                try:\n",
    "                    cite_key = build_cite_key(reference.authors[0].name + ' and', reference.year, reference.title)\n",
    "                except:\n",
    "                    print(f'ERROR GENERATING KEY: {reference.authors[0].name} --- {reference.title}')\n",
    "                    print(f'Progress: {progress_count}')\n",
    "                    continue\n",
    "                    #raise Exception('Error generating key')\n",
    "                \n",
    "                if(selection_only): selection_cond = bool((cite_key in init_selection) or (cite_key in fs_selection) or (cite_key in bs_selection))\n",
    "                else: selection_cond = bool(cite_key in glossary)\n",
    "\n",
    "                if(selection_cond):\n",
    "                    if(G.has_node(cite_key) == False): # If node from initial set was not printed because initially isolated, create it now\n",
    "                        G.add_node(str(cite_key), id=cite_key, color='skyblue', style='filled', label=cite_key_trim_regex.search(cite_key).group(), shape='circle', width=2, height=2)\n",
    "                    if(G.has_node(key) == False):\n",
    "                        G.add_node(str(key), id=key, color='skyblue', style='filled', label=cite_key_trim_regex.search(key).group(), shape='circle', width=2, height=2)\n",
    "                        \n",
    "                    G.add_edge(key, cite_key)\n",
    "        progress_count += 1\n",
    "    return resize_nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_nodes(G):\n",
    "    # Iterate through all nodes\n",
    "    for node in G.nodes():\n",
    "        if(G.in_degree(node) != 0):\n",
    "            size_increase = math.log(G.in_degree(node)) + 1\n",
    "            G.nodes[node]['width'] = 2 * size_increase\n",
    "            G.nodes[node]['height'] = 2 * size_increase\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = build_bs_graph('../References/Snowballing/Backward snowballing (Outgoing references)', venues, bs_selection, glossary, min_incoming_edges=1, selection_only=True)\n",
    "BS_A = generate_graphviz_syntax(BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = build_fs_graph('../References/Snowballing/Forward snowballing (Incoming citations)', venues, fs_selection, glossary, min_outgoing_edges=1, selection_only=True)\n",
    "FS_A = generate_graphviz_syntax(FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL = nx.compose(BS,FS)\n",
    "FULL = resize_nodes(FULL)\n",
    "FULL_A = generate_graphviz_syntax(FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_E = add_snowballing_edges(FULL, init_selection, glossary)\n",
    "FULL_E = add_snowballing_edges(FULL_E, fs_selection, glossary)\n",
    "FULL_E = add_snowballing_edges(FULL_E, bs_selection, glossary)\n",
    "FULL_E = resize_nodes(FULL_E)\n",
    "FULL_E_A = generate_graphviz_syntax(FULL_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_COMPLETE = build_bs_graph('../References/Snowballing/Backward snowballing (Outgoing references)', venues, bs_selection, glossary, min_incoming_edges=1, selection_only=False)\n",
    "BS_COMPLETE_A = generate_graphviz_syntax(BS)\n",
    "\n",
    "FS_COMPLETE = build_fs_graph('../References/Snowballing/Forward snowballing (Incoming citations)', venues, fs_selection, glossary, min_outgoing_edges=1, selection_only=False)\n",
    "FS_COMPLETE_A = generate_graphviz_syntax(FS)\n",
    "\n",
    "COMPLETE = nx.compose(BS_COMPLETE,FS_COMPLETE)\n",
    "COMPLETE = resize_nodes(COMPLETE)\n",
    "COMPLETE_A = generate_graphviz_syntax(COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.25100/cdea.v37i70.10811\n",
      "`guevara_2021_creacion_de_valor_con_practicas_de_economia_circular_en_la_produccion_de_viche' at Glossary.bib:10222:0\n",
      "10.1016/j.najef.2020.101179\n",
      "jing_2017_guess_you_like_course_recommendation_in_moocs\n",
      "`honnibal_2018_spacy_2_natural_language_understanding_with_bloom_embeddings_convolutional_neural_networks_and_incremental_parsing' at Glossary.bib:24707:0\n",
      "`bhattacharjee_2018_fasttext_quick_start_guide_get_started_with_facebooks_library_for_text_representation_and_classification' at Glossary.bib:25135:0\n",
      "`knoos_2021_sentiment_analysis_of_mooc_learner_reviews_what_motivates_learners_to_complete_a_course' at Glossary.bib:25206:0\n",
      "`lohr_2020_remember_the_moocs_after_near_death_theyre_booming' at Glossary.bib:25225:0\n",
      "10.1007/978-3-319-07124-4_24\n",
      "10.1007/978-3-319-93073-2_4\n",
      "li_2017_feature_selection_a_data_perspective\n",
      "`epaminonda_2021_a_review_of_the_sociocultural_profile_of_cyprus_historical_background_current_features_change_and_diversity' at Glossary.bib:25818:0\n",
      "10.1007/s11191-022-00332-4\n",
      "950850e22e42201f152d90dc6f53d53e39d37657\n",
      "7ab2166f6cdb1737e000df66d29c6538afc6811d\n",
      "`dieng_2016_topicrnn_a_recurrent_neural_network_with_long_range_semantic_dependency' at Glossary.bib:30650:0\n",
      "eb0c0001de406772f89502b0e2c4e0677d268dca\n",
      "10.5281/zenodo.7310816\n",
      "`montani_2022_explosionspacy_v343_extended_typer_support_and_bug_fixes' at Glossary.bib:31972:0\n",
      "10.18653/v1/p18-1042\n",
      "`berg_2018_digital_labour_platforms_and_the_future_of_work_towards_decent_work_in_the_online_world' at Glossary.bib:32943:0\n",
      "xia_2017_our_privacy_needs_to_be_protected_at_all_costs_crowd_workers_privacy_experiences_on_amazon_mechanical_turk\n",
      "10.1162/tacl_a_00325\n",
      "woodruff_2018_a_qualitative_exploration_of_perceptions_of_algorithmic_fairness\n",
      "hoffman_2017_evaluating_a_computational_approach_to_labeling_politeness_challenges_for_the_application_of_machine_classification_to_social_computing_data\n",
      "`hoffman_2017_evaluating_a_computational_approach_to_labeling_politeness_challenges_for_the_application_of_machine_classification_to_social_computing_data' at Glossary.bib:33625:0\n"
     ]
    }
   ],
   "source": [
    "COMPLETE_WITH_SNOWB_EDGES = add_snowballing_edges(COMPLETE, selection=glossary, glossary=glossary, selection_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETE_WITH_SNOWB_EDGES_A = generate_graphviz_syntax(COMPLETE_WITH_SNOWB_EDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'is_multigraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6998195d2fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMPLETE_WITH_SNOWB_EDGES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselfloop_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMPLETE_WITH_SNOWB_EDGES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/algorithms/core.py\u001b[0m in \u001b[0;36mk_core\u001b[0;34m(G, k, core_number)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_core_subgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/algorithms/core.py\u001b[0m in \u001b[0;36m_core_subgraph\u001b[0;34m(G, k_filter, k, core)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \"\"\"\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mcore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36margmap_core_number_1\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         if (mval is None or mval == g.is_multigraph()) and (\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mdval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         ):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_multigraph'"
     ]
    }
   ],
   "source": [
    "nx.k_core(COMPLETE_WITH_SNOWB_EDGES.remove_edges_from(nx.selfloop_edges(COMPLETE_WITH_SNOWB_EDGES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADED = nx.Graph(nx.nx_pydot.read_dot('ALL.dot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADED.remove_edges_from(nx.selfloop_edges(LOADED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
